---
layout:     post
title:     SparkSQL 文件读取及划分分区源码解析
subtitle:   SparkSQL 源码解析
date:       2019-12-13
author:     jiulongzhu
header-img: img/moon_night.jpg
catalog: true
tags:
    - Spark 2.3.0
    - Spark SQL
    - 源码解析
---


不考虑 SQL 文本在 SparkSQL 中的全生命周期，仅关注 SQL 物理计划读取文件及分区划分的部分逻辑    

```
linux> spark-shell --master yarn --queue high
scala> val sql="SELECT count(1) FROM utest.u_tbl_ifs_cost_aggregations"
scala> val df = spark.sql(sql)
scala> df.queryExecution.sparkPlan
res0: org.apache.spark.sql.execution.SparkPlan =
HashAggregate(keys=[], functions=[count(1)], output=[count(1)#47L])
+- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#96L])
   +- FileScan orc utest.u_tbl_ifs_cost_aggregations[] Batched: true, Format: ORC, Location: InMemoryFileIndex[hdfs://bj2/user/hive/warehouse/utest.db/u_tbl_ifs_cost_aggregations], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>
```

即 SparkSQL 中读取表部分实际是委托给 FileSourceScanExec 来处理  

<!-- more -->

## 源码解析  

第一步: 读取逻辑分区(可能包含多个物理文件)做为数据迭代器    
第二步: 按照是否分桶分别处理(分桶情况下，分区数和桶数量相关)    
[FileSourceScanExec.scala]  

```
private lazy val inputRDD: RDD[InternalRow] = {
	// 第一步
       // filters 为可下推到物理文件的过滤器(e.g. 分区)
    val readFile: (PartitionedFile) => Iterator[InternalRow] =
      relation.fileFormat.buildReaderWithPartitionValues(
        sparkSession = relation.sparkSession,
        dataSchema = relation.dataSchema,
        partitionSchema = relation.partitionSchema,
        requiredSchema = requiredSchema,
        filters = pushedDownFilters,
        options = relation.options,
        hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))
        // 第二步
    relation.bucketSpec match {
      case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled =>
        createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)
      case _ =>
        createNonBucketedReadRDD(readFile, selectedPartitions, relation)
    }
  }
```
按照未分桶的情况来看  

```
private def createNonBucketedReadRDD(
      readFile: (PartitionedFile) => Iterator[InternalRow],
      selectedPartitions: Seq[PartitionDirectory],
      fsRelation: HadoopFsRelation): RDD[InternalRow] = {
      // 单(逻辑)分区最大数据量，默认 128 MB
    val defaultMaxSplitBytes =
      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes
      // 打开文件的代价,默认 4MB。一般理解为单(逻辑)分区最小数据量，物理小文件较多时合并多个物理文件为一个逻辑分区
      // 避免过多的小任务
    val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes
    // 默认并行度,取的是 CoarseGrainedSchedulerBackend#defaultParallelism
    val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism
    // 文件夹下所有文件累计大小
    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum
    // 每个 core 分均处理数据量  
    val bytesPerCore = totalBytes / defaultParallelism
    val files = fsRelation.inputFiles.mkString(",")
    logInfo(s"${fsRelation.toString}, sizeInBytes: ${fsRelation.location.sizeInBytes}, files: ${files}")
    val log = s"defaultParallelism: ${defaultParallelism}, totalBytes: ${totalBytes}, bytesPerCore: ${bytesPerCore}"
    logInfo(log)
    // 期望分区大小。 公式含义: 在单分区处理数据量不超过 128M 且不低于 4MB的条件下,所有数据负载均衡给各并发处理
    val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))
    logInfo(s"Planning scan with bin packing, max size: $maxSplitBytes bytes, " +
      s"open cost is considered as scanning $openCostInBytes bytes.")
	
	// selectedPartitions 是分区下推之后的所有分区目录,非分区表或无需分区下推的情况下,为表在 hdfs 目录
    val splitFiles = selectedPartitions.flatMap { partition =>
      partition.files.flatMap { file =>
        val blockLocations = getBlockLocations(file)
        // 若文件可分割,则按照"期望分区大小"来划分作为 逻辑上的文件
        // 若文件不可分割,每个物理文件作为一个 逻辑上的文件
        if (fsRelation.fileFormat.isSplitable(
            fsRelation.sparkSession, fsRelation.options, file.getPath)) {
          (0L until file.getLen by maxSplitBytes).map { offset =>
            val remaining = file.getLen - offset
            // 文件尾部不足则剩余作为一个逻辑上的文件
            val size = if (remaining > maxSplitBytes) maxSplitBytes else remaining
            val hosts = getBlockHosts(blockLocations, offset, size)
            PartitionedFile(
              partition.values, file.getPath.toUri.toString, offset, size, hosts)
          }
        } else {
          val hosts = getBlockHosts(blockLocations, 0, file.getLen)
          Seq(PartitionedFile(
            partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))
        }
      }
    }.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)
 	
    // splitFiles 对大文件切分有效,但是如果小文件较多的话,则会产生与文件数相同的逻辑文件
    // 需要将逻辑文件合并为逻辑分区,每个逻辑分区对于一个 spark task  
    
    val partitions = new ArrayBuffer[FilePartition]
    val currentFiles = new ArrayBuffer[PartitionedFile]
    var currentSize = 0L
   
    // 截断并新建一个逻辑分区
   def closePartition(): Unit = {
      if (currentFiles.nonEmpty) {
        val newPartition =
          FilePartition(
            partitions.size,
            currentFiles.toArray.toSeq) 
        partitions += newPartition
      }
      currentFiles.clear()
      currentSize = 0
    }
    
    splitFiles.foreach { file =>
      // 如果已有数据量加上当前逻辑文件的长度超过了 期望分区大小,则不添加当前文件 截断为一个新的逻辑分区
      if (currentSize + file.length > maxSplitBytes) {
        closePartition()
      }
      
      currentSize += file.length + openCostInBytes
      currentFiles += file
    }
    // 余量作为一个新的逻辑分区
    closePartition()
    
    // 创建 RDD,分区为所有的逻辑分区,readFile为文件读取逻辑 
    new FileScanRDD(fsRelation.sparkSession, readFile, partitions)
  }
```

[CoarseGrainedSchedulerBackend.scala]

```
override def defaultParallelism(): Int = {
    conf.getInt("spark.default.parallelism", math.max(totalCoreCount.get(), 2))
  }
```

>
综上:  
val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))    
即分区大小的逻辑取决于三个值:    
defaultMaxSplitBytes: 单分区最大数据量,默认128MB  
openCostInBytes: 单分区最小数据量,默认 4MB    
bytesPerCore: 平均每 core 处理数据量, totalBytes/defaultParallelism    
defaultParallelism: 取值为配置项 spark.default.parallelism (默认未配置),然后是 totalCoreCount 与 2 的较大者    
totalCoreCount: 随 executor 的注册/注销动态变化(变化量为 spark.executor.cores),开启动态资源管理的情况下难以预估。  
即在单分区数据量不低于 4MB,不高于 128MB 的情况下,所有数据均衡给所有并发数处理   

## 修改分区数
defaultMaxSplitBytes 决定了上限，openCostInBytes决定了下限  

1. 减小分区数  
提高下限 openCostInBytes 值，并注意上限 defaultMaxSplitBytes即可  
2. 增加分区数  
配置合理的 spark.default.parallelism 值, 并注意下限 openCostInBytes 值即可  

注: 高压缩率 ORC 文件 4MB 解压后有 4G 左右的大小，可能需要降低下限    

 
