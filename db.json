{"meta":{"version":1,"warehouse":"3.0.1"},"models":{"Asset":[{"_id":"source/img/8981.log","path":"img/8981.log","modified":0,"renderable":0},{"_id":"source/img/image_1564500797.115948.jpg","path":"img/image_1564500797.115948.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500820.121955.jpg","path":"img/image_1564500820.121955.jpg","modified":0,"renderable":0},{"_id":"source/img/moon_night3.jpg","path":"img/moon_night3.jpg","modified":0,"renderable":0},{"_id":"source/img/cat-323262_960_720.jpg","path":"img/cat-323262_960_720.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564499567.339448.jpg","path":"img/image_1564499567.339448.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500798.413998.jpg","path":"img/image_1564500798.413998.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500801.667575.jpg","path":"img/image_1564500801.667575.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500802.493830.jpg","path":"img/image_1564500802.493830.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500804.092850.jpg","path":"img/image_1564500804.092850.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500810.633156.jpg","path":"img/image_1564500810.633156.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500811.368373.jpg","path":"img/image_1564500811.368373.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500821.860539.jpg","path":"img/image_1564500821.860539.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500826.315305.jpg","path":"img/image_1564500826.315305.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500825.238104.jpg","path":"img/image_1564500825.238104.jpg","modified":0,"renderable":0},{"_id":"source/img/moon_night.jpg","path":"img/moon_night.jpg","modified":0,"renderable":0},{"_id":"source/img/404-bg.jpg","path":"img/404-bg.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564499571.510152.jpg","path":"img/image_1564499571.510152.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500795.033385.jpg","path":"img/image_1564500795.033385.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500799.286733.jpg","path":"img/image_1564500799.286733.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500800.014160.jpg","path":"img/image_1564500800.014160.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500800.739319.jpg","path":"img/image_1564500800.739319.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500803.271657.jpg","path":"img/image_1564500803.271657.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500823.906395.jpg","path":"img/image_1564500823.906395.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500829.801306.jpg","path":"img/image_1564500829.801306.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"source/file/Spark.pdf","path":"file/Spark.pdf","modified":0,"renderable":0},{"_id":"source/img/cat-2332202_960_720.jpg","path":"img/cat-2332202_960_720.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500806.619420.jpg","path":"img/image_1564500806.619420.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500809.014874.jpg","path":"img/image_1564500809.014874.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500809.688791.jpg","path":"img/image_1564500809.688791.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500827.867988.jpg","path":"img/image_1564500827.867988.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500805.768255.jpg","path":"img/image_1564500805.768255.jpg","modified":0,"renderable":0},{"_id":"source/img/moon_night2.png","path":"img/moon_night2.png","modified":0,"renderable":0},{"_id":"source/img/pictures/negative/yarn_node_update.eddx","path":"img/pictures/negative/yarn_node_update.eddx","modified":0,"renderable":0},{"_id":"source/img/pictures/reservation/reservationSystem.png","path":"img/pictures/reservation/reservationSystem.png","modified":0,"renderable":0},{"_id":"source/img/pictures/spark/auto_merge_small_file.png","path":"img/pictures/spark/auto_merge_small_file.png","modified":0,"renderable":0},{"_id":"source/img/pictures/yarn_preempt/preempt_queue.png","path":"img/pictures/yarn_preempt/preempt_queue.png","modified":0,"renderable":0},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":0},{"_id":"source/img/image_1564500808.251661.jpg","path":"img/image_1564500808.251661.jpg","modified":0,"renderable":0},{"_id":"source/img/jerry.icon","path":"img/jerry.icon","modified":0,"renderable":0},{"_id":"source/img/pictures/flink/flink_connector.png","path":"img/pictures/flink/flink_connector.png","modified":0,"renderable":0},{"_id":"source/img/pictures/negative/yarn_node_update_pic.png","path":"img/pictures/negative/yarn_node_update_pic.png","modified":0,"renderable":0},{"_id":"source/img/pictures/reservation/yarn_reservation_system.png","path":"img/pictures/reservation/yarn_reservation_system.png","modified":0,"renderable":0},{"_id":"source/img/pictures/yarn_headroom/headroom_1.jpeg","path":"img/pictures/yarn_headroom/headroom_1.jpeg","modified":0,"renderable":0},{"_id":"source/img/pictures/yarn_headroom/headroom_2.jpeg","path":"img/pictures/yarn_headroom/headroom_2.jpeg","modified":0,"renderable":0},{"_id":"source/img/image_1564500822.829172.jpg","path":"img/image_1564500822.829172.jpg","modified":0,"renderable":0},{"_id":"source/img/image_1564500828.736853.jpg","path":"img/image_1564500828.736853.jpg","modified":0,"renderable":0},{"_id":"source/img/pictures/flink/flink_src_connector.png","path":"img/pictures/flink/flink_src_connector.png","modified":0,"renderable":0},{"_id":"source/img/pictures/flink/flink_watermark.png","path":"img/pictures/flink/flink_watermark.png","modified":0,"renderable":0},{"_id":"source/img/pictures/negative/negative_aead-04d8e67b1357.png","path":"img/pictures/negative/negative_aead-04d8e67b1357.png","modified":0,"renderable":0},{"_id":"source/img/pictures/negative/negative_e7a766275896.png","path":"img/pictures/negative/negative_e7a766275896.png","modified":0,"renderable":0},{"_id":"source/img/pictures/yarn_preempt/preemption_scheduler.png","path":"img/pictures/yarn_preempt/preemption_scheduler.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"source/img/image_1564500807.490268.jpg","path":"img/image_1564500807.490268.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"source/file/SparkSQL.pdf","path":"file/SparkSQL.pdf","modified":0,"renderable":0},{"_id":"source/file/RDDs.pdf","path":"file/RDDs.pdf","modified":0,"renderable":0},{"_id":"source/img/pictures/delegation/cycle.png","path":"img/pictures/delegation/cycle.png","modified":0,"renderable":0},{"_id":"source/img/pictures/delegation/private.png","path":"img/pictures/delegation/private.png","modified":0,"renderable":0},{"_id":"source/img/pictures/delegation/public.png","path":"img/pictures/delegation/public.png","modified":0,"renderable":0},{"_id":"source/img/pictures/delegation/auth.png","path":"img/pictures/delegation/auth.png","modified":0,"renderable":0},{"_id":"themes/yilia/source/main.0cf68a.css","path":"main.0cf68a.css","modified":1,"renderable":1},{"_id":"themes/yilia/source/slider.e37972.js","path":"slider.e37972.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/main.0cf68a.js","path":"main.0cf68a.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/mobile.992cbe.js","path":"mobile.992cbe.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.gitignore","hash":"7b68ca7a46104cf9aa84ec0541a4856ab1836eca","modified":1584352888437},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1584352888430},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1584352888430},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1584352888438},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1584352888437},{"_id":"themes/next/_config.yml","hash":"eff733406dacf386dbe7dcdf8282aa5b4e199424","modified":1594117031687},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1584352888438},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1584352888430},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1584352888445},{"_id":"themes/next/package.json","hash":"61aa2ac422e92ad435d1bfedea8d41824f5fe85e","modified":1584352888498},{"_id":"themes/next/gulpfile.js","hash":"72e6d5a6e32d5f95d82e4c4d0c963d39555bb760","modified":1584352888457},{"_id":"themes/next/README.md","hash":"dc026053a4d9fb97a58dbc3e9060e480f6852b23","modified":1584352888439},{"_id":"source/img/8981.log","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584352888215},{"_id":"source/_posts/2019-08-09-ExecutorAllocationManager.md","hash":"55a85bdb9545c5ab47b1ae460fe071f732b2bd12","modified":1584352888105},{"_id":"source/_posts/2019-10-05-Yarn PreemptionSystem.md","hash":"d9804ace17c39f7bdedf47c2f02db241a9de470b","modified":1592289651649},{"_id":"source/_posts/2019-11-12-Yarn User Headroom.md","hash":"f4212d83003289e07695e4c6a2944b53833eac31","modified":1592289694143},{"_id":"source/_posts/2019-11-18-SparkOnYarn 任务退出码.md","hash":"6e0833be96b8daf1d903563941e65178e0981b19","modified":1592290051005},{"_id":"source/_posts/2019-11-22-Improved MapPartitions In Spark.md","hash":"fd5530f43935a8b5f1f0b268b806d36d886f8893","modified":1584352888109},{"_id":"source/_posts/2019-11-25-基于队列优先级的小集群大容器资源抢占方案.md","hash":"48af51ab576357d4af411ce5a0a83c46a13d0042","modified":1593775203369},{"_id":"source/_posts/2019-11-28-SparkSQL 源码解析(1):从 SQL 到 UnResolvedLogicPlan.md","hash":"3e455dd97598910000b3a0b4c04d7a57e2afe0ed","modified":1592289752975},{"_id":"source/_posts/2019-11-29-SparkSQL 源码解析(2):从 UnresolvedLogicPlan 到 ResolvedLogicPlan.md","hash":"0bb8c7fcedbabf5cbfac05428d9616d1d3c1151a","modified":1584352888110},{"_id":"source/_posts/2019-12-10-SparkSQL 源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan.md","hash":"c73eabc451d0c3829fef868b514064c1edf59f60","modified":1584352888111},{"_id":"source/_posts/2019-12-13-SparkSQL 文件读取分区源码解析.md","hash":"800da4fda9e601f5b8447b2559bfc5ffb7395ce1","modified":1584352888111},{"_id":"source/_posts/2019-12-27-Apache Spark 3.0 Preview 新特性和改动.md","hash":"acbcd70e5d62d67408b5e92a33f70e03f05d1627","modified":1584352888111},{"_id":"source/_posts/2020-01-10-Spark Executor 内存动态拓展方案.md","hash":"43cc51c43090ea8e72dd1bd1aa581d7a51ab28c7","modified":1593775225462},{"_id":"source/_posts/2020-02-14-Apache Spark TPCDS性能测试数据集.md","hash":"844f9a9c794e3d4563365556eee98a53f2a0b389","modified":1592289830842},{"_id":"source/_posts/2020-02-23-Maven 常用命令.md","hash":"0b837fcb631cc8635ba0407933374b6f64ad1a73","modified":1584353735783},{"_id":"source/_posts/2020-03-18-.git目录过大解决方案.md","hash":"754cc360a3575df6c8c8d8748da4f4d41241f5ed","modified":1588764599473},{"_id":"source/_posts/2020-03-18-Spark-Adaptive-Execution-TPCDS-3TB-BenchMark-md.md","hash":"7a76f2c2ae485da2d2b9347003d9db935517dd1d","modified":1584430339915},{"_id":"source/_posts/2020-04-08-Linux 服务器分析 JVM Dump 文件.md","hash":"2e136c13f85ba7418723cca08eacfb81a2495d33","modified":1586320565556},{"_id":"source/_posts/2020-07-03-Spark 自动合并小文件.md","hash":"7495f590bfa420e0bf940e6e68e475955b1d0246","modified":1596425675699},{"_id":"source/_posts/2020-08-15-01Flink Connector.md","hash":"4f644083f05655868a95a6a7aaf822bffc742209","modified":1597920937535},{"_id":"source/_posts/2020-08-20-02FlinkKafkaConnector.md","hash":"80755ada3c134eca4b0dcc743a7b6dd33ef7e22f","modified":1597925573370},{"_id":"source/_posts/hello-world.md","hash":"e728ef72d7e3b6d5307c63349103b5bdb7bf512d","modified":1584352888112},{"_id":"source/categories/index.md","hash":"3d6bd1e28004f43336a4e5dc1fd460c7ba7e3cd0","modified":1584352888113},{"_id":"source/about/index.md","hash":"beda479fb1e873dd635ada5a68f417a76fb46345","modified":1584352888112},{"_id":"source/img/image_1564500797.115948.jpg","hash":"00b681fb2d32122336dffe0484ea34423596c410","modified":1584352888248},{"_id":"source/img/image_1564500820.121955.jpg","hash":"bac682b53eed8a44ebf2512c734d217a21c0e322","modified":1584352888335},{"_id":"source/img/moon_night3.jpg","hash":"bed20dc33569d47c3e7f1a22b9f2c018bec1d645","modified":1584352888388},{"_id":"source/tags/index.md","hash":"55b424f1987d6af0b132507aeb4bbfa9eb25b9ea","modified":1584352888424},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1584352888431},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"f4825dd07e8c524f4be3fe99d36069828c1b8535","modified":1584352888432},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"2a4ff75ccb91d9001032c08e22489096b850bf4f","modified":1584352888434},{"_id":"themes/next/.github/config.yml","hash":"e4f4b9afe59bc508c4f7634895b33d7d460a7cb1","modified":1584352888435},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"16541fb7b80f5ab90135db96285badb63c4d7d3e","modified":1584352888435},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1584352888436},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1584352888436},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"b09c85aa449f46bc025317ed196ec33a877f7c23","modified":1584352888435},{"_id":"themes/next/.github/stale.yml","hash":"941209526c2f7d916c76163c9e1ac1af9d956679","modified":1584352888437},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1584352888436},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1584352888437},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"5b4c013e0598b3211ebd899265936cfdaf7c139f","modified":1584352888446},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1584352888448},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"08cda41b4bcf687facfda19ab39718ec7a05ae54","modified":1584352888448},{"_id":"themes/next/docs/DATA-FILES.md","hash":"d0d46cb265f0efceffce82214cd36d8b82ddce13","modified":1584352888447},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1584352888448},{"_id":"themes/next/docs/MATH.md","hash":"f520b336f16665e164d6edf075bdcc6aa17b31bc","modified":1584352888449},{"_id":"themes/next/languages/de.yml","hash":"285b294b1dd2ce49b0694beb4fd75ed05dc20ccb","modified":1584352888457},{"_id":"themes/next/languages/default.yml","hash":"89eb0cff04435307e4eccb0d72a653499627d02d","modified":1584352888458},{"_id":"themes/next/languages/en.yml","hash":"89eb0cff04435307e4eccb0d72a653499627d02d","modified":1584352888458},{"_id":"themes/next/languages/es.yml","hash":"3bd0977225a9b9dc3cdc856161b3a77e5993ac4a","modified":1584352888458},{"_id":"themes/next/languages/fa.yml","hash":"35468531ad0236df5b188f28bd1b4bb0c0537faf","modified":1584352888459},{"_id":"themes/next/languages/fr.yml","hash":"5fe560d14824c71ea90b2a76d0c17b2332b9d08a","modified":1584352888459},{"_id":"themes/next/languages/hu.yml","hash":"dd9e4cd2873ed9a9ae7cb89962eeff911de396b4","modified":1584352888459},{"_id":"themes/next/languages/id.yml","hash":"80c3db4c2dbbea0703f6c8f1191218f692899507","modified":1584352888459},{"_id":"themes/next/languages/it.yml","hash":"68ac5a0aa361210f51915e101c842ed1c9464889","modified":1584352888460},{"_id":"themes/next/languages/ja.yml","hash":"e61529ddf80426a5362ed07642216027615bd740","modified":1584352888460},{"_id":"themes/next/languages/ko.yml","hash":"14a162509dc71bb364335c20281d3bccbd2a51f3","modified":1584352888461},{"_id":"themes/next/languages/nl.yml","hash":"4d9b09aff03648cbbbfeb0c51a2d1214320b5ec6","modified":1584352888461},{"_id":"themes/next/languages/pt-BR.yml","hash":"024dcdeb30c93e57cd1ba4cd707f8988dcad2468","modified":1584352888461},{"_id":"themes/next/languages/pt.yml","hash":"88fb58e537a949c013a2adcdaebcbe09543c0bd2","modified":1584352888462},{"_id":"themes/next/languages/ru.yml","hash":"f7d5efd6289845f2375bbee8fe883813de745767","modified":1584352888462},{"_id":"themes/next/languages/tr.yml","hash":"e08d8d6ee9507a7c7d4450bc1c76b435470a3739","modified":1584352888462},{"_id":"themes/next/languages/vi.yml","hash":"0c0890ccffcb5cca0893de63ebad66e3ce740707","modified":1584352888463},{"_id":"themes/next/languages/zh-CN.yml","hash":"4b36a51556e87d236c6ac6ee6975de1beb95724c","modified":1584352888463},{"_id":"themes/next/languages/uk.yml","hash":"e58393cf726401bf50a772392e23507d9e11049e","modified":1584352888462},{"_id":"themes/next/languages/zh-hk.yml","hash":"6d83daf9df1811e3ae6cde5ad778c888d8027b13","modified":1584352888463},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"199e9ef3cb5928af0fe801d03d2d1aeea704cea4","modified":1584352888449},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1584352888447},{"_id":"themes/next/layout/_layout.swig","hash":"29ee038b0d5ffdb45327598733ea968588367769","modified":1584352888464},{"_id":"themes/next/languages/zh-tw.yml","hash":"5e395cb492a0b6284f8d2ffdd93f7380db31486f","modified":1584352888463},{"_id":"themes/next/layout/archive.swig","hash":"26526c09a4334099e2141456697696fcd1f9783f","modified":1584352888497},{"_id":"themes/next/layout/category.swig","hash":"c55debb2588e4746b02d31ec249bf0a84fdea260","modified":1584352888497},{"_id":"themes/next/layout/index.swig","hash":"3bc6fb1e9707d74b96e1346d3f03fe6584f764f4","modified":1584352888497},{"_id":"themes/next/layout/page.swig","hash":"e61d64c055b6497a04affc143f47fdd0a6dc495b","modified":1584352888498},{"_id":"themes/next/layout/post.swig","hash":"382d9f9a9b35e1f369585f7f9f9b5dd6fa58d2f0","modified":1584352888498},{"_id":"themes/next/layout/tag.swig","hash":"7ff6e34d557a3da1c6a29ecd97842bf73ff213dc","modified":1584352888498},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1584352888504},{"_id":"source/_posts/2019-09-15-Yarn:movetoqueue导致的UIcontainer泄露问题的修复思路.md","hash":"3588f97231f9a7efdffd1b31b26408c8da2f8d8e","modified":1592289636884},{"_id":"source/_posts/2019-10-28-Yarn ReservationSystem.md","hash":"a3009532effa5428309ca14f38ecddf313a63028","modified":1592289670445},{"_id":"source/img/cat-323262_960_720.jpg","hash":"fc20d7411066c1871cff1e5b1f78bb434c231d8b","modified":1584352888222},{"_id":"source/img/image_1564499567.339448.jpg","hash":"936d01884900678e41c20344b7fdbbc58226a733","modified":1584352888236},{"_id":"source/img/image_1564500798.413998.jpg","hash":"b83dae7a1d0acbd89c4ac29251d82e02e59c440f","modified":1584352888253},{"_id":"source/img/image_1564500801.667575.jpg","hash":"a2909084fda60e6cfff7fa9919aebd9e8dcc6548","modified":1584352888277},{"_id":"source/img/image_1564500802.493830.jpg","hash":"b33b25b55b10fef256848816de477666e1bfd347","modified":1584352888281},{"_id":"source/img/image_1564500804.092850.jpg","hash":"936d01884900678e41c20344b7fdbbc58226a733","modified":1584352888283},{"_id":"source/img/image_1564500810.633156.jpg","hash":"79a5bbed126917f555213230308f2a5b936e0eee","modified":1584352888334},{"_id":"source/img/image_1564500811.368373.jpg","hash":"79a5bbed126917f555213230308f2a5b936e0eee","modified":1584352888335},{"_id":"source/img/image_1564500821.860539.jpg","hash":"e4297c3dcaef3f44d2ed45ca1e728eea11984cbf","modified":1584352888338},{"_id":"source/img/image_1564500826.315305.jpg","hash":"b9c01d5bd9d8c40aa80ccdfb25183ce00e9bee83","modified":1584352888355},{"_id":"source/img/image_1564500825.238104.jpg","hash":"27039207f2adbd1cf24d1400f1156ffc599b5584","modified":1584352888350},{"_id":"source/img/moon_night.jpg","hash":"1ad487defd427542ada22176769749c4589ea935","modified":1584352888383},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1584352888446},{"_id":"source/img/404-bg.jpg","hash":"68f7d525269a94287e0ad18713ae232fb59dcf71","modified":1584352888214},{"_id":"source/img/image_1564499571.510152.jpg","hash":"d0ad27d7b4c58e91bc0cf107cddf505e11320e11","modified":1584352888240},{"_id":"source/img/image_1564500795.033385.jpg","hash":"edb6d785ca69bd642b88983cc7ac4b5d47b226ce","modified":1584352888244},{"_id":"source/img/image_1564500799.286733.jpg","hash":"bb40887121838860a6b2b340d6d3683aa50b8fcf","modified":1584352888259},{"_id":"source/img/image_1564500800.014160.jpg","hash":"bc907858a286ae07c7734ee12381202db4ca8cea","modified":1584352888264},{"_id":"source/img/image_1564500800.739319.jpg","hash":"99b95505abc4189ad536956dd1686ba57f442ebb","modified":1584352888271},{"_id":"source/img/image_1564500803.271657.jpg","hash":"d0ad27d7b4c58e91bc0cf107cddf505e11320e11","modified":1584352888282},{"_id":"source/img/image_1564500823.906395.jpg","hash":"75f2851fa20a836dea96b002c54480f00d70b07e","modified":1584352888345},{"_id":"source/img/image_1564500829.801306.jpg","hash":"edb6d785ca69bd642b88983cc7ac4b5d47b226ce","modified":1584352888377},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"bf8b10715f5529cc6997501d821307895710ba58","modified":1584352888433},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"2f85f500781124b1848cc419f864797399fcdabc","modified":1584352888433},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"80c31b258f404a16ffc19706c5e5aa3ccb8c887e","modified":1584352888432},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"6d79701dd0bb3757722650d9df30eeaacd83344e","modified":1584352888433},{"_id":"themes/next/docs/ru/README.md","hash":"41b1bef32fb991410ebf559b4c45022549f95215","modified":1584352888450},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"2665f6a9d72090f9452a8811544ff2596e8899be","modified":1584352888449},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"fe3f5cda1975114884d84bef384a562920d70335","modified":1584352888451},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"4245fe9472647226692fcbdd5a52d6e6dcd251bc","modified":1584352888452},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"ebacdc94f6f9724a7c6ef7c3cdde41ff7c37931a","modified":1584352888455},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"bc159df1260e0c912c509090d54dd20a08201100","modified":1584352888455},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"3b4dbf0d2ca12ab442b7e0e227769ce100b8a444","modified":1584352888455},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1584352888455},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"264d131b8865bae069f8e4d78a33a0f59d28e36b","modified":1584352888456},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"2d60a18340c9260cd4dc81fe8acc7983d6b2d011","modified":1584352888456},{"_id":"themes/next/docs/zh-CN/README.md","hash":"b6a3611d40863c12804c5846994786119ce3b79f","modified":1584352888457},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"8109a531d3a7f5a306e36d4304c11f0c7c180c87","modified":1584352888457},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"815676d904f92748ddf4f529bed2baf066997bc6","modified":1584352888464},{"_id":"themes/next/layout/_macro/post.swig","hash":"049caf88e2cb8dde780b0c6196db984c7bc3a4cc","modified":1584352888465},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"e2c9db54cc9e154e882008fde6588b065fadc9a7","modified":1584352888466},{"_id":"themes/next/layout/_partials/footer.swig","hash":"434a4979c376c1ff27380c778d0261fea8d0a0a4","modified":1584352888473},{"_id":"themes/next/layout/_partials/comments.swig","hash":"0c4914a5fd08f15beec71940218c814ad9a89f3f","modified":1584352888468},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1584352888479},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1584352888450},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1584352888483},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1584352888483},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1584352888484},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"9675acc599ffa546f05a60375c1637b0327be4fd","modified":1584352888484},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1584352888486},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1584352888486},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"8627c8c8b031ecee16c522433b66fa4d6979b8ea","modified":1584352888489},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1584352888493},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"dd6bd817cb69b5ad5e9746498146314b54054ff8","modified":1584352888494},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"da6a9d14ed10203e378c6e2c00a7b5e7afabca58","modified":1584352888494},{"_id":"themes/next/scripts/events/index.js","hash":"9047d8ae2670e43429b16a7919a08a0a0a81afe0","modified":1584352888499},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1584352888502},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1584352888502},{"_id":"themes/next/scripts/filters/locals.js","hash":"5bbfdc1c373542159660b7a68ed0b57ca18ad10b","modified":1584352888502},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1584352888502},{"_id":"themes/next/scripts/filters/post.js","hash":"5e5035372b3d94a65309334c93303c858e072012","modified":1584352888502},{"_id":"themes/next/scripts/helpers/engine.js","hash":"c8c8afde59df587f883dd9b47a3c629042bc4c6e","modified":1584352888503},{"_id":"themes/next/scripts/helpers/font.js","hash":"32268fb4c59c5b37c1eb1c9582ab630e09e5cc7d","modified":1584352888504},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"58347687b02f7ab5e64bef07525c8efa97c9e8fb","modified":1584352888504},{"_id":"themes/next/scripts/tags/button.js","hash":"1d1d25f7e579d92fa563778dd0f163e8eda190da","modified":1584352888505},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"20e392b8583ba6ae5037449c2c7e191d3927641b","modified":1584352888505},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1584352888505},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1584352888505},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1584352888505},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1584352888506},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1584352888506},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1584352888506},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1584352888506},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1584352888507},{"_id":"themes/next/source/css/main.styl","hash":"68c3377b643162aeaae2b60c196486fdb3b509c3","modified":1584352888557},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1584352888558},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1584352888558},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1584352888561},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1584352888561},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1584352888562},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1584352888562},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1584352888563},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1584352888563},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1584352888563},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1584352888563},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1584352888564},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1584352888564},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1584352888564},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1584352888565},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1584352888565},{"_id":"themes/next/source/js/algolia-search.js","hash":"813afcc30feee11d59f297f2d5a96f98fbbd4743","modified":1584352888565},{"_id":"themes/next/source/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1584352888566},{"_id":"themes/next/source/js/local-search.js","hash":"9f3d3ddff86e7b2828772aba915a5ebfd7d9f55f","modified":1584352888566},{"_id":"themes/next/source/js/motion.js","hash":"09f8be6bcc6920546bb06e3f8c0181ec62c70b90","modified":1584352888567},{"_id":"themes/next/source/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1584352888567},{"_id":"themes/next/source/js/utils.js","hash":"4ea7d83e2b26b810ee4e847033c4f7e91278f37c","modified":1584352888569},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1584352888569},{"_id":"source/file/Spark.pdf","hash":"4b1b374b887611c2e276ba226440f2441ac8c9a9","modified":1584352888152},{"_id":"source/img/cat-2332202_960_720.jpg","hash":"2f6f828b8780f73486cd7ae2850cfda76cd0be8a","modified":1584352888219},{"_id":"source/img/image_1564500806.619420.jpg","hash":"a7fb0dd14790887971022a724051ffc0b39d3cdc","modified":1584352888293},{"_id":"source/img/image_1564500809.014874.jpg","hash":"9fa234f2810c15ac1e2bd920ddbf9b4bb8c0adeb","modified":1584352888323},{"_id":"source/img/image_1564500809.688791.jpg","hash":"565b0558d85d4f59ea41d745e27237f8cf87c429","modified":1584352888329},{"_id":"source/img/image_1564500827.867988.jpg","hash":"a7aa7f53dafa8154012d59c9c2edf091d05d4c2e","modified":1584352888368},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584352888547},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584352888548},{"_id":"source/img/image_1564500805.768255.jpg","hash":"af225677fb6289395b84ebeb2c7e1ac95635654f","modified":1584352888288},{"_id":"source/img/moon_night2.png","hash":"8862d2483da9aec4fd0e2c92606703d99311d5df","modified":1584352888387},{"_id":"source/img/pictures/negative/yarn_node_update.eddx","hash":"3e3e5cafac04ffb0b4ea65266ad80ffa06f862c9","modified":1584352888400},{"_id":"source/img/pictures/reservation/reservationSystem.png","hash":"37663f45f6b9e91a296de596b33742a43ea01888","modified":1584352888403},{"_id":"source/img/pictures/spark/auto_merge_small_file.png","hash":"ff4458dd63a546cbfd2e483e40c3271e4f9bba16","modified":1596425531392},{"_id":"source/img/pictures/yarn_preempt/preempt_queue.png","hash":"99bc3913e044f3451a66ae97b9e609284bef0d11","modified":1584352888421},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"1638483d2d2dad1da4c841a6fb9f6ee96b850187","modified":1584352888475},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"0172055d118d1d7f4c8379c8495c1ee1aa50c7d9","modified":1584352888475},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"7487ca8f0e4b16351ea0d6b35dc52b0d32176d57","modified":1584352888476},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"30528a8da30994b1ef9355a72b09b2cd85a7c0e9","modified":1584352888476},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"ff33b5797f5e4b5cbcb3c222c17bc636c6b88df6","modified":1584352888477},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"d488664bc16608b5f829f959c2058b4381be244a","modified":1584352888477},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"fbec9f77139e1f300509c38446416b4b26350cfa","modified":1584352888478},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1584352888478},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"a56e4f6ad95c106f361d354f828d1ef4810b1d76","modified":1584352888478},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"94d54b0c65d504f772af1e62424952e092b6c21d","modified":1584352888479},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1584352888479},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1584352888480},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"78cb2077f50bc3b81130a516ffc04bc1fcea5834","modified":1584352888480},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"716b78cd90addc4216413719554721cb362b0c18","modified":1584352888483},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"7fa01334a0ba84500e920bb9202baa08067d2ee1","modified":1584352888482},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"d6fa9e4432b87004c5678dfe2d4b2c1f4a702b93","modified":1584352888482},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"733d6874aa4f50d1071e670a554508a5a0094eb3","modified":1584352888482},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"d5a5f45ef0b41332cad069d263df2c449e2f1e7e","modified":1584352888484},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1584352888484},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1584352888485},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1584352888485},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1584352888485},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1584352888486},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1584352888487},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1584352888488},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1584352888489},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"fdcf006e1ba2e53eab65e901b6c63159538307ef","modified":1584352888490},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1584352888491},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1584352888491},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1ef172bc041d6f6b6c8c642ea64496a188d79029","modified":1584352888492},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"d3dc3e051e6816cdd576d00cc70b18b8a4c6a495","modified":1584352888492},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"0992d51dff4b8f4f4e69834c8cd1604593536c5d","modified":1584352888492},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1584352888492},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4d6f9e09ca4056ff6a5d4923e202126a75242183","modified":1584352888493},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1584352888493},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1584352888494},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"fb27a38f9a4b8fcba4f637b03904f7a83cc73416","modified":1584352888494},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"e456d7a2aaabe55447f78cd952b30d70a6c1e742","modified":1584352888495},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1584352888495},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1584352888495},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d7258d02bcf0dac6c0fd8377c0909ddecb09d1d4","modified":1584352888495},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1584352888496},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"0ea0bac09b0747bc16fde852164c0eaab2efe02c","modified":1584352888496},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1584352888496},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"4a7207c6f9ab0e56ea8f59aca3ed4c2cb2fb7450","modified":1584352888496},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"4958fa997ff6df2b2ce05341f40cc3a81b0f91bb","modified":1584352888497},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"7fa72dc60c078842979861622839b109683e05a3","modified":1584352888497},{"_id":"themes/next/scripts/events/lib/config.js","hash":"b205d72a56b1827681f0a260c266e0c02065fd08","modified":1584352888499},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1584352888499},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1584352888500},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"1f20213af8da3127701e6bb9da995e5c91be2051","modified":1584352888500},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"0803d4f4d3d02c24417c163ad0b27b60fda79250","modified":1584352888500},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1584352888500},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"5ba0480c2dbb0626501c90c920be85ce2308103c","modified":1584352888501},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1584352888501},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1584352888501},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1584352888501},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"e24c4dd78f6e4adee0e7c15eb8bca12eb92452e4","modified":1584352888501},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0a25f3df1b5c39794365efde387647da81da884a","modified":1584352888548},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2315dd8a7a2c7aabd29efa6193df08e805cb15fc","modified":1584352888547},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c261d685c5ed0df34718d94bb2ba977c0ed443e6","modified":1584352888556},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"644c1f0b02be9bf59955ebdf496136b3fa4b660b","modified":1584352888548},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"52550138127ae9ebbe049bcdacd94d767c003855","modified":1584352888556},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"b9d7058db61df7bbd2b58779efe45621a06ffc18","modified":1584352888556},{"_id":"themes/next/source/css/_variables/base.styl","hash":"6903ec07803e188ea08e38a707b7e70920c1c1b7","modified":1584352888557},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"50bc57e66331c0f15a4527010b4ca3316ac92403","modified":1584352888557},{"_id":"themes/next/source/js/schemes/muse.js","hash":"792f4522a3b0b96584a041a4aca20fcded410079","modified":1584352888568},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"52d139e6f3bc212c8c9b71bc9e57e931920434a6","modified":1584352888569},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1584352888570},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1584352888570},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1584352888571},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1584352888586},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1584352888586},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1584352888570},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1584352888570},{"_id":"source/img/favicon.ico","hash":"daca33db516830ba37442e3d02b3a36e3921900b","modified":1584352888232},{"_id":"source/img/image_1564500808.251661.jpg","hash":"db9ba11b894cbac7486f2d1746aba4ee2cd7984b","modified":1584352888312},{"_id":"source/img/jerry.icon","hash":"daca33db516830ba37442e3d02b3a36e3921900b","modified":1584352888380},{"_id":"source/img/pictures/flink/flink_connector.png","hash":"13cf3e2e48105c425b3e516dd1a10e1243ec9c5b","modified":1597492524011},{"_id":"source/img/pictures/negative/yarn_node_update_pic.png","hash":"8a33503b2964f8a8959ce895a56421d3d0e75037","modified":1584352888401},{"_id":"source/img/pictures/reservation/yarn_reservation_system.png","hash":"897b98ca4f490fee1f72a6ea68b163007bebf63a","modified":1584352888408},{"_id":"source/img/pictures/yarn_headroom/headroom_1.jpeg","hash":"1b8bb002b173e60913c04345e1f322a1dc20af1e","modified":1584352888414},{"_id":"source/img/pictures/yarn_headroom/headroom_2.jpeg","hash":"2051ca4668992bd148445c20c472120fe7f25008","modified":1584352888417},{"_id":"source/img/image_1564500822.829172.jpg","hash":"0ade9245b1340cdc2c49677c09c8991cfb4ae121","modified":1584352888341},{"_id":"source/img/image_1564500828.736853.jpg","hash":"2add48386f879a46dabf7d12e9399d59b7030afd","modified":1584352888376},{"_id":"source/img/pictures/flink/flink_src_connector.png","hash":"c1f1d2369795f44b0e6941ae42fd1efecd21ce1f","modified":1597491255354},{"_id":"source/img/pictures/flink/flink_watermark.png","hash":"362d6d5165773ad684f772b5b9f0b03e3d03a1a2","modified":1597917903610},{"_id":"source/img/pictures/negative/negative_aead-04d8e67b1357.png","hash":"5f08b5050a25b8f69f2534129d794884fb591908","modified":1584352888395},{"_id":"source/img/pictures/negative/negative_e7a766275896.png","hash":"9852a80850d9edbc40144b45e5091f938c2d8d9b","modified":1584352888399},{"_id":"source/img/pictures/yarn_preempt/preemption_scheduler.png","hash":"96e15b4e8070dc58816090ff936589647d164588","modified":1584352888424},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"47ee915d7b0a97e74140a25fbfc01c04d6781534","modified":1584352888509},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"112f5e8f1fe5cec4419e87acfbdef0e615ed23f3","modified":1584352888509},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"80d359661d08b80ad561b97f8508766b3e1f6d01","modified":1584352888508},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"41c7cd1b63d49476ed5fbdd26ab9411d8f44bd05","modified":1584352888520},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"170c4598cbbe49cd1527f94158d97d2320a6b906","modified":1584352888520},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"44fe82eadbdbb2f66adda37ac83ebd0f85876bfc","modified":1584352888527},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"4f7879a50d4608c46cc2061c725a2564597a45bd","modified":1584352888536},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"d8ba44b8e1a0332c5c1079ff65fc83d2918a5865","modified":1584352888536},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"3faa8a7cdb05ef3f3b02920d381773dfd54270a5","modified":1584352888539},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1584352888543},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1584352888543},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1584352888544},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"2b52b0eb4c66e06762cf115450da0ca1dd435e75","modified":1584352888544},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1584352888547},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"990bd301ce2de0a6b936781c58318f3945d81bc2","modified":1584352888526},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"1693ec6b53758ac15d2c7798c789d6ae8af913ea","modified":1584352888548},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"4fb385d8533877678440c8faa08b68fa793cb1a2","modified":1584352888549},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"24a086a6904bbf5355a354403c9b0e6069f7eb01","modified":1584352888549},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"3646e915b0a55f3b66e41d802b082aba88a76e06","modified":1584352888552},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1584352888552},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"c6905c694c6fbbf372b53456a0d219701010a110","modified":1584352888553},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"eeab294e14abbae231107e1a327e907b25323136","modified":1584352888553},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1584352888553},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1584352888553},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"e321bd62f5e04d1fdc101a470ec13604e99816a5","modified":1584352888550},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1584352888551},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"f1f81bca31e82ccbd375f0cb9fb8dbb3beac810d","modified":1584352888554},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"48743ac61af37a4de2026667e15a65de5e8cf542","modified":1584352888549},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"71a3d7f2242706f7a6b79933a67ef3664cca3a24","modified":1584352888555},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"f5abb2ea7746586738fb4e82107fceed041708ee","modified":1584352888554},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d7587df06b30042091316d8b1cc58ba361211492","modified":1584352888555},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0424a1fcffa1ae82fe70935972a894aca885bf9a","modified":1584352888555},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1584352888555},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1584352888575},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1584352888575},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1584352888576},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1584352888584},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1584352888585},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1584352888511},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1584352888511},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1584352888511},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"6336c2b129db802221b1fd75e5fbe8aab85c0a1f","modified":1584352888512},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"7ddb7453bf9b85b01bff136e9d10a7f06baac9e8","modified":1584352888512},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"98227b4de364b48b11e21efcf4f1beb2ed3ab329","modified":1584352888513},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"e3ade812b5541eca5b863ad3ff234ea95925bf31","modified":1584352888516},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"5d5c022aa3b2f89c2f2a178212338bb64804dd75","modified":1584352888516},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1584352888516},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"bcba503e956e4b737b062faa66341bd880f16c10","modified":1584352888517},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"9df5ba77e6cf36129bddc270407215c23c60ff38","modified":1584352888517},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"eca4d80dd0df1c3b1bc06bd39e6a4bd6c56198df","modified":1584352888518},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1584352888518},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1584352888518},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1584352888519},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"284249dcfa3b49c5d76210bbb0f74f65fc2f12ea","modified":1584352888519},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"4b84f35e7040f9adb5cc540c366d7f9eb4c48bcc","modified":1584352888513},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1584352888521},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1584352888521},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1584352888521},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1584352888522},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"f5821481440a0624c8aec5fc85f093de1527095f","modified":1584352888523},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"4b237e2344f35e9d1f6dbc3842d5e432d478ebfd","modified":1584352888524},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"d5d85d3646d184e0340924addcfd2523fb289d00","modified":1584352888525},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1584352888525},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"a7ed54e2f52185a7b6bb9a8201f6c3aa74b0cb00","modified":1584352888526},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"c27b3056d5e22d3c66d8a152a23634314d5c4a60","modified":1584352888526},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1584352888527},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"9b3ff4aa24069eab0e9771437013f45e450d4217","modified":1584352888528},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"a237c290e8934d1a8cbbf22b3f30503d9663021d","modified":1584352888529},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"95339b71ac90553fb1634c536c9749055e0c788a","modified":1584352888528},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"1db4ce981fe9fcb9ee4279395c29553efbb43947","modified":1584352888531},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1584352888531},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"e2ad7ccf1865a45548e3f31c70fac2c65d6ef534","modified":1584352888531},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1584352888533},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"c83d750238d6ac13b65869a909465fe621c464c3","modified":1584352888533},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"9a18b186b08ec220d1b17cf83812bcdd06077814","modified":1584352888533},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"0672ea2acf28dcc2cfc5244da36d3387d71a17cb","modified":1584352888524},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"2d9d68a431a334626d463bb1bdfbcd2ea8242e94","modified":1584352888521},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1584352888541},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e02b1097a72a7d2ddc45ea8d53aa6d77c25ac407","modified":1584352888523},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1584352888542},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"ae13712e313aacfce989d97ed6cba0606c776fb3","modified":1584352888542},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"7213e3d0ad7c95717ecd4e701d6ee9248ef2bf9f","modified":1584352888545},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1584352888545},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"5bf28a03ef021c27cfd713971caca5a0b9466fd1","modified":1584352888545},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"1a14c1b92d8a4dd8aabb5949333ac0ac79094c6c","modified":1584352888546},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"d2f0f2171722533bba308f944a2ec727b083582c","modified":1584352888546},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1584352888547},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"b492a45422773ab2af06ee345d527ba4c6bbc608","modified":1584352888541},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"2dc2a5b7becb11de1d4bdab6b5195588ae878cfc","modified":1584352888546},{"_id":"source/img/image_1564500807.490268.jpg","hash":"9962f71900b4d0963d28dba9ed9155edb3e27055","modified":1584352888301},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1584352888581},{"_id":"source/file/SparkSQL.pdf","hash":"080ed793c12d97436ae29851b5e34c54c07e3816","modified":1584352888191},{"_id":"source/file/RDDs.pdf","hash":"290974ef18b8672824b09c626e1b9847ef58a5b0","modified":1584352888144},{"_id":"public/search.xml","hash":"d43767c276066f59fe45da00606275acda3f8528","modified":1597925578719},{"_id":"public/img/8981.log","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1597925578719},{"_id":"public/categories/index.html","hash":"3a1f2169155a31f70bdabac78304f1f7a7c4525a","modified":1597925578719},{"_id":"public/about/index.html","hash":"ee6a5a7e8e33cedab2fa0f807802867d0f5545d8","modified":1597925578719},{"_id":"public/tags/index.html","hash":"698a4d47aa0638643de3d0d5bfd948c4e1766b02","modified":1597925578719},{"_id":"public/2020-03-18-Spark-Adaptive-Execution-TPCDS-3TB-BenchMark-md/index.html","hash":"5aae4db1de3ab3669d4d05cb77ab1affea6fd1a9","modified":1597925578719},{"_id":"public/archives/index.html","hash":"25af790d5356192125f6a44d71b5abaa5e7612ac","modified":1597925578719},{"_id":"public/archives/page/3/index.html","hash":"abfc4c5fae2564067bd226e28d5086efbc01bf3c","modified":1597925578719},{"_id":"public/archives/2019/page/2/index.html","hash":"a6abfc350c87b1c99bb51ed3b5b9106af58e187a","modified":1597925578719},{"_id":"public/archives/2019/08/index.html","hash":"dd778753bd10999545256ce66278e5b2e47f3ea2","modified":1597925578719},{"_id":"public/archives/2019/09/index.html","hash":"2b442b5514783d0ec5170de2b34219c51444744e","modified":1597925578719},{"_id":"public/archives/2019/10/index.html","hash":"d2148bf4e70e78f89d1bc5d91ff791281e590040","modified":1597925578719},{"_id":"public/archives/2019/11/index.html","hash":"119cd77ff1df44542fb03c87df729a902483cc7e","modified":1597925578719},{"_id":"public/archives/2019/12/index.html","hash":"a94395dcb6b72951c926562b927deca45b361462","modified":1597925578719},{"_id":"public/archives/2020/index.html","hash":"07ea27519a2df74c68a9eee29949b943d9c4a862","modified":1597925578719},{"_id":"public/archives/2020/01/index.html","hash":"732c466834a00fefd9edb902fb5530f18d57eb3f","modified":1597925578719},{"_id":"public/archives/2020/02/index.html","hash":"03a3c54e820d6f9d4ae36e4ff60744e96d082349","modified":1597925578719},{"_id":"public/archives/2020/03/index.html","hash":"6407d69cd91d77eb669378ce56695b2d318bca98","modified":1597925578719},{"_id":"public/archives/2020/04/index.html","hash":"bb79ba124c2e678e6a2f565ab113e9bb7aee826d","modified":1597925578719},{"_id":"public/archives/2020/07/index.html","hash":"4ae004da223c6a8ca89adc6e31c1d182379562d6","modified":1597925578719},{"_id":"public/archives/2020/08/index.html","hash":"f52e897d7dd8560ad33bab254d5db4c07893c8ce","modified":1597925578719},{"_id":"public/tags/Spark-2-3-0/page/2/index.html","hash":"64b815d348cb5ab7c6921d77ff179626379b677a","modified":1597925578719},{"_id":"public/tags/源码解析/index.html","hash":"4404c92ef42ecb37d549554c4887718ca14a4a5f","modified":1597925578719},{"_id":"public/tags/Yarn-2-7-3/index.html","hash":"11f59a554efed9cfe628f9860ed5d1d12f5d8514","modified":1597925578719},{"_id":"public/tags/Spark-On-Yarn/index.html","hash":"f27a83dec8ab4819392a1436897df56df20fd8ad","modified":1597925578719},{"_id":"public/tags/方案/index.html","hash":"e613356a6ef1bfbb7cb82c62c429789f76b87d21","modified":1597925578719},{"_id":"public/tags/Spark-SQL/index.html","hash":"9d34fa9fcce69b37d908d50a9ef731f8d0394b1e","modified":1597925578719},{"_id":"public/tags/Spark-3-0/index.html","hash":"1cccff6241032f38d5467c0f6854d64f0a27a2d6","modified":1597925578719},{"_id":"public/tags/Adaptive-Execution/index.html","hash":"49db6858831e2a024d1f2855969972c0eb19e7c7","modified":1597925578719},{"_id":"public/tags/tools/index.html","hash":"d2e23e03b86b4bbdfa54980b1e074cf17bd32451","modified":1597925578719},{"_id":"public/tags/Flink-1-12/index.html","hash":"63accb25538cab9d2a147a72888dd6a308b7ce18","modified":1597925578719},{"_id":"public/2020-08-20-02FlinkKafkaConnector/index.html","hash":"c65c5ef7eb3bec03f24ac6bc5ef364fbe1832107","modified":1597925578719},{"_id":"public/2020-08-15-01Flink Connector/index.html","hash":"88dee4487a9ffeb680f8a7f9167222db077219f2","modified":1597925578719},{"_id":"public/2020-07-03-Spark 自动合并小文件/index.html","hash":"1379f9da9a0d4746e9b7e580b5adefa0b8d65884","modified":1597925578719},{"_id":"public/2020-04-08-Linux 服务器分析 JVM Dump 文件/index.html","hash":"e49995d897a5c2fe025a1353fad6f484972ee48c","modified":1597925578719},{"_id":"public/2020-03-18-.git目录过大解决方案/index.html","hash":"6e1d982148701413f2ed836c7264dadb9e10a0ea","modified":1597925578719},{"_id":"public/hello-world/index.html","hash":"a1c4dd7a77634e3b143991aefb28363a18da69b9","modified":1597925578719},{"_id":"public/2020-02-23-Maven 常用命令/index.html","hash":"48edce78004fce092475c3490158127a9c6feef5","modified":1597925578719},{"_id":"public/2020-02-14-Apache Spark TPCDS性能测试数据集/index.html","hash":"b7af0c535ca888c33956f17ba1b73ddb3c53e676","modified":1597925578719},{"_id":"public/2020-01-10-Spark Executor 内存动态拓展方案/index.html","hash":"a36616ef4339eafb77f050bea9745da06532f2fb","modified":1597925578719},{"_id":"public/2019-12-27-Apache Spark 3.0 Preview 新特性和改动/index.html","hash":"93a840559caea5bb7e8a51dd4c6bea295cf16911","modified":1597925578719},{"_id":"public/2019-12-13-SparkSQL 文件读取分区源码解析/index.html","hash":"2e3fff6d00302f44c22a222c51d5dddf644066a3","modified":1597925578719},{"_id":"public/2019-12-10-SparkSQL 源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan/index.html","hash":"b94793224ee6135994706afc3662de8b6aa13b2c","modified":1597925578719},{"_id":"public/2019-11-29-SparkSQL 源码解析(2):从 UnresolvedLogicPlan 到 ResolvedLogicPlan/index.html","hash":"c0906035c9956f30e997f7a4bcde0e1a78ee1593","modified":1597925578719},{"_id":"public/2019-11-28-SparkSQL 源码解析(1):从 SQL 到 UnResolvedLogicPlan/index.html","hash":"4b0a28de398198553d06080544fe7df1cb78fed5","modified":1597925578719},{"_id":"public/2019-11-25-基于队列优先级的小集群大容器资源抢占方案/index.html","hash":"756903169c301a1f495cf89b90782ba37de5be5a","modified":1597925578719},{"_id":"public/2019-11-22-Improved MapPartitions In Spark/index.html","hash":"e80f75e8145df618c2823d0aea4a55e7427cf34b","modified":1597925578719},{"_id":"public/2019-11-18-SparkOnYarn 任务退出码/index.html","hash":"509bef3e7a372c3c53cefeba49c26335ebfafe47","modified":1597925578719},{"_id":"public/2019-11-12-Yarn User Headroom/index.html","hash":"488d4978f1619ebe05bc491ef1e30b936ad5fa86","modified":1597925578719},{"_id":"public/2019-10-28-Yarn ReservationSystem/index.html","hash":"3cddce6a9234a22a802d271e85089524686486ae","modified":1597925578719},{"_id":"public/2019-10-05-Yarn PreemptionSystem/index.html","hash":"f855e3bf3b99017d310168f6246f8f3bb9bbf578","modified":1597925578719},{"_id":"public/2019-09-15-Yarn:movetoqueue导致的UIcontainer泄露问题的修复思路/index.html","hash":"50c0a9360e0347f33043acc7fcc4f2bb51251021","modified":1597925578719},{"_id":"public/2019-08-09-ExecutorAllocationManager/index.html","hash":"8e37f907c2a7a2478a86442c2eb674525336e567","modified":1597925578719},{"_id":"public/archives/page/2/index.html","hash":"de24b22842763c1676a0ee28fd3e97adc203fd18","modified":1597925578719},{"_id":"public/archives/2019/index.html","hash":"47ca8fb62b515ee7467c2e831be312dee2f2a310","modified":1597925578719},{"_id":"public/tags/Spark-2-3-0/index.html","hash":"1be228191e1f97936fe4b28a4d83d08c13dd257a","modified":1597925578719},{"_id":"public/index.html","hash":"fcdde53e0809cf4c7d6abed188c3ccf2dbc9330b","modified":1597925578719},{"_id":"public/page/2/index.html","hash":"bcec5f18abd0fa71c047ee21427fe6df7bfd6c03","modified":1597925578719},{"_id":"public/page/3/index.html","hash":"eaa830062bdd0c536785b91bc9ebdc254390f5b2","modified":1597925578719},{"_id":"public/img/image_1564500797.115948.jpg","hash":"00b681fb2d32122336dffe0484ea34423596c410","modified":1597925578719},{"_id":"public/img/image_1564500820.121955.jpg","hash":"bac682b53eed8a44ebf2512c734d217a21c0e322","modified":1597925578719},{"_id":"public/img/moon_night3.jpg","hash":"bed20dc33569d47c3e7f1a22b9f2c018bec1d645","modified":1597925578719},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1597925578719},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1597925578719},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1597925578719},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1597925578719},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1597925578719},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1597925578719},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1597925578719},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1597925578719},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1597925578719},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1597925578719},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1597925578719},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1597925578719},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1597925578719},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1597925578719},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1597925578719},{"_id":"public/img/pictures/negative/yarn_node_update.eddx","hash":"3e3e5cafac04ffb0b4ea65266ad80ffa06f862c9","modified":1597925578719},{"_id":"public/img/pictures/reservation/reservationSystem.png","hash":"37663f45f6b9e91a296de596b33742a43ea01888","modified":1597925578719},{"_id":"public/img/pictures/yarn_preempt/preempt_queue.png","hash":"99bc3913e044f3451a66ae97b9e609284bef0d11","modified":1597925578719},{"_id":"public/img/pictures/spark/auto_merge_small_file.png","hash":"ff4458dd63a546cbfd2e483e40c3271e4f9bba16","modified":1597925578719},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1597925578719},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1597925578719},{"_id":"public/img/image_1564499567.339448.jpg","hash":"936d01884900678e41c20344b7fdbbc58226a733","modified":1597925578719},{"_id":"public/img/image_1564500802.493830.jpg","hash":"b33b25b55b10fef256848816de477666e1bfd347","modified":1597925578719},{"_id":"public/img/image_1564500801.667575.jpg","hash":"a2909084fda60e6cfff7fa9919aebd9e8dcc6548","modified":1597925578719},{"_id":"public/img/image_1564500804.092850.jpg","hash":"936d01884900678e41c20344b7fdbbc58226a733","modified":1597925578719},{"_id":"public/img/image_1564500810.633156.jpg","hash":"79a5bbed126917f555213230308f2a5b936e0eee","modified":1597925578719},{"_id":"public/img/image_1564500811.368373.jpg","hash":"79a5bbed126917f555213230308f2a5b936e0eee","modified":1597925578719},{"_id":"public/img/image_1564500821.860539.jpg","hash":"e4297c3dcaef3f44d2ed45ca1e728eea11984cbf","modified":1597925578719},{"_id":"public/img/moon_night.jpg","hash":"1ad487defd427542ada22176769749c4589ea935","modified":1597925578719},{"_id":"public/img/pictures/flink/flink_connector.png","hash":"13cf3e2e48105c425b3e516dd1a10e1243ec9c5b","modified":1597925578719},{"_id":"public/img/pictures/negative/yarn_node_update_pic.png","hash":"8a33503b2964f8a8959ce895a56421d3d0e75037","modified":1597925578719},{"_id":"public/img/pictures/reservation/yarn_reservation_system.png","hash":"897b98ca4f490fee1f72a6ea68b163007bebf63a","modified":1597925578719},{"_id":"public/img/pictures/yarn_headroom/headroom_1.jpeg","hash":"1b8bb002b173e60913c04345e1f322a1dc20af1e","modified":1597925578719},{"_id":"public/img/pictures/yarn_headroom/headroom_2.jpeg","hash":"2051ca4668992bd148445c20c472120fe7f25008","modified":1597925578719},{"_id":"public/img/image_1564500798.413998.jpg","hash":"b83dae7a1d0acbd89c4ac29251d82e02e59c440f","modified":1597925578719},{"_id":"public/img/image_1564500825.238104.jpg","hash":"27039207f2adbd1cf24d1400f1156ffc599b5584","modified":1597925578719},{"_id":"public/img/image_1564500826.315305.jpg","hash":"b9c01d5bd9d8c40aa80ccdfb25183ce00e9bee83","modified":1597925578719},{"_id":"public/img/image_1564499571.510152.jpg","hash":"d0ad27d7b4c58e91bc0cf107cddf505e11320e11","modified":1597925578719},{"_id":"public/img/404-bg.jpg","hash":"68f7d525269a94287e0ad18713ae232fb59dcf71","modified":1597925578719},{"_id":"public/img/image_1564500799.286733.jpg","hash":"bb40887121838860a6b2b340d6d3683aa50b8fcf","modified":1597925578719},{"_id":"public/img/image_1564500795.033385.jpg","hash":"edb6d785ca69bd642b88983cc7ac4b5d47b226ce","modified":1597925578719},{"_id":"public/img/image_1564500800.014160.jpg","hash":"bc907858a286ae07c7734ee12381202db4ca8cea","modified":1597925578719},{"_id":"public/img/image_1564500800.739319.jpg","hash":"99b95505abc4189ad536956dd1686ba57f442ebb","modified":1597925578719},{"_id":"public/img/image_1564500803.271657.jpg","hash":"d0ad27d7b4c58e91bc0cf107cddf505e11320e11","modified":1597925578719},{"_id":"public/img/image_1564500829.801306.jpg","hash":"edb6d785ca69bd642b88983cc7ac4b5d47b226ce","modified":1597925578719},{"_id":"public/img/image_1564500823.906395.jpg","hash":"75f2851fa20a836dea96b002c54480f00d70b07e","modified":1597925578719},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1597925578719},{"_id":"public/js/algolia-search.js","hash":"813afcc30feee11d59f297f2d5a96f98fbbd4743","modified":1597925578719},{"_id":"public/js/motion.js","hash":"09f8be6bcc6920546bb06e3f8c0181ec62c70b90","modified":1597925578719},{"_id":"public/js/local-search.js","hash":"9f3d3ddff86e7b2828772aba915a5ebfd7d9f55f","modified":1597925578719},{"_id":"public/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1597925578719},{"_id":"public/js/utils.js","hash":"4ea7d83e2b26b810ee4e847033c4f7e91278f37c","modified":1597925578719},{"_id":"public/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1597925578719},{"_id":"public/js/schemes/muse.js","hash":"792f4522a3b0b96584a041a4aca20fcded410079","modified":1597925578719},{"_id":"public/js/schemes/pisces.js","hash":"52d139e6f3bc212c8c9b71bc9e57e931920434a6","modified":1597925578719},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1597925578719},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1597925578719},{"_id":"public/css/main.css","hash":"818b97c2eeee90ab96610d5bc6ed42a9d9e182d0","modified":1597925578719},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1597925578719},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1597925578719},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1597925578719},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1597925578719},{"_id":"public/img/pictures/flink/flink_src_connector.png","hash":"c1f1d2369795f44b0e6941ae42fd1efecd21ce1f","modified":1597925578719},{"_id":"public/img/pictures/flink/flink_watermark.png","hash":"362d6d5165773ad684f772b5b9f0b03e3d03a1a2","modified":1597925578719},{"_id":"public/img/pictures/negative/negative_aead-04d8e67b1357.png","hash":"5f08b5050a25b8f69f2534129d794884fb591908","modified":1597925578719},{"_id":"public/img/pictures/yarn_preempt/preemption_scheduler.png","hash":"96e15b4e8070dc58816090ff936589647d164588","modified":1597925578719},{"_id":"public/img/pictures/negative/negative_e7a766275896.png","hash":"9852a80850d9edbc40144b45e5091f938c2d8d9b","modified":1597925578719},{"_id":"public/img/cat-323262_960_720.jpg","hash":"fc20d7411066c1871cff1e5b1f78bb434c231d8b","modified":1597925578719},{"_id":"public/img/image_1564500809.688791.jpg","hash":"565b0558d85d4f59ea41d745e27237f8cf87c429","modified":1597925578719},{"_id":"public/img/image_1564500827.867988.jpg","hash":"a7aa7f53dafa8154012d59c9c2edf091d05d4c2e","modified":1597925578719},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1597925578719},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1597925578719},{"_id":"public/img/cat-2332202_960_720.jpg","hash":"2f6f828b8780f73486cd7ae2850cfda76cd0be8a","modified":1597925578719},{"_id":"public/img/moon_night2.png","hash":"8862d2483da9aec4fd0e2c92606703d99311d5df","modified":1597925578719},{"_id":"public/img/image_1564500808.251661.jpg","hash":"db9ba11b894cbac7486f2d1746aba4ee2cd7984b","modified":1597925578719},{"_id":"public/img/image_1564500822.829172.jpg","hash":"0ade9245b1340cdc2c49677c09c8991cfb4ae121","modified":1597925578719},{"_id":"public/file/Spark.pdf","hash":"4b1b374b887611c2e276ba226440f2441ac8c9a9","modified":1597925578719},{"_id":"public/img/image_1564500809.014874.jpg","hash":"9fa234f2810c15ac1e2bd920ddbf9b4bb8c0adeb","modified":1597925578719},{"_id":"public/img/favicon.ico","hash":"daca33db516830ba37442e3d02b3a36e3921900b","modified":1597925578719},{"_id":"public/img/jerry.icon","hash":"daca33db516830ba37442e3d02b3a36e3921900b","modified":1597925578719},{"_id":"public/img/image_1564500806.619420.jpg","hash":"a7fb0dd14790887971022a724051ffc0b39d3cdc","modified":1597925578719},{"_id":"public/img/image_1564500828.736853.jpg","hash":"2add48386f879a46dabf7d12e9399d59b7030afd","modified":1597925578719},{"_id":"public/img/image_1564500805.768255.jpg","hash":"af225677fb6289395b84ebeb2c7e1ac95635654f","modified":1597925578719},{"_id":"public/img/image_1564500807.490268.jpg","hash":"9962f71900b4d0963d28dba9ed9155edb3e27055","modified":1597925578719},{"_id":"public/file/SparkSQL.pdf","hash":"080ed793c12d97436ae29851b5e34c54c07e3816","modified":1597925578719},{"_id":"public/file/RDDs.pdf","hash":"290974ef18b8672824b09c626e1b9847ef58a5b0","modified":1597925578719},{"_id":"source/img/pictures/delegation/cycle.png","hash":"e86535ffeb33872bd194cf0e316fabc435eac591","modified":1610507701055},{"_id":"source/img/pictures/delegation/private.png","hash":"f6014f90b353c3f35c65e337e0686620ea1f7f33","modified":1610506558487},{"_id":"source/img/pictures/delegation/public.png","hash":"b6284d2c271239de4296a0123a0c32ad8e3cf4fd","modified":1610506549985},{"_id":"source/img/pictures/delegation/auth.png","hash":"d932c632fc8ad4b41b0d7c8e3742748d6eff1fc6","modified":1610506871603},{"_id":"themes/yilia/.babelrc","hash":"b1b76475ac17dc9e2fa50af96c9e31eea2d0f2b4","modified":1597492793358},{"_id":"themes/yilia/.editorconfig","hash":"da6d022b8f4d9c961e2f8f80677e92af8de0db4d","modified":1597492793358},{"_id":"themes/yilia/.eslintignore","hash":"df0a50b13cc00acb749226fee3cee6e0351fb1d9","modified":1597492793358},{"_id":"themes/yilia/.eslintrc.js","hash":"5696ae049de010ed3786768b0c359f14c05b5ec6","modified":1597492793358},{"_id":"themes/yilia/.gitattributes","hash":"e0f24dceeb1e6878a1dd9b01a2b9df1bc037a867","modified":1597492793358},{"_id":"themes/yilia/_config.yml","hash":"ecfcbf2609f3e842361b8e2bf442b7eed55e0232","modified":1597493890560},{"_id":"themes/yilia/.gitignore","hash":"9c4b7d27a1e3e5efa0c8ed143a032a85d586b03b","modified":1597492793359},{"_id":"themes/yilia/README.md","hash":"f7708aaaf9a5e1cecaa2f72e834d0e69fcdb8488","modified":1597922120863},{"_id":"themes/yilia/webpack.config.js","hash":"05ba46a4ae744272f5312e684928910dccad3755","modified":1597492793425},{"_id":"themes/yilia/package.json","hash":"367cb9579d35968a942c243ab248a5f5ebfaf462","modified":1597492793378},{"_id":"themes/yilia/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1597492780499},{"_id":"themes/yilia/.git/config","hash":"767b47bb9fdd310868b70dddd0b6f79fd0e182c6","modified":1597492793345},{"_id":"themes/yilia/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1597492793340},{"_id":"themes/yilia/.git/index","hash":"c37ff996df415bbc68d7f3a1ae38e453eb34d48f","modified":1597494146688},{"_id":"themes/yilia/.git/packed-refs","hash":"723c2bc5caf1542b62b5311cbbb1c35e78311d80","modified":1597492793332},{"_id":"themes/yilia/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1597492793360},{"_id":"themes/yilia/.git/shallow","hash":"4ed77da1a2617db0e77c3e3e190a1c79c16dfb9a","modified":1597492784455},{"_id":"themes/yilia/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1597492793361},{"_id":"themes/yilia/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1597492793360},{"_id":"themes/yilia/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1597492793361},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1597492793361},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1597492793362},{"_id":"themes/yilia/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1597492793360},{"_id":"themes/yilia/layout/layout.ejs","hash":"0a332bdbd3b86c231d690614687f5b97186b85d5","modified":1597492793376},{"_id":"themes/yilia/layout/index.ejs","hash":"a35dc900203f9d8dd863ea4c1722198d6d457ec8","modified":1597492793376},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1597492793377},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1597492793377},{"_id":"themes/yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1597492793377},{"_id":"themes/yilia/source/main.0cf68a.css","hash":"ddf6e2c6b953c2c59a3c271e6070010a4cc81cf9","modified":1597492793423},{"_id":"themes/yilia/source/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1597492793425},{"_id":"themes/yilia/source-src/css.ejs","hash":"94dbdb02ca11849e415d54fb28546a598f2cffb1","modified":1597492793378},{"_id":"themes/yilia/source-src/script.ejs","hash":"c21381e1317db7bb157f1d182b8c088cb7cba411","modified":1597492793414},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1597492793376},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1597492793376},{"_id":"themes/yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1597492793375},{"_id":"themes/yilia/source/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1597492793423},{"_id":"themes/yilia/source/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1597492793424},{"_id":"themes/yilia/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1597492780503},{"_id":"themes/yilia/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1597492780500},{"_id":"themes/yilia/.git/hooks/fsmonitor-watchman.sample","hash":"f7c0aa40cb0d620ff0bca3efe3521ec79e5d7156","modified":1597492780503},{"_id":"themes/yilia/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1597492780504},{"_id":"themes/yilia/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1597492780505},{"_id":"themes/yilia/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1597492780505},{"_id":"themes/yilia/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1597492780501},{"_id":"themes/yilia/.git/hooks/pre-commit.sample","hash":"33729ad4ce51acda35094e581e4088f3167a0af8","modified":1597492780502},{"_id":"themes/yilia/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1597492780504},{"_id":"themes/yilia/.git/info/exclude","hash":"bb5a85730dcf100facee799c05cc4f6affec0745","modified":1597492780499},{"_id":"themes/yilia/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1597492780504},{"_id":"themes/yilia/.git/logs/HEAD","hash":"694f0a6384f1666e94dd4825f10147625cc96b5b","modified":1597492793342},{"_id":"themes/yilia/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1597492780506},{"_id":"themes/yilia/layout/_partial/aside.ejs","hash":"8edbd7993b9b061611a193533a664e2e85eae748","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"b86b248720ad415ec1b5fee53fb583776c776f83","modified":1597492793362},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a6e94061ac55b9eb55275f87b608d62f6ea35659","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"630c6ec866d056657d3d91e34b4c64eb993c0654","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"64f092186b5a744aa1603ce22bb1d44a34446add","modified":1597492793364},{"_id":"themes/yilia/layout/_partial/baidu-analytics.ejs","hash":"f0e6e88f9f7eb08b8fe51449a8a3016273507924","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"1f7d4819b7f67602c4a1b99871808d2160b60978","modified":1597492793363},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1597492793364},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"183d7ca4ba8e7c80694ffdc8cf39957092238346","modified":1597492793365},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"151a1ef2173ba7b6789d349f0f8da89616cc1394","modified":1597492793366},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"7fbbfabf5e29525b24ada14613c21a26789132b4","modified":1597492793370},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"f2994e0acd1d606ebf4680afc4fa652e148ccf4e","modified":1597492793364},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"6387a93dad7c3d778eb91e3821852fbf6813880c","modified":1597492793365},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"e495790b2abe2290875817e42bd505bc611d3e26","modified":1597492793375},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"c41341b9618e591538e1136a2d1637587c1bbd90","modified":1597492793375},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1597492793415},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1597492793416},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1597492793419},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1597492793418},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1597492793420},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1597492793422},{"_id":"themes/yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1597492793421},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1597492793421},{"_id":"themes/yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1597492793422},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"24f347a2412abbf58318369152504da9538f8d3b","modified":1597492793379},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"7d27e22ac898e8fafec14549e940c73cbea1fba8","modified":1597492793380},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"93a50dd19a93485712da1f8d0a1672482dd1eabc","modified":1597492793380},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"3fad68bd74260326f83090b0974dd80707e7bac7","modified":1597492793381},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"43e507f2a48504079afd9471353337e23ca47470","modified":1597492793381},{"_id":"themes/yilia/source-src/css/aside.scss","hash":"578a67464dd0f542197f7fcee158c991db058563","modified":1597492793387},{"_id":"themes/yilia/source-src/css/comment.scss","hash":"cafe3834017a3bf47420f37543725025225a2c89","modified":1597492793391},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"d79f2d35a06de83a2a226ca790b7a0a34789c115","modified":1597492793381},{"_id":"themes/yilia/source-src/css/article.scss","hash":"0f6d61af99ed4db87f8589db1feaea7747b55963","modified":1597492793383},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7c995410b25baaf61dfc5e148e22ca60330abcd3","modified":1597492793398},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"97b8fba41c914145710b90091f400b845879577f","modified":1597492793396},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1597492793398},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"3719994c2c9393813cc1d42b657205c368a329cb","modified":1597492793399},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"f053c609d84df0dd9eee1d11ddf0c19163a456be","modified":1597492793401},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"849a29fcd7150214fcf7b9715fa5dc71d1f9b896","modified":1597492793399},{"_id":"themes/yilia/source-src/css/left.scss","hash":"0d30c0e7cdb831c3881a017006c782f2214ac195","modified":1597492793401},{"_id":"themes/yilia/source-src/css/main.scss","hash":"2f86a014af93583caba78a563d9549826bf28294","modified":1597492793401},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"9c8dfd1c76854ef063494ca76fac6360b391ed6d","modified":1597492793403},{"_id":"themes/yilia/source-src/css/share.scss","hash":"150c6425f6582e7ec78a873256ce49c9930e8805","modified":1597492793403},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"ace041d72f95b419f6a5e443191703c2b62007f4","modified":1597492793402},{"_id":"themes/yilia/source-src/css/social.scss","hash":"724162ccf3977e70a45d189abfaa20b6e2fba87b","modified":1597492793404},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"ac67a3c7097849206244db9b0ba91daaba017ef5","modified":1597492793405},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"80a4fcf9171d4a33235da96ac8a2b7dcabc45dfb","modified":1597492793402},{"_id":"themes/yilia/source-src/css/page.scss","hash":"bf206bb7f7d0967bc8b7fdf01b7ffc99aff9ba88","modified":1597492793402},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"c8aa84fca93862d3caae77c552873b8610f33327","modified":1597492793405},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"1b1aa0908e58cf942b28e3881d07c5573c4129e1","modified":1597492793406},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"53d5a554bc2f38e9bb3d26400a47767013c05216","modified":1597492793406},{"_id":"themes/yilia/source-src/js/aside.js","hash":"754f771264548a6c5a8ad842908e59ae4e7ed099","modified":1597492793407},{"_id":"themes/yilia/source-src/js/anm.js","hash":"4a4c5d82b09a3063f91a434388e6aa064fd7fd98","modified":1597492793407},{"_id":"themes/yilia/source-src/js/browser.js","hash":"04095b38cfd4316a23f8eb14b1ffaf95f78a4260","modified":1597492793408},{"_id":"themes/yilia/source-src/js/Q.js","hash":"d011af172064b6c6e0c7051d8f9879373ddac113","modified":1597492793407},{"_id":"themes/yilia/source-src/js/main.js","hash":"3894e60827c817319e43c9ff3ed045fc3d7336ce","modified":1597492793410},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"4d823b039fd296d24a454eae5a798b93c44560cb","modified":1597492793411},{"_id":"themes/yilia/source-src/js/fix.js","hash":"d6782d53c992e712af39c84e804eccaf38830b94","modified":1597492793410},{"_id":"themes/yilia/source-src/js/report.js","hash":"4f1d9a18a936ce40b037f636a39127dd19175b6e","modified":1597492793411},{"_id":"themes/yilia/source-src/js/share.js","hash":"b090f82cf80cba7da764753906d9e2cc2acdf30d","modified":1597492793411},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"2577deb6a9fe4f5436360b2ce9afcc7f9a7f0116","modified":1597492793413},{"_id":"themes/yilia/source-src/js/slider.js","hash":"e846bcc5aac9c68b93f7b8de353df54d8d29f666","modified":1597492793412},{"_id":"themes/yilia/source-src/js/util.js","hash":"8456e9d6b19532742582c99b2fb9d09e146e1c58","modified":1597492793413},{"_id":"themes/yilia/layout/_partial/script.ejs","hash":"4cb685f07e89dd5175c2a576e73a1a957aec5637","modified":1597492793375},{"_id":"themes/yilia/.git/objects/pack/pack-4adf69ef2cf96845a8f9496c9bd27ab63ed1847b.idx","hash":"e89d34194272a482225fe7968ed6185e0a6b4321","modified":1597492793302},{"_id":"themes/yilia/.git/refs/heads/master","hash":"4ed77da1a2617db0e77c3e3e190a1c79c16dfb9a","modified":1597492793342},{"_id":"themes/yilia/layout/_partial/post/changyan.ejs","hash":"5f99b55980da64a723a8e14d5a7daba0d6504647","modified":1597492793372},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"ef71c4081e866a494367575c59610e7e6339ece0","modified":1597492793372},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"0809a4829aabeb4e911a3ed04ec28db4df7dfe3f","modified":1597492793371},{"_id":"themes/yilia/layout/_partial/post/gitment.ejs","hash":"e68bbac9ffb1ad27b56837c9abad6ed6bb7daa0c","modified":1597492793373},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"5dccfbe165b23a101f1333cc65ed8efbd197453c","modified":1597492793373},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"e8399025ed3b980aedb821c92855889f5f12fd5b","modified":1597492793372},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"1036c8e4e1a7bc935ba173744da735a0d6ed09cd","modified":1597492793373},{"_id":"themes/yilia/layout/_partial/post/wangyiyun.ejs","hash":"ea41c462168d9697caef9485862e9cac718a12c1","modified":1597492793374},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1597492793374},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"2e783e68755abb852760eb0e627a3fbb50a05a55","modified":1597492793374},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"63a37f26276f9207405afe0f2d65339ce295bbaf","modified":1597492793392},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"491ab3378d5c11005ba65c607608bb36b368a9d5","modified":1597492793393},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"3bba5c77bad5981eac859fe05c9561d580ba7fa9","modified":1597492793394},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"fab871fa93bd542e76a71a56428f2994a4aaf443","modified":1597492793395},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1597492793397},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1597492793398},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"fb511c505d1309249f21dc577d4ad2bad99a764f","modified":1597492793396},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1597492793400},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1597492793399},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1597492793400},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1597492793398},{"_id":"themes/yilia/.git/logs/refs/heads/master","hash":"694f0a6384f1666e94dd4825f10147625cc96b5b","modified":1597492793342},{"_id":"themes/yilia/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1597492793339},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1597492793397},{"_id":"themes/yilia/.git/objects/pack/pack-4adf69ef2cf96845a8f9496c9bd27ab63ed1847b.pack","hash":"ffac752817755631f917c96acafe6ce324cb058a","modified":1597492793302},{"_id":"themes/yilia/.git/logs/refs/remotes/origin/HEAD","hash":"694f0a6384f1666e94dd4825f10147625cc96b5b","modified":1597492793339}],"Category":[],"Data":[],"Page":[{"title":"categories","date":"2019-12-10T11:00:00.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-12-10 19:00:00\ntype: \"categories\"\ncomments: false\n---\n","updated":"2020-03-16T10:01:28.113Z","path":"categories/index.html","layout":"page","_id":"cke2rms4f0001y7c712rw8w48","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"李斯特","date":"2019-12-10T11:00:00.000Z","type":"about","comments":0,"_content":"\n主要研究方向：分布式计算系统(Spark,Flink),资源调度系统(Yarn)  \n主要兴趣方向: 大数据,深度学习  \n\n她那个时候还太年轻,不知道所有命运馈赠的礼物,早已标好了价格  \n","source":"about/index.md","raw":"---\ntitle: 李斯特\ndate: 2019-12-10 19:00:00\ntype: about\ncomments: false\n---\n\n主要研究方向：分布式计算系统(Spark,Flink),资源调度系统(Yarn)  \n主要兴趣方向: 大数据,深度学习  \n\n她那个时候还太年轻,不知道所有命运馈赠的礼物,早已标好了价格  \n","updated":"2020-03-16T10:01:28.112Z","path":"about/index.html","layout":"page","_id":"cke2rms4i0003y7c74fkm8ldl","content":"<p>主要研究方向：分布式计算系统(Spark,Flink),资源调度系统(Yarn)<br>主要兴趣方向: 大数据,深度学习  </p>\n<p>她那个时候还太年轻,不知道所有命运馈赠的礼物,早已标好了价格  </p>\n","site":{"data":{}},"excerpt":"","more":"<p>主要研究方向：分布式计算系统(Spark,Flink),资源调度系统(Yarn)<br>主要兴趣方向: 大数据,深度学习  </p>\n<p>她那个时候还太年轻,不知道所有命运馈赠的礼物,早已标好了价格  </p>\n"},{"title":"标签","date":"2019-12-10T11:00:00.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2019-12-10 19:00:00\ntype: \"tags\"\ncomments: false\n---\n","updated":"2020-03-16T10:01:28.424Z","path":"tags/index.html","layout":"page","_id":"cke2rms8n002ey7c7embm81w2","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"post","title":"ExecutorAllocationManager源码分析","subtitle":"Spark 源码分析","date":"2019-08-08T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n## 摘要\n\nSpark 为 Standalone 和 CoarseGrained(Yarn/Mesos)调度模式提供了动态资源(executor)分配功能,可以根据 Application 的工作负载动态调整其占用的资源.在多 Application 同时运行的情况下,可以提高资源利用率.原理是Spark 定时调度,利用正在运行和积压的任务数推算出需要的executor 数量并和已持有的数量对比,增加 executor;利用维护的节点空闲时间信息,释放掉空闲的 executor.\n\n<!-- more -->\n\n## 背景\n\n\t在SparkOnYarn任务执行的时候,发现了下面几条日志:\n\tExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)\n\tExecutorAllocationManager: Removing executor 6 because it has been idle for 180 seconds (new desired total will be 5)\n\t上述日志没有时间先后及因果关系,发现ExecutorAllocationManager 会依据积压的任务申请新的 executor,并在 executor 空闲一段时间后释放 executor.\n\t\n## 开启方法\n\n### Cluster\n\n\t仅以资源调度平台最通用的 yarn 举例\n       1.spark 源代码编译时需支持 yarn 特性  \n       2.将 $SPARK_HOME/common/network-yarn/target/scala-<version>/spark-<version>-yarn-shuffle.jar移动到各 NodeManager 的 classpath 下  \n       3.各节点的yarn-site.xml 配置文件中,设置 yarn.nodemanager.aux-services为spark_shuffle,yarn.nodemanager.aux-services.spark_shuffle.class为org.apache.spark.network.yarn.YarnShuffleService\n       4.etc/hadoop/yarn-env.sh文件中,增加NodeManager 的堆内存YARN_HEAPSIZE,以减少 shuffle 期间的 GC 频率\n       5.重启所有的 NodeManager\n\n### Application\n\n\t必须把下面两个参数默认都是 false,需要设置为 true\n\tspark.dynamicAllocation.enabled true\t\t开启动态资源分配机制\n\tspark.shuffle.service.enabled true \t 使用额外的 shuffle service 服务,可以使 executor 被移除时,不会删除该 executor写入的 shuffle数据.\n\tspark.shuffle.service.port 7337   额外的 shuffle service 服务所占用的端口\n\n### 配置参数\t\n\nspark.dynamicAllocation.enabled true\t  \nspark.shuffle.service.enabled true    \nspark.dynamicAllocation.minExecutors 2\t\t动态数量下界  \nspark.dynamicAllocation.maxExecutors 100 \t动态数量上界  \nspark.dynamicAllocation.initialExecutors 2 \texecutor初始化时申请的数量   \nspark.dynamicAllocation.schedulerBacklogTimeout 60s  被积压的任务等待时间超过此值时,触发一次executor申请   \nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 60s 和spark.dynamicAllocation.schedulerBacklogTimeout值相同,用于后续的executor申请   \nspark.dynamicAllocation.executorIdleTimeout 180s \texecutor在空闲了该时间之后,释放该 executor   \nspark.dynamicAllocation.cachedExecutorIdleTimeout infinity  缓存了数据的 executor 在空闲了该时间之后,释放该 executor.一般不修改即不删除   \n\n## 源码解析\nExecutorAllocationManager 被调用的入口是 SparkContext\n\n```\n[SparkContext.scala]\n    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) //判断 spark.dynamicAllocation.enable 参数\n    _executorAllocationManager =\n      if (dynamicAllocationEnabled) {\n        schedulerBackend match {\n          case b: ExecutorAllocationClient =>\t//子类是 StandaloneSchdulerBackend 和 CoarseGrainedSchedulerBackend,没有 Local\n            Some(new ExecutorAllocationManager(\n              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,\n              _env.blockManager.master))\n          case _ =>\n            None\n        }\n      } else {\n        None\n      }\n    _executorAllocationManager.foreach(_.start())\n```\n\nExecutorAllocationManager中的关键成员变量,主要用于申请/注销 executor 时的信息维护\n\n```\n[ExecutorAllocationManager.scala]\n  //在下次申请 executor 时的滚动增量\t\n  private var numExecutorsToAdd = 1\n  //当前已持有的 executor 数量.如果运行中所有的 executor 都挂了,该值正是 application 向 clustermanager 申请的 executor 数量,而非再从 initExecutorNum 和 numExecutorToAdd 滚动增加至挂掉前状态\n  private var numExecutorsTarget = initialNumExecutors\n  //已经请求释放但是还没被释放的 executor 列表\n  private val executorsPendingToRemove = new mutable.HashSet[String]\n  //application 所有的 executor\n  private val executorIds = new mutable.HashSet[String]\n  // A timestamp of when an addition should be triggered, or NOT_SET if it is not set\n  // This is set when pending tasks are added but not scheduled yet\n  private var addTime: Long = NOT_SET\n   //<executorId,最大存活时间>信息.在 executor 第一次注册或不再运行任务时设置\n  private val removeTimes = new mutable.HashMap[String, Long]\n  //是否是 application初始化时期.为 true 时,不会申请executor;当开始执行任务后或者executor 已经过了超时时间 时为 false\n  @volatile private var initializing: Boolean = true\n  //用于 executor 本地化计算分配,调用 clustermanager 接口的必须参数\n  private var localityAwareTasks = 0\n  //用于 executor 本地化计算分配,调用cm 接口的必须参数\n  private var hostToLocalTaskCount: Map[String, Int] = Map.empty\n```\n\n周期性检测 application 的工作负载,动态调节executor 的数量\n\n```\nExecutorAllocationManager.scala\n def start(): Unit = {\n    listenerBus.addToManagementQueue(listener)\n    val scheduleTask = new Runnable() {\n      override def run(): Unit = {\n             ...\n             schedule()\n             ...\n      \t}\n    }\n    //定时调度,间隔100ms\n    executor.scheduleWithFixedDelay(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)\n    //先向集群申请初始化数量的 executor. numExecutorTarget 初始值是 max(spark.dynamicAllocation.minExecutors,spark.dynamicAllocation.initialExecutors,spark.executor.instances),其他两个参数是 0 和 Map.Empty\n    client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n  }\n  \n```\nschedule 方法的逻辑分为两块,1.检测是否要申请 executor 以及申请 2.检测是否要释放 executor 以及释放\n\n```\nprivate def schedule(): Unit = synchronized {\n    val now = clock.getTimeMillis\n    //申请\n    updateAndSyncNumExecutorsTarget(now)\n    //待释放列表\n    val executorIdsToBeRemoved = ArrayBuffer[String]()\n    //保留未超时的 executor,将超时的 executor 加入到待释放列表\n    removeTimes.retain { case (executorId, expireTime) =>\n      val expired = now >= expireTime\n      if (expired) {\n        initializing = false\n        executorIdsToBeRemoved += executorId\n      }\n      !expired\n    }\n    //释放\n    if (executorIdsToBeRemoved.nonEmpty) {\n      removeExecutors(executorIdsToBeRemoved)\n    }\n  }\n```\n先看释放 executor 的逻辑,比较简单\n\n```\n  private def removeExecutors(executors: Seq[String]): Seq[String] = synchronized {\n    val executorIdsToBeRemoved = new ArrayBuffer[String]\n    logInfo(\"Request to remove executorIds: \" + executors.mkString(\", \"))\n    //当前存活的节点数\n    val numExistingExecutors = allocationManager.executorIds.size - executorsPendingToRemove.size\n    var newExecutorTotal = numExistingExecutors\n    executors.foreach { executorIdToBeRemoved =>\n    //如果删除该节点,导致存活节点数小于动态资源下界或者小于当前需要的资源数,则不删除\n      if (newExecutorTotal - 1 < minNumExecutors) {\n        logDebug(s\"Not removing idle executor $executorIdToBeRemoved because there are only \" +\n          s\"$newExecutorTotal executor(s) left (minimum number of executor limit $minNumExecutors)\")\n      } else if (newExecutorTotal - 1 < numExecutorsTarget) {\n        logDebug(s\"Not removing idle executor $executorIdToBeRemoved because there are only \" +\n          s\"$newExecutorTotal executor(s) left (number of executor target $numExecutorsTarget)\")\n      } else if (canBeKilled(executorIdToBeRemoved)) {\n        executorIdsToBeRemoved += executorIdToBeRemoved\n        newExecutorTotal -= 1\n      }\n    }\n    if (executorIdsToBeRemoved.isEmpty) {\n      return Seq.empty[String]\n    }\n    val executorsRemoved = if (testing) {\n      executorIdsToBeRemoved\n    } else {\n        client.killExecutors(executorIdsToBeRemoved, adjustTargetNumExecutors = false,\n        countFailures = false, force = false)\n    }\n     client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n    newExecutorTotal = numExistingExecutors\n    if (testing || executorsRemoved.nonEmpty) {\n      executorsRemoved.foreach { removedExecutorId =>\n        newExecutorTotal -= 1\n        logInfo(s\"Removing executor $removedExecutorId because it has been idle for \" +\n          s\"$executorIdleTimeoutS seconds (new desired total will be $newExecutorTotal)\")\n        executorsPendingToRemove.add(removedExecutorId)\n      }\n      executorsRemoved\n    } else {\n      logWarning(s\"Unable to reach the cluster manager to kill executor/s \" +\n        s\"${executorIdsToBeRemoved.mkString(\",\")} or no executor eligible to kill!\")\n      Seq.empty[String]\n    }\n  }\n```\nkillExecutors 的逻辑在对接底层资源调度平台(Standalone,CoarseGrained)的实现类中,以CoarseGrainedSchedulerBackend为例\n\n```\n[CoarseGrainedSchedulerBackend.scala]\nfinal override def killExecutors(\n      executorIds: Seq[String],\n      adjustTargetNumExecutors: Boolean,\n      countFailures: Boolean,\n      force: Boolean): Seq[String] = {\n    val response = synchronized {\n      val (knownExecutors, unknownExecutors) = executorIds.partition(executorDataMap.contains)\n      unknownExecutors.foreach { id =>\n        logWarning(s\"Executor to kill $id does not exist!\")\n      }\n     //从待删除的列表中过滤掉确定被删除但还未被删除的和没有正在跑task 的 executor,作为此次请求 cm 释放的executor\n      val executorsToKill = knownExecutors\n        .filter { id => !executorsPendingToRemove.contains(id) }\n        .filter { id => force || !scheduler.isExecutorBusy(id) }\n      executorsToKill.foreach { id => executorsPendingToRemove(id) = !countFailures }\n      logInfo(s\"Actual list of executor(s) to be killed is ${executorsToKill.mkString(\", \")}\")\n      \n      // If we do not wish to replace the executors we kill, sync the target number of executors\n      // with the cluster manager to avoid allocating new ones. When computing the new target,\n      // take into account executors that are pending to be added or removed.\n      val adjustTotalExecutors =\n        if (adjustTargetNumExecutors) {\n          requestedTotalExecutors = math.max(requestedTotalExecutors - executorsToKill.size, 0)\n          ..\n          doRequestTotalExecutors(requestedTotalExecutors)\n        } else {\n          numPendingExecutors += knownExecutors.size\n          Future.successful(true)\n        }\n        \n      val killExecutors: Boolean => Future[Boolean] =\n        if (!executorsToKill.isEmpty) {\n          _ => doKillExecutors(executorsToKill)\n        } else {\n          _ => Future.successful(false)\n        }\n      val killResponse = adjustTotalExecutors.flatMap(killExecutors)(ThreadUtils.sameThread)\n      killResponse.flatMap(killSuccessful =>\n        Future.successful (if (killSuccessful) executorsToKill else Seq.empty[String])\n      )(ThreadUtils.sameThread)\n    }\n    defaultAskTimeout.awaitResult(response)\n  }\n   protected def doKillExecutors(executorIds: Seq[String]): Future[Boolean] =\n    Future.successful(false)\n```\ndoKillExecutor和doRequestTotalExecutors在 CoarseGrainedSchedulerBackend 中有默认实现,里面内容比较有迷惑性,其实逻辑在YarnSchedulerBackend中,封装了YarnClientSchedulerBackend 和 YarnClusterSchedulerBackend 实现类的通用逻辑.ResourceManager 端的逻辑不在此讨论\n\n```\n[YarnSchedulerBackend.scala]\n  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](KillExecutors(executorIds))\n  }\n  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))\n  \n```\n再看申请 executor 的逻辑,对 ExecutorAllocationManager#start#schedule#updateAndSyncNumExecutorsTarget(now)方法做解析\n\n```\nprivate def updateAndSyncNumExecutorsTarget(now: Long): Int = synchronized {\n    val maxNeeded = maxNumExecutorsNeeded\n    if (initializing) {\n      //application job 还没有开始执行第一个 stage,所以不需要申请额外的资源\n       0\n    } else if (maxNeeded < numExecutorsTarget) {\n      //如果当前计算出所需的资源小于已经拥有的资源,那么没有必要再申请新的,申请了也是空闲\n      //可以代入一些值做debug\n      val oldNumExecutorsTarget = numExecutorsTarget\n      numExecutorsTarget = math.max(maxNeeded, minNumExecutors)\n      numExecutorsToAdd = 1\n\n       //math.max(maxNeeded, minNumExecutors)与numExecutorsTarget且maxNeeded < numExecutorsTarget\n      if (numExecutorsTarget < oldNumExecutorsTarget) {\n        // We lower the target number of executors but don't actively kill any yet.  Killing is\n        // controlled separately by an idle timeout.  It's still *helpful* to reduce the target number\n        // in case an executor just happens to get lost (eg., bad hardware, or the cluster manager\n        // preempts it) -- in that case, there is no point in trying to immediately  get a new\n        // executor, since we wouldn't even use it yet.\n        //同步当前需要的 executor 数量给 cm\n        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n        logDebug(s\"Lowering target number of executors to $numExecutorsTarget (previously \" +\n          s\"$oldNumExecutorsTarget) because not all requested executors are actually needed\")\n      }\n      numExecutorsTarget - oldNumExecutorsTarget\n    } else if (addTime != NOT_SET && now >= addTime) {\n    //在 application 已经开始运行,申请过资源;且当前计算出来的需要的 executor 数量大于已有的 executor 数量;且任务已经被积压过(onSchedulerBacklogged事件会设置 add_time)那么会申请 executor\n      val delta = addExecutors(maxNeeded)\n      logDebug(s\"Starting timer to add more executors (to \" +\n        s\"expire in $sustainedSchedulerBacklogTimeoutS seconds)\")\n      addTime = now + (sustainedSchedulerBacklogTimeoutS * 1000)\n      delta\n    } else {\n      0\n    }\n  }\n  \n  //依据当前stage正在运行的task数量和堆积的task数量来预估需要的executor数量\n  //之所以是当前 stage,是因为onStageSubmitted和onStageCompleted方法会维护stageIdToNumTasks(hashmap),所以这个 map 里面存储的是DAG中无依赖关系的 stage-tasknum 数据\n   private def maxNumExecutorsNeeded(): Int = {\n    val numRunningOrPendingTasks = listener.totalPendingTasks + listener.totalRunningTasks\n    (numRunningOrPendingTasks + tasksPerExecutor - 1) / tasksPerExecutor\n  }\n//申请 executor.如果当前需要的资源数超过动态最大资源数,该次不申请并且将 numExecutorsToAdd(申请 executor 的滚动增量)设置为1,而不是乘以2.\n//numExecutorsToAdd:1->2->4->8->1(已持有的数量不小于动态最大数量,则没必要需要太快的增速)->2->4\nprivate def addExecutors(maxNumExecutorsNeeded: Int): Int = {\n    // Do not request more executors if it would put our target over the upper bound\n    if (numExecutorsTarget >= maxNumExecutors) {\n      logDebug(s\"Not adding executors because our current target total \" +\n        s\"is already $numExecutorsTarget (limit $maxNumExecutors)\")\n      numExecutorsToAdd = 1\n      return 0\n    }\n\t\n    val oldNumExecutorsTarget = numExecutorsTarget\n    // There's no point in wasting time ramping up to the number of executors we already have, so\n    // make sure our target is at least as much as our current allocation:\n    numExecutorsTarget = math.max(numExecutorsTarget, executorIds.size)\n    // Boost our target with the number to add for this round:   滚动增量\n    numExecutorsTarget += numExecutorsToAdd\n    // Ensure that our target doesn't exceed what we need at the present moment:\n    numExecutorsTarget = math.min(numExecutorsTarget, maxNumExecutorsNeeded)\n    // Ensure that our target fits within configured bounds:\n    numExecutorsTarget = math.max(math.min(numExecutorsTarget, maxNumExecutors), minNumExecutors)\n\n    val delta = numExecutorsTarget - oldNumExecutorsTarget\n    if (delta == 0) {\n      // Check if there is any speculative jobs pending\n      if (listener.pendingTasks == 0 && listener.pendingSpeculativeTasks > 0) {\n        numExecutorsTarget =\n          math.max(math.min(maxNumExecutorsNeeded + 1, maxNumExecutors), minNumExecutors)\n      } else {\n        numExecutorsToAdd = 1\n        return 0\n      }\n    }\n    val addRequestAcknowledged = try {\n      testing ||\n        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n    } catch {\n        ....\n        false\n    }\n    if (addRequestAcknowledged) {\n      val executorsString = \"executor\" + { if (delta > 1) \"s\" else \"\" }\n      logInfo(s\"Requesting $delta new $executorsString because tasks are backlogged\" +\n        s\" (new desired total will be $numExecutorsTarget)\")\n      numExecutorsToAdd = if (delta == numExecutorsToAdd) {\n        numExecutorsToAdd * 2\n      } else {\n        1\n      }\n      delta\n    } else {\n      logWarning(\n        s\"Unable to reach the cluster manager to request $numExecutorsTarget total executors!\")\n      numExecutorsTarget = oldNumExecutorsTarget\n      0\n    }\n  }\n```\n资源申请\n\n```\n[YarnSchedulerBackend.scala]\noverride def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))\n }\n```\n\n## ExecutorAllocationListener\nExecutorAllocationListener监听器监听各类事件,触发相应的回调函数,对自身以及ExecutorAllocationManager 内维护的信息做修改,辅助 ExecutorAllocationManager 做动态资源调度.看一些回调函数的源码:\n\n```\n/**\n   * Callback invoked when the specified executor has been added.\n   */\nprivate def onExecutorAdded(executorId: String): Unit = synchronized {\n    if (!executorIds.contains(executorId)) {\n      executorIds.add(executorId)\n      // If an executor (call this executor X) is not removed because the lower bound\n      // has been reached, it will no longer be marked as idle. When new executors join,\n      // however, we are no longer at the lower bound, and so we must mark executor X\n      // as idle again so as not to forget that it is a candidate for removal. (see SPARK-4951)\n      executorIds.filter(listener.isExecutorIdle).foreach(onExecutorIdle)\n      logInfo(s\"New executor $executorId has registered (new total is ${executorIds.size})\")\n    } else {\n      logWarning(s\"Duplicate executor $executorId has registered\")\n    }\n  }\n  private def onExecutorRemoved(executorId: String): Unit = synchronized {\n    if (executorIds.contains(executorId)) {\n      executorIds.remove(executorId)\n      removeTimes.remove(executorId)\n      logInfo(s\"Existing executor $executorId has been removed (new total is ${executorIds.size})\")\n      if (executorsPendingToRemove.contains(executorId)) {\n        executorsPendingToRemove.remove(executorId)\n        logDebug(s\"Executor $executorId is no longer pending to \" +\n          s\"be removed (${executorsPendingToRemove.size} left)\")\n      }\n    } else {\n      logWarning(s\"Unknown executor $executorId has been removed!\")\n    }\n  }\n  private def onSchedulerBacklogged(): Unit = synchronized {\n    if (addTime == NOT_SET) {\n      logDebug(s\"Starting timer to add executors because pending tasks \" +\n        s\"are building up (to expire in $schedulerBacklogTimeoutS seconds)\")\n      addTime = clock.getTimeMillis + schedulerBacklogTimeoutS * 1000\n    }\n  }\n  private def onExecutorIdle(executorId: String): Unit = synchronized {\n    if (executorIds.contains(executorId)) {\n      if (!removeTimes.contains(executorId) && !executorsPendingToRemove.contains(executorId)) {\n        val hasCachedBlocks = blockManagerMaster.hasCachedBlocks(executorId)\n        val now = clock.getTimeMillis()\n        val timeout = {\n          if (hasCachedBlocks) {\t//缓存了数据,shuffle write 数据等\n            now + cachedExecutorIdleTimeoutS * 1000\n          } else {\n            now + executorIdleTimeoutS * 1000\n          }\n        }\n        val realTimeout = if (timeout <= 0) Long.MaxValue else timeout // overflow\n        removeTimes(executorId) = realTimeout\n        logDebug(s\"Starting idle timer for $executorId because there are no more tasks \" +\n          s\"scheduled to run on the executor (to expire in ${(realTimeout - now)/1000} seconds)\")\n      }\n    } else {\n      logWarning(s\"Attempted to mark unknown executor $executorId idle\")\n    }\n  }\n  private def onExecutorBusy(executorId: String): Unit = synchronized {\n    logDebug(s\"Clearing idle timer for $executorId because it is now running a task\")\n    removeTimes.remove(executorId)\n  }\n```\n\n## 其他\n\n>\n动态资源调度设计的一点思考\n1. 如何增加资源  \n\t负载与当前资源的权衡;资源增加算法:线性增加,对数等;资源快速满足需求快速下降  \n2. 如何释放资源  \n\t如何判断空闲;资源上有不好处理的逻辑怎么办,数据或计算   \n3. 设计模式   \n\t事件驱动的设计模式  \n4. 解耦的重要性  \n\t曾经设计过一个主从式数据处理框架,但是在可拓展性上考虑的比较少.如果现在在那个系统上增加动态资源调度功能,在设计模式和解耦上比不上 spark 之万一...  \n\t\n## 参考\n\n http://spark.apache.org/docs/2.3.1/configuration.html   \n http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service  \n\n\n\n\n\n\n\n\n\n","source":"_posts/2019-08-09-ExecutorAllocationManager.md","raw":"---\nlayout:     post   \t\t\t\t   \ntitle:      ExecutorAllocationManager源码分析 \t\nsubtitle:   Spark 源码分析 \t\t\ndate:       2019-08-09 \t\nauthor:     jiulongzhu \t\t\t\t\t\t\nheader-img: img/moon_night.jpg  \ncatalog: true \t\t\t\t\ntags:\t\t\t\n    - Spark 2.3.0\n    - 源码解析\n---\n\n## 摘要\n\nSpark 为 Standalone 和 CoarseGrained(Yarn/Mesos)调度模式提供了动态资源(executor)分配功能,可以根据 Application 的工作负载动态调整其占用的资源.在多 Application 同时运行的情况下,可以提高资源利用率.原理是Spark 定时调度,利用正在运行和积压的任务数推算出需要的executor 数量并和已持有的数量对比,增加 executor;利用维护的节点空闲时间信息,释放掉空闲的 executor.\n\n<!-- more -->\n\n## 背景\n\n\t在SparkOnYarn任务执行的时候,发现了下面几条日志:\n\tExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)\n\tExecutorAllocationManager: Removing executor 6 because it has been idle for 180 seconds (new desired total will be 5)\n\t上述日志没有时间先后及因果关系,发现ExecutorAllocationManager 会依据积压的任务申请新的 executor,并在 executor 空闲一段时间后释放 executor.\n\t\n## 开启方法\n\n### Cluster\n\n\t仅以资源调度平台最通用的 yarn 举例\n       1.spark 源代码编译时需支持 yarn 特性  \n       2.将 $SPARK_HOME/common/network-yarn/target/scala-<version>/spark-<version>-yarn-shuffle.jar移动到各 NodeManager 的 classpath 下  \n       3.各节点的yarn-site.xml 配置文件中,设置 yarn.nodemanager.aux-services为spark_shuffle,yarn.nodemanager.aux-services.spark_shuffle.class为org.apache.spark.network.yarn.YarnShuffleService\n       4.etc/hadoop/yarn-env.sh文件中,增加NodeManager 的堆内存YARN_HEAPSIZE,以减少 shuffle 期间的 GC 频率\n       5.重启所有的 NodeManager\n\n### Application\n\n\t必须把下面两个参数默认都是 false,需要设置为 true\n\tspark.dynamicAllocation.enabled true\t\t开启动态资源分配机制\n\tspark.shuffle.service.enabled true \t 使用额外的 shuffle service 服务,可以使 executor 被移除时,不会删除该 executor写入的 shuffle数据.\n\tspark.shuffle.service.port 7337   额外的 shuffle service 服务所占用的端口\n\n### 配置参数\t\n\nspark.dynamicAllocation.enabled true\t  \nspark.shuffle.service.enabled true    \nspark.dynamicAllocation.minExecutors 2\t\t动态数量下界  \nspark.dynamicAllocation.maxExecutors 100 \t动态数量上界  \nspark.dynamicAllocation.initialExecutors 2 \texecutor初始化时申请的数量   \nspark.dynamicAllocation.schedulerBacklogTimeout 60s  被积压的任务等待时间超过此值时,触发一次executor申请   \nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 60s 和spark.dynamicAllocation.schedulerBacklogTimeout值相同,用于后续的executor申请   \nspark.dynamicAllocation.executorIdleTimeout 180s \texecutor在空闲了该时间之后,释放该 executor   \nspark.dynamicAllocation.cachedExecutorIdleTimeout infinity  缓存了数据的 executor 在空闲了该时间之后,释放该 executor.一般不修改即不删除   \n\n## 源码解析\nExecutorAllocationManager 被调用的入口是 SparkContext\n\n```\n[SparkContext.scala]\n    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) //判断 spark.dynamicAllocation.enable 参数\n    _executorAllocationManager =\n      if (dynamicAllocationEnabled) {\n        schedulerBackend match {\n          case b: ExecutorAllocationClient =>\t//子类是 StandaloneSchdulerBackend 和 CoarseGrainedSchedulerBackend,没有 Local\n            Some(new ExecutorAllocationManager(\n              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,\n              _env.blockManager.master))\n          case _ =>\n            None\n        }\n      } else {\n        None\n      }\n    _executorAllocationManager.foreach(_.start())\n```\n\nExecutorAllocationManager中的关键成员变量,主要用于申请/注销 executor 时的信息维护\n\n```\n[ExecutorAllocationManager.scala]\n  //在下次申请 executor 时的滚动增量\t\n  private var numExecutorsToAdd = 1\n  //当前已持有的 executor 数量.如果运行中所有的 executor 都挂了,该值正是 application 向 clustermanager 申请的 executor 数量,而非再从 initExecutorNum 和 numExecutorToAdd 滚动增加至挂掉前状态\n  private var numExecutorsTarget = initialNumExecutors\n  //已经请求释放但是还没被释放的 executor 列表\n  private val executorsPendingToRemove = new mutable.HashSet[String]\n  //application 所有的 executor\n  private val executorIds = new mutable.HashSet[String]\n  // A timestamp of when an addition should be triggered, or NOT_SET if it is not set\n  // This is set when pending tasks are added but not scheduled yet\n  private var addTime: Long = NOT_SET\n   //<executorId,最大存活时间>信息.在 executor 第一次注册或不再运行任务时设置\n  private val removeTimes = new mutable.HashMap[String, Long]\n  //是否是 application初始化时期.为 true 时,不会申请executor;当开始执行任务后或者executor 已经过了超时时间 时为 false\n  @volatile private var initializing: Boolean = true\n  //用于 executor 本地化计算分配,调用 clustermanager 接口的必须参数\n  private var localityAwareTasks = 0\n  //用于 executor 本地化计算分配,调用cm 接口的必须参数\n  private var hostToLocalTaskCount: Map[String, Int] = Map.empty\n```\n\n周期性检测 application 的工作负载,动态调节executor 的数量\n\n```\nExecutorAllocationManager.scala\n def start(): Unit = {\n    listenerBus.addToManagementQueue(listener)\n    val scheduleTask = new Runnable() {\n      override def run(): Unit = {\n             ...\n             schedule()\n             ...\n      \t}\n    }\n    //定时调度,间隔100ms\n    executor.scheduleWithFixedDelay(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)\n    //先向集群申请初始化数量的 executor. numExecutorTarget 初始值是 max(spark.dynamicAllocation.minExecutors,spark.dynamicAllocation.initialExecutors,spark.executor.instances),其他两个参数是 0 和 Map.Empty\n    client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n  }\n  \n```\nschedule 方法的逻辑分为两块,1.检测是否要申请 executor 以及申请 2.检测是否要释放 executor 以及释放\n\n```\nprivate def schedule(): Unit = synchronized {\n    val now = clock.getTimeMillis\n    //申请\n    updateAndSyncNumExecutorsTarget(now)\n    //待释放列表\n    val executorIdsToBeRemoved = ArrayBuffer[String]()\n    //保留未超时的 executor,将超时的 executor 加入到待释放列表\n    removeTimes.retain { case (executorId, expireTime) =>\n      val expired = now >= expireTime\n      if (expired) {\n        initializing = false\n        executorIdsToBeRemoved += executorId\n      }\n      !expired\n    }\n    //释放\n    if (executorIdsToBeRemoved.nonEmpty) {\n      removeExecutors(executorIdsToBeRemoved)\n    }\n  }\n```\n先看释放 executor 的逻辑,比较简单\n\n```\n  private def removeExecutors(executors: Seq[String]): Seq[String] = synchronized {\n    val executorIdsToBeRemoved = new ArrayBuffer[String]\n    logInfo(\"Request to remove executorIds: \" + executors.mkString(\", \"))\n    //当前存活的节点数\n    val numExistingExecutors = allocationManager.executorIds.size - executorsPendingToRemove.size\n    var newExecutorTotal = numExistingExecutors\n    executors.foreach { executorIdToBeRemoved =>\n    //如果删除该节点,导致存活节点数小于动态资源下界或者小于当前需要的资源数,则不删除\n      if (newExecutorTotal - 1 < minNumExecutors) {\n        logDebug(s\"Not removing idle executor $executorIdToBeRemoved because there are only \" +\n          s\"$newExecutorTotal executor(s) left (minimum number of executor limit $minNumExecutors)\")\n      } else if (newExecutorTotal - 1 < numExecutorsTarget) {\n        logDebug(s\"Not removing idle executor $executorIdToBeRemoved because there are only \" +\n          s\"$newExecutorTotal executor(s) left (number of executor target $numExecutorsTarget)\")\n      } else if (canBeKilled(executorIdToBeRemoved)) {\n        executorIdsToBeRemoved += executorIdToBeRemoved\n        newExecutorTotal -= 1\n      }\n    }\n    if (executorIdsToBeRemoved.isEmpty) {\n      return Seq.empty[String]\n    }\n    val executorsRemoved = if (testing) {\n      executorIdsToBeRemoved\n    } else {\n        client.killExecutors(executorIdsToBeRemoved, adjustTargetNumExecutors = false,\n        countFailures = false, force = false)\n    }\n     client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n    newExecutorTotal = numExistingExecutors\n    if (testing || executorsRemoved.nonEmpty) {\n      executorsRemoved.foreach { removedExecutorId =>\n        newExecutorTotal -= 1\n        logInfo(s\"Removing executor $removedExecutorId because it has been idle for \" +\n          s\"$executorIdleTimeoutS seconds (new desired total will be $newExecutorTotal)\")\n        executorsPendingToRemove.add(removedExecutorId)\n      }\n      executorsRemoved\n    } else {\n      logWarning(s\"Unable to reach the cluster manager to kill executor/s \" +\n        s\"${executorIdsToBeRemoved.mkString(\",\")} or no executor eligible to kill!\")\n      Seq.empty[String]\n    }\n  }\n```\nkillExecutors 的逻辑在对接底层资源调度平台(Standalone,CoarseGrained)的实现类中,以CoarseGrainedSchedulerBackend为例\n\n```\n[CoarseGrainedSchedulerBackend.scala]\nfinal override def killExecutors(\n      executorIds: Seq[String],\n      adjustTargetNumExecutors: Boolean,\n      countFailures: Boolean,\n      force: Boolean): Seq[String] = {\n    val response = synchronized {\n      val (knownExecutors, unknownExecutors) = executorIds.partition(executorDataMap.contains)\n      unknownExecutors.foreach { id =>\n        logWarning(s\"Executor to kill $id does not exist!\")\n      }\n     //从待删除的列表中过滤掉确定被删除但还未被删除的和没有正在跑task 的 executor,作为此次请求 cm 释放的executor\n      val executorsToKill = knownExecutors\n        .filter { id => !executorsPendingToRemove.contains(id) }\n        .filter { id => force || !scheduler.isExecutorBusy(id) }\n      executorsToKill.foreach { id => executorsPendingToRemove(id) = !countFailures }\n      logInfo(s\"Actual list of executor(s) to be killed is ${executorsToKill.mkString(\", \")}\")\n      \n      // If we do not wish to replace the executors we kill, sync the target number of executors\n      // with the cluster manager to avoid allocating new ones. When computing the new target,\n      // take into account executors that are pending to be added or removed.\n      val adjustTotalExecutors =\n        if (adjustTargetNumExecutors) {\n          requestedTotalExecutors = math.max(requestedTotalExecutors - executorsToKill.size, 0)\n          ..\n          doRequestTotalExecutors(requestedTotalExecutors)\n        } else {\n          numPendingExecutors += knownExecutors.size\n          Future.successful(true)\n        }\n        \n      val killExecutors: Boolean => Future[Boolean] =\n        if (!executorsToKill.isEmpty) {\n          _ => doKillExecutors(executorsToKill)\n        } else {\n          _ => Future.successful(false)\n        }\n      val killResponse = adjustTotalExecutors.flatMap(killExecutors)(ThreadUtils.sameThread)\n      killResponse.flatMap(killSuccessful =>\n        Future.successful (if (killSuccessful) executorsToKill else Seq.empty[String])\n      )(ThreadUtils.sameThread)\n    }\n    defaultAskTimeout.awaitResult(response)\n  }\n   protected def doKillExecutors(executorIds: Seq[String]): Future[Boolean] =\n    Future.successful(false)\n```\ndoKillExecutor和doRequestTotalExecutors在 CoarseGrainedSchedulerBackend 中有默认实现,里面内容比较有迷惑性,其实逻辑在YarnSchedulerBackend中,封装了YarnClientSchedulerBackend 和 YarnClusterSchedulerBackend 实现类的通用逻辑.ResourceManager 端的逻辑不在此讨论\n\n```\n[YarnSchedulerBackend.scala]\n  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](KillExecutors(executorIds))\n  }\n  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))\n  \n```\n再看申请 executor 的逻辑,对 ExecutorAllocationManager#start#schedule#updateAndSyncNumExecutorsTarget(now)方法做解析\n\n```\nprivate def updateAndSyncNumExecutorsTarget(now: Long): Int = synchronized {\n    val maxNeeded = maxNumExecutorsNeeded\n    if (initializing) {\n      //application job 还没有开始执行第一个 stage,所以不需要申请额外的资源\n       0\n    } else if (maxNeeded < numExecutorsTarget) {\n      //如果当前计算出所需的资源小于已经拥有的资源,那么没有必要再申请新的,申请了也是空闲\n      //可以代入一些值做debug\n      val oldNumExecutorsTarget = numExecutorsTarget\n      numExecutorsTarget = math.max(maxNeeded, minNumExecutors)\n      numExecutorsToAdd = 1\n\n       //math.max(maxNeeded, minNumExecutors)与numExecutorsTarget且maxNeeded < numExecutorsTarget\n      if (numExecutorsTarget < oldNumExecutorsTarget) {\n        // We lower the target number of executors but don't actively kill any yet.  Killing is\n        // controlled separately by an idle timeout.  It's still *helpful* to reduce the target number\n        // in case an executor just happens to get lost (eg., bad hardware, or the cluster manager\n        // preempts it) -- in that case, there is no point in trying to immediately  get a new\n        // executor, since we wouldn't even use it yet.\n        //同步当前需要的 executor 数量给 cm\n        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n        logDebug(s\"Lowering target number of executors to $numExecutorsTarget (previously \" +\n          s\"$oldNumExecutorsTarget) because not all requested executors are actually needed\")\n      }\n      numExecutorsTarget - oldNumExecutorsTarget\n    } else if (addTime != NOT_SET && now >= addTime) {\n    //在 application 已经开始运行,申请过资源;且当前计算出来的需要的 executor 数量大于已有的 executor 数量;且任务已经被积压过(onSchedulerBacklogged事件会设置 add_time)那么会申请 executor\n      val delta = addExecutors(maxNeeded)\n      logDebug(s\"Starting timer to add more executors (to \" +\n        s\"expire in $sustainedSchedulerBacklogTimeoutS seconds)\")\n      addTime = now + (sustainedSchedulerBacklogTimeoutS * 1000)\n      delta\n    } else {\n      0\n    }\n  }\n  \n  //依据当前stage正在运行的task数量和堆积的task数量来预估需要的executor数量\n  //之所以是当前 stage,是因为onStageSubmitted和onStageCompleted方法会维护stageIdToNumTasks(hashmap),所以这个 map 里面存储的是DAG中无依赖关系的 stage-tasknum 数据\n   private def maxNumExecutorsNeeded(): Int = {\n    val numRunningOrPendingTasks = listener.totalPendingTasks + listener.totalRunningTasks\n    (numRunningOrPendingTasks + tasksPerExecutor - 1) / tasksPerExecutor\n  }\n//申请 executor.如果当前需要的资源数超过动态最大资源数,该次不申请并且将 numExecutorsToAdd(申请 executor 的滚动增量)设置为1,而不是乘以2.\n//numExecutorsToAdd:1->2->4->8->1(已持有的数量不小于动态最大数量,则没必要需要太快的增速)->2->4\nprivate def addExecutors(maxNumExecutorsNeeded: Int): Int = {\n    // Do not request more executors if it would put our target over the upper bound\n    if (numExecutorsTarget >= maxNumExecutors) {\n      logDebug(s\"Not adding executors because our current target total \" +\n        s\"is already $numExecutorsTarget (limit $maxNumExecutors)\")\n      numExecutorsToAdd = 1\n      return 0\n    }\n\t\n    val oldNumExecutorsTarget = numExecutorsTarget\n    // There's no point in wasting time ramping up to the number of executors we already have, so\n    // make sure our target is at least as much as our current allocation:\n    numExecutorsTarget = math.max(numExecutorsTarget, executorIds.size)\n    // Boost our target with the number to add for this round:   滚动增量\n    numExecutorsTarget += numExecutorsToAdd\n    // Ensure that our target doesn't exceed what we need at the present moment:\n    numExecutorsTarget = math.min(numExecutorsTarget, maxNumExecutorsNeeded)\n    // Ensure that our target fits within configured bounds:\n    numExecutorsTarget = math.max(math.min(numExecutorsTarget, maxNumExecutors), minNumExecutors)\n\n    val delta = numExecutorsTarget - oldNumExecutorsTarget\n    if (delta == 0) {\n      // Check if there is any speculative jobs pending\n      if (listener.pendingTasks == 0 && listener.pendingSpeculativeTasks > 0) {\n        numExecutorsTarget =\n          math.max(math.min(maxNumExecutorsNeeded + 1, maxNumExecutors), minNumExecutors)\n      } else {\n        numExecutorsToAdd = 1\n        return 0\n      }\n    }\n    val addRequestAcknowledged = try {\n      testing ||\n        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)\n    } catch {\n        ....\n        false\n    }\n    if (addRequestAcknowledged) {\n      val executorsString = \"executor\" + { if (delta > 1) \"s\" else \"\" }\n      logInfo(s\"Requesting $delta new $executorsString because tasks are backlogged\" +\n        s\" (new desired total will be $numExecutorsTarget)\")\n      numExecutorsToAdd = if (delta == numExecutorsToAdd) {\n        numExecutorsToAdd * 2\n      } else {\n        1\n      }\n      delta\n    } else {\n      logWarning(\n        s\"Unable to reach the cluster manager to request $numExecutorsTarget total executors!\")\n      numExecutorsTarget = oldNumExecutorsTarget\n      0\n    }\n  }\n```\n资源申请\n\n```\n[YarnSchedulerBackend.scala]\noverride def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] = {\n    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))\n }\n```\n\n## ExecutorAllocationListener\nExecutorAllocationListener监听器监听各类事件,触发相应的回调函数,对自身以及ExecutorAllocationManager 内维护的信息做修改,辅助 ExecutorAllocationManager 做动态资源调度.看一些回调函数的源码:\n\n```\n/**\n   * Callback invoked when the specified executor has been added.\n   */\nprivate def onExecutorAdded(executorId: String): Unit = synchronized {\n    if (!executorIds.contains(executorId)) {\n      executorIds.add(executorId)\n      // If an executor (call this executor X) is not removed because the lower bound\n      // has been reached, it will no longer be marked as idle. When new executors join,\n      // however, we are no longer at the lower bound, and so we must mark executor X\n      // as idle again so as not to forget that it is a candidate for removal. (see SPARK-4951)\n      executorIds.filter(listener.isExecutorIdle).foreach(onExecutorIdle)\n      logInfo(s\"New executor $executorId has registered (new total is ${executorIds.size})\")\n    } else {\n      logWarning(s\"Duplicate executor $executorId has registered\")\n    }\n  }\n  private def onExecutorRemoved(executorId: String): Unit = synchronized {\n    if (executorIds.contains(executorId)) {\n      executorIds.remove(executorId)\n      removeTimes.remove(executorId)\n      logInfo(s\"Existing executor $executorId has been removed (new total is ${executorIds.size})\")\n      if (executorsPendingToRemove.contains(executorId)) {\n        executorsPendingToRemove.remove(executorId)\n        logDebug(s\"Executor $executorId is no longer pending to \" +\n          s\"be removed (${executorsPendingToRemove.size} left)\")\n      }\n    } else {\n      logWarning(s\"Unknown executor $executorId has been removed!\")\n    }\n  }\n  private def onSchedulerBacklogged(): Unit = synchronized {\n    if (addTime == NOT_SET) {\n      logDebug(s\"Starting timer to add executors because pending tasks \" +\n        s\"are building up (to expire in $schedulerBacklogTimeoutS seconds)\")\n      addTime = clock.getTimeMillis + schedulerBacklogTimeoutS * 1000\n    }\n  }\n  private def onExecutorIdle(executorId: String): Unit = synchronized {\n    if (executorIds.contains(executorId)) {\n      if (!removeTimes.contains(executorId) && !executorsPendingToRemove.contains(executorId)) {\n        val hasCachedBlocks = blockManagerMaster.hasCachedBlocks(executorId)\n        val now = clock.getTimeMillis()\n        val timeout = {\n          if (hasCachedBlocks) {\t//缓存了数据,shuffle write 数据等\n            now + cachedExecutorIdleTimeoutS * 1000\n          } else {\n            now + executorIdleTimeoutS * 1000\n          }\n        }\n        val realTimeout = if (timeout <= 0) Long.MaxValue else timeout // overflow\n        removeTimes(executorId) = realTimeout\n        logDebug(s\"Starting idle timer for $executorId because there are no more tasks \" +\n          s\"scheduled to run on the executor (to expire in ${(realTimeout - now)/1000} seconds)\")\n      }\n    } else {\n      logWarning(s\"Attempted to mark unknown executor $executorId idle\")\n    }\n  }\n  private def onExecutorBusy(executorId: String): Unit = synchronized {\n    logDebug(s\"Clearing idle timer for $executorId because it is now running a task\")\n    removeTimes.remove(executorId)\n  }\n```\n\n## 其他\n\n>\n动态资源调度设计的一点思考\n1. 如何增加资源  \n\t负载与当前资源的权衡;资源增加算法:线性增加,对数等;资源快速满足需求快速下降  \n2. 如何释放资源  \n\t如何判断空闲;资源上有不好处理的逻辑怎么办,数据或计算   \n3. 设计模式   \n\t事件驱动的设计模式  \n4. 解耦的重要性  \n\t曾经设计过一个主从式数据处理框架,但是在可拓展性上考虑的比较少.如果现在在那个系统上增加动态资源调度功能,在设计模式和解耦上比不上 spark 之万一...  \n\t\n## 参考\n\n http://spark.apache.org/docs/2.3.1/configuration.html   \n http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service  \n\n\n\n\n\n\n\n\n\n","slug":"2019-08-09-ExecutorAllocationManager","published":1,"updated":"2020-03-16T10:01:28.105Z","comments":1,"photos":[],"link":"","_id":"cke2rms470000y7c73cf7dhjf","content":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>Spark 为 Standalone 和 CoarseGrained(Yarn/Mesos)调度模式提供了动态资源(executor)分配功能,可以根据 Application 的工作负载动态调整其占用的资源.在多 Application 同时运行的情况下,可以提高资源利用率.原理是Spark 定时调度,利用正在运行和积压的任务数推算出需要的executor 数量并和已持有的数量对比,增加 executor;利用维护的节点空闲时间信息,释放掉空闲的 executor.</p>\n<a id=\"more\"></a>\n\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><pre><code>在SparkOnYarn任务执行的时候,发现了下面几条日志:\nExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)\nExecutorAllocationManager: Removing executor 6 because it has been idle for 180 seconds (new desired total will be 5)\n上述日志没有时间先后及因果关系,发现ExecutorAllocationManager 会依据积压的任务申请新的 executor,并在 executor 空闲一段时间后释放 executor.</code></pre><h2 id=\"开启方法\"><a href=\"#开启方法\" class=\"headerlink\" title=\"开启方法\"></a>开启方法</h2><h3 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a>Cluster</h3><pre><code>仅以资源调度平台最通用的 yarn 举例\n   1.spark 源代码编译时需支持 yarn 特性  \n   2.将 $SPARK_HOME/common/network-yarn/target/scala-&lt;version&gt;/spark-&lt;version&gt;-yarn-shuffle.jar移动到各 NodeManager 的 classpath 下  \n   3.各节点的yarn-site.xml 配置文件中,设置 yarn.nodemanager.aux-services为spark_shuffle,yarn.nodemanager.aux-services.spark_shuffle.class为org.apache.spark.network.yarn.YarnShuffleService\n   4.etc/hadoop/yarn-env.sh文件中,增加NodeManager 的堆内存YARN_HEAPSIZE,以减少 shuffle 期间的 GC 频率\n   5.重启所有的 NodeManager</code></pre><h3 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h3><pre><code>必须把下面两个参数默认都是 false,需要设置为 true\nspark.dynamicAllocation.enabled true        开启动态资源分配机制\nspark.shuffle.service.enabled true      使用额外的 shuffle service 服务,可以使 executor 被移除时,不会删除该 executor写入的 shuffle数据.\nspark.shuffle.service.port 7337   额外的 shuffle service 服务所占用的端口</code></pre><h3 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h3><p>spark.dynamicAllocation.enabled true<br>spark.shuffle.service.enabled true<br>spark.dynamicAllocation.minExecutors 2        动态数量下界<br>spark.dynamicAllocation.maxExecutors 100     动态数量上界<br>spark.dynamicAllocation.initialExecutors 2     executor初始化时申请的数量<br>spark.dynamicAllocation.schedulerBacklogTimeout 60s  被积压的任务等待时间超过此值时,触发一次executor申请<br>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 60s 和spark.dynamicAllocation.schedulerBacklogTimeout值相同,用于后续的executor申请<br>spark.dynamicAllocation.executorIdleTimeout 180s     executor在空闲了该时间之后,释放该 executor<br>spark.dynamicAllocation.cachedExecutorIdleTimeout infinity  缓存了数据的 executor 在空闲了该时间之后,释放该 executor.一般不修改即不删除   </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>ExecutorAllocationManager 被调用的入口是 SparkContext</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[SparkContext.scala]</span><br><span class=\"line\">    val dynamicAllocationEnabled &#x3D; Utils.isDynamicAllocationEnabled(_conf) &#x2F;&#x2F;判断 spark.dynamicAllocation.enable 参数</span><br><span class=\"line\">    _executorAllocationManager &#x3D;</span><br><span class=\"line\">      if (dynamicAllocationEnabled) &#123;</span><br><span class=\"line\">        schedulerBackend match &#123;</span><br><span class=\"line\">          case b: ExecutorAllocationClient &#x3D;&gt;\t&#x2F;&#x2F;子类是 StandaloneSchdulerBackend 和 CoarseGrainedSchedulerBackend,没有 Local</span><br><span class=\"line\">            Some(new ExecutorAllocationManager(</span><br><span class=\"line\">              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,</span><br><span class=\"line\">              _env.blockManager.master))</span><br><span class=\"line\">          case _ &#x3D;&gt;</span><br><span class=\"line\">            None</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        None</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    _executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></figure>\n\n<p>ExecutorAllocationManager中的关键成员变量,主要用于申请/注销 executor 时的信息维护</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ExecutorAllocationManager.scala]</span><br><span class=\"line\">  &#x2F;&#x2F;在下次申请 executor 时的滚动增量\t</span><br><span class=\"line\">  private var numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">  &#x2F;&#x2F;当前已持有的 executor 数量.如果运行中所有的 executor 都挂了,该值正是 application 向 clustermanager 申请的 executor 数量,而非再从 initExecutorNum 和 numExecutorToAdd 滚动增加至挂掉前状态</span><br><span class=\"line\">  private var numExecutorsTarget &#x3D; initialNumExecutors</span><br><span class=\"line\">  &#x2F;&#x2F;已经请求释放但是还没被释放的 executor 列表</span><br><span class=\"line\">  private val executorsPendingToRemove &#x3D; new mutable.HashSet[String]</span><br><span class=\"line\">  &#x2F;&#x2F;application 所有的 executor</span><br><span class=\"line\">  private val executorIds &#x3D; new mutable.HashSet[String]</span><br><span class=\"line\">  &#x2F;&#x2F; A timestamp of when an addition should be triggered, or NOT_SET if it is not set</span><br><span class=\"line\">  &#x2F;&#x2F; This is set when pending tasks are added but not scheduled yet</span><br><span class=\"line\">  private var addTime: Long &#x3D; NOT_SET</span><br><span class=\"line\">   &#x2F;&#x2F;&lt;executorId,最大存活时间&gt;信息.在 executor 第一次注册或不再运行任务时设置</span><br><span class=\"line\">  private val removeTimes &#x3D; new mutable.HashMap[String, Long]</span><br><span class=\"line\">  &#x2F;&#x2F;是否是 application初始化时期.为 true 时,不会申请executor;当开始执行任务后或者executor 已经过了超时时间 时为 false</span><br><span class=\"line\">  @volatile private var initializing: Boolean &#x3D; true</span><br><span class=\"line\">  &#x2F;&#x2F;用于 executor 本地化计算分配,调用 clustermanager 接口的必须参数</span><br><span class=\"line\">  private var localityAwareTasks &#x3D; 0</span><br><span class=\"line\">  &#x2F;&#x2F;用于 executor 本地化计算分配,调用cm 接口的必须参数</span><br><span class=\"line\">  private var hostToLocalTaskCount: Map[String, Int] &#x3D; Map.empty</span><br></pre></td></tr></table></figure>\n\n<p>周期性检测 application 的工作负载,动态调节executor 的数量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorAllocationManager.scala</span><br><span class=\"line\"> def start(): Unit &#x3D; &#123;</span><br><span class=\"line\">    listenerBus.addToManagementQueue(listener)</span><br><span class=\"line\">    val scheduleTask &#x3D; new Runnable() &#123;</span><br><span class=\"line\">      override def run(): Unit &#x3D; &#123;</span><br><span class=\"line\">             ...</span><br><span class=\"line\">             schedule()</span><br><span class=\"line\">             ...</span><br><span class=\"line\">      \t&#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;定时调度,间隔100ms</span><br><span class=\"line\">    executor.scheduleWithFixedDelay(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)</span><br><span class=\"line\">    &#x2F;&#x2F;先向集群申请初始化数量的 executor. numExecutorTarget 初始值是 max(spark.dynamicAllocation.minExecutors,spark.dynamicAllocation.initialExecutors,spark.executor.instances),其他两个参数是 0 和 Map.Empty</span><br><span class=\"line\">    client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>schedule 方法的逻辑分为两块,1.检测是否要申请 executor 以及申请 2.检测是否要释放 executor 以及释放</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def schedule(): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    val now &#x3D; clock.getTimeMillis</span><br><span class=\"line\">    &#x2F;&#x2F;申请</span><br><span class=\"line\">    updateAndSyncNumExecutorsTarget(now)</span><br><span class=\"line\">    &#x2F;&#x2F;待释放列表</span><br><span class=\"line\">    val executorIdsToBeRemoved &#x3D; ArrayBuffer[String]()</span><br><span class=\"line\">    &#x2F;&#x2F;保留未超时的 executor,将超时的 executor 加入到待释放列表</span><br><span class=\"line\">    removeTimes.retain &#123; case (executorId, expireTime) &#x3D;&gt;</span><br><span class=\"line\">      val expired &#x3D; now &gt;&#x3D; expireTime</span><br><span class=\"line\">      if (expired) &#123;</span><br><span class=\"line\">        initializing &#x3D; false</span><br><span class=\"line\">        executorIdsToBeRemoved +&#x3D; executorId</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      !expired</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;释放</span><br><span class=\"line\">    if (executorIdsToBeRemoved.nonEmpty) &#123;</span><br><span class=\"line\">      removeExecutors(executorIdsToBeRemoved)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看释放 executor 的逻辑,比较简单</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def removeExecutors(executors: Seq[String]): Seq[String] &#x3D; synchronized &#123;</span><br><span class=\"line\">  val executorIdsToBeRemoved &#x3D; new ArrayBuffer[String]</span><br><span class=\"line\">  logInfo(&quot;Request to remove executorIds: &quot; + executors.mkString(&quot;, &quot;))</span><br><span class=\"line\">  &#x2F;&#x2F;当前存活的节点数</span><br><span class=\"line\">  val numExistingExecutors &#x3D; allocationManager.executorIds.size - executorsPendingToRemove.size</span><br><span class=\"line\">  var newExecutorTotal &#x3D; numExistingExecutors</span><br><span class=\"line\">  executors.foreach &#123; executorIdToBeRemoved &#x3D;&gt;</span><br><span class=\"line\">  &#x2F;&#x2F;如果删除该节点,导致存活节点数小于动态资源下界或者小于当前需要的资源数,则不删除</span><br><span class=\"line\">    if (newExecutorTotal - 1 &lt; minNumExecutors) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not removing idle executor $executorIdToBeRemoved because there are only &quot; +</span><br><span class=\"line\">        s&quot;$newExecutorTotal executor(s) left (minimum number of executor limit $minNumExecutors)&quot;)</span><br><span class=\"line\">    &#125; else if (newExecutorTotal - 1 &lt; numExecutorsTarget) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not removing idle executor $executorIdToBeRemoved because there are only &quot; +</span><br><span class=\"line\">        s&quot;$newExecutorTotal executor(s) left (number of executor target $numExecutorsTarget)&quot;)</span><br><span class=\"line\">    &#125; else if (canBeKilled(executorIdToBeRemoved)) &#123;</span><br><span class=\"line\">      executorIdsToBeRemoved +&#x3D; executorIdToBeRemoved</span><br><span class=\"line\">      newExecutorTotal -&#x3D; 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if (executorIdsToBeRemoved.isEmpty) &#123;</span><br><span class=\"line\">    return Seq.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  val executorsRemoved &#x3D; if (testing) &#123;</span><br><span class=\"line\">    executorIdsToBeRemoved</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">      client.killExecutors(executorIdsToBeRemoved, adjustTargetNumExecutors &#x3D; false,</span><br><span class=\"line\">      countFailures &#x3D; false, force &#x3D; false)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">  newExecutorTotal &#x3D; numExistingExecutors</span><br><span class=\"line\">  if (testing || executorsRemoved.nonEmpty) &#123;</span><br><span class=\"line\">    executorsRemoved.foreach &#123; removedExecutorId &#x3D;&gt;</span><br><span class=\"line\">      newExecutorTotal -&#x3D; 1</span><br><span class=\"line\">      logInfo(s&quot;Removing executor $removedExecutorId because it has been idle for &quot; +</span><br><span class=\"line\">        s&quot;$executorIdleTimeoutS seconds (new desired total will be $newExecutorTotal)&quot;)</span><br><span class=\"line\">      executorsPendingToRemove.add(removedExecutorId)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    executorsRemoved</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    logWarning(s&quot;Unable to reach the cluster manager to kill executor&#x2F;s &quot; +</span><br><span class=\"line\">      s&quot;$&#123;executorIdsToBeRemoved.mkString(&quot;,&quot;)&#125; or no executor eligible to kill!&quot;)</span><br><span class=\"line\">    Seq.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>killExecutors 的逻辑在对接底层资源调度平台(Standalone,CoarseGrained)的实现类中,以CoarseGrainedSchedulerBackend为例</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[CoarseGrainedSchedulerBackend.scala]</span><br><span class=\"line\">final override def killExecutors(</span><br><span class=\"line\">      executorIds: Seq[String],</span><br><span class=\"line\">      adjustTargetNumExecutors: Boolean,</span><br><span class=\"line\">      countFailures: Boolean,</span><br><span class=\"line\">      force: Boolean): Seq[String] &#x3D; &#123;</span><br><span class=\"line\">    val response &#x3D; synchronized &#123;</span><br><span class=\"line\">      val (knownExecutors, unknownExecutors) &#x3D; executorIds.partition(executorDataMap.contains)</span><br><span class=\"line\">      unknownExecutors.foreach &#123; id &#x3D;&gt;</span><br><span class=\"line\">        logWarning(s&quot;Executor to kill $id does not exist!&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">     &#x2F;&#x2F;从待删除的列表中过滤掉确定被删除但还未被删除的和没有正在跑task 的 executor,作为此次请求 cm 释放的executor</span><br><span class=\"line\">      val executorsToKill &#x3D; knownExecutors</span><br><span class=\"line\">        .filter &#123; id &#x3D;&gt; !executorsPendingToRemove.contains(id) &#125;</span><br><span class=\"line\">        .filter &#123; id &#x3D;&gt; force || !scheduler.isExecutorBusy(id) &#125;</span><br><span class=\"line\">      executorsToKill.foreach &#123; id &#x3D;&gt; executorsPendingToRemove(id) &#x3D; !countFailures &#125;</span><br><span class=\"line\">      logInfo(s&quot;Actual list of executor(s) to be killed is $&#123;executorsToKill.mkString(&quot;, &quot;)&#125;&quot;)</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; If we do not wish to replace the executors we kill, sync the target number of executors</span><br><span class=\"line\">      &#x2F;&#x2F; with the cluster manager to avoid allocating new ones. When computing the new target,</span><br><span class=\"line\">      &#x2F;&#x2F; take into account executors that are pending to be added or removed.</span><br><span class=\"line\">      val adjustTotalExecutors &#x3D;</span><br><span class=\"line\">        if (adjustTargetNumExecutors) &#123;</span><br><span class=\"line\">          requestedTotalExecutors &#x3D; math.max(requestedTotalExecutors - executorsToKill.size, 0)</span><br><span class=\"line\">          ..</span><br><span class=\"line\">          doRequestTotalExecutors(requestedTotalExecutors)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          numPendingExecutors +&#x3D; knownExecutors.size</span><br><span class=\"line\">          Future.successful(true)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">      val killExecutors: Boolean &#x3D;&gt; Future[Boolean] &#x3D;</span><br><span class=\"line\">        if (!executorsToKill.isEmpty) &#123;</span><br><span class=\"line\">          _ &#x3D;&gt; doKillExecutors(executorsToKill)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          _ &#x3D;&gt; Future.successful(false)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      val killResponse &#x3D; adjustTotalExecutors.flatMap(killExecutors)(ThreadUtils.sameThread)</span><br><span class=\"line\">      killResponse.flatMap(killSuccessful &#x3D;&gt;</span><br><span class=\"line\">        Future.successful (if (killSuccessful) executorsToKill else Seq.empty[String])</span><br><span class=\"line\">      )(ThreadUtils.sameThread)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    defaultAskTimeout.awaitResult(response)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   protected def doKillExecutors(executorIds: Seq[String]): Future[Boolean] &#x3D;</span><br><span class=\"line\">    Future.successful(false)</span><br></pre></td></tr></table></figure>\n<p>doKillExecutor和doRequestTotalExecutors在 CoarseGrainedSchedulerBackend 中有默认实现,里面内容比较有迷惑性,其实逻辑在YarnSchedulerBackend中,封装了YarnClientSchedulerBackend 和 YarnClusterSchedulerBackend 实现类的通用逻辑.ResourceManager 端的逻辑不在此讨论</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[YarnSchedulerBackend.scala]</span><br><span class=\"line\">  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](KillExecutors(executorIds))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))</span><br></pre></td></tr></table></figure>\n<p>再看申请 executor 的逻辑,对 ExecutorAllocationManager#start#schedule#updateAndSyncNumExecutorsTarget(now)方法做解析</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def updateAndSyncNumExecutorsTarget(now: Long): Int &#x3D; synchronized &#123;</span><br><span class=\"line\">    val maxNeeded &#x3D; maxNumExecutorsNeeded</span><br><span class=\"line\">    if (initializing) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;application job 还没有开始执行第一个 stage,所以不需要申请额外的资源</span><br><span class=\"line\">       0</span><br><span class=\"line\">    &#125; else if (maxNeeded &lt; numExecutorsTarget) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;如果当前计算出所需的资源小于已经拥有的资源,那么没有必要再申请新的,申请了也是空闲</span><br><span class=\"line\">      &#x2F;&#x2F;可以代入一些值做debug</span><br><span class=\"line\">      val oldNumExecutorsTarget &#x3D; numExecutorsTarget</span><br><span class=\"line\">      numExecutorsTarget &#x3D; math.max(maxNeeded, minNumExecutors)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\"></span><br><span class=\"line\">       &#x2F;&#x2F;math.max(maxNeeded, minNumExecutors)与numExecutorsTarget且maxNeeded &lt; numExecutorsTarget</span><br><span class=\"line\">      if (numExecutorsTarget &lt; oldNumExecutorsTarget) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; We lower the target number of executors but don&#39;t actively kill any yet.  Killing is</span><br><span class=\"line\">        &#x2F;&#x2F; controlled separately by an idle timeout.  It&#39;s still *helpful* to reduce the target number</span><br><span class=\"line\">        &#x2F;&#x2F; in case an executor just happens to get lost (eg., bad hardware, or the cluster manager</span><br><span class=\"line\">        &#x2F;&#x2F; preempts it) -- in that case, there is no point in trying to immediately  get a new</span><br><span class=\"line\">        &#x2F;&#x2F; executor, since we wouldn&#39;t even use it yet.</span><br><span class=\"line\">        &#x2F;&#x2F;同步当前需要的 executor 数量给 cm</span><br><span class=\"line\">        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">        logDebug(s&quot;Lowering target number of executors to $numExecutorsTarget (previously &quot; +</span><br><span class=\"line\">          s&quot;$oldNumExecutorsTarget) because not all requested executors are actually needed&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      numExecutorsTarget - oldNumExecutorsTarget</span><br><span class=\"line\">    &#125; else if (addTime !&#x3D; NOT_SET &amp;&amp; now &gt;&#x3D; addTime) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;在 application 已经开始运行,申请过资源;且当前计算出来的需要的 executor 数量大于已有的 executor 数量;且任务已经被积压过(onSchedulerBacklogged事件会设置 add_time)那么会申请 executor</span><br><span class=\"line\">      val delta &#x3D; addExecutors(maxNeeded)</span><br><span class=\"line\">      logDebug(s&quot;Starting timer to add more executors (to &quot; +</span><br><span class=\"line\">        s&quot;expire in $sustainedSchedulerBacklogTimeoutS seconds)&quot;)</span><br><span class=\"line\">      addTime &#x3D; now + (sustainedSchedulerBacklogTimeoutS * 1000)</span><br><span class=\"line\">      delta</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;依据当前stage正在运行的task数量和堆积的task数量来预估需要的executor数量</span><br><span class=\"line\">  &#x2F;&#x2F;之所以是当前 stage,是因为onStageSubmitted和onStageCompleted方法会维护stageIdToNumTasks(hashmap),所以这个 map 里面存储的是DAG中无依赖关系的 stage-tasknum 数据</span><br><span class=\"line\">   private def maxNumExecutorsNeeded(): Int &#x3D; &#123;</span><br><span class=\"line\">    val numRunningOrPendingTasks &#x3D; listener.totalPendingTasks + listener.totalRunningTasks</span><br><span class=\"line\">    (numRunningOrPendingTasks + tasksPerExecutor - 1) &#x2F; tasksPerExecutor</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#x2F;&#x2F;申请 executor.如果当前需要的资源数超过动态最大资源数,该次不申请并且将 numExecutorsToAdd(申请 executor 的滚动增量)设置为1,而不是乘以2.</span><br><span class=\"line\">&#x2F;&#x2F;numExecutorsToAdd:1-&gt;2-&gt;4-&gt;8-&gt;1(已持有的数量不小于动态最大数量,则没必要需要太快的增速)-&gt;2-&gt;4</span><br><span class=\"line\">private def addExecutors(maxNumExecutorsNeeded: Int): Int &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Do not request more executors if it would put our target over the upper bound</span><br><span class=\"line\">    if (numExecutorsTarget &gt;&#x3D; maxNumExecutors) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not adding executors because our current target total &quot; +</span><br><span class=\"line\">        s&quot;is already $numExecutorsTarget (limit $maxNumExecutors)&quot;)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">      return 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    val oldNumExecutorsTarget &#x3D; numExecutorsTarget</span><br><span class=\"line\">    &#x2F;&#x2F; There&#39;s no point in wasting time ramping up to the number of executors we already have, so</span><br><span class=\"line\">    &#x2F;&#x2F; make sure our target is at least as much as our current allocation:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.max(numExecutorsTarget, executorIds.size)</span><br><span class=\"line\">    &#x2F;&#x2F; Boost our target with the number to add for this round:   滚动增量</span><br><span class=\"line\">    numExecutorsTarget +&#x3D; numExecutorsToAdd</span><br><span class=\"line\">    &#x2F;&#x2F; Ensure that our target doesn&#39;t exceed what we need at the present moment:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.min(numExecutorsTarget, maxNumExecutorsNeeded)</span><br><span class=\"line\">    &#x2F;&#x2F; Ensure that our target fits within configured bounds:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.max(math.min(numExecutorsTarget, maxNumExecutors), minNumExecutors)</span><br><span class=\"line\"></span><br><span class=\"line\">    val delta &#x3D; numExecutorsTarget - oldNumExecutorsTarget</span><br><span class=\"line\">    if (delta &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; Check if there is any speculative jobs pending</span><br><span class=\"line\">      if (listener.pendingTasks &#x3D;&#x3D; 0 &amp;&amp; listener.pendingSpeculativeTasks &gt; 0) &#123;</span><br><span class=\"line\">        numExecutorsTarget &#x3D;</span><br><span class=\"line\">          math.max(math.min(maxNumExecutorsNeeded + 1, maxNumExecutors), minNumExecutors)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">        return 0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val addRequestAcknowledged &#x3D; try &#123;</span><br><span class=\"line\">      testing ||</span><br><span class=\"line\">        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">        ....</span><br><span class=\"line\">        false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (addRequestAcknowledged) &#123;</span><br><span class=\"line\">      val executorsString &#x3D; &quot;executor&quot; + &#123; if (delta &gt; 1) &quot;s&quot; else &quot;&quot; &#125;</span><br><span class=\"line\">      logInfo(s&quot;Requesting $delta new $executorsString because tasks are backlogged&quot; +</span><br><span class=\"line\">        s&quot; (new desired total will be $numExecutorsTarget)&quot;)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; if (delta &#x3D;&#x3D; numExecutorsToAdd) &#123;</span><br><span class=\"line\">        numExecutorsToAdd * 2</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      delta</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(</span><br><span class=\"line\">        s&quot;Unable to reach the cluster manager to request $numExecutorsTarget total executors!&quot;)</span><br><span class=\"line\">      numExecutorsTarget &#x3D; oldNumExecutorsTarget</span><br><span class=\"line\">      0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>资源申请</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[YarnSchedulerBackend.scala]</span><br><span class=\"line\">override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ExecutorAllocationListener\"><a href=\"#ExecutorAllocationListener\" class=\"headerlink\" title=\"ExecutorAllocationListener\"></a>ExecutorAllocationListener</h2><p>ExecutorAllocationListener监听器监听各类事件,触发相应的回调函数,对自身以及ExecutorAllocationManager 内维护的信息做修改,辅助 ExecutorAllocationManager 做动态资源调度.看一些回调函数的源码:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\">   * Callback invoked when the specified executor has been added.</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">private def onExecutorAdded(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (!executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      executorIds.add(executorId)</span><br><span class=\"line\">      &#x2F;&#x2F; If an executor (call this executor X) is not removed because the lower bound</span><br><span class=\"line\">      &#x2F;&#x2F; has been reached, it will no longer be marked as idle. When new executors join,</span><br><span class=\"line\">      &#x2F;&#x2F; however, we are no longer at the lower bound, and so we must mark executor X</span><br><span class=\"line\">      &#x2F;&#x2F; as idle again so as not to forget that it is a candidate for removal. (see SPARK-4951)</span><br><span class=\"line\">      executorIds.filter(listener.isExecutorIdle).foreach(onExecutorIdle)</span><br><span class=\"line\">      logInfo(s&quot;New executor $executorId has registered (new total is $&#123;executorIds.size&#125;)&quot;)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Duplicate executor $executorId has registered&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorRemoved(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      executorIds.remove(executorId)</span><br><span class=\"line\">      removeTimes.remove(executorId)</span><br><span class=\"line\">      logInfo(s&quot;Existing executor $executorId has been removed (new total is $&#123;executorIds.size&#125;)&quot;)</span><br><span class=\"line\">      if (executorsPendingToRemove.contains(executorId)) &#123;</span><br><span class=\"line\">        executorsPendingToRemove.remove(executorId)</span><br><span class=\"line\">        logDebug(s&quot;Executor $executorId is no longer pending to &quot; +</span><br><span class=\"line\">          s&quot;be removed ($&#123;executorsPendingToRemove.size&#125; left)&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Unknown executor $executorId has been removed!&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onSchedulerBacklogged(): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (addTime &#x3D;&#x3D; NOT_SET) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Starting timer to add executors because pending tasks &quot; +</span><br><span class=\"line\">        s&quot;are building up (to expire in $schedulerBacklogTimeoutS seconds)&quot;)</span><br><span class=\"line\">      addTime &#x3D; clock.getTimeMillis + schedulerBacklogTimeoutS * 1000</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorIdle(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      if (!removeTimes.contains(executorId) &amp;&amp; !executorsPendingToRemove.contains(executorId)) &#123;</span><br><span class=\"line\">        val hasCachedBlocks &#x3D; blockManagerMaster.hasCachedBlocks(executorId)</span><br><span class=\"line\">        val now &#x3D; clock.getTimeMillis()</span><br><span class=\"line\">        val timeout &#x3D; &#123;</span><br><span class=\"line\">          if (hasCachedBlocks) &#123;\t&#x2F;&#x2F;缓存了数据,shuffle write 数据等</span><br><span class=\"line\">            now + cachedExecutorIdleTimeoutS * 1000</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            now + executorIdleTimeoutS * 1000</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        val realTimeout &#x3D; if (timeout &lt;&#x3D; 0) Long.MaxValue else timeout &#x2F;&#x2F; overflow</span><br><span class=\"line\">        removeTimes(executorId) &#x3D; realTimeout</span><br><span class=\"line\">        logDebug(s&quot;Starting idle timer for $executorId because there are no more tasks &quot; +</span><br><span class=\"line\">          s&quot;scheduled to run on the executor (to expire in $&#123;(realTimeout - now)&#x2F;1000&#125; seconds)&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Attempted to mark unknown executor $executorId idle&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorBusy(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    logDebug(s&quot;Clearing idle timer for $executorId because it is now running a task&quot;)</span><br><span class=\"line\">    removeTimes.remove(executorId)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><blockquote>\n</blockquote>\n<p>动态资源调度设计的一点思考</p>\n<ol>\n<li>如何增加资源<br> 负载与当前资源的权衡;资源增加算法:线性增加,对数等;资源快速满足需求快速下降  </li>\n<li>如何释放资源<br> 如何判断空闲;资源上有不好处理的逻辑怎么办,数据或计算   </li>\n<li>设计模式<br> 事件驱动的设计模式  </li>\n<li>解耦的重要性<br> 曾经设计过一个主从式数据处理框架,但是在可拓展性上考虑的比较少.如果现在在那个系统上增加动态资源调度功能,在设计模式和解耦上比不上 spark 之万一…  </li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p> <a href=\"http://spark.apache.org/docs/2.3.1/configuration.html\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/2.3.1/configuration.html</a><br> <a href=\"http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service</a>  </p>\n","site":{"data":{}},"excerpt":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>Spark 为 Standalone 和 CoarseGrained(Yarn/Mesos)调度模式提供了动态资源(executor)分配功能,可以根据 Application 的工作负载动态调整其占用的资源.在多 Application 同时运行的情况下,可以提高资源利用率.原理是Spark 定时调度,利用正在运行和积压的任务数推算出需要的executor 数量并和已持有的数量对比,增加 executor;利用维护的节点空闲时间信息,释放掉空闲的 executor.</p>","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><pre><code>在SparkOnYarn任务执行的时候,发现了下面几条日志:\nExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)\nExecutorAllocationManager: Removing executor 6 because it has been idle for 180 seconds (new desired total will be 5)\n上述日志没有时间先后及因果关系,发现ExecutorAllocationManager 会依据积压的任务申请新的 executor,并在 executor 空闲一段时间后释放 executor.</code></pre><h2 id=\"开启方法\"><a href=\"#开启方法\" class=\"headerlink\" title=\"开启方法\"></a>开启方法</h2><h3 id=\"Cluster\"><a href=\"#Cluster\" class=\"headerlink\" title=\"Cluster\"></a>Cluster</h3><pre><code>仅以资源调度平台最通用的 yarn 举例\n   1.spark 源代码编译时需支持 yarn 特性  \n   2.将 $SPARK_HOME/common/network-yarn/target/scala-&lt;version&gt;/spark-&lt;version&gt;-yarn-shuffle.jar移动到各 NodeManager 的 classpath 下  \n   3.各节点的yarn-site.xml 配置文件中,设置 yarn.nodemanager.aux-services为spark_shuffle,yarn.nodemanager.aux-services.spark_shuffle.class为org.apache.spark.network.yarn.YarnShuffleService\n   4.etc/hadoop/yarn-env.sh文件中,增加NodeManager 的堆内存YARN_HEAPSIZE,以减少 shuffle 期间的 GC 频率\n   5.重启所有的 NodeManager</code></pre><h3 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h3><pre><code>必须把下面两个参数默认都是 false,需要设置为 true\nspark.dynamicAllocation.enabled true        开启动态资源分配机制\nspark.shuffle.service.enabled true      使用额外的 shuffle service 服务,可以使 executor 被移除时,不会删除该 executor写入的 shuffle数据.\nspark.shuffle.service.port 7337   额外的 shuffle service 服务所占用的端口</code></pre><h3 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h3><p>spark.dynamicAllocation.enabled true<br>spark.shuffle.service.enabled true<br>spark.dynamicAllocation.minExecutors 2        动态数量下界<br>spark.dynamicAllocation.maxExecutors 100     动态数量上界<br>spark.dynamicAllocation.initialExecutors 2     executor初始化时申请的数量<br>spark.dynamicAllocation.schedulerBacklogTimeout 60s  被积压的任务等待时间超过此值时,触发一次executor申请<br>spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 60s 和spark.dynamicAllocation.schedulerBacklogTimeout值相同,用于后续的executor申请<br>spark.dynamicAllocation.executorIdleTimeout 180s     executor在空闲了该时间之后,释放该 executor<br>spark.dynamicAllocation.cachedExecutorIdleTimeout infinity  缓存了数据的 executor 在空闲了该时间之后,释放该 executor.一般不修改即不删除   </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>ExecutorAllocationManager 被调用的入口是 SparkContext</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[SparkContext.scala]</span><br><span class=\"line\">    val dynamicAllocationEnabled &#x3D; Utils.isDynamicAllocationEnabled(_conf) &#x2F;&#x2F;判断 spark.dynamicAllocation.enable 参数</span><br><span class=\"line\">    _executorAllocationManager &#x3D;</span><br><span class=\"line\">      if (dynamicAllocationEnabled) &#123;</span><br><span class=\"line\">        schedulerBackend match &#123;</span><br><span class=\"line\">          case b: ExecutorAllocationClient &#x3D;&gt;\t&#x2F;&#x2F;子类是 StandaloneSchdulerBackend 和 CoarseGrainedSchedulerBackend,没有 Local</span><br><span class=\"line\">            Some(new ExecutorAllocationManager(</span><br><span class=\"line\">              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,</span><br><span class=\"line\">              _env.blockManager.master))</span><br><span class=\"line\">          case _ &#x3D;&gt;</span><br><span class=\"line\">            None</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        None</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    _executorAllocationManager.foreach(_.start())</span><br></pre></td></tr></table></figure>\n\n<p>ExecutorAllocationManager中的关键成员变量,主要用于申请/注销 executor 时的信息维护</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ExecutorAllocationManager.scala]</span><br><span class=\"line\">  &#x2F;&#x2F;在下次申请 executor 时的滚动增量\t</span><br><span class=\"line\">  private var numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">  &#x2F;&#x2F;当前已持有的 executor 数量.如果运行中所有的 executor 都挂了,该值正是 application 向 clustermanager 申请的 executor 数量,而非再从 initExecutorNum 和 numExecutorToAdd 滚动增加至挂掉前状态</span><br><span class=\"line\">  private var numExecutorsTarget &#x3D; initialNumExecutors</span><br><span class=\"line\">  &#x2F;&#x2F;已经请求释放但是还没被释放的 executor 列表</span><br><span class=\"line\">  private val executorsPendingToRemove &#x3D; new mutable.HashSet[String]</span><br><span class=\"line\">  &#x2F;&#x2F;application 所有的 executor</span><br><span class=\"line\">  private val executorIds &#x3D; new mutable.HashSet[String]</span><br><span class=\"line\">  &#x2F;&#x2F; A timestamp of when an addition should be triggered, or NOT_SET if it is not set</span><br><span class=\"line\">  &#x2F;&#x2F; This is set when pending tasks are added but not scheduled yet</span><br><span class=\"line\">  private var addTime: Long &#x3D; NOT_SET</span><br><span class=\"line\">   &#x2F;&#x2F;&lt;executorId,最大存活时间&gt;信息.在 executor 第一次注册或不再运行任务时设置</span><br><span class=\"line\">  private val removeTimes &#x3D; new mutable.HashMap[String, Long]</span><br><span class=\"line\">  &#x2F;&#x2F;是否是 application初始化时期.为 true 时,不会申请executor;当开始执行任务后或者executor 已经过了超时时间 时为 false</span><br><span class=\"line\">  @volatile private var initializing: Boolean &#x3D; true</span><br><span class=\"line\">  &#x2F;&#x2F;用于 executor 本地化计算分配,调用 clustermanager 接口的必须参数</span><br><span class=\"line\">  private var localityAwareTasks &#x3D; 0</span><br><span class=\"line\">  &#x2F;&#x2F;用于 executor 本地化计算分配,调用cm 接口的必须参数</span><br><span class=\"line\">  private var hostToLocalTaskCount: Map[String, Int] &#x3D; Map.empty</span><br></pre></td></tr></table></figure>\n\n<p>周期性检测 application 的工作负载,动态调节executor 的数量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorAllocationManager.scala</span><br><span class=\"line\"> def start(): Unit &#x3D; &#123;</span><br><span class=\"line\">    listenerBus.addToManagementQueue(listener)</span><br><span class=\"line\">    val scheduleTask &#x3D; new Runnable() &#123;</span><br><span class=\"line\">      override def run(): Unit &#x3D; &#123;</span><br><span class=\"line\">             ...</span><br><span class=\"line\">             schedule()</span><br><span class=\"line\">             ...</span><br><span class=\"line\">      \t&#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;定时调度,间隔100ms</span><br><span class=\"line\">    executor.scheduleWithFixedDelay(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)</span><br><span class=\"line\">    &#x2F;&#x2F;先向集群申请初始化数量的 executor. numExecutorTarget 初始值是 max(spark.dynamicAllocation.minExecutors,spark.dynamicAllocation.initialExecutors,spark.executor.instances),其他两个参数是 0 和 Map.Empty</span><br><span class=\"line\">    client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>schedule 方法的逻辑分为两块,1.检测是否要申请 executor 以及申请 2.检测是否要释放 executor 以及释放</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def schedule(): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    val now &#x3D; clock.getTimeMillis</span><br><span class=\"line\">    &#x2F;&#x2F;申请</span><br><span class=\"line\">    updateAndSyncNumExecutorsTarget(now)</span><br><span class=\"line\">    &#x2F;&#x2F;待释放列表</span><br><span class=\"line\">    val executorIdsToBeRemoved &#x3D; ArrayBuffer[String]()</span><br><span class=\"line\">    &#x2F;&#x2F;保留未超时的 executor,将超时的 executor 加入到待释放列表</span><br><span class=\"line\">    removeTimes.retain &#123; case (executorId, expireTime) &#x3D;&gt;</span><br><span class=\"line\">      val expired &#x3D; now &gt;&#x3D; expireTime</span><br><span class=\"line\">      if (expired) &#123;</span><br><span class=\"line\">        initializing &#x3D; false</span><br><span class=\"line\">        executorIdsToBeRemoved +&#x3D; executorId</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      !expired</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;释放</span><br><span class=\"line\">    if (executorIdsToBeRemoved.nonEmpty) &#123;</span><br><span class=\"line\">      removeExecutors(executorIdsToBeRemoved)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看释放 executor 的逻辑,比较简单</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def removeExecutors(executors: Seq[String]): Seq[String] &#x3D; synchronized &#123;</span><br><span class=\"line\">  val executorIdsToBeRemoved &#x3D; new ArrayBuffer[String]</span><br><span class=\"line\">  logInfo(&quot;Request to remove executorIds: &quot; + executors.mkString(&quot;, &quot;))</span><br><span class=\"line\">  &#x2F;&#x2F;当前存活的节点数</span><br><span class=\"line\">  val numExistingExecutors &#x3D; allocationManager.executorIds.size - executorsPendingToRemove.size</span><br><span class=\"line\">  var newExecutorTotal &#x3D; numExistingExecutors</span><br><span class=\"line\">  executors.foreach &#123; executorIdToBeRemoved &#x3D;&gt;</span><br><span class=\"line\">  &#x2F;&#x2F;如果删除该节点,导致存活节点数小于动态资源下界或者小于当前需要的资源数,则不删除</span><br><span class=\"line\">    if (newExecutorTotal - 1 &lt; minNumExecutors) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not removing idle executor $executorIdToBeRemoved because there are only &quot; +</span><br><span class=\"line\">        s&quot;$newExecutorTotal executor(s) left (minimum number of executor limit $minNumExecutors)&quot;)</span><br><span class=\"line\">    &#125; else if (newExecutorTotal - 1 &lt; numExecutorsTarget) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not removing idle executor $executorIdToBeRemoved because there are only &quot; +</span><br><span class=\"line\">        s&quot;$newExecutorTotal executor(s) left (number of executor target $numExecutorsTarget)&quot;)</span><br><span class=\"line\">    &#125; else if (canBeKilled(executorIdToBeRemoved)) &#123;</span><br><span class=\"line\">      executorIdsToBeRemoved +&#x3D; executorIdToBeRemoved</span><br><span class=\"line\">      newExecutorTotal -&#x3D; 1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if (executorIdsToBeRemoved.isEmpty) &#123;</span><br><span class=\"line\">    return Seq.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  val executorsRemoved &#x3D; if (testing) &#123;</span><br><span class=\"line\">    executorIdsToBeRemoved</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">      client.killExecutors(executorIdsToBeRemoved, adjustTargetNumExecutors &#x3D; false,</span><br><span class=\"line\">      countFailures &#x3D; false, force &#x3D; false)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">  newExecutorTotal &#x3D; numExistingExecutors</span><br><span class=\"line\">  if (testing || executorsRemoved.nonEmpty) &#123;</span><br><span class=\"line\">    executorsRemoved.foreach &#123; removedExecutorId &#x3D;&gt;</span><br><span class=\"line\">      newExecutorTotal -&#x3D; 1</span><br><span class=\"line\">      logInfo(s&quot;Removing executor $removedExecutorId because it has been idle for &quot; +</span><br><span class=\"line\">        s&quot;$executorIdleTimeoutS seconds (new desired total will be $newExecutorTotal)&quot;)</span><br><span class=\"line\">      executorsPendingToRemove.add(removedExecutorId)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    executorsRemoved</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    logWarning(s&quot;Unable to reach the cluster manager to kill executor&#x2F;s &quot; +</span><br><span class=\"line\">      s&quot;$&#123;executorIdsToBeRemoved.mkString(&quot;,&quot;)&#125; or no executor eligible to kill!&quot;)</span><br><span class=\"line\">    Seq.empty[String]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>killExecutors 的逻辑在对接底层资源调度平台(Standalone,CoarseGrained)的实现类中,以CoarseGrainedSchedulerBackend为例</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[CoarseGrainedSchedulerBackend.scala]</span><br><span class=\"line\">final override def killExecutors(</span><br><span class=\"line\">      executorIds: Seq[String],</span><br><span class=\"line\">      adjustTargetNumExecutors: Boolean,</span><br><span class=\"line\">      countFailures: Boolean,</span><br><span class=\"line\">      force: Boolean): Seq[String] &#x3D; &#123;</span><br><span class=\"line\">    val response &#x3D; synchronized &#123;</span><br><span class=\"line\">      val (knownExecutors, unknownExecutors) &#x3D; executorIds.partition(executorDataMap.contains)</span><br><span class=\"line\">      unknownExecutors.foreach &#123; id &#x3D;&gt;</span><br><span class=\"line\">        logWarning(s&quot;Executor to kill $id does not exist!&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">     &#x2F;&#x2F;从待删除的列表中过滤掉确定被删除但还未被删除的和没有正在跑task 的 executor,作为此次请求 cm 释放的executor</span><br><span class=\"line\">      val executorsToKill &#x3D; knownExecutors</span><br><span class=\"line\">        .filter &#123; id &#x3D;&gt; !executorsPendingToRemove.contains(id) &#125;</span><br><span class=\"line\">        .filter &#123; id &#x3D;&gt; force || !scheduler.isExecutorBusy(id) &#125;</span><br><span class=\"line\">      executorsToKill.foreach &#123; id &#x3D;&gt; executorsPendingToRemove(id) &#x3D; !countFailures &#125;</span><br><span class=\"line\">      logInfo(s&quot;Actual list of executor(s) to be killed is $&#123;executorsToKill.mkString(&quot;, &quot;)&#125;&quot;)</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; If we do not wish to replace the executors we kill, sync the target number of executors</span><br><span class=\"line\">      &#x2F;&#x2F; with the cluster manager to avoid allocating new ones. When computing the new target,</span><br><span class=\"line\">      &#x2F;&#x2F; take into account executors that are pending to be added or removed.</span><br><span class=\"line\">      val adjustTotalExecutors &#x3D;</span><br><span class=\"line\">        if (adjustTargetNumExecutors) &#123;</span><br><span class=\"line\">          requestedTotalExecutors &#x3D; math.max(requestedTotalExecutors - executorsToKill.size, 0)</span><br><span class=\"line\">          ..</span><br><span class=\"line\">          doRequestTotalExecutors(requestedTotalExecutors)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          numPendingExecutors +&#x3D; knownExecutors.size</span><br><span class=\"line\">          Future.successful(true)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">      val killExecutors: Boolean &#x3D;&gt; Future[Boolean] &#x3D;</span><br><span class=\"line\">        if (!executorsToKill.isEmpty) &#123;</span><br><span class=\"line\">          _ &#x3D;&gt; doKillExecutors(executorsToKill)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          _ &#x3D;&gt; Future.successful(false)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      val killResponse &#x3D; adjustTotalExecutors.flatMap(killExecutors)(ThreadUtils.sameThread)</span><br><span class=\"line\">      killResponse.flatMap(killSuccessful &#x3D;&gt;</span><br><span class=\"line\">        Future.successful (if (killSuccessful) executorsToKill else Seq.empty[String])</span><br><span class=\"line\">      )(ThreadUtils.sameThread)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    defaultAskTimeout.awaitResult(response)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   protected def doKillExecutors(executorIds: Seq[String]): Future[Boolean] &#x3D;</span><br><span class=\"line\">    Future.successful(false)</span><br></pre></td></tr></table></figure>\n<p>doKillExecutor和doRequestTotalExecutors在 CoarseGrainedSchedulerBackend 中有默认实现,里面内容比较有迷惑性,其实逻辑在YarnSchedulerBackend中,封装了YarnClientSchedulerBackend 和 YarnClusterSchedulerBackend 实现类的通用逻辑.ResourceManager 端的逻辑不在此讨论</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[YarnSchedulerBackend.scala]</span><br><span class=\"line\">  override def doKillExecutors(executorIds: Seq[String]): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](KillExecutors(executorIds))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))</span><br></pre></td></tr></table></figure>\n<p>再看申请 executor 的逻辑,对 ExecutorAllocationManager#start#schedule#updateAndSyncNumExecutorsTarget(now)方法做解析</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def updateAndSyncNumExecutorsTarget(now: Long): Int &#x3D; synchronized &#123;</span><br><span class=\"line\">    val maxNeeded &#x3D; maxNumExecutorsNeeded</span><br><span class=\"line\">    if (initializing) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;application job 还没有开始执行第一个 stage,所以不需要申请额外的资源</span><br><span class=\"line\">       0</span><br><span class=\"line\">    &#125; else if (maxNeeded &lt; numExecutorsTarget) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;如果当前计算出所需的资源小于已经拥有的资源,那么没有必要再申请新的,申请了也是空闲</span><br><span class=\"line\">      &#x2F;&#x2F;可以代入一些值做debug</span><br><span class=\"line\">      val oldNumExecutorsTarget &#x3D; numExecutorsTarget</span><br><span class=\"line\">      numExecutorsTarget &#x3D; math.max(maxNeeded, minNumExecutors)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\"></span><br><span class=\"line\">       &#x2F;&#x2F;math.max(maxNeeded, minNumExecutors)与numExecutorsTarget且maxNeeded &lt; numExecutorsTarget</span><br><span class=\"line\">      if (numExecutorsTarget &lt; oldNumExecutorsTarget) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; We lower the target number of executors but don&#39;t actively kill any yet.  Killing is</span><br><span class=\"line\">        &#x2F;&#x2F; controlled separately by an idle timeout.  It&#39;s still *helpful* to reduce the target number</span><br><span class=\"line\">        &#x2F;&#x2F; in case an executor just happens to get lost (eg., bad hardware, or the cluster manager</span><br><span class=\"line\">        &#x2F;&#x2F; preempts it) -- in that case, there is no point in trying to immediately  get a new</span><br><span class=\"line\">        &#x2F;&#x2F; executor, since we wouldn&#39;t even use it yet.</span><br><span class=\"line\">        &#x2F;&#x2F;同步当前需要的 executor 数量给 cm</span><br><span class=\"line\">        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">        logDebug(s&quot;Lowering target number of executors to $numExecutorsTarget (previously &quot; +</span><br><span class=\"line\">          s&quot;$oldNumExecutorsTarget) because not all requested executors are actually needed&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      numExecutorsTarget - oldNumExecutorsTarget</span><br><span class=\"line\">    &#125; else if (addTime !&#x3D; NOT_SET &amp;&amp; now &gt;&#x3D; addTime) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;在 application 已经开始运行,申请过资源;且当前计算出来的需要的 executor 数量大于已有的 executor 数量;且任务已经被积压过(onSchedulerBacklogged事件会设置 add_time)那么会申请 executor</span><br><span class=\"line\">      val delta &#x3D; addExecutors(maxNeeded)</span><br><span class=\"line\">      logDebug(s&quot;Starting timer to add more executors (to &quot; +</span><br><span class=\"line\">        s&quot;expire in $sustainedSchedulerBacklogTimeoutS seconds)&quot;)</span><br><span class=\"line\">      addTime &#x3D; now + (sustainedSchedulerBacklogTimeoutS * 1000)</span><br><span class=\"line\">      delta</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;依据当前stage正在运行的task数量和堆积的task数量来预估需要的executor数量</span><br><span class=\"line\">  &#x2F;&#x2F;之所以是当前 stage,是因为onStageSubmitted和onStageCompleted方法会维护stageIdToNumTasks(hashmap),所以这个 map 里面存储的是DAG中无依赖关系的 stage-tasknum 数据</span><br><span class=\"line\">   private def maxNumExecutorsNeeded(): Int &#x3D; &#123;</span><br><span class=\"line\">    val numRunningOrPendingTasks &#x3D; listener.totalPendingTasks + listener.totalRunningTasks</span><br><span class=\"line\">    (numRunningOrPendingTasks + tasksPerExecutor - 1) &#x2F; tasksPerExecutor</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#x2F;&#x2F;申请 executor.如果当前需要的资源数超过动态最大资源数,该次不申请并且将 numExecutorsToAdd(申请 executor 的滚动增量)设置为1,而不是乘以2.</span><br><span class=\"line\">&#x2F;&#x2F;numExecutorsToAdd:1-&gt;2-&gt;4-&gt;8-&gt;1(已持有的数量不小于动态最大数量,则没必要需要太快的增速)-&gt;2-&gt;4</span><br><span class=\"line\">private def addExecutors(maxNumExecutorsNeeded: Int): Int &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Do not request more executors if it would put our target over the upper bound</span><br><span class=\"line\">    if (numExecutorsTarget &gt;&#x3D; maxNumExecutors) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Not adding executors because our current target total &quot; +</span><br><span class=\"line\">        s&quot;is already $numExecutorsTarget (limit $maxNumExecutors)&quot;)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">      return 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    val oldNumExecutorsTarget &#x3D; numExecutorsTarget</span><br><span class=\"line\">    &#x2F;&#x2F; There&#39;s no point in wasting time ramping up to the number of executors we already have, so</span><br><span class=\"line\">    &#x2F;&#x2F; make sure our target is at least as much as our current allocation:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.max(numExecutorsTarget, executorIds.size)</span><br><span class=\"line\">    &#x2F;&#x2F; Boost our target with the number to add for this round:   滚动增量</span><br><span class=\"line\">    numExecutorsTarget +&#x3D; numExecutorsToAdd</span><br><span class=\"line\">    &#x2F;&#x2F; Ensure that our target doesn&#39;t exceed what we need at the present moment:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.min(numExecutorsTarget, maxNumExecutorsNeeded)</span><br><span class=\"line\">    &#x2F;&#x2F; Ensure that our target fits within configured bounds:</span><br><span class=\"line\">    numExecutorsTarget &#x3D; math.max(math.min(numExecutorsTarget, maxNumExecutors), minNumExecutors)</span><br><span class=\"line\"></span><br><span class=\"line\">    val delta &#x3D; numExecutorsTarget - oldNumExecutorsTarget</span><br><span class=\"line\">    if (delta &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; Check if there is any speculative jobs pending</span><br><span class=\"line\">      if (listener.pendingTasks &#x3D;&#x3D; 0 &amp;&amp; listener.pendingSpeculativeTasks &gt; 0) &#123;</span><br><span class=\"line\">        numExecutorsTarget &#x3D;</span><br><span class=\"line\">          math.max(math.min(maxNumExecutorsNeeded + 1, maxNumExecutors), minNumExecutors)</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        numExecutorsToAdd &#x3D; 1</span><br><span class=\"line\">        return 0</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val addRequestAcknowledged &#x3D; try &#123;</span><br><span class=\"line\">      testing ||</span><br><span class=\"line\">        client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">        ....</span><br><span class=\"line\">        false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (addRequestAcknowledged) &#123;</span><br><span class=\"line\">      val executorsString &#x3D; &quot;executor&quot; + &#123; if (delta &gt; 1) &quot;s&quot; else &quot;&quot; &#125;</span><br><span class=\"line\">      logInfo(s&quot;Requesting $delta new $executorsString because tasks are backlogged&quot; +</span><br><span class=\"line\">        s&quot; (new desired total will be $numExecutorsTarget)&quot;)</span><br><span class=\"line\">      numExecutorsToAdd &#x3D; if (delta &#x3D;&#x3D; numExecutorsToAdd) &#123;</span><br><span class=\"line\">        numExecutorsToAdd * 2</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        1</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      delta</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(</span><br><span class=\"line\">        s&quot;Unable to reach the cluster manager to request $numExecutorsTarget total executors!&quot;)</span><br><span class=\"line\">      numExecutorsTarget &#x3D; oldNumExecutorsTarget</span><br><span class=\"line\">      0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>资源申请</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[YarnSchedulerBackend.scala]</span><br><span class=\"line\">override def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] &#x3D; &#123;</span><br><span class=\"line\">    yarnSchedulerEndpointRef.ask[Boolean](prepareRequestExecutors(requestedTotal))</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ExecutorAllocationListener\"><a href=\"#ExecutorAllocationListener\" class=\"headerlink\" title=\"ExecutorAllocationListener\"></a>ExecutorAllocationListener</h2><p>ExecutorAllocationListener监听器监听各类事件,触发相应的回调函数,对自身以及ExecutorAllocationManager 内维护的信息做修改,辅助 ExecutorAllocationManager 做动态资源调度.看一些回调函数的源码:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\">   * Callback invoked when the specified executor has been added.</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">private def onExecutorAdded(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (!executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      executorIds.add(executorId)</span><br><span class=\"line\">      &#x2F;&#x2F; If an executor (call this executor X) is not removed because the lower bound</span><br><span class=\"line\">      &#x2F;&#x2F; has been reached, it will no longer be marked as idle. When new executors join,</span><br><span class=\"line\">      &#x2F;&#x2F; however, we are no longer at the lower bound, and so we must mark executor X</span><br><span class=\"line\">      &#x2F;&#x2F; as idle again so as not to forget that it is a candidate for removal. (see SPARK-4951)</span><br><span class=\"line\">      executorIds.filter(listener.isExecutorIdle).foreach(onExecutorIdle)</span><br><span class=\"line\">      logInfo(s&quot;New executor $executorId has registered (new total is $&#123;executorIds.size&#125;)&quot;)</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Duplicate executor $executorId has registered&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorRemoved(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      executorIds.remove(executorId)</span><br><span class=\"line\">      removeTimes.remove(executorId)</span><br><span class=\"line\">      logInfo(s&quot;Existing executor $executorId has been removed (new total is $&#123;executorIds.size&#125;)&quot;)</span><br><span class=\"line\">      if (executorsPendingToRemove.contains(executorId)) &#123;</span><br><span class=\"line\">        executorsPendingToRemove.remove(executorId)</span><br><span class=\"line\">        logDebug(s&quot;Executor $executorId is no longer pending to &quot; +</span><br><span class=\"line\">          s&quot;be removed ($&#123;executorsPendingToRemove.size&#125; left)&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Unknown executor $executorId has been removed!&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onSchedulerBacklogged(): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (addTime &#x3D;&#x3D; NOT_SET) &#123;</span><br><span class=\"line\">      logDebug(s&quot;Starting timer to add executors because pending tasks &quot; +</span><br><span class=\"line\">        s&quot;are building up (to expire in $schedulerBacklogTimeoutS seconds)&quot;)</span><br><span class=\"line\">      addTime &#x3D; clock.getTimeMillis + schedulerBacklogTimeoutS * 1000</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorIdle(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (executorIds.contains(executorId)) &#123;</span><br><span class=\"line\">      if (!removeTimes.contains(executorId) &amp;&amp; !executorsPendingToRemove.contains(executorId)) &#123;</span><br><span class=\"line\">        val hasCachedBlocks &#x3D; blockManagerMaster.hasCachedBlocks(executorId)</span><br><span class=\"line\">        val now &#x3D; clock.getTimeMillis()</span><br><span class=\"line\">        val timeout &#x3D; &#123;</span><br><span class=\"line\">          if (hasCachedBlocks) &#123;\t&#x2F;&#x2F;缓存了数据,shuffle write 数据等</span><br><span class=\"line\">            now + cachedExecutorIdleTimeoutS * 1000</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            now + executorIdleTimeoutS * 1000</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        val realTimeout &#x3D; if (timeout &lt;&#x3D; 0) Long.MaxValue else timeout &#x2F;&#x2F; overflow</span><br><span class=\"line\">        removeTimes(executorId) &#x3D; realTimeout</span><br><span class=\"line\">        logDebug(s&quot;Starting idle timer for $executorId because there are no more tasks &quot; +</span><br><span class=\"line\">          s&quot;scheduled to run on the executor (to expire in $&#123;(realTimeout - now)&#x2F;1000&#125; seconds)&quot;)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      logWarning(s&quot;Attempted to mark unknown executor $executorId idle&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def onExecutorBusy(executorId: String): Unit &#x3D; synchronized &#123;</span><br><span class=\"line\">    logDebug(s&quot;Clearing idle timer for $executorId because it is now running a task&quot;)</span><br><span class=\"line\">    removeTimes.remove(executorId)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><blockquote>\n</blockquote>\n<p>动态资源调度设计的一点思考</p>\n<ol>\n<li>如何增加资源<br> 负载与当前资源的权衡;资源增加算法:线性增加,对数等;资源快速满足需求快速下降  </li>\n<li>如何释放资源<br> 如何判断空闲;资源上有不好处理的逻辑怎么办,数据或计算   </li>\n<li>设计模式<br> 事件驱动的设计模式  </li>\n<li>解耦的重要性<br> 曾经设计过一个主从式数据处理框架,但是在可拓展性上考虑的比较少.如果现在在那个系统上增加动态资源调度功能,在设计模式和解耦上比不上 spark 之万一…  </li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p> <a href=\"http://spark.apache.org/docs/2.3.1/configuration.html\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/2.3.1/configuration.html</a><br> <a href=\"http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/2.3.1/running-on-yarn.html#configuring-the-external-shuffle-service</a>  </p>"},{"layout":"post","title":"Yarn PreemptionSystem","subtitle":"Yarn 抢占调度","date":"2019-10-04T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 前言\nYarn 使用树状层级队列组织方式来管理资源,所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源,root可有有多个子队列,子队列也可以有自己的子队列,树状层级结构的最底层为叶子队列.除 root 外,每个子队列都会持有父队列一定比例的资源 即最低配额(Configured Capacity),以及最多可持有的资源 即最高配额(Configured Max Capacity).以上是 Yarn 的资源组织方式。     \nYarn 采用两阶段资源调度模型.第一阶段 客户端向 ResourceManager 申请资源启动 ApplicationMaster 作为 Application 的资源协调和 task 监控角色;第二阶段 计算框架驱动角色通过 AppliationMaster 向 ResourceManager 申请资源运行 Worker 角色。 ApplicationMaster 和 Worker角色资源申请及分配都是异步的过程,当 NodeManager 向 ResourceManager 汇报心跳时,附带了自身正在运行和已经完成的 Container 信息,并触发了资源调度器的 NodeUpdate 事件,调度器通过以下过程尝试分配资源     \n1. 调度器委派 RootQueue 尝试进行资源分配  \n2. RootQueue 委派其 ChildQueue(直至 LeafQueue) 尝试进行资源分配  \n3. 遍历 Application 的 ResourceRequest,尝试进行分配或保留  \n4. 若分配或保留成功,则同步 Queue/Application/NodeManager 信息.若分配成功,则触发 Container 的转变为 ALLOCATED 状态:RM 已经分配了资源给 APP(Attempt),但是 AM 此时还不知道,直至 AM 和 RM 的下次心跳时 AM 将对此 Container 做调度:下载依赖,准备运行环境,启动主类等;若保留成功,则该节点后续上报心跳时,将会跳过其分配调度,直至其可用资源量达到了保留资源的要求,则被保留的 Container 将会由 RESERVED 转换为 ALLOCATED 状态.保留资源固然在某种程度上算是浪费,但是保证了大 container 的 application 不会饿死      \n5. NM 监控自身运行的所有 Container 状态,对于新启动和运行状态的 Container 以及运行完成的 Container 都会通过心跳告知 RM,RM 同步信息,更新各 Container 状态.若 Container 已经完成则在 RM 端同步 Queue/Application/NodeManager 信息  \n以上是 Yarn 常规调度机制。         \n层级队列中除 root 外,每个队列可以设置其拥有父队列的最低配额和最高配额.最高配额是队列无论在任何情况下都不会超出的用量,最低配额是在队列有任务时尽可能保证的配额.为了提高集群整体的资源利用率,Yarn 引入了抢占机制:  \n1. 在队列中无任务运行时,队列没有必要硬遵守保留最低配额,可以将资源借给负载较高的其他队列     \n2. 当借出资源的队列接收到了新提交的应用时,将出借的资源收回以满足资源需求      \n以上是Yarn 抢占调度机制。第 1 点由 Yarn 资源调度器来实现;第 2 点只有调度器实现了 PreemptableResourceScheduler 接口,且抢占策略实现了SchedulingEditPolicy时才能实现   \n本文侧重于第 2 点,并试图解决一些在小集群申请大容器的问题    \n\n<!-- more -->\n\n## 概述  \n\n当队列 A 负载比较大时,可以临时借用负载较低队列 B 的资源.但如果有任务提交到 B 队列上,调度平台处理 B 借出的资源的方式一般有三种      \n\n| 处理方式 | B 队列应用延时 | A 队列应用恢复代价 |浪费/重复工作量|\n| ------------ | ------------ | ------------|------------|\n| 等待应用完成 | 高 | 无 |无|\n| 杀死应用             | 无 | 高 |高|\n| 杀死应用部分容器    |无| 低| 一般 |\n\n对于借出资源的处理,传统做法一般是等待 A 队列应用完成,将相应的资源还回,这样对于 B 队列应用程序来说有较高的延迟;直接杀死 A 队列应用是一种比较直接的做法,这对于一些特殊服务的影响很大 e.g. 7*24 的流式应用,优先级较高的服务(设计较好的公司会将这部分放在单独的队列甚至单独集群中以保证高可用).Yarn 抢占机制试图不彻底杀死整个应用,在考虑优先级和浪费工作量的情况下,从超分配队列中选出一些容器,释放掉资源达到还回原队列的目的   \n\n## 源码解析\n\n源码解析主要参照:  https://www.aboutyun.com/thread-24628-1-1.html ,与 Hadoop2.7.3版本有两处差异:  \n1.Hadoop2.7.3 版本在 TempQueue 中加入了untouchableExtra变量,用以保存在队列不允许抢占时的(used absCapacity)值  \n2.Hadoop2.7.3 版本在 计算各队列瓜分空闲资源时加了一些逻辑,在ProportionalCapacityPreemptionPolicy#computeFixpointAllocation中      \n\ncomputeFixpointAllocation是抢占调度中最核心的逻辑: 计算每个队列的理想容量,决定了是否在该队列抢占及抢占数量的问题         \n[ProportionalCapacityPreemptionPolicy.java]  \n\n```\n/**\n* 重新计算每个子队列的理想配额,存储于每个子队列的 idealAssigned 成员变量中\n* tot_guarant:父队列的理想配额\n* qAlloc:父队列的子队列\n* unassigned: tot_guarant 的 clone 对象,父队列的理想配额.将此配额分配给所有子队列\n* ignoreGuarantee:该批次子队列是否配置了最低配额.将会影响到每个队列在分配空闲配额时的权重.\n* 若为 true,没有配置最低配额,则每个队列均分空闲配额\n* 若为 false,配置了最低配额,则每个队列按 最低配额/sum(最低配额) 的权重分配最低配额  \n*/\nprivate void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection<TempQueue> qAlloc, Resource unassigned, \n      boolean ignoreGuarantee) {\n      // 按照 欠分配的程度,做排序,下文引用了类内容\n    TQComparator tqComparator = new TQComparator(rc, tot_guarant);\n    PriorityQueue<TempQueue> orderedByNeed =\n                                 new PriorityQueue<TempQueue>(10,tqComparator);\n    for (Iterator<TempQueue> i = qAlloc.iterator(); i.hasNext();) {\n      TempQueue q = i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n       // 在配置了队列可抢占后,untouchableExtra=0;若队列不可抢占,则超出最低配额的量算到理想配额内\n        q.idealAssigned = Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned = Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      Resource curPlusPend = Resources.add(q.current, q.pending);\n      // 欠分配的队列\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n    // 欠分配的队列 瓜分 空闲配额  \n    while (!orderedByNeed.isEmpty()\n       && Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned = Resource.newInstance(0, 0);\n      // 计算每个队列应得的权重  \n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n      // 使用tqComparator得到最欠分配的队列  依据是 idealAssigned/absCapacity\n      //  超分配队列初始值为 1，欠分配队列初始小于 1 \n      Collection<TempQueue> underserved =\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\n        TempQueue sub = i.next();\n       // 当前轮次, 空闲配额*权重  \n        Resource wQavail = Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        //  min{空闲配额*权重,最高配额-理想配额,需求配额}做理想配额的增量;下文引用了 offer 方法  \n        Resource wQidle = sub.offer(wQavail, rc, tot_guarant);\n        // 队列得到的最终值,即上面的 min{三元组}\n        Resource wQdone = Resources.subtract(wQavail, wQidle);\n\t    // 在该轮得到了资源,那么可能还能得到资源,需要参加下一轮迭代;如果没得到资源,那么就不会再得到资源了(等于最高配额或满足了需求)\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      // 经过这一轮迭代后,全局空闲资源变化\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }\n  \n  Resource offer(Resource avail, ResourceCalculator rc,\n        Resource clusterResource) {\n        // 最大配额-理想配额:表示该队列最多只能拿到这么多额外资源 \n      Resource absMaxCapIdealAssignedDelta = Resources.componentwiseMax(\n                      Resources.subtract(maxCapacity, idealAssigned),\n                      Resource.newInstance(0, 0));\n      Resource accepted = \n          Resources.min(rc, clusterResource, \n              absMaxCapIdealAssignedDelta,\n              // avail:是传入值, 全局空闲资源*队列权重,表明在当前迭代 最多可以给的空闲资源\n          Resources.min(rc, clusterResource, avail, \n           // current+pending-idealAssigned:表示 满足队列中所有应用程序需要的 额外资源 \n          Resources.subtract(\n              Resources.add(current, pending), idealAssigned)));\n              \n      Resource remain = Resources.subtract(avail, accepted);\n      // 修改当前队列的理想配额  \n      Resources.addTo(idealAssigned, accepted);\n      return remain;\n    }\n```\n欠分配比较器  \n[TQComparator.java]  \n\n```\nstatic class TQComparator implements Comparator<TempQueue> {\n    private ResourceCalculator rc;\n    private Resource clusterRes;\n    @Override\n    public int compare(TempQueue tq1, TempQueue tq2) {\n      if (getIdealPctOfGuaranteed(tq1) < getIdealPctOfGuaranteed(tq2)) {\n        return -1;\n      }\n      if (getIdealPctOfGuaranteed(tq1) > getIdealPctOfGuaranteed(tq2)) {\n        return 1;\n      }\n      return 0;\n    }\n    // 计算 idealAssigned / guaranteed\n    private double getIdealPctOfGuaranteed(TempQueue q) {\n      double pctOver = Integer.MAX_VALUE;\n      if (q != null && Resources.greaterThan(\n          rc, clusterRes, q.guaranteed, Resources.none())) {\n        pctOver =\n            Resources.divide(rc, clusterRes, q.idealAssigned, q.guaranteed);\n      }\n      return (pctOver);\n    }\n  }\n```\n\n## 抢占流程 \n\n第一步: 递归获取所有队列的快照信息，重点包括 used/pending/absCapacity/maxAbsCapacity 信息  \n第二步: 设置 root 队列的初始 idealAssigned 为 absCapacity(100%)，以此为依据将 root 队列的 idealAssigned 分摊给各层队列,即第三步     \n第三步: 根据是否配置了最低配额将所有子队列分为两类，优先满足配置了最低配额的队列需求，然后剩余配额满足未配置最低配额队列的需求。对这两类队列进行资源再平衡 迭代计算出每个子队列的理想配额，即第四步。如果子队列的 used > absCapacity * (1+maxIgnoredOverCapacity)，则需要从其中掠夺资源，即第五步  \n第四步: 首先按照当前使用状态(min{used,capacity})，计算出全局空闲资源量(弹性)，获得欠分配的队列；对欠分配的队列集合进行迭代以求出每个队列的 idealAssigned，在每一轮迭代中，设置每个欠分配队列所占的权重(capacity 为依据)，该权重影响队列在获取空闲资源时的增量；优先对欠分配程度最高的队列进行分配，配额为 min{全局空闲资源量 * 队列权重, maxAbsCapacity-idealAssigned, used+pending-idealAssigned},三元组中第一个值表示可分配给队列的额外配额，第二个值表示在不超过最高配额的条件下能接受的最多额外配额，第三个值表示满足应用程序资源需求的额外配额。若满足了队列的需求则将其剔除出欠分配的队列集合  \n第五步: 如果队列 used > absCapacity * (1+maxIgnoredOverCapacity),那么将会释放掉 (used - idealAssigned) * naturalTerminationFactor (自然终止因子,默认0.2)的资源,则按照 <b>最小化影响 application </b>的原则(maxIgnoredOverCapacity 和 naturalTerminationFactor 都是遵从该原则)，优先释放最晚启动的 application 中的最晚分配的container，优先释放 reserved contaienr，优先释放非 AM 的 container。向常规调度器发送抢占 container 事件，并追踪这些 container 的存活时间，如果在超过了 maxWaitTime(15s)之后还没有释放，则强制常规调度器杀死   \n\n## 一些误区\n\n1. <b>最小化影响 application 原则是抢占过程中最优先考虑的原则</b>        \n2. 抢占调度仅仅是所有队列的资源再平衡过程，<b>不为具体的应用程序调度，不为某个容器做分配或保留</b>。从设计模式原则的角度来看是合理的；抢占调度和常规调度松耦合，仅仅使用总线向常规调度器单向发送抢占/杀死容器事件，从架构分层分模块的角度来看也是合理的      \n3. 抢占调度<b>不会平衡队列内部的用户资源使用量</b>，用户资源量的平衡应使用 user limit percent 或者 Fair 去做。e.g. user1 先向队列提交了 10 个 application,user2 后向队列提交了 2 个 application ,在抢占调度时 不会把 user1 占用的资源平衡给 user2 使用;且 如果队列要释放资源,那么由于 user2 的 application 后提交,反而会先于 user1 的 application 被抢占    \n4. 抢占调度<b>不会使繁忙队列的资源达到最高配额，且不会使空闲队列在任意条件下都得到最低配额</b>。比如,队列 A 占用了队列 B 的所有资源，那么当 B 上有任务提交时，是 A 和 B 一起抢占 B 的最低配额资源，只不过 B 由于欠分配程度(idealAssigned/absCapacity)比较高，在抢占时多次迭代中都更优先于 A，体现出来就是 B 抢回了资源。在 B used + pending >> capacity 且 idealAssigned < capacity 时，B 的 idealAssigned 会尽快收敛到 capacity，然后和 A 一起瓜分剩余资源。        \n5. 关于抢占的量，控制该值的配置主要是两个: yarn.resourcemanager.monitor.capacity.preemption.total\\_preemption\\_per\\_round 每轮总抢占 默认 0.1，表示最多每轮抢占的资源占集群总资源的比例。若每轮抢占总量超过此值，则按比例在每个队列中缩减； yarn.resourcemanager.monitor.capacity.preemption.natural\\_termination\\_factor 自然终止因子 默认 0.2。考虑到容器自然终止的情况,即使不杀死容器也能在 5 * maxWaitTime(15s) 内收回 95%的资源，所以使用(used - idealAssigned)乘以该因子作为队列内最终抢占资源量，以尽量减少对 已启动 application 的影响  \n6. 在 AM-RM 心跳协议 ApplicationMasterProtocol#allocate 中，AM 通过 AllocateRequest更新自己的需求，包括新增与释放 container。RM通过 AllocateResponse 响应请求，包括新分配的容器集合，释放的容器集合和<b>需要 AM 释放的被抢占容器集合</b>。抢占容器的消息类型分为两类:StrictPreemptionContract，AM 必须释放 RM 指定的容器；PreemptionContract。 AM 可以在满足同等大小资源的条件下灵活的替换 RM 指定的待抢占容器。目前 Yarn 采用的是 PreemptionContract类型，从 AM(用户)的角度来看这是可操作性很高的设计:待抢占的 worker 角色可能存储着 shuffle 文件、可能开启着重要的服务...... 恢复这些 worker 可能需要较多额外的工作，所以 RM 让 AM 去选择杀死哪些 container，只关注 AM 能不能还回等量的资源。但是抢占调度里面追踪着 RM 给 AM 指定的待抢占容器集合，如果 AM 替换掉这些容器，那么如果在抢占调度后续的周期发现这些追踪的容器存活大于 maxWaitTime，那么会直接杀死。这是一个有歧义的设计，目前 SparkOnYarn 的 YarnAllocator 没有对 RM 发给 AM 的待抢占容器集合做任何处理，即等同于等待容器超时被 RM 端杀死      \n7. <b>抢占得到的资源可能不足以任何资源请求，但是抢占依旧会进行</b>。控制每轮抢占立即得到的资源总量的参数参考 \"一些误区 5\"。假设集群中只有两个叶子队列 A和B，最低配额都是 50%。初始状态，A 完全空闲，B 负载很高 抢占了 A 的所有资源。A 队列突然被提交了大量应用，那么在第一轮抢占时 B 是超分配的队列，idealAssigned 初值为50%，A是借出所有资源的队列 idealAssigned 初值为 0，但是 idealAssigned/absCapacity 小于 B，是最欠分配的队列 迭代时每轮都优先分配A，A 能得到的资源为: B 队列 (used 100% - idealAssigned 50%) *自然终止因子 0.2 =10%，且不能超过 集群资源100% * 每轮总抢占比例 0.1 = 10%。抢占资源量可能不足以满足任何 ResourceRequest，对于大型集群来说这个问题可能不足为虑，但是在小集群上可能会有一些难以预知的后果，尤其是在小集群上很大的资源需求(e.g. 40G)       \n\n## 小集群大容器问题  \n\n### 问题场景 \n\n![](img/pictures/yarn_preempt/preempt_queue.png)   \n\n为简单起见，Yarn 使用默认配置且在不考虑: 虚拟核，AM/Worker 的区别和分配/保留的区别。集群有三台服务器，每台服务器有 80G 内存，共计有 240G 的内存资源。有两个叶子队列: A 和 B，最低配额都是 50%，最高配额都是 100%。当 A 负载较高而 B 空闲时，A 使用了整个集群的资源，A 中只有一个应用 app\\_A，启动的 container 都是 4G 大小。此时有新任务 app\\_B 提交到队列 B，且需要的资源量大于 B 的最低配额，则在第一次抢占时 B 队列会获得(240-120) * 0.2 自然终止因子 = 24G 的资源，且该轮抢占最大资源量不能超过 240 * 0.1 每轮抢占资源量比例 = 24G，这些抢占的 contaienr 均匀分布在集群的所有节点上，即图中红色 container 为被抢占，绿色 container 为未被抢占    \n \n- 场景 1: 假设 app\\_B 待分配的 container 大小为 40G，每个节点上只有 8G 的空闲资源，不足以启动该 container。且即使这些被抢占的 container 都分布在一个节点上，也不足以启动 40G JVM。 抢占虽然发生，但是 B 没有使用，最终这些抢占释放的资源会被 app\\_A 利用。这样就陷入了由周期性抢占导致的困局: 抢占调度 平衡资源-> B 无法利用资源-> A 可以利用资源->A 超分配,B 欠分配->抢占调度 平衡资源->.....  具体表现是: B 中的任务无法启动，A及app\\_A 中最近启动的容器一直处于 被抢占->启动成为最新->被抢占......的循环中  \n\n- 场景 2: 假设 app\\_B 待分配的 container 大小为 20G，每个节点上只有 8G 的空闲资源，不足以启动 20G 的 container，也会陷入周期性抢占导致的困局。但是如果这些被抢占的 container 分布在一个节点上，足以启动 20G 的 JVM。这是因为 JVM 大小不高于每轮抢占资源总量且不大于每台服务器内存总量的缘故    \n\n注: 常规调度器不希望一个容器占有节点太多的资源，所以对于大 container，不一定会做保留  \n[LeafQueue.java]\n\n```\nboolean shouldAllocOrReserveNewContainer(FiCaSchedulerApp application,\n      Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers > 0) {\n      float nodeFactor = \n          Resources.ratio(\n              resourceCalculator, required, getMaximumAllocation()\n              );      \n       // 使用所需节点的百分比来对大型容器施加偏差…\n       // 防止需要使用整个节点的极端情况\n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );  \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"needsContainers:\" +\n            \" app.#re-reserve=\" + application.getReReservations(priority) + \n            \" reserved=\" + reservedContainers + \n            \" nodeFactor=\" + nodeFactor + \n            \" minAllocFactor=\" + getMinimumAllocationFactor() +\n            \" starvation=\" + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) > 0);\n``` \n\n### 解决方案  \n\n<b>方案 1</b>   \n对于场景 1，可以从两个角度优化: 提高配置增加每轮抢占资源量及自然终止因子，以提高每轮抢占可以立即得到的资源量；规定应用程序单个容器的大小不能高于每轮抢占资源量。将场景 1 的问题转换问场景 2 的问题      \n对于场景 2，尽可能使被抢占容器分布在同一个节点上，为\"最小化影响 Application\"，可以在每个节点上按照设定的选取规则尝试抢占容器(但是并未真的杀死)以满足大容器并计算抢占代价(sum(currentTime-containerStartTime))，选择抢占代价最低的节点抢占。  \n \n \n优点:\n\n1. 可以提高启动大容器的可能性，减少大容器调度延时    \n\n缺点:  \n\n1. 增加每轮抢占资源量和自然终止因子,将增加集群和队列资源在平衡点附近抖动程度(队列释放->阶梯式下降,队列获取->阶梯式上升)。最好配合yarn.resourcemanager.monitor.capacity.preemption.max\\_ignored\\_over\\_capacity 属性使用。还是最小化影响原则，减小借用资源不多的队列的抖动      \n2. 提高抢占配置，提供能够启动大容器的资源量，将增加了被杀死的容器数量，扩大了影响范围，恢复任务产生的无效工作量也会增加      \n3. 尽可能在节点上抢占容器启动大容器，而不是在集群所有节点上抢占最晚启动的应用程序，导致这些节点资源利用率抖动极大  \n4. 由于抢占调度和常规调度解耦，抢占调度在节点上抢占出大容器的空间，常规调度也未必在该空间内分配大容器  \n  \n仅限于抢占频率较低的场景，并使用某些参量(欠分配队列 totalPending, minResourceRequest)来决定 走默认抢占逻辑还是集中在节点上释放资源      \n\n<b>方案 2</b>  \n\n[资源预订系统](https://jiulongzhu.github.io/2019/10/28/Yarn-ReservationSystem/)\n \n<b>方案 3</b>  \n标签系统：对节点进行分类或分组的一种方式，应用程序可以指定在特定标签的节点执行。可以配置资源队列可以访问的节点标签集合，应用程序只能提交到这些包含该标签的队列上来使用这些具有标签的节点的计算资源  \nyarn.scheduler.capacity.\\<queue\\_path\\>.capacity 属性是<b>默认标签</b>(无标签)的资源配置，所有的队列都可以访问无标签的节点；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels 属性用于指定队列可访问的标签节点，在不指定的情况下可以继承父队列的权限，若希望让某队列只能访问无标签节点，设置为空格即可；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels.\\<label\\>.capacity 指定队列可以使用该标签资源的最低配额；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels.\\<label\\>.maximum-capacity 指定队列可以使用该标签资源的最高配额     \n\n适用场景 \n \n1. 机构路由: BU A的任务，只能运行在具有 A 标签的 NM 上    \n2. 服务路由: Hbase 的数据分析任务，只能运行在具有 Hbase 标签的 NM 上    \n3. 特殊机器: 只有深度学习任务可以运行在具有 GPU 标签的 NM 上        \n\n新增或修改一些节点，配置其标签，并设置仅有某些队列具有该标签资源的使用权限，大内存应用程序专用    \n\n优点:\n  \n1. 可以运行大内存应用，且 SLA 高  \n\n缺点:  \n1. 标签资源空闲时其他应用无法使用，集群资源利用率相对较低  \n2. 需要额外的服务器成本  \n  \n## 相关参数\n\n- yarn.resourcemanager.scheduler.monitor.enable   \n设置为 true ,启用抢占  \n- yarn.resourcemanager.scheduler.monitor.policies  \n启用抢占时,抢占策略主类.RM 将配置的主类启动在独立线程中,周期性的执行抢占.默认是org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy  \n- yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval  \n周期性执行抢占策略的间隔,单位毫秒,默认是 3000  \n- yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill  \n自容器被标记为被抢占到抢占策略强制杀死该容器的时间,单位毫秒,默认15000.留给 AM 灵活杀死 container 的时间,如果希望快速收回资源可以适当调小    \n- yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round  \n每轮抢占最大资源总量占集群无标签资源的比例,默认 0.1. 即每轮抢占最多可以收回相当于集群无标签资源总量的10%,若每轮抢占资源量超过此值,则按比例在每个待抢占队列进行缩减    \n- yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity  \n忽略抢占的阈值,默认 0.1.当队列资源使用量 used > absCapacity(1+0.1)时,认为其应该还回资源,才会抢占容器.用于避免资源消耗和配额的剧烈波动,最小化影响      \n- yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor  \n自然终止因子,统计发现值为 0.5 或者为 0 时,都会在 5* 15秒(上述第三配置项)内收回 95%的资源,默认 0.2.为每个队列设置抢占目标后,再乘以此值作为最终的抢占目标.假设第一周期,某队列待抢占值为 10G,那么最终抢占为 2G.第二周期待抢占值为 7G,最终抢占为 1.4G...以一种平滑的方式回收,降低波动.可以适当增加此值以加快回收速度  \n\n## 一点思考 \n\n1. 方案设计时考虑了很多，从尝试借鉴 CPU 抢占、内存 SWAP 模式、用户体验、架构分层等角度思考设计一个抢占调度系统的想法。有时间再整理吧     \n2. 关于开源抢占系统通用优化的初步想法     \n调度系统抢占的容器先不杀死，而是打一个标记，比如:MARK\\_PREEMPTED\\_KILL,在资源分配时优先使用这些标记的节点或者资源紧缺时使用 以尽可能使容器能自然终止(类似 jvm软引用)，无资源分配时则不杀死。其一可以保证 在周期性抢占造成的困局中，队列使用不到这些资源时，这些资源不会被反复无效调度，而是继续运行任务；其二 或许能提高资源抢占相应速度，代价是需要完善常规调度的逻辑，分配时如何处理MARK\\_PREEMPTED\\_KILL，保留时如何处理MARK\\_PREEMPTED\\_KILL...    \n\n## 参考 \nhttps://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html  \nhttps://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption  \nhttps://www.aboutyun.com/thread-24628-1-1.html\n\n\n\n","source":"_posts/2019-10-05-Yarn PreemptionSystem.md","raw":"---\nlayout:     post  \ntitle:      Yarn PreemptionSystem  \nsubtitle:   Yarn 抢占调度  \ndate:       2019-10-05  \nauthor:     jiulongzhu  \nheader-img: img/moon_night.jpg  \ncatalog: true  \ntags:  \n    - Yarn 2.7.3   \n    - 源码解析  \n---\n\n\n## 前言\nYarn 使用树状层级队列组织方式来管理资源,所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源,root可有有多个子队列,子队列也可以有自己的子队列,树状层级结构的最底层为叶子队列.除 root 外,每个子队列都会持有父队列一定比例的资源 即最低配额(Configured Capacity),以及最多可持有的资源 即最高配额(Configured Max Capacity).以上是 Yarn 的资源组织方式。     \nYarn 采用两阶段资源调度模型.第一阶段 客户端向 ResourceManager 申请资源启动 ApplicationMaster 作为 Application 的资源协调和 task 监控角色;第二阶段 计算框架驱动角色通过 AppliationMaster 向 ResourceManager 申请资源运行 Worker 角色。 ApplicationMaster 和 Worker角色资源申请及分配都是异步的过程,当 NodeManager 向 ResourceManager 汇报心跳时,附带了自身正在运行和已经完成的 Container 信息,并触发了资源调度器的 NodeUpdate 事件,调度器通过以下过程尝试分配资源     \n1. 调度器委派 RootQueue 尝试进行资源分配  \n2. RootQueue 委派其 ChildQueue(直至 LeafQueue) 尝试进行资源分配  \n3. 遍历 Application 的 ResourceRequest,尝试进行分配或保留  \n4. 若分配或保留成功,则同步 Queue/Application/NodeManager 信息.若分配成功,则触发 Container 的转变为 ALLOCATED 状态:RM 已经分配了资源给 APP(Attempt),但是 AM 此时还不知道,直至 AM 和 RM 的下次心跳时 AM 将对此 Container 做调度:下载依赖,准备运行环境,启动主类等;若保留成功,则该节点后续上报心跳时,将会跳过其分配调度,直至其可用资源量达到了保留资源的要求,则被保留的 Container 将会由 RESERVED 转换为 ALLOCATED 状态.保留资源固然在某种程度上算是浪费,但是保证了大 container 的 application 不会饿死      \n5. NM 监控自身运行的所有 Container 状态,对于新启动和运行状态的 Container 以及运行完成的 Container 都会通过心跳告知 RM,RM 同步信息,更新各 Container 状态.若 Container 已经完成则在 RM 端同步 Queue/Application/NodeManager 信息  \n以上是 Yarn 常规调度机制。         \n层级队列中除 root 外,每个队列可以设置其拥有父队列的最低配额和最高配额.最高配额是队列无论在任何情况下都不会超出的用量,最低配额是在队列有任务时尽可能保证的配额.为了提高集群整体的资源利用率,Yarn 引入了抢占机制:  \n1. 在队列中无任务运行时,队列没有必要硬遵守保留最低配额,可以将资源借给负载较高的其他队列     \n2. 当借出资源的队列接收到了新提交的应用时,将出借的资源收回以满足资源需求      \n以上是Yarn 抢占调度机制。第 1 点由 Yarn 资源调度器来实现;第 2 点只有调度器实现了 PreemptableResourceScheduler 接口,且抢占策略实现了SchedulingEditPolicy时才能实现   \n本文侧重于第 2 点,并试图解决一些在小集群申请大容器的问题    \n\n<!-- more -->\n\n## 概述  \n\n当队列 A 负载比较大时,可以临时借用负载较低队列 B 的资源.但如果有任务提交到 B 队列上,调度平台处理 B 借出的资源的方式一般有三种      \n\n| 处理方式 | B 队列应用延时 | A 队列应用恢复代价 |浪费/重复工作量|\n| ------------ | ------------ | ------------|------------|\n| 等待应用完成 | 高 | 无 |无|\n| 杀死应用             | 无 | 高 |高|\n| 杀死应用部分容器    |无| 低| 一般 |\n\n对于借出资源的处理,传统做法一般是等待 A 队列应用完成,将相应的资源还回,这样对于 B 队列应用程序来说有较高的延迟;直接杀死 A 队列应用是一种比较直接的做法,这对于一些特殊服务的影响很大 e.g. 7*24 的流式应用,优先级较高的服务(设计较好的公司会将这部分放在单独的队列甚至单独集群中以保证高可用).Yarn 抢占机制试图不彻底杀死整个应用,在考虑优先级和浪费工作量的情况下,从超分配队列中选出一些容器,释放掉资源达到还回原队列的目的   \n\n## 源码解析\n\n源码解析主要参照:  https://www.aboutyun.com/thread-24628-1-1.html ,与 Hadoop2.7.3版本有两处差异:  \n1.Hadoop2.7.3 版本在 TempQueue 中加入了untouchableExtra变量,用以保存在队列不允许抢占时的(used absCapacity)值  \n2.Hadoop2.7.3 版本在 计算各队列瓜分空闲资源时加了一些逻辑,在ProportionalCapacityPreemptionPolicy#computeFixpointAllocation中      \n\ncomputeFixpointAllocation是抢占调度中最核心的逻辑: 计算每个队列的理想容量,决定了是否在该队列抢占及抢占数量的问题         \n[ProportionalCapacityPreemptionPolicy.java]  \n\n```\n/**\n* 重新计算每个子队列的理想配额,存储于每个子队列的 idealAssigned 成员变量中\n* tot_guarant:父队列的理想配额\n* qAlloc:父队列的子队列\n* unassigned: tot_guarant 的 clone 对象,父队列的理想配额.将此配额分配给所有子队列\n* ignoreGuarantee:该批次子队列是否配置了最低配额.将会影响到每个队列在分配空闲配额时的权重.\n* 若为 true,没有配置最低配额,则每个队列均分空闲配额\n* 若为 false,配置了最低配额,则每个队列按 最低配额/sum(最低配额) 的权重分配最低配额  \n*/\nprivate void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection<TempQueue> qAlloc, Resource unassigned, \n      boolean ignoreGuarantee) {\n      // 按照 欠分配的程度,做排序,下文引用了类内容\n    TQComparator tqComparator = new TQComparator(rc, tot_guarant);\n    PriorityQueue<TempQueue> orderedByNeed =\n                                 new PriorityQueue<TempQueue>(10,tqComparator);\n    for (Iterator<TempQueue> i = qAlloc.iterator(); i.hasNext();) {\n      TempQueue q = i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n       // 在配置了队列可抢占后,untouchableExtra=0;若队列不可抢占,则超出最低配额的量算到理想配额内\n        q.idealAssigned = Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned = Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      Resource curPlusPend = Resources.add(q.current, q.pending);\n      // 欠分配的队列\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n    // 欠分配的队列 瓜分 空闲配额  \n    while (!orderedByNeed.isEmpty()\n       && Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned = Resource.newInstance(0, 0);\n      // 计算每个队列应得的权重  \n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n      // 使用tqComparator得到最欠分配的队列  依据是 idealAssigned/absCapacity\n      //  超分配队列初始值为 1，欠分配队列初始小于 1 \n      Collection<TempQueue> underserved =\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\n        TempQueue sub = i.next();\n       // 当前轮次, 空闲配额*权重  \n        Resource wQavail = Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        //  min{空闲配额*权重,最高配额-理想配额,需求配额}做理想配额的增量;下文引用了 offer 方法  \n        Resource wQidle = sub.offer(wQavail, rc, tot_guarant);\n        // 队列得到的最终值,即上面的 min{三元组}\n        Resource wQdone = Resources.subtract(wQavail, wQidle);\n\t    // 在该轮得到了资源,那么可能还能得到资源,需要参加下一轮迭代;如果没得到资源,那么就不会再得到资源了(等于最高配额或满足了需求)\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      // 经过这一轮迭代后,全局空闲资源变化\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }\n  \n  Resource offer(Resource avail, ResourceCalculator rc,\n        Resource clusterResource) {\n        // 最大配额-理想配额:表示该队列最多只能拿到这么多额外资源 \n      Resource absMaxCapIdealAssignedDelta = Resources.componentwiseMax(\n                      Resources.subtract(maxCapacity, idealAssigned),\n                      Resource.newInstance(0, 0));\n      Resource accepted = \n          Resources.min(rc, clusterResource, \n              absMaxCapIdealAssignedDelta,\n              // avail:是传入值, 全局空闲资源*队列权重,表明在当前迭代 最多可以给的空闲资源\n          Resources.min(rc, clusterResource, avail, \n           // current+pending-idealAssigned:表示 满足队列中所有应用程序需要的 额外资源 \n          Resources.subtract(\n              Resources.add(current, pending), idealAssigned)));\n              \n      Resource remain = Resources.subtract(avail, accepted);\n      // 修改当前队列的理想配额  \n      Resources.addTo(idealAssigned, accepted);\n      return remain;\n    }\n```\n欠分配比较器  \n[TQComparator.java]  \n\n```\nstatic class TQComparator implements Comparator<TempQueue> {\n    private ResourceCalculator rc;\n    private Resource clusterRes;\n    @Override\n    public int compare(TempQueue tq1, TempQueue tq2) {\n      if (getIdealPctOfGuaranteed(tq1) < getIdealPctOfGuaranteed(tq2)) {\n        return -1;\n      }\n      if (getIdealPctOfGuaranteed(tq1) > getIdealPctOfGuaranteed(tq2)) {\n        return 1;\n      }\n      return 0;\n    }\n    // 计算 idealAssigned / guaranteed\n    private double getIdealPctOfGuaranteed(TempQueue q) {\n      double pctOver = Integer.MAX_VALUE;\n      if (q != null && Resources.greaterThan(\n          rc, clusterRes, q.guaranteed, Resources.none())) {\n        pctOver =\n            Resources.divide(rc, clusterRes, q.idealAssigned, q.guaranteed);\n      }\n      return (pctOver);\n    }\n  }\n```\n\n## 抢占流程 \n\n第一步: 递归获取所有队列的快照信息，重点包括 used/pending/absCapacity/maxAbsCapacity 信息  \n第二步: 设置 root 队列的初始 idealAssigned 为 absCapacity(100%)，以此为依据将 root 队列的 idealAssigned 分摊给各层队列,即第三步     \n第三步: 根据是否配置了最低配额将所有子队列分为两类，优先满足配置了最低配额的队列需求，然后剩余配额满足未配置最低配额队列的需求。对这两类队列进行资源再平衡 迭代计算出每个子队列的理想配额，即第四步。如果子队列的 used > absCapacity * (1+maxIgnoredOverCapacity)，则需要从其中掠夺资源，即第五步  \n第四步: 首先按照当前使用状态(min{used,capacity})，计算出全局空闲资源量(弹性)，获得欠分配的队列；对欠分配的队列集合进行迭代以求出每个队列的 idealAssigned，在每一轮迭代中，设置每个欠分配队列所占的权重(capacity 为依据)，该权重影响队列在获取空闲资源时的增量；优先对欠分配程度最高的队列进行分配，配额为 min{全局空闲资源量 * 队列权重, maxAbsCapacity-idealAssigned, used+pending-idealAssigned},三元组中第一个值表示可分配给队列的额外配额，第二个值表示在不超过最高配额的条件下能接受的最多额外配额，第三个值表示满足应用程序资源需求的额外配额。若满足了队列的需求则将其剔除出欠分配的队列集合  \n第五步: 如果队列 used > absCapacity * (1+maxIgnoredOverCapacity),那么将会释放掉 (used - idealAssigned) * naturalTerminationFactor (自然终止因子,默认0.2)的资源,则按照 <b>最小化影响 application </b>的原则(maxIgnoredOverCapacity 和 naturalTerminationFactor 都是遵从该原则)，优先释放最晚启动的 application 中的最晚分配的container，优先释放 reserved contaienr，优先释放非 AM 的 container。向常规调度器发送抢占 container 事件，并追踪这些 container 的存活时间，如果在超过了 maxWaitTime(15s)之后还没有释放，则强制常规调度器杀死   \n\n## 一些误区\n\n1. <b>最小化影响 application 原则是抢占过程中最优先考虑的原则</b>        \n2. 抢占调度仅仅是所有队列的资源再平衡过程，<b>不为具体的应用程序调度，不为某个容器做分配或保留</b>。从设计模式原则的角度来看是合理的；抢占调度和常规调度松耦合，仅仅使用总线向常规调度器单向发送抢占/杀死容器事件，从架构分层分模块的角度来看也是合理的      \n3. 抢占调度<b>不会平衡队列内部的用户资源使用量</b>，用户资源量的平衡应使用 user limit percent 或者 Fair 去做。e.g. user1 先向队列提交了 10 个 application,user2 后向队列提交了 2 个 application ,在抢占调度时 不会把 user1 占用的资源平衡给 user2 使用;且 如果队列要释放资源,那么由于 user2 的 application 后提交,反而会先于 user1 的 application 被抢占    \n4. 抢占调度<b>不会使繁忙队列的资源达到最高配额，且不会使空闲队列在任意条件下都得到最低配额</b>。比如,队列 A 占用了队列 B 的所有资源，那么当 B 上有任务提交时，是 A 和 B 一起抢占 B 的最低配额资源，只不过 B 由于欠分配程度(idealAssigned/absCapacity)比较高，在抢占时多次迭代中都更优先于 A，体现出来就是 B 抢回了资源。在 B used + pending >> capacity 且 idealAssigned < capacity 时，B 的 idealAssigned 会尽快收敛到 capacity，然后和 A 一起瓜分剩余资源。        \n5. 关于抢占的量，控制该值的配置主要是两个: yarn.resourcemanager.monitor.capacity.preemption.total\\_preemption\\_per\\_round 每轮总抢占 默认 0.1，表示最多每轮抢占的资源占集群总资源的比例。若每轮抢占总量超过此值，则按比例在每个队列中缩减； yarn.resourcemanager.monitor.capacity.preemption.natural\\_termination\\_factor 自然终止因子 默认 0.2。考虑到容器自然终止的情况,即使不杀死容器也能在 5 * maxWaitTime(15s) 内收回 95%的资源，所以使用(used - idealAssigned)乘以该因子作为队列内最终抢占资源量，以尽量减少对 已启动 application 的影响  \n6. 在 AM-RM 心跳协议 ApplicationMasterProtocol#allocate 中，AM 通过 AllocateRequest更新自己的需求，包括新增与释放 container。RM通过 AllocateResponse 响应请求，包括新分配的容器集合，释放的容器集合和<b>需要 AM 释放的被抢占容器集合</b>。抢占容器的消息类型分为两类:StrictPreemptionContract，AM 必须释放 RM 指定的容器；PreemptionContract。 AM 可以在满足同等大小资源的条件下灵活的替换 RM 指定的待抢占容器。目前 Yarn 采用的是 PreemptionContract类型，从 AM(用户)的角度来看这是可操作性很高的设计:待抢占的 worker 角色可能存储着 shuffle 文件、可能开启着重要的服务...... 恢复这些 worker 可能需要较多额外的工作，所以 RM 让 AM 去选择杀死哪些 container，只关注 AM 能不能还回等量的资源。但是抢占调度里面追踪着 RM 给 AM 指定的待抢占容器集合，如果 AM 替换掉这些容器，那么如果在抢占调度后续的周期发现这些追踪的容器存活大于 maxWaitTime，那么会直接杀死。这是一个有歧义的设计，目前 SparkOnYarn 的 YarnAllocator 没有对 RM 发给 AM 的待抢占容器集合做任何处理，即等同于等待容器超时被 RM 端杀死      \n7. <b>抢占得到的资源可能不足以任何资源请求，但是抢占依旧会进行</b>。控制每轮抢占立即得到的资源总量的参数参考 \"一些误区 5\"。假设集群中只有两个叶子队列 A和B，最低配额都是 50%。初始状态，A 完全空闲，B 负载很高 抢占了 A 的所有资源。A 队列突然被提交了大量应用，那么在第一轮抢占时 B 是超分配的队列，idealAssigned 初值为50%，A是借出所有资源的队列 idealAssigned 初值为 0，但是 idealAssigned/absCapacity 小于 B，是最欠分配的队列 迭代时每轮都优先分配A，A 能得到的资源为: B 队列 (used 100% - idealAssigned 50%) *自然终止因子 0.2 =10%，且不能超过 集群资源100% * 每轮总抢占比例 0.1 = 10%。抢占资源量可能不足以满足任何 ResourceRequest，对于大型集群来说这个问题可能不足为虑，但是在小集群上可能会有一些难以预知的后果，尤其是在小集群上很大的资源需求(e.g. 40G)       \n\n## 小集群大容器问题  \n\n### 问题场景 \n\n![](img/pictures/yarn_preempt/preempt_queue.png)   \n\n为简单起见，Yarn 使用默认配置且在不考虑: 虚拟核，AM/Worker 的区别和分配/保留的区别。集群有三台服务器，每台服务器有 80G 内存，共计有 240G 的内存资源。有两个叶子队列: A 和 B，最低配额都是 50%，最高配额都是 100%。当 A 负载较高而 B 空闲时，A 使用了整个集群的资源，A 中只有一个应用 app\\_A，启动的 container 都是 4G 大小。此时有新任务 app\\_B 提交到队列 B，且需要的资源量大于 B 的最低配额，则在第一次抢占时 B 队列会获得(240-120) * 0.2 自然终止因子 = 24G 的资源，且该轮抢占最大资源量不能超过 240 * 0.1 每轮抢占资源量比例 = 24G，这些抢占的 contaienr 均匀分布在集群的所有节点上，即图中红色 container 为被抢占，绿色 container 为未被抢占    \n \n- 场景 1: 假设 app\\_B 待分配的 container 大小为 40G，每个节点上只有 8G 的空闲资源，不足以启动该 container。且即使这些被抢占的 container 都分布在一个节点上，也不足以启动 40G JVM。 抢占虽然发生，但是 B 没有使用，最终这些抢占释放的资源会被 app\\_A 利用。这样就陷入了由周期性抢占导致的困局: 抢占调度 平衡资源-> B 无法利用资源-> A 可以利用资源->A 超分配,B 欠分配->抢占调度 平衡资源->.....  具体表现是: B 中的任务无法启动，A及app\\_A 中最近启动的容器一直处于 被抢占->启动成为最新->被抢占......的循环中  \n\n- 场景 2: 假设 app\\_B 待分配的 container 大小为 20G，每个节点上只有 8G 的空闲资源，不足以启动 20G 的 container，也会陷入周期性抢占导致的困局。但是如果这些被抢占的 container 分布在一个节点上，足以启动 20G 的 JVM。这是因为 JVM 大小不高于每轮抢占资源总量且不大于每台服务器内存总量的缘故    \n\n注: 常规调度器不希望一个容器占有节点太多的资源，所以对于大 container，不一定会做保留  \n[LeafQueue.java]\n\n```\nboolean shouldAllocOrReserveNewContainer(FiCaSchedulerApp application,\n      Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers > 0) {\n      float nodeFactor = \n          Resources.ratio(\n              resourceCalculator, required, getMaximumAllocation()\n              );      \n       // 使用所需节点的百分比来对大型容器施加偏差…\n       // 防止需要使用整个节点的极端情况\n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );  \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"needsContainers:\" +\n            \" app.#re-reserve=\" + application.getReReservations(priority) + \n            \" reserved=\" + reservedContainers + \n            \" nodeFactor=\" + nodeFactor + \n            \" minAllocFactor=\" + getMinimumAllocationFactor() +\n            \" starvation=\" + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) > 0);\n``` \n\n### 解决方案  \n\n<b>方案 1</b>   \n对于场景 1，可以从两个角度优化: 提高配置增加每轮抢占资源量及自然终止因子，以提高每轮抢占可以立即得到的资源量；规定应用程序单个容器的大小不能高于每轮抢占资源量。将场景 1 的问题转换问场景 2 的问题      \n对于场景 2，尽可能使被抢占容器分布在同一个节点上，为\"最小化影响 Application\"，可以在每个节点上按照设定的选取规则尝试抢占容器(但是并未真的杀死)以满足大容器并计算抢占代价(sum(currentTime-containerStartTime))，选择抢占代价最低的节点抢占。  \n \n \n优点:\n\n1. 可以提高启动大容器的可能性，减少大容器调度延时    \n\n缺点:  \n\n1. 增加每轮抢占资源量和自然终止因子,将增加集群和队列资源在平衡点附近抖动程度(队列释放->阶梯式下降,队列获取->阶梯式上升)。最好配合yarn.resourcemanager.monitor.capacity.preemption.max\\_ignored\\_over\\_capacity 属性使用。还是最小化影响原则，减小借用资源不多的队列的抖动      \n2. 提高抢占配置，提供能够启动大容器的资源量，将增加了被杀死的容器数量，扩大了影响范围，恢复任务产生的无效工作量也会增加      \n3. 尽可能在节点上抢占容器启动大容器，而不是在集群所有节点上抢占最晚启动的应用程序，导致这些节点资源利用率抖动极大  \n4. 由于抢占调度和常规调度解耦，抢占调度在节点上抢占出大容器的空间，常规调度也未必在该空间内分配大容器  \n  \n仅限于抢占频率较低的场景，并使用某些参量(欠分配队列 totalPending, minResourceRequest)来决定 走默认抢占逻辑还是集中在节点上释放资源      \n\n<b>方案 2</b>  \n\n[资源预订系统](https://jiulongzhu.github.io/2019/10/28/Yarn-ReservationSystem/)\n \n<b>方案 3</b>  \n标签系统：对节点进行分类或分组的一种方式，应用程序可以指定在特定标签的节点执行。可以配置资源队列可以访问的节点标签集合，应用程序只能提交到这些包含该标签的队列上来使用这些具有标签的节点的计算资源  \nyarn.scheduler.capacity.\\<queue\\_path\\>.capacity 属性是<b>默认标签</b>(无标签)的资源配置，所有的队列都可以访问无标签的节点；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels 属性用于指定队列可访问的标签节点，在不指定的情况下可以继承父队列的权限，若希望让某队列只能访问无标签节点，设置为空格即可；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels.\\<label\\>.capacity 指定队列可以使用该标签资源的最低配额；yarn.scheduler.capacity.\\<queue\\_path\\>.accessible-node-labels.\\<label\\>.maximum-capacity 指定队列可以使用该标签资源的最高配额     \n\n适用场景 \n \n1. 机构路由: BU A的任务，只能运行在具有 A 标签的 NM 上    \n2. 服务路由: Hbase 的数据分析任务，只能运行在具有 Hbase 标签的 NM 上    \n3. 特殊机器: 只有深度学习任务可以运行在具有 GPU 标签的 NM 上        \n\n新增或修改一些节点，配置其标签，并设置仅有某些队列具有该标签资源的使用权限，大内存应用程序专用    \n\n优点:\n  \n1. 可以运行大内存应用，且 SLA 高  \n\n缺点:  \n1. 标签资源空闲时其他应用无法使用，集群资源利用率相对较低  \n2. 需要额外的服务器成本  \n  \n## 相关参数\n\n- yarn.resourcemanager.scheduler.monitor.enable   \n设置为 true ,启用抢占  \n- yarn.resourcemanager.scheduler.monitor.policies  \n启用抢占时,抢占策略主类.RM 将配置的主类启动在独立线程中,周期性的执行抢占.默认是org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy  \n- yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval  \n周期性执行抢占策略的间隔,单位毫秒,默认是 3000  \n- yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill  \n自容器被标记为被抢占到抢占策略强制杀死该容器的时间,单位毫秒,默认15000.留给 AM 灵活杀死 container 的时间,如果希望快速收回资源可以适当调小    \n- yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round  \n每轮抢占最大资源总量占集群无标签资源的比例,默认 0.1. 即每轮抢占最多可以收回相当于集群无标签资源总量的10%,若每轮抢占资源量超过此值,则按比例在每个待抢占队列进行缩减    \n- yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity  \n忽略抢占的阈值,默认 0.1.当队列资源使用量 used > absCapacity(1+0.1)时,认为其应该还回资源,才会抢占容器.用于避免资源消耗和配额的剧烈波动,最小化影响      \n- yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor  \n自然终止因子,统计发现值为 0.5 或者为 0 时,都会在 5* 15秒(上述第三配置项)内收回 95%的资源,默认 0.2.为每个队列设置抢占目标后,再乘以此值作为最终的抢占目标.假设第一周期,某队列待抢占值为 10G,那么最终抢占为 2G.第二周期待抢占值为 7G,最终抢占为 1.4G...以一种平滑的方式回收,降低波动.可以适当增加此值以加快回收速度  \n\n## 一点思考 \n\n1. 方案设计时考虑了很多，从尝试借鉴 CPU 抢占、内存 SWAP 模式、用户体验、架构分层等角度思考设计一个抢占调度系统的想法。有时间再整理吧     \n2. 关于开源抢占系统通用优化的初步想法     \n调度系统抢占的容器先不杀死，而是打一个标记，比如:MARK\\_PREEMPTED\\_KILL,在资源分配时优先使用这些标记的节点或者资源紧缺时使用 以尽可能使容器能自然终止(类似 jvm软引用)，无资源分配时则不杀死。其一可以保证 在周期性抢占造成的困局中，队列使用不到这些资源时，这些资源不会被反复无效调度，而是继续运行任务；其二 或许能提高资源抢占相应速度，代价是需要完善常规调度的逻辑，分配时如何处理MARK\\_PREEMPTED\\_KILL，保留时如何处理MARK\\_PREEMPTED\\_KILL...    \n\n## 参考 \nhttps://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html  \nhttps://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption  \nhttps://www.aboutyun.com/thread-24628-1-1.html\n\n\n\n","slug":"2019-10-05-Yarn PreemptionSystem","published":1,"updated":"2020-06-16T06:40:51.649Z","comments":1,"photos":[],"link":"","_id":"cke2rms4g0002y7c7gjeh8i34","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Yarn 使用树状层级队列组织方式来管理资源,所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源,root可有有多个子队列,子队列也可以有自己的子队列,树状层级结构的最底层为叶子队列.除 root 外,每个子队列都会持有父队列一定比例的资源 即最低配额(Configured Capacity),以及最多可持有的资源 即最高配额(Configured Max Capacity).以上是 Yarn 的资源组织方式。<br>Yarn 采用两阶段资源调度模型.第一阶段 客户端向 ResourceManager 申请资源启动 ApplicationMaster 作为 Application 的资源协调和 task 监控角色;第二阶段 计算框架驱动角色通过 AppliationMaster 向 ResourceManager 申请资源运行 Worker 角色。 ApplicationMaster 和 Worker角色资源申请及分配都是异步的过程,当 NodeManager 向 ResourceManager 汇报心跳时,附带了自身正在运行和已经完成的 Container 信息,并触发了资源调度器的 NodeUpdate 事件,调度器通过以下过程尝试分配资源     </p>\n<ol>\n<li>调度器委派 RootQueue 尝试进行资源分配  </li>\n<li>RootQueue 委派其 ChildQueue(直至 LeafQueue) 尝试进行资源分配  </li>\n<li>遍历 Application 的 ResourceRequest,尝试进行分配或保留  </li>\n<li>若分配或保留成功,则同步 Queue/Application/NodeManager 信息.若分配成功,则触发 Container 的转变为 ALLOCATED 状态:RM 已经分配了资源给 APP(Attempt),但是 AM 此时还不知道,直至 AM 和 RM 的下次心跳时 AM 将对此 Container 做调度:下载依赖,准备运行环境,启动主类等;若保留成功,则该节点后续上报心跳时,将会跳过其分配调度,直至其可用资源量达到了保留资源的要求,则被保留的 Container 将会由 RESERVED 转换为 ALLOCATED 状态.保留资源固然在某种程度上算是浪费,但是保证了大 container 的 application 不会饿死      </li>\n<li>NM 监控自身运行的所有 Container 状态,对于新启动和运行状态的 Container 以及运行完成的 Container 都会通过心跳告知 RM,RM 同步信息,更新各 Container 状态.若 Container 已经完成则在 RM 端同步 Queue/Application/NodeManager 信息<br>以上是 Yarn 常规调度机制。<br>层级队列中除 root 外,每个队列可以设置其拥有父队列的最低配额和最高配额.最高配额是队列无论在任何情况下都不会超出的用量,最低配额是在队列有任务时尽可能保证的配额.为了提高集群整体的资源利用率,Yarn 引入了抢占机制:  </li>\n<li>在队列中无任务运行时,队列没有必要硬遵守保留最低配额,可以将资源借给负载较高的其他队列     </li>\n<li>当借出资源的队列接收到了新提交的应用时,将出借的资源收回以满足资源需求<br>以上是Yarn 抢占调度机制。第 1 点由 Yarn 资源调度器来实现;第 2 点只有调度器实现了 PreemptableResourceScheduler 接口,且抢占策略实现了SchedulingEditPolicy时才能实现<br>本文侧重于第 2 点,并试图解决一些在小集群申请大容器的问题    </li>\n</ol>\n<a id=\"more\"></a>\n\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>当队列 A 负载比较大时,可以临时借用负载较低队列 B 的资源.但如果有任务提交到 B 队列上,调度平台处理 B 借出的资源的方式一般有三种      </p>\n<table>\n<thead>\n<tr>\n<th>处理方式</th>\n<th>B 队列应用延时</th>\n<th>A 队列应用恢复代价</th>\n<th>浪费/重复工作量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>等待应用完成</td>\n<td>高</td>\n<td>无</td>\n<td>无</td>\n</tr>\n<tr>\n<td>杀死应用</td>\n<td>无</td>\n<td>高</td>\n<td>高</td>\n</tr>\n<tr>\n<td>杀死应用部分容器</td>\n<td>无</td>\n<td>低</td>\n<td>一般</td>\n</tr>\n</tbody></table>\n<p>对于借出资源的处理,传统做法一般是等待 A 队列应用完成,将相应的资源还回,这样对于 B 队列应用程序来说有较高的延迟;直接杀死 A 队列应用是一种比较直接的做法,这对于一些特殊服务的影响很大 e.g. 7*24 的流式应用,优先级较高的服务(设计较好的公司会将这部分放在单独的队列甚至单独集群中以保证高可用).Yarn 抢占机制试图不彻底杀死整个应用,在考虑优先级和浪费工作量的情况下,从超分配队列中选出一些容器,释放掉资源达到还回原队列的目的   </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>源码解析主要参照:  <a href=\"https://www.aboutyun.com/thread-24628-1-1.html\" target=\"_blank\" rel=\"noopener\">https://www.aboutyun.com/thread-24628-1-1.html</a> ,与 Hadoop2.7.3版本有两处差异:<br>1.Hadoop2.7.3 版本在 TempQueue 中加入了untouchableExtra变量,用以保存在队列不允许抢占时的(used absCapacity)值<br>2.Hadoop2.7.3 版本在 计算各队列瓜分空闲资源时加了一些逻辑,在ProportionalCapacityPreemptionPolicy#computeFixpointAllocation中      </p>\n<p>computeFixpointAllocation是抢占调度中最核心的逻辑: 计算每个队列的理想容量,决定了是否在该队列抢占及抢占数量的问题<br>[ProportionalCapacityPreemptionPolicy.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\">* 重新计算每个子队列的理想配额,存储于每个子队列的 idealAssigned 成员变量中</span><br><span class=\"line\">* tot_guarant:父队列的理想配额</span><br><span class=\"line\">* qAlloc:父队列的子队列</span><br><span class=\"line\">* unassigned: tot_guarant 的 clone 对象,父队列的理想配额.将此配额分配给所有子队列</span><br><span class=\"line\">* ignoreGuarantee:该批次子队列是否配置了最低配额.将会影响到每个队列在分配空闲配额时的权重.</span><br><span class=\"line\">* 若为 true,没有配置最低配额,则每个队列均分空闲配额</span><br><span class=\"line\">* 若为 false,配置了最低配额,则每个队列按 最低配额&#x2F;sum(最低配额) 的权重分配最低配额  </span><br><span class=\"line\">*&#x2F;</span><br><span class=\"line\">private void computeFixpointAllocation(ResourceCalculator rc,</span><br><span class=\"line\">      Resource tot_guarant, Collection&lt;TempQueue&gt; qAlloc, Resource unassigned, </span><br><span class=\"line\">      boolean ignoreGuarantee) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 按照 欠分配的程度,做排序,下文引用了类内容</span><br><span class=\"line\">    TQComparator tqComparator &#x3D; new TQComparator(rc, tot_guarant);</span><br><span class=\"line\">    PriorityQueue&lt;TempQueue&gt; orderedByNeed &#x3D;</span><br><span class=\"line\">                                 new PriorityQueue&lt;TempQueue&gt;(10,tqComparator);</span><br><span class=\"line\">    for (Iterator&lt;TempQueue&gt; i &#x3D; qAlloc.iterator(); i.hasNext();) &#123;</span><br><span class=\"line\">      TempQueue q &#x3D; i.next();</span><br><span class=\"line\">      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) &#123;</span><br><span class=\"line\">       &#x2F;&#x2F; 在配置了队列可抢占后,untouchableExtra&#x3D;0;若队列不可抢占,则超出最低配额的量算到理想配额内</span><br><span class=\"line\">        q.idealAssigned &#x3D; Resources.add(q.guaranteed, q.untouchableExtra);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        q.idealAssigned &#x3D; Resources.clone(q.current);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      Resources.subtractFrom(unassigned, q.idealAssigned);</span><br><span class=\"line\">      Resource curPlusPend &#x3D; Resources.add(q.current, q.pending);</span><br><span class=\"line\">      &#x2F;&#x2F; 欠分配的队列</span><br><span class=\"line\">      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) &#123;</span><br><span class=\"line\">        orderedByNeed.add(q);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 欠分配的队列 瓜分 空闲配额  </span><br><span class=\"line\">    while (!orderedByNeed.isEmpty()</span><br><span class=\"line\">       &amp;&amp; Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) &#123;</span><br><span class=\"line\">      Resource wQassigned &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">      &#x2F;&#x2F; 计算每个队列应得的权重  </span><br><span class=\"line\">      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);</span><br><span class=\"line\">      &#x2F;&#x2F; 使用tqComparator得到最欠分配的队列  依据是 idealAssigned&#x2F;absCapacity</span><br><span class=\"line\">      &#x2F;&#x2F;  超分配队列初始值为 1，欠分配队列初始小于 1 </span><br><span class=\"line\">      Collection&lt;TempQueue&gt; underserved &#x3D;</span><br><span class=\"line\">          getMostUnderservedQueues(orderedByNeed, tqComparator);</span><br><span class=\"line\">      for (Iterator&lt;TempQueue&gt; i &#x3D; underserved.iterator(); i.hasNext();) &#123;</span><br><span class=\"line\">        TempQueue sub &#x3D; i.next();</span><br><span class=\"line\">       &#x2F;&#x2F; 当前轮次, 空闲配额*权重  </span><br><span class=\"line\">        Resource wQavail &#x3D; Resources.multiplyAndNormalizeUp(rc,</span><br><span class=\"line\">            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));</span><br><span class=\"line\">        &#x2F;&#x2F;  min&#123;空闲配额*权重,最高配额-理想配额,需求配额&#125;做理想配额的增量;下文引用了 offer 方法  </span><br><span class=\"line\">        Resource wQidle &#x3D; sub.offer(wQavail, rc, tot_guarant);</span><br><span class=\"line\">        &#x2F;&#x2F; 队列得到的最终值,即上面的 min&#123;三元组&#125;</span><br><span class=\"line\">        Resource wQdone &#x3D; Resources.subtract(wQavail, wQidle);</span><br><span class=\"line\">\t    &#x2F;&#x2F; 在该轮得到了资源,那么可能还能得到资源,需要参加下一轮迭代;如果没得到资源,那么就不会再得到资源了(等于最高配额或满足了需求)</span><br><span class=\"line\">        if (Resources.greaterThan(rc, tot_guarant,</span><br><span class=\"line\">              wQdone, Resources.none())) &#123;</span><br><span class=\"line\">          orderedByNeed.add(sub);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Resources.addTo(wQassigned, wQdone);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 经过这一轮迭代后,全局空闲资源变化</span><br><span class=\"line\">      Resources.subtractFrom(unassigned, wQassigned);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  Resource offer(Resource avail, ResourceCalculator rc,</span><br><span class=\"line\">        Resource clusterResource) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 最大配额-理想配额:表示该队列最多只能拿到这么多额外资源 </span><br><span class=\"line\">      Resource absMaxCapIdealAssignedDelta &#x3D; Resources.componentwiseMax(</span><br><span class=\"line\">                      Resources.subtract(maxCapacity, idealAssigned),</span><br><span class=\"line\">                      Resource.newInstance(0, 0));</span><br><span class=\"line\">      Resource accepted &#x3D; </span><br><span class=\"line\">          Resources.min(rc, clusterResource, </span><br><span class=\"line\">              absMaxCapIdealAssignedDelta,</span><br><span class=\"line\">              &#x2F;&#x2F; avail:是传入值, 全局空闲资源*队列权重,表明在当前迭代 最多可以给的空闲资源</span><br><span class=\"line\">          Resources.min(rc, clusterResource, avail, </span><br><span class=\"line\">           &#x2F;&#x2F; current+pending-idealAssigned:表示 满足队列中所有应用程序需要的 额外资源 </span><br><span class=\"line\">          Resources.subtract(</span><br><span class=\"line\">              Resources.add(current, pending), idealAssigned)));</span><br><span class=\"line\">              </span><br><span class=\"line\">      Resource remain &#x3D; Resources.subtract(avail, accepted);</span><br><span class=\"line\">      &#x2F;&#x2F; 修改当前队列的理想配额  </span><br><span class=\"line\">      Resources.addTo(idealAssigned, accepted);</span><br><span class=\"line\">      return remain;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>欠分配比较器<br>[TQComparator.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static class TQComparator implements Comparator&lt;TempQueue&gt; &#123;</span><br><span class=\"line\">    private ResourceCalculator rc;</span><br><span class=\"line\">    private Resource clusterRes;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public int compare(TempQueue tq1, TempQueue tq2) &#123;</span><br><span class=\"line\">      if (getIdealPctOfGuaranteed(tq1) &lt; getIdealPctOfGuaranteed(tq2)) &#123;</span><br><span class=\"line\">        return -1;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (getIdealPctOfGuaranteed(tq1) &gt; getIdealPctOfGuaranteed(tq2)) &#123;</span><br><span class=\"line\">        return 1;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return 0;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 计算 idealAssigned &#x2F; guaranteed</span><br><span class=\"line\">    private double getIdealPctOfGuaranteed(TempQueue q) &#123;</span><br><span class=\"line\">      double pctOver &#x3D; Integer.MAX_VALUE;</span><br><span class=\"line\">      if (q !&#x3D; null &amp;&amp; Resources.greaterThan(</span><br><span class=\"line\">          rc, clusterRes, q.guaranteed, Resources.none())) &#123;</span><br><span class=\"line\">        pctOver &#x3D;</span><br><span class=\"line\">            Resources.divide(rc, clusterRes, q.idealAssigned, q.guaranteed);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return (pctOver);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"抢占流程\"><a href=\"#抢占流程\" class=\"headerlink\" title=\"抢占流程\"></a>抢占流程</h2><p>第一步: 递归获取所有队列的快照信息，重点包括 used/pending/absCapacity/maxAbsCapacity 信息<br>第二步: 设置 root 队列的初始 idealAssigned 为 absCapacity(100%)，以此为依据将 root 队列的 idealAssigned 分摊给各层队列,即第三步<br>第三步: 根据是否配置了最低配额将所有子队列分为两类，优先满足配置了最低配额的队列需求，然后剩余配额满足未配置最低配额队列的需求。对这两类队列进行资源再平衡 迭代计算出每个子队列的理想配额，即第四步。如果子队列的 used &gt; absCapacity * (1+maxIgnoredOverCapacity)，则需要从其中掠夺资源，即第五步<br>第四步: 首先按照当前使用状态(min{used,capacity})，计算出全局空闲资源量(弹性)，获得欠分配的队列；对欠分配的队列集合进行迭代以求出每个队列的 idealAssigned，在每一轮迭代中，设置每个欠分配队列所占的权重(capacity 为依据)，该权重影响队列在获取空闲资源时的增量；优先对欠分配程度最高的队列进行分配，配额为 min{全局空闲资源量 * 队列权重, maxAbsCapacity-idealAssigned, used+pending-idealAssigned},三元组中第一个值表示可分配给队列的额外配额，第二个值表示在不超过最高配额的条件下能接受的最多额外配额，第三个值表示满足应用程序资源需求的额外配额。若满足了队列的需求则将其剔除出欠分配的队列集合<br>第五步: 如果队列 used &gt; absCapacity * (1+maxIgnoredOverCapacity),那么将会释放掉 (used - idealAssigned) * naturalTerminationFactor (自然终止因子,默认0.2)的资源,则按照 <b>最小化影响 application </b>的原则(maxIgnoredOverCapacity 和 naturalTerminationFactor 都是遵从该原则)，优先释放最晚启动的 application 中的最晚分配的container，优先释放 reserved contaienr，优先释放非 AM 的 container。向常规调度器发送抢占 container 事件，并追踪这些 container 的存活时间，如果在超过了 maxWaitTime(15s)之后还没有释放，则强制常规调度器杀死   </p>\n<h2 id=\"一些误区\"><a href=\"#一些误区\" class=\"headerlink\" title=\"一些误区\"></a>一些误区</h2><ol>\n<li><b>最小化影响 application 原则是抢占过程中最优先考虑的原则</b>        </li>\n<li>抢占调度仅仅是所有队列的资源再平衡过程，<b>不为具体的应用程序调度，不为某个容器做分配或保留</b>。从设计模式原则的角度来看是合理的；抢占调度和常规调度松耦合，仅仅使用总线向常规调度器单向发送抢占/杀死容器事件，从架构分层分模块的角度来看也是合理的      </li>\n<li>抢占调度<b>不会平衡队列内部的用户资源使用量</b>，用户资源量的平衡应使用 user limit percent 或者 Fair 去做。e.g. user1 先向队列提交了 10 个 application,user2 后向队列提交了 2 个 application ,在抢占调度时 不会把 user1 占用的资源平衡给 user2 使用;且 如果队列要释放资源,那么由于 user2 的 application 后提交,反而会先于 user1 的 application 被抢占    </li>\n<li>抢占调度<b>不会使繁忙队列的资源达到最高配额，且不会使空闲队列在任意条件下都得到最低配额</b>。比如,队列 A 占用了队列 B 的所有资源，那么当 B 上有任务提交时，是 A 和 B 一起抢占 B 的最低配额资源，只不过 B 由于欠分配程度(idealAssigned/absCapacity)比较高，在抢占时多次迭代中都更优先于 A，体现出来就是 B 抢回了资源。在 B used + pending &gt;&gt; capacity 且 idealAssigned &lt; capacity 时，B 的 idealAssigned 会尽快收敛到 capacity，然后和 A 一起瓜分剩余资源。        </li>\n<li>关于抢占的量，控制该值的配置主要是两个: yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round 每轮总抢占 默认 0.1，表示最多每轮抢占的资源占集群总资源的比例。若每轮抢占总量超过此值，则按比例在每个队列中缩减； yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor 自然终止因子 默认 0.2。考虑到容器自然终止的情况,即使不杀死容器也能在 5 * maxWaitTime(15s) 内收回 95%的资源，所以使用(used - idealAssigned)乘以该因子作为队列内最终抢占资源量，以尽量减少对 已启动 application 的影响  </li>\n<li>在 AM-RM 心跳协议 ApplicationMasterProtocol#allocate 中，AM 通过 AllocateRequest更新自己的需求，包括新增与释放 container。RM通过 AllocateResponse 响应请求，包括新分配的容器集合，释放的容器集合和<b>需要 AM 释放的被抢占容器集合</b>。抢占容器的消息类型分为两类:StrictPreemptionContract，AM 必须释放 RM 指定的容器；PreemptionContract。 AM 可以在满足同等大小资源的条件下灵活的替换 RM 指定的待抢占容器。目前 Yarn 采用的是 PreemptionContract类型，从 AM(用户)的角度来看这是可操作性很高的设计:待抢占的 worker 角色可能存储着 shuffle 文件、可能开启着重要的服务…… 恢复这些 worker 可能需要较多额外的工作，所以 RM 让 AM 去选择杀死哪些 container，只关注 AM 能不能还回等量的资源。但是抢占调度里面追踪着 RM 给 AM 指定的待抢占容器集合，如果 AM 替换掉这些容器，那么如果在抢占调度后续的周期发现这些追踪的容器存活大于 maxWaitTime，那么会直接杀死。这是一个有歧义的设计，目前 SparkOnYarn 的 YarnAllocator 没有对 RM 发给 AM 的待抢占容器集合做任何处理，即等同于等待容器超时被 RM 端杀死      </li>\n<li><b>抢占得到的资源可能不足以任何资源请求，但是抢占依旧会进行</b>。控制每轮抢占立即得到的资源总量的参数参考 “一些误区 5”。假设集群中只有两个叶子队列 A和B，最低配额都是 50%。初始状态，A 完全空闲，B 负载很高 抢占了 A 的所有资源。A 队列突然被提交了大量应用，那么在第一轮抢占时 B 是超分配的队列，idealAssigned 初值为50%，A是借出所有资源的队列 idealAssigned 初值为 0，但是 idealAssigned/absCapacity 小于 B，是最欠分配的队列 迭代时每轮都优先分配A，A 能得到的资源为: B 队列 (used 100% - idealAssigned 50%) *自然终止因子 0.2 =10%，且不能超过 集群资源100% * 每轮总抢占比例 0.1 = 10%。抢占资源量可能不足以满足任何 ResourceRequest，对于大型集群来说这个问题可能不足为虑，但是在小集群上可能会有一些难以预知的后果，尤其是在小集群上很大的资源需求(e.g. 40G)       </li>\n</ol>\n<h2 id=\"小集群大容器问题\"><a href=\"#小集群大容器问题\" class=\"headerlink\" title=\"小集群大容器问题\"></a>小集群大容器问题</h2><h3 id=\"问题场景\"><a href=\"#问题场景\" class=\"headerlink\" title=\"问题场景\"></a>问题场景</h3><p><img src=\"img/pictures/yarn_preempt/preempt_queue.png\" alt=\"\">   </p>\n<p>为简单起见，Yarn 使用默认配置且在不考虑: 虚拟核，AM/Worker 的区别和分配/保留的区别。集群有三台服务器，每台服务器有 80G 内存，共计有 240G 的内存资源。有两个叶子队列: A 和 B，最低配额都是 50%，最高配额都是 100%。当 A 负载较高而 B 空闲时，A 使用了整个集群的资源，A 中只有一个应用 app_A，启动的 container 都是 4G 大小。此时有新任务 app_B 提交到队列 B，且需要的资源量大于 B 的最低配额，则在第一次抢占时 B 队列会获得(240-120) * 0.2 自然终止因子 = 24G 的资源，且该轮抢占最大资源量不能超过 240 * 0.1 每轮抢占资源量比例 = 24G，这些抢占的 contaienr 均匀分布在集群的所有节点上，即图中红色 container 为被抢占，绿色 container 为未被抢占    </p>\n<ul>\n<li><p>场景 1: 假设 app_B 待分配的 container 大小为 40G，每个节点上只有 8G 的空闲资源，不足以启动该 container。且即使这些被抢占的 container 都分布在一个节点上，也不足以启动 40G JVM。 抢占虽然发生，但是 B 没有使用，最终这些抢占释放的资源会被 app_A 利用。这样就陷入了由周期性抢占导致的困局: 抢占调度 平衡资源-&gt; B 无法利用资源-&gt; A 可以利用资源-&gt;A 超分配,B 欠分配-&gt;抢占调度 平衡资源-&gt;…..  具体表现是: B 中的任务无法启动，A及app_A 中最近启动的容器一直处于 被抢占-&gt;启动成为最新-&gt;被抢占……的循环中  </p>\n</li>\n<li><p>场景 2: 假设 app_B 待分配的 container 大小为 20G，每个节点上只有 8G 的空闲资源，不足以启动 20G 的 container，也会陷入周期性抢占导致的困局。但是如果这些被抢占的 container 分布在一个节点上，足以启动 20G 的 JVM。这是因为 JVM 大小不高于每轮抢占资源总量且不大于每台服务器内存总量的缘故    </p>\n</li>\n</ul>\n<p>注: 常规调度器不希望一个容器占有节点太多的资源，所以对于大 container，不一定会做保留<br>[LeafQueue.java]</p>\n<pre><code>boolean shouldAllocOrReserveNewContainer(FiCaSchedulerApp application,\n      Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers &gt; 0) {\n      float nodeFactor = \n          Resources.ratio(\n              resourceCalculator, required, getMaximumAllocation()\n              );      \n       // 使用所需节点的百分比来对大型容器施加偏差…\n       // 防止需要使用整个节点的极端情况\n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );  \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(&quot;needsContainers:&quot; +\n            &quot; app.#re-reserve=&quot; + application.getReReservations(priority) + \n            &quot; reserved=&quot; + reservedContainers + \n            &quot; nodeFactor=&quot; + nodeFactor + \n            &quot; minAllocFactor=&quot; + getMinimumAllocationFactor() +\n            &quot; starvation=&quot; + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) &gt; 0);</code></pre><h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p><b>方案 1</b><br>对于场景 1，可以从两个角度优化: 提高配置增加每轮抢占资源量及自然终止因子，以提高每轮抢占可以立即得到的资源量；规定应用程序单个容器的大小不能高于每轮抢占资源量。将场景 1 的问题转换问场景 2 的问题<br>对于场景 2，尽可能使被抢占容器分布在同一个节点上，为”最小化影响 Application”，可以在每个节点上按照设定的选取规则尝试抢占容器(但是并未真的杀死)以满足大容器并计算抢占代价(sum(currentTime-containerStartTime))，选择抢占代价最低的节点抢占。  </p>\n<p>优点:</p>\n<ol>\n<li>可以提高启动大容器的可能性，减少大容器调度延时    </li>\n</ol>\n<p>缺点:  </p>\n<ol>\n<li>增加每轮抢占资源量和自然终止因子,将增加集群和队列资源在平衡点附近抖动程度(队列释放-&gt;阶梯式下降,队列获取-&gt;阶梯式上升)。最好配合yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity 属性使用。还是最小化影响原则，减小借用资源不多的队列的抖动      </li>\n<li>提高抢占配置，提供能够启动大容器的资源量，将增加了被杀死的容器数量，扩大了影响范围，恢复任务产生的无效工作量也会增加      </li>\n<li>尽可能在节点上抢占容器启动大容器，而不是在集群所有节点上抢占最晚启动的应用程序，导致这些节点资源利用率抖动极大  </li>\n<li>由于抢占调度和常规调度解耦，抢占调度在节点上抢占出大容器的空间，常规调度也未必在该空间内分配大容器  </li>\n</ol>\n<p>仅限于抢占频率较低的场景，并使用某些参量(欠分配队列 totalPending, minResourceRequest)来决定 走默认抢占逻辑还是集中在节点上释放资源      </p>\n<p><b>方案 2</b>  </p>\n<p><a href=\"https://jiulongzhu.github.io/2019/10/28/Yarn-ReservationSystem/\">资源预订系统</a></p>\n<p><b>方案 3</b><br>标签系统：对节点进行分类或分组的一种方式，应用程序可以指定在特定标签的节点执行。可以配置资源队列可以访问的节点标签集合，应用程序只能提交到这些包含该标签的队列上来使用这些具有标签的节点的计算资源<br>yarn.scheduler.capacity.&lt;queue_path&gt;.capacity 属性是<b>默认标签</b>(无标签)的资源配置，所有的队列都可以访问无标签的节点；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels 属性用于指定队列可访问的标签节点，在不指定的情况下可以继承父队列的权限，若希望让某队列只能访问无标签节点，设置为空格即可；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels.&lt;label&gt;.capacity 指定队列可以使用该标签资源的最低配额；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels.&lt;label&gt;.maximum-capacity 指定队列可以使用该标签资源的最高配额     </p>\n<p>适用场景 </p>\n<ol>\n<li>机构路由: BU A的任务，只能运行在具有 A 标签的 NM 上    </li>\n<li>服务路由: Hbase 的数据分析任务，只能运行在具有 Hbase 标签的 NM 上    </li>\n<li>特殊机器: 只有深度学习任务可以运行在具有 GPU 标签的 NM 上        </li>\n</ol>\n<p>新增或修改一些节点，配置其标签，并设置仅有某些队列具有该标签资源的使用权限，大内存应用程序专用    </p>\n<p>优点:</p>\n<ol>\n<li>可以运行大内存应用，且 SLA 高  </li>\n</ol>\n<p>缺点:  </p>\n<ol>\n<li>标签资源空闲时其他应用无法使用，集群资源利用率相对较低  </li>\n<li>需要额外的服务器成本  </li>\n</ol>\n<h2 id=\"相关参数\"><a href=\"#相关参数\" class=\"headerlink\" title=\"相关参数\"></a>相关参数</h2><ul>\n<li>yarn.resourcemanager.scheduler.monitor.enable<br>设置为 true ,启用抢占  </li>\n<li>yarn.resourcemanager.scheduler.monitor.policies<br>启用抢占时,抢占策略主类.RM 将配置的主类启动在独立线程中,周期性的执行抢占.默认是org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy  </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval<br>周期性执行抢占策略的间隔,单位毫秒,默认是 3000  </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill<br>自容器被标记为被抢占到抢占策略强制杀死该容器的时间,单位毫秒,默认15000.留给 AM 灵活杀死 container 的时间,如果希望快速收回资源可以适当调小    </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round<br>每轮抢占最大资源总量占集群无标签资源的比例,默认 0.1. 即每轮抢占最多可以收回相当于集群无标签资源总量的10%,若每轮抢占资源量超过此值,则按比例在每个待抢占队列进行缩减    </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity<br>忽略抢占的阈值,默认 0.1.当队列资源使用量 used &gt; absCapacity(1+0.1)时,认为其应该还回资源,才会抢占容器.用于避免资源消耗和配额的剧烈波动,最小化影响      </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor<br>自然终止因子,统计发现值为 0.5 或者为 0 时,都会在 5* 15秒(上述第三配置项)内收回 95%的资源,默认 0.2.为每个队列设置抢占目标后,再乘以此值作为最终的抢占目标.假设第一周期,某队列待抢占值为 10G,那么最终抢占为 2G.第二周期待抢占值为 7G,最终抢占为 1.4G…以一种平滑的方式回收,降低波动.可以适当增加此值以加快回收速度  </li>\n</ul>\n<h2 id=\"一点思考\"><a href=\"#一点思考\" class=\"headerlink\" title=\"一点思考\"></a>一点思考</h2><ol>\n<li>方案设计时考虑了很多，从尝试借鉴 CPU 抢占、内存 SWAP 模式、用户体验、架构分层等角度思考设计一个抢占调度系统的想法。有时间再整理吧     </li>\n<li>关于开源抢占系统通用优化的初步想法<br>调度系统抢占的容器先不杀死，而是打一个标记，比如:MARK_PREEMPTED_KILL,在资源分配时优先使用这些标记的节点或者资源紧缺时使用 以尽可能使容器能自然终止(类似 jvm软引用)，无资源分配时则不杀死。其一可以保证 在周期性抢占造成的困局中，队列使用不到这些资源时，这些资源不会被反复无效调度，而是继续运行任务；其二 或许能提高资源抢占相应速度，代价是需要完善常规调度的逻辑，分配时如何处理MARK_PREEMPTED_KILL，保留时如何处理MARK_PREEMPTED_KILL…    </li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html</a><br><a href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption</a><br><a href=\"https://www.aboutyun.com/thread-24628-1-1.html\" target=\"_blank\" rel=\"noopener\">https://www.aboutyun.com/thread-24628-1-1.html</a></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>Yarn 使用树状层级队列组织方式来管理资源,所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源,root可有有多个子队列,子队列也可以有自己的子队列,树状层级结构的最底层为叶子队列.除 root 外,每个子队列都会持有父队列一定比例的资源 即最低配额(Configured Capacity),以及最多可持有的资源 即最高配额(Configured Max Capacity).以上是 Yarn 的资源组织方式。<br>Yarn 采用两阶段资源调度模型.第一阶段 客户端向 ResourceManager 申请资源启动 ApplicationMaster 作为 Application 的资源协调和 task 监控角色;第二阶段 计算框架驱动角色通过 AppliationMaster 向 ResourceManager 申请资源运行 Worker 角色。 ApplicationMaster 和 Worker角色资源申请及分配都是异步的过程,当 NodeManager 向 ResourceManager 汇报心跳时,附带了自身正在运行和已经完成的 Container 信息,并触发了资源调度器的 NodeUpdate 事件,调度器通过以下过程尝试分配资源     </p>\n<ol>\n<li>调度器委派 RootQueue 尝试进行资源分配  </li>\n<li>RootQueue 委派其 ChildQueue(直至 LeafQueue) 尝试进行资源分配  </li>\n<li>遍历 Application 的 ResourceRequest,尝试进行分配或保留  </li>\n<li>若分配或保留成功,则同步 Queue/Application/NodeManager 信息.若分配成功,则触发 Container 的转变为 ALLOCATED 状态:RM 已经分配了资源给 APP(Attempt),但是 AM 此时还不知道,直至 AM 和 RM 的下次心跳时 AM 将对此 Container 做调度:下载依赖,准备运行环境,启动主类等;若保留成功,则该节点后续上报心跳时,将会跳过其分配调度,直至其可用资源量达到了保留资源的要求,则被保留的 Container 将会由 RESERVED 转换为 ALLOCATED 状态.保留资源固然在某种程度上算是浪费,但是保证了大 container 的 application 不会饿死      </li>\n<li>NM 监控自身运行的所有 Container 状态,对于新启动和运行状态的 Container 以及运行完成的 Container 都会通过心跳告知 RM,RM 同步信息,更新各 Container 状态.若 Container 已经完成则在 RM 端同步 Queue/Application/NodeManager 信息<br>以上是 Yarn 常规调度机制。<br>层级队列中除 root 外,每个队列可以设置其拥有父队列的最低配额和最高配额.最高配额是队列无论在任何情况下都不会超出的用量,最低配额是在队列有任务时尽可能保证的配额.为了提高集群整体的资源利用率,Yarn 引入了抢占机制:  </li>\n<li>在队列中无任务运行时,队列没有必要硬遵守保留最低配额,可以将资源借给负载较高的其他队列     </li>\n<li>当借出资源的队列接收到了新提交的应用时,将出借的资源收回以满足资源需求<br>以上是Yarn 抢占调度机制。第 1 点由 Yarn 资源调度器来实现;第 2 点只有调度器实现了 PreemptableResourceScheduler 接口,且抢占策略实现了SchedulingEditPolicy时才能实现<br>本文侧重于第 2 点,并试图解决一些在小集群申请大容器的问题    </li>\n</ol>","more":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>当队列 A 负载比较大时,可以临时借用负载较低队列 B 的资源.但如果有任务提交到 B 队列上,调度平台处理 B 借出的资源的方式一般有三种      </p>\n<table>\n<thead>\n<tr>\n<th>处理方式</th>\n<th>B 队列应用延时</th>\n<th>A 队列应用恢复代价</th>\n<th>浪费/重复工作量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>等待应用完成</td>\n<td>高</td>\n<td>无</td>\n<td>无</td>\n</tr>\n<tr>\n<td>杀死应用</td>\n<td>无</td>\n<td>高</td>\n<td>高</td>\n</tr>\n<tr>\n<td>杀死应用部分容器</td>\n<td>无</td>\n<td>低</td>\n<td>一般</td>\n</tr>\n</tbody></table>\n<p>对于借出资源的处理,传统做法一般是等待 A 队列应用完成,将相应的资源还回,这样对于 B 队列应用程序来说有较高的延迟;直接杀死 A 队列应用是一种比较直接的做法,这对于一些特殊服务的影响很大 e.g. 7*24 的流式应用,优先级较高的服务(设计较好的公司会将这部分放在单独的队列甚至单独集群中以保证高可用).Yarn 抢占机制试图不彻底杀死整个应用,在考虑优先级和浪费工作量的情况下,从超分配队列中选出一些容器,释放掉资源达到还回原队列的目的   </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>源码解析主要参照:  <a href=\"https://www.aboutyun.com/thread-24628-1-1.html\" target=\"_blank\" rel=\"noopener\">https://www.aboutyun.com/thread-24628-1-1.html</a> ,与 Hadoop2.7.3版本有两处差异:<br>1.Hadoop2.7.3 版本在 TempQueue 中加入了untouchableExtra变量,用以保存在队列不允许抢占时的(used absCapacity)值<br>2.Hadoop2.7.3 版本在 计算各队列瓜分空闲资源时加了一些逻辑,在ProportionalCapacityPreemptionPolicy#computeFixpointAllocation中      </p>\n<p>computeFixpointAllocation是抢占调度中最核心的逻辑: 计算每个队列的理想容量,决定了是否在该队列抢占及抢占数量的问题<br>[ProportionalCapacityPreemptionPolicy.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\">* 重新计算每个子队列的理想配额,存储于每个子队列的 idealAssigned 成员变量中</span><br><span class=\"line\">* tot_guarant:父队列的理想配额</span><br><span class=\"line\">* qAlloc:父队列的子队列</span><br><span class=\"line\">* unassigned: tot_guarant 的 clone 对象,父队列的理想配额.将此配额分配给所有子队列</span><br><span class=\"line\">* ignoreGuarantee:该批次子队列是否配置了最低配额.将会影响到每个队列在分配空闲配额时的权重.</span><br><span class=\"line\">* 若为 true,没有配置最低配额,则每个队列均分空闲配额</span><br><span class=\"line\">* 若为 false,配置了最低配额,则每个队列按 最低配额&#x2F;sum(最低配额) 的权重分配最低配额  </span><br><span class=\"line\">*&#x2F;</span><br><span class=\"line\">private void computeFixpointAllocation(ResourceCalculator rc,</span><br><span class=\"line\">      Resource tot_guarant, Collection&lt;TempQueue&gt; qAlloc, Resource unassigned, </span><br><span class=\"line\">      boolean ignoreGuarantee) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 按照 欠分配的程度,做排序,下文引用了类内容</span><br><span class=\"line\">    TQComparator tqComparator &#x3D; new TQComparator(rc, tot_guarant);</span><br><span class=\"line\">    PriorityQueue&lt;TempQueue&gt; orderedByNeed &#x3D;</span><br><span class=\"line\">                                 new PriorityQueue&lt;TempQueue&gt;(10,tqComparator);</span><br><span class=\"line\">    for (Iterator&lt;TempQueue&gt; i &#x3D; qAlloc.iterator(); i.hasNext();) &#123;</span><br><span class=\"line\">      TempQueue q &#x3D; i.next();</span><br><span class=\"line\">      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) &#123;</span><br><span class=\"line\">       &#x2F;&#x2F; 在配置了队列可抢占后,untouchableExtra&#x3D;0;若队列不可抢占,则超出最低配额的量算到理想配额内</span><br><span class=\"line\">        q.idealAssigned &#x3D; Resources.add(q.guaranteed, q.untouchableExtra);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        q.idealAssigned &#x3D; Resources.clone(q.current);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      Resources.subtractFrom(unassigned, q.idealAssigned);</span><br><span class=\"line\">      Resource curPlusPend &#x3D; Resources.add(q.current, q.pending);</span><br><span class=\"line\">      &#x2F;&#x2F; 欠分配的队列</span><br><span class=\"line\">      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) &#123;</span><br><span class=\"line\">        orderedByNeed.add(q);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 欠分配的队列 瓜分 空闲配额  </span><br><span class=\"line\">    while (!orderedByNeed.isEmpty()</span><br><span class=\"line\">       &amp;&amp; Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) &#123;</span><br><span class=\"line\">      Resource wQassigned &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">      &#x2F;&#x2F; 计算每个队列应得的权重  </span><br><span class=\"line\">      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);</span><br><span class=\"line\">      &#x2F;&#x2F; 使用tqComparator得到最欠分配的队列  依据是 idealAssigned&#x2F;absCapacity</span><br><span class=\"line\">      &#x2F;&#x2F;  超分配队列初始值为 1，欠分配队列初始小于 1 </span><br><span class=\"line\">      Collection&lt;TempQueue&gt; underserved &#x3D;</span><br><span class=\"line\">          getMostUnderservedQueues(orderedByNeed, tqComparator);</span><br><span class=\"line\">      for (Iterator&lt;TempQueue&gt; i &#x3D; underserved.iterator(); i.hasNext();) &#123;</span><br><span class=\"line\">        TempQueue sub &#x3D; i.next();</span><br><span class=\"line\">       &#x2F;&#x2F; 当前轮次, 空闲配额*权重  </span><br><span class=\"line\">        Resource wQavail &#x3D; Resources.multiplyAndNormalizeUp(rc,</span><br><span class=\"line\">            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));</span><br><span class=\"line\">        &#x2F;&#x2F;  min&#123;空闲配额*权重,最高配额-理想配额,需求配额&#125;做理想配额的增量;下文引用了 offer 方法  </span><br><span class=\"line\">        Resource wQidle &#x3D; sub.offer(wQavail, rc, tot_guarant);</span><br><span class=\"line\">        &#x2F;&#x2F; 队列得到的最终值,即上面的 min&#123;三元组&#125;</span><br><span class=\"line\">        Resource wQdone &#x3D; Resources.subtract(wQavail, wQidle);</span><br><span class=\"line\">\t    &#x2F;&#x2F; 在该轮得到了资源,那么可能还能得到资源,需要参加下一轮迭代;如果没得到资源,那么就不会再得到资源了(等于最高配额或满足了需求)</span><br><span class=\"line\">        if (Resources.greaterThan(rc, tot_guarant,</span><br><span class=\"line\">              wQdone, Resources.none())) &#123;</span><br><span class=\"line\">          orderedByNeed.add(sub);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Resources.addTo(wQassigned, wQdone);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 经过这一轮迭代后,全局空闲资源变化</span><br><span class=\"line\">      Resources.subtractFrom(unassigned, wQassigned);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  Resource offer(Resource avail, ResourceCalculator rc,</span><br><span class=\"line\">        Resource clusterResource) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 最大配额-理想配额:表示该队列最多只能拿到这么多额外资源 </span><br><span class=\"line\">      Resource absMaxCapIdealAssignedDelta &#x3D; Resources.componentwiseMax(</span><br><span class=\"line\">                      Resources.subtract(maxCapacity, idealAssigned),</span><br><span class=\"line\">                      Resource.newInstance(0, 0));</span><br><span class=\"line\">      Resource accepted &#x3D; </span><br><span class=\"line\">          Resources.min(rc, clusterResource, </span><br><span class=\"line\">              absMaxCapIdealAssignedDelta,</span><br><span class=\"line\">              &#x2F;&#x2F; avail:是传入值, 全局空闲资源*队列权重,表明在当前迭代 最多可以给的空闲资源</span><br><span class=\"line\">          Resources.min(rc, clusterResource, avail, </span><br><span class=\"line\">           &#x2F;&#x2F; current+pending-idealAssigned:表示 满足队列中所有应用程序需要的 额外资源 </span><br><span class=\"line\">          Resources.subtract(</span><br><span class=\"line\">              Resources.add(current, pending), idealAssigned)));</span><br><span class=\"line\">              </span><br><span class=\"line\">      Resource remain &#x3D; Resources.subtract(avail, accepted);</span><br><span class=\"line\">      &#x2F;&#x2F; 修改当前队列的理想配额  </span><br><span class=\"line\">      Resources.addTo(idealAssigned, accepted);</span><br><span class=\"line\">      return remain;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>欠分配比较器<br>[TQComparator.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">static class TQComparator implements Comparator&lt;TempQueue&gt; &#123;</span><br><span class=\"line\">    private ResourceCalculator rc;</span><br><span class=\"line\">    private Resource clusterRes;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public int compare(TempQueue tq1, TempQueue tq2) &#123;</span><br><span class=\"line\">      if (getIdealPctOfGuaranteed(tq1) &lt; getIdealPctOfGuaranteed(tq2)) &#123;</span><br><span class=\"line\">        return -1;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (getIdealPctOfGuaranteed(tq1) &gt; getIdealPctOfGuaranteed(tq2)) &#123;</span><br><span class=\"line\">        return 1;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return 0;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 计算 idealAssigned &#x2F; guaranteed</span><br><span class=\"line\">    private double getIdealPctOfGuaranteed(TempQueue q) &#123;</span><br><span class=\"line\">      double pctOver &#x3D; Integer.MAX_VALUE;</span><br><span class=\"line\">      if (q !&#x3D; null &amp;&amp; Resources.greaterThan(</span><br><span class=\"line\">          rc, clusterRes, q.guaranteed, Resources.none())) &#123;</span><br><span class=\"line\">        pctOver &#x3D;</span><br><span class=\"line\">            Resources.divide(rc, clusterRes, q.idealAssigned, q.guaranteed);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return (pctOver);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"抢占流程\"><a href=\"#抢占流程\" class=\"headerlink\" title=\"抢占流程\"></a>抢占流程</h2><p>第一步: 递归获取所有队列的快照信息，重点包括 used/pending/absCapacity/maxAbsCapacity 信息<br>第二步: 设置 root 队列的初始 idealAssigned 为 absCapacity(100%)，以此为依据将 root 队列的 idealAssigned 分摊给各层队列,即第三步<br>第三步: 根据是否配置了最低配额将所有子队列分为两类，优先满足配置了最低配额的队列需求，然后剩余配额满足未配置最低配额队列的需求。对这两类队列进行资源再平衡 迭代计算出每个子队列的理想配额，即第四步。如果子队列的 used &gt; absCapacity * (1+maxIgnoredOverCapacity)，则需要从其中掠夺资源，即第五步<br>第四步: 首先按照当前使用状态(min{used,capacity})，计算出全局空闲资源量(弹性)，获得欠分配的队列；对欠分配的队列集合进行迭代以求出每个队列的 idealAssigned，在每一轮迭代中，设置每个欠分配队列所占的权重(capacity 为依据)，该权重影响队列在获取空闲资源时的增量；优先对欠分配程度最高的队列进行分配，配额为 min{全局空闲资源量 * 队列权重, maxAbsCapacity-idealAssigned, used+pending-idealAssigned},三元组中第一个值表示可分配给队列的额外配额，第二个值表示在不超过最高配额的条件下能接受的最多额外配额，第三个值表示满足应用程序资源需求的额外配额。若满足了队列的需求则将其剔除出欠分配的队列集合<br>第五步: 如果队列 used &gt; absCapacity * (1+maxIgnoredOverCapacity),那么将会释放掉 (used - idealAssigned) * naturalTerminationFactor (自然终止因子,默认0.2)的资源,则按照 <b>最小化影响 application </b>的原则(maxIgnoredOverCapacity 和 naturalTerminationFactor 都是遵从该原则)，优先释放最晚启动的 application 中的最晚分配的container，优先释放 reserved contaienr，优先释放非 AM 的 container。向常规调度器发送抢占 container 事件，并追踪这些 container 的存活时间，如果在超过了 maxWaitTime(15s)之后还没有释放，则强制常规调度器杀死   </p>\n<h2 id=\"一些误区\"><a href=\"#一些误区\" class=\"headerlink\" title=\"一些误区\"></a>一些误区</h2><ol>\n<li><b>最小化影响 application 原则是抢占过程中最优先考虑的原则</b>        </li>\n<li>抢占调度仅仅是所有队列的资源再平衡过程，<b>不为具体的应用程序调度，不为某个容器做分配或保留</b>。从设计模式原则的角度来看是合理的；抢占调度和常规调度松耦合，仅仅使用总线向常规调度器单向发送抢占/杀死容器事件，从架构分层分模块的角度来看也是合理的      </li>\n<li>抢占调度<b>不会平衡队列内部的用户资源使用量</b>，用户资源量的平衡应使用 user limit percent 或者 Fair 去做。e.g. user1 先向队列提交了 10 个 application,user2 后向队列提交了 2 个 application ,在抢占调度时 不会把 user1 占用的资源平衡给 user2 使用;且 如果队列要释放资源,那么由于 user2 的 application 后提交,反而会先于 user1 的 application 被抢占    </li>\n<li>抢占调度<b>不会使繁忙队列的资源达到最高配额，且不会使空闲队列在任意条件下都得到最低配额</b>。比如,队列 A 占用了队列 B 的所有资源，那么当 B 上有任务提交时，是 A 和 B 一起抢占 B 的最低配额资源，只不过 B 由于欠分配程度(idealAssigned/absCapacity)比较高，在抢占时多次迭代中都更优先于 A，体现出来就是 B 抢回了资源。在 B used + pending &gt;&gt; capacity 且 idealAssigned &lt; capacity 时，B 的 idealAssigned 会尽快收敛到 capacity，然后和 A 一起瓜分剩余资源。        </li>\n<li>关于抢占的量，控制该值的配置主要是两个: yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round 每轮总抢占 默认 0.1，表示最多每轮抢占的资源占集群总资源的比例。若每轮抢占总量超过此值，则按比例在每个队列中缩减； yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor 自然终止因子 默认 0.2。考虑到容器自然终止的情况,即使不杀死容器也能在 5 * maxWaitTime(15s) 内收回 95%的资源，所以使用(used - idealAssigned)乘以该因子作为队列内最终抢占资源量，以尽量减少对 已启动 application 的影响  </li>\n<li>在 AM-RM 心跳协议 ApplicationMasterProtocol#allocate 中，AM 通过 AllocateRequest更新自己的需求，包括新增与释放 container。RM通过 AllocateResponse 响应请求，包括新分配的容器集合，释放的容器集合和<b>需要 AM 释放的被抢占容器集合</b>。抢占容器的消息类型分为两类:StrictPreemptionContract，AM 必须释放 RM 指定的容器；PreemptionContract。 AM 可以在满足同等大小资源的条件下灵活的替换 RM 指定的待抢占容器。目前 Yarn 采用的是 PreemptionContract类型，从 AM(用户)的角度来看这是可操作性很高的设计:待抢占的 worker 角色可能存储着 shuffle 文件、可能开启着重要的服务…… 恢复这些 worker 可能需要较多额外的工作，所以 RM 让 AM 去选择杀死哪些 container，只关注 AM 能不能还回等量的资源。但是抢占调度里面追踪着 RM 给 AM 指定的待抢占容器集合，如果 AM 替换掉这些容器，那么如果在抢占调度后续的周期发现这些追踪的容器存活大于 maxWaitTime，那么会直接杀死。这是一个有歧义的设计，目前 SparkOnYarn 的 YarnAllocator 没有对 RM 发给 AM 的待抢占容器集合做任何处理，即等同于等待容器超时被 RM 端杀死      </li>\n<li><b>抢占得到的资源可能不足以任何资源请求，但是抢占依旧会进行</b>。控制每轮抢占立即得到的资源总量的参数参考 “一些误区 5”。假设集群中只有两个叶子队列 A和B，最低配额都是 50%。初始状态，A 完全空闲，B 负载很高 抢占了 A 的所有资源。A 队列突然被提交了大量应用，那么在第一轮抢占时 B 是超分配的队列，idealAssigned 初值为50%，A是借出所有资源的队列 idealAssigned 初值为 0，但是 idealAssigned/absCapacity 小于 B，是最欠分配的队列 迭代时每轮都优先分配A，A 能得到的资源为: B 队列 (used 100% - idealAssigned 50%) *自然终止因子 0.2 =10%，且不能超过 集群资源100% * 每轮总抢占比例 0.1 = 10%。抢占资源量可能不足以满足任何 ResourceRequest，对于大型集群来说这个问题可能不足为虑，但是在小集群上可能会有一些难以预知的后果，尤其是在小集群上很大的资源需求(e.g. 40G)       </li>\n</ol>\n<h2 id=\"小集群大容器问题\"><a href=\"#小集群大容器问题\" class=\"headerlink\" title=\"小集群大容器问题\"></a>小集群大容器问题</h2><h3 id=\"问题场景\"><a href=\"#问题场景\" class=\"headerlink\" title=\"问题场景\"></a>问题场景</h3><p><img src=\"img/pictures/yarn_preempt/preempt_queue.png\" alt=\"\">   </p>\n<p>为简单起见，Yarn 使用默认配置且在不考虑: 虚拟核，AM/Worker 的区别和分配/保留的区别。集群有三台服务器，每台服务器有 80G 内存，共计有 240G 的内存资源。有两个叶子队列: A 和 B，最低配额都是 50%，最高配额都是 100%。当 A 负载较高而 B 空闲时，A 使用了整个集群的资源，A 中只有一个应用 app_A，启动的 container 都是 4G 大小。此时有新任务 app_B 提交到队列 B，且需要的资源量大于 B 的最低配额，则在第一次抢占时 B 队列会获得(240-120) * 0.2 自然终止因子 = 24G 的资源，且该轮抢占最大资源量不能超过 240 * 0.1 每轮抢占资源量比例 = 24G，这些抢占的 contaienr 均匀分布在集群的所有节点上，即图中红色 container 为被抢占，绿色 container 为未被抢占    </p>\n<ul>\n<li><p>场景 1: 假设 app_B 待分配的 container 大小为 40G，每个节点上只有 8G 的空闲资源，不足以启动该 container。且即使这些被抢占的 container 都分布在一个节点上，也不足以启动 40G JVM。 抢占虽然发生，但是 B 没有使用，最终这些抢占释放的资源会被 app_A 利用。这样就陷入了由周期性抢占导致的困局: 抢占调度 平衡资源-&gt; B 无法利用资源-&gt; A 可以利用资源-&gt;A 超分配,B 欠分配-&gt;抢占调度 平衡资源-&gt;…..  具体表现是: B 中的任务无法启动，A及app_A 中最近启动的容器一直处于 被抢占-&gt;启动成为最新-&gt;被抢占……的循环中  </p>\n</li>\n<li><p>场景 2: 假设 app_B 待分配的 container 大小为 20G，每个节点上只有 8G 的空闲资源，不足以启动 20G 的 container，也会陷入周期性抢占导致的困局。但是如果这些被抢占的 container 分布在一个节点上，足以启动 20G 的 JVM。这是因为 JVM 大小不高于每轮抢占资源总量且不大于每台服务器内存总量的缘故    </p>\n</li>\n</ul>\n<p>注: 常规调度器不希望一个容器占有节点太多的资源，所以对于大 container，不一定会做保留<br>[LeafQueue.java]</p>\n<pre><code>boolean shouldAllocOrReserveNewContainer(FiCaSchedulerApp application,\n      Priority priority, Resource required) {\n    int requiredContainers = application.getTotalRequiredResources(priority);\n    int reservedContainers = application.getNumReservedContainers(priority);\n    int starvation = 0;\n    if (reservedContainers &gt; 0) {\n      float nodeFactor = \n          Resources.ratio(\n              resourceCalculator, required, getMaximumAllocation()\n              );      \n       // 使用所需节点的百分比来对大型容器施加偏差…\n       // 防止需要使用整个节点的极端情况\n      // Use percentage of node required to bias against large containers...\n      // Protect against corner case where you need the whole node with\n      // Math.min(nodeFactor, minimumAllocationFactor)\n      starvation = \n          (int)((application.getReReservations(priority) / (float)reservedContainers) * \n                (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))\n               );  \n      if (LOG.isDebugEnabled()) {\n        LOG.debug(&quot;needsContainers:&quot; +\n            &quot; app.#re-reserve=&quot; + application.getReReservations(priority) + \n            &quot; reserved=&quot; + reservedContainers + \n            &quot; nodeFactor=&quot; + nodeFactor + \n            &quot; minAllocFactor=&quot; + getMinimumAllocationFactor() +\n            &quot; starvation=&quot; + starvation);\n      }\n    }\n    return (((starvation + requiredContainers) - reservedContainers) &gt; 0);</code></pre><h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p><b>方案 1</b><br>对于场景 1，可以从两个角度优化: 提高配置增加每轮抢占资源量及自然终止因子，以提高每轮抢占可以立即得到的资源量；规定应用程序单个容器的大小不能高于每轮抢占资源量。将场景 1 的问题转换问场景 2 的问题<br>对于场景 2，尽可能使被抢占容器分布在同一个节点上，为”最小化影响 Application”，可以在每个节点上按照设定的选取规则尝试抢占容器(但是并未真的杀死)以满足大容器并计算抢占代价(sum(currentTime-containerStartTime))，选择抢占代价最低的节点抢占。  </p>\n<p>优点:</p>\n<ol>\n<li>可以提高启动大容器的可能性，减少大容器调度延时    </li>\n</ol>\n<p>缺点:  </p>\n<ol>\n<li>增加每轮抢占资源量和自然终止因子,将增加集群和队列资源在平衡点附近抖动程度(队列释放-&gt;阶梯式下降,队列获取-&gt;阶梯式上升)。最好配合yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity 属性使用。还是最小化影响原则，减小借用资源不多的队列的抖动      </li>\n<li>提高抢占配置，提供能够启动大容器的资源量，将增加了被杀死的容器数量，扩大了影响范围，恢复任务产生的无效工作量也会增加      </li>\n<li>尽可能在节点上抢占容器启动大容器，而不是在集群所有节点上抢占最晚启动的应用程序，导致这些节点资源利用率抖动极大  </li>\n<li>由于抢占调度和常规调度解耦，抢占调度在节点上抢占出大容器的空间，常规调度也未必在该空间内分配大容器  </li>\n</ol>\n<p>仅限于抢占频率较低的场景，并使用某些参量(欠分配队列 totalPending, minResourceRequest)来决定 走默认抢占逻辑还是集中在节点上释放资源      </p>\n<p><b>方案 2</b>  </p>\n<p><a href=\"https://jiulongzhu.github.io/2019/10/28/Yarn-ReservationSystem/\">资源预订系统</a></p>\n<p><b>方案 3</b><br>标签系统：对节点进行分类或分组的一种方式，应用程序可以指定在特定标签的节点执行。可以配置资源队列可以访问的节点标签集合，应用程序只能提交到这些包含该标签的队列上来使用这些具有标签的节点的计算资源<br>yarn.scheduler.capacity.&lt;queue_path&gt;.capacity 属性是<b>默认标签</b>(无标签)的资源配置，所有的队列都可以访问无标签的节点；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels 属性用于指定队列可访问的标签节点，在不指定的情况下可以继承父队列的权限，若希望让某队列只能访问无标签节点，设置为空格即可；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels.&lt;label&gt;.capacity 指定队列可以使用该标签资源的最低配额；yarn.scheduler.capacity.&lt;queue_path&gt;.accessible-node-labels.&lt;label&gt;.maximum-capacity 指定队列可以使用该标签资源的最高配额     </p>\n<p>适用场景 </p>\n<ol>\n<li>机构路由: BU A的任务，只能运行在具有 A 标签的 NM 上    </li>\n<li>服务路由: Hbase 的数据分析任务，只能运行在具有 Hbase 标签的 NM 上    </li>\n<li>特殊机器: 只有深度学习任务可以运行在具有 GPU 标签的 NM 上        </li>\n</ol>\n<p>新增或修改一些节点，配置其标签，并设置仅有某些队列具有该标签资源的使用权限，大内存应用程序专用    </p>\n<p>优点:</p>\n<ol>\n<li>可以运行大内存应用，且 SLA 高  </li>\n</ol>\n<p>缺点:  </p>\n<ol>\n<li>标签资源空闲时其他应用无法使用，集群资源利用率相对较低  </li>\n<li>需要额外的服务器成本  </li>\n</ol>\n<h2 id=\"相关参数\"><a href=\"#相关参数\" class=\"headerlink\" title=\"相关参数\"></a>相关参数</h2><ul>\n<li>yarn.resourcemanager.scheduler.monitor.enable<br>设置为 true ,启用抢占  </li>\n<li>yarn.resourcemanager.scheduler.monitor.policies<br>启用抢占时,抢占策略主类.RM 将配置的主类启动在独立线程中,周期性的执行抢占.默认是org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy  </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval<br>周期性执行抢占策略的间隔,单位毫秒,默认是 3000  </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill<br>自容器被标记为被抢占到抢占策略强制杀死该容器的时间,单位毫秒,默认15000.留给 AM 灵活杀死 container 的时间,如果希望快速收回资源可以适当调小    </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round<br>每轮抢占最大资源总量占集群无标签资源的比例,默认 0.1. 即每轮抢占最多可以收回相当于集群无标签资源总量的10%,若每轮抢占资源量超过此值,则按比例在每个待抢占队列进行缩减    </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity<br>忽略抢占的阈值,默认 0.1.当队列资源使用量 used &gt; absCapacity(1+0.1)时,认为其应该还回资源,才会抢占容器.用于避免资源消耗和配额的剧烈波动,最小化影响      </li>\n<li>yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor<br>自然终止因子,统计发现值为 0.5 或者为 0 时,都会在 5* 15秒(上述第三配置项)内收回 95%的资源,默认 0.2.为每个队列设置抢占目标后,再乘以此值作为最终的抢占目标.假设第一周期,某队列待抢占值为 10G,那么最终抢占为 2G.第二周期待抢占值为 7G,最终抢占为 1.4G…以一种平滑的方式回收,降低波动.可以适当增加此值以加快回收速度  </li>\n</ul>\n<h2 id=\"一点思考\"><a href=\"#一点思考\" class=\"headerlink\" title=\"一点思考\"></a>一点思考</h2><ol>\n<li>方案设计时考虑了很多，从尝试借鉴 CPU 抢占、内存 SWAP 模式、用户体验、架构分层等角度思考设计一个抢占调度系统的想法。有时间再整理吧     </li>\n<li>关于开源抢占系统通用优化的初步想法<br>调度系统抢占的容器先不杀死，而是打一个标记，比如:MARK_PREEMPTED_KILL,在资源分配时优先使用这些标记的节点或者资源紧缺时使用 以尽可能使容器能自然终止(类似 jvm软引用)，无资源分配时则不杀死。其一可以保证 在周期性抢占造成的困局中，队列使用不到这些资源时，这些资源不会被反复无效调度，而是继续运行任务；其二 或许能提高资源抢占相应速度，代价是需要完善常规调度的逻辑，分配时如何处理MARK_PREEMPTED_KILL，保留时如何处理MARK_PREEMPTED_KILL…    </li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeLabel.html</a><br><a href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Capacity\\_Scheduler\\_container\\_preemption</a><br><a href=\"https://www.aboutyun.com/thread-24628-1-1.html\" target=\"_blank\" rel=\"noopener\">https://www.aboutyun.com/thread-24628-1-1.html</a></p>"},{"layout":"post","title":"Spark On Yarn 任务退出码","subtitle":"Spark On Yarn  ExitCode","date":"2019-11-17T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"## 概述\n\nSparkOnYarn 程序的退出码种类比较繁多逻辑杂乱，主要包括：   \n\n1. Yarn 退出码      \n2. Spark 退出码    \n3. JVM 退出码   \n\n其中一些退出码可能是多个环节共同作用最终展现到日志上，e.g. SparkOnYarn 程序封装 driver/executor 的 ContainerLauchContext 时启动命令时加入\"-XX:OnOutOfMemoryError=kill %p\"，所以当程序 OOM 使用默认 signal(15) 终止 Container 脚本启动的 JVM，上报到 SparkOnYarn 日志的退出码是 143(128+15)。 \n\n<!-- more -->\n\n以下均基于 Hadoop2.7.3，Spark 2.3-SNAPSHOT，jdk1.8 \n\n## Yarn 退出码\n\nYarn container 的退出码分为两类: container-executor 和 container。container 是 Yarn 资源调度的基础，也是资源隔离的基础，在同一台服务器上运行的多个container 之间内存、CPU和权限等方面不应该相互影响(实际上内存和 CPU 没有做到绝对的资源隔离,仅仅监控内存/不管 CPU,在此不细说)。container-executor 负责使用 Yarn 参考 ContainerLaunchContext 上下文环境组装的脚本 初始化、启动和终止 Container，因此其错误码体系更像是操作系统的进程退出码。container-executor 共分两类: 通用的 DefaultContainerExecutor，所有的 Container 的用户均为启动 NodeManager 的用户；LinuxContainerExecutor，每个 Container 进程可以由不同用户启动，并支持 CGROUP 和 ACL。container 错误码体系适用于所有容器，是 Yarn 框架的一部分，应用程序错误码体系的补充。即 container-executor 错误码体系更靠近操作系统，container 错误码体系更靠近应用程序。  \n\n### container-executor\n\n仅列出 LinuxContainerExecutor 错误码体系  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|1|INVALID\\_ARGUMENT\\_NUMBER|1.启动脚本给定输入参数数量不符 2.未能成功初始化|\n|2|INVALID\\_USER\\_NAME|启动脚本所属用户不存在|\n|3| INVALID\\_COMMAND\\_PROVIDED |无法识别被提供的启动命令|\n|4|SUPER\\_USER\\_NOT\\_ALLOWED\\_TO\\_RUN\\_TASKS|未启用|\n|5|INVALID\\_NM\\_ROOT\\_DIRS|启动脚本参数定义的 NM root 目录不存在|\n|6|SETUID\\_OPER\\_FAILED|无法设置 UID 或 GID|\n|7|UNABLE\\_TO\\_EXECUTE\\_CONTAINER\\_SCRIPT|无法运行启动脚本|\n|8|UNABLE\\_TO\\_SIGNAL\\_CONTAINER|无法向指定容器发送信号|\n|9|INVALID\\_CONTAINER\\_PID|设置的启动命令 PID 不大于 0或不合法|\n|10| ERROR\\_RESOLVING\\_FILE\\_PATH |未启用|\n|11| RELATIVE\\_PATH\\_COMPONENTS\\_IN\\_FILE\\_PATH |未启用|\n|12| UNABLE\\_TO\\_STAT\\_FILE |未启用|\n|13| FILE\\_NOT\\_OWNED\\_BY\\_ROOT |未启用|\n|14| PREPARE\\_CONTAINER\\_DIRECTORIES\\_FAILED |未启用|\n|15| INITIALIZE\\_CONTAINER\\_FAILED |未启用|\n|16| PREPARE\\_CONTAINER\\_LOGS\\_FAILED |未启用|\n|17| INVALID\\_LOG\\_DIR |未启用|\n|18| OUT\\_OF\\_MEMORY |启动命令未能获得足够的内存|\n|19| INITIALIZE\\_DISTCACHEFILE\\_FAILED |未启用|\n|20| INITIALIZE\\_USER\\_FAILED |无法获取用户的 NM 目录|\n|21| UNABLE\\_TO\\_BUILD\\_PATH|启动脚本创建路径失败|\n|22| INVALID\\_CONTAINER\\_EXEC\\_PERMISSIONS |启动脚本没有设置正确的执行权限|\n|23| PREPARE\\_JOB\\_LOGS\\_FAILED |未启用|\n|24| INVALID\\_CONFIG\\_FILE |container-executor.cfg文件不存在或者权限不正确|\n|25| SETSID\\_OPER\\_FAILED|设置容器的 SID(sessionId?)失败|\n|26| WRITE\\_PIDFILE\\_FAILED|无法将 PID 写入 PID 文件|\n|27| WRITE\\_CGROUP\\_FAILED|无法写入CGROUP 信息(/sys/fs/cgroup ?)|\n|其他|UNKNOWN\\_ERROR|需要依据错误信息具体分析|\n\n以上主要参考 hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   \n\n### container\n\nLinuxContainerExecutor 和 DefaultContainerExecutor 通用的错误码体系，Yarn 框架的一部分，可作为应用程序错误码的补充。  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|0|SUCCESS|容器成功退出|\n|-1000|INVALID|容器退出码的初始值|\n|-100|ABORTED|容器被框架杀死,可能是 app 释放容器或 nm 失败丢失容器导致|\n|-101|DISKS\\_FAILED|NM 本地目录数量超过阈值|\n|-102|PREEMPTED|容器被抢占|\n|-103|KILLED\\_EXCEEDED\\_VMEM|虚拟内存超限被杀|\n|-104|KILLED\\_EXCEEDED\\_PMEM|物理内存超限被杀|\n|-105|KILLED\\_BY\\_APPMASTER|被 am 请求终止|\n|-106|KILLED\\_BY\\_RESOURCEMANAGER|被 rm 请求终止|\n|-107|KILLED\\_AFTER\\_APP\\_COMPLETION|app 完成后被杀死|\n\n以上主要参考 org.apache.hadoop.yarn.api.records.ContainerExitStatus.java  \n\n### Application  \n\n通用的 Yarn Application 状态码定义  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n||UNDEFINED|APP 未完成时的状态码(初始值)|\n||SUCCEEDED|APP 成功完成|\n||FAILED|APP 失败|\n||KILLED|APP 被提交用户或者管理员用户终止|\n\n以上参考 org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  \n\n## Spark 退出码\n\n### Executor 退出码\n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|50|UNCAUGHT_EXCEPTION|触发了默认的异常处理器(?)|\n|51|UNCAUGHT\\_EXCEPTION\\_TWICE|触发了默认的异常处理器,并在记录异常时触发了新的异常(?)|\n|52|OOM|触发了默认的异常处理器,且异常是 OutOfMemoryError|\n|53|DISK\\_STORE\\_FAILED\\_TO\\_CREATE\\_DIR|DiskStore 未能创建本地临时目录|\n|54|EXTERNAL\\_BLOCK\\_STORE\\_FAILED\\_TO\\_INITIALIZE|ExternalBlockStore 未能初始化|\n|55|EXTERNAL\\_BLOCK\\_STORE\\_FAILED\\_TO\\_CREATE\\_DIR|ExternalBlockStore 未能创建本地临时目录|\n|56|HEARTBEAT_FAILURE|executor 向 driver 发送心跳信号失败次数超过了 spark.executor.heartbeat.maxFailures|\n\n以上主要参考 org.apache.spark.executor.ExecutorExitCode.scala  \n\n### ApplicationMaster 退出码\n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|0|EXIT_SUCCESS||\n|10|EXIT_UNCAUGHT_EXCEPTION|未捕获的异常(此列表之外的异常)退出|\n|11|EXIT_MAX_EXECUTOR_FAILURES|失败的 executor 数量超过目标值。目标值优先取 spark.yarn.max.executor.failures(默认无),未配置 且 开启动态资源管理时取spark.dynamicAllocation.maxExecutors 的 2 倍,其他情况则取 spark.executor.instances 的 2 倍|\n|12|EXIT_REPORTER_FAILURE|应用进度汇报线程(progress reporter)的失败次数超过spark.yarn.scheduler.reporterThread.maxFailures(默认 5)|\n|13|EXIT_SC_NOT_INITED|SparkContext 初始化时间超过 spark.yarn.am.waitTime(默认 100s)|\n|14|EXIT_SECURITY|未启用|\n|15|EXIT_EXCEPTION_USER_CLASS|用户MainClass 抛出异常|\n|16|EXIT_EARLY|在 APP 结束前调用了 ShutdownHook|\n\n以上参考 org.apache.spark.deploy.yarn.ApplicationMaster.scala  \n\n## JVM 退出码\n\n数值上超过 128 的退出码很有可能是由 Unix Signal 触发的程序关闭导致的，可以通过用退出码减去 128来计算 Unix Signal。例如最常见的 137 退出码是由 kill -9 强制杀死抛出的，143 退出码一般是由 kill 或 kill -15 杀死抛出的。 \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|137|FORCE_KILLED|128+9,即是由 kill -9 命令强制终止,一般是虚拟或物理内存超限导致、也有可能是操作系统整体内存紧张杀死进程|\n|143|TERMINATED|128+15,即是由 kill 命令终止的,一般是 OOM/FGC 导致|\n|154|LOST|128+27,虚拟计时器过期(?)|\n\n```\nThen I ran the program in one terminal window (java Death; echo $?) while iterating through all kill signals (0-31) in another:\nkill -$SIGNAL $(jps | grep Death | cut -d\\  -f1)\n\nsignal\t    shutdown runs hook exit code    comment\ndefault (15)\tyes\tyes\t143\tSIGTERM is the default unix kill signal\n0\tno\t-\t-\t\n1 (SIGHUP)\tyes\tyes\t129\t\n2 (SIGINT)\tyes\tyes\t130\tSIGINT is the signal sent on ^C\n3 (SIGQUIT)\tno\t-\t-\tMakes the JVM dump threads / stack-traces\n4 (SIGILL)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6\n5\tyes\tno\t133\tMakes the JVM exit with \"Trace/BPT trap: 5\"\n6 (SIGABRT)\tyes\tno\t134\tMakes the JVM exit with \"Abort trap: 6\"\n7\tyes\tno\t135\tMakes the JVM exit with \"EMT trap: 7\"\n8 (SIGFPE)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6\n9 (SIGKILL)\tyes\tno\t137\tThe JVM is forcibly killed (exits with \"Killed: 9\")\n10 (SIGBUS)\tyes\tno\t134\tEmulates a \"Bus Error\"\n11 (SIGSEGV)\tyes\tno\t134\tEmulates a \"Segmentation fault\"\n12\tyes\tno\t140\tMakes the JVM exit with \"Bad system call: 12\"\n13\tno\t-\t-\t\n14\tyes\tno\t142\tMakes the JVM exit with \"Alarm clock: 14\"\n15 (SIGTERM)\tyes\tyes\t143\tThis is the default unix kill signal\n16\tno\t-\t-\t\n17\tno\t-\t145\tStops the application (sends it to the background), same as ^Z\n18\tno\t-\t146\tStops the application (sends it to the background), same as ^Z\n19\tno\t-\t-\t\n20\tno\t-\t-\t\n21\tno\t-\t149\tStops the application (sends it to the background), same as ^Z\n22\tno\t-\t150\tStops the application (sends it to the background), same as ^Z\n23\tno\t-\t-\t\n24\tyes\tno\t152\tMakes the JVM exit with \"Cputime limit exceeded: 24\"\n25\tno\t-\t-\t\n26\tyes\tno\t154\tMakes the JVM exit with \"Virtual timer expired: 26\"\n27\tyes\tno\t155\tMakes the JVM exit with \"Profiling timer expired: 27\"\n28\tno\t-\t-\t\n29\tno\t-\t-\t\n30\tyes\tno\t158\tMakes the JVM exit with \"User defined signal 1: 30\"\n31\tyes\tno\t134\tMakes the JVM exit on Segmentation fault\n\n\nThis list was compiled using (a quite old) Oracle Hotspot Java 8 EA on Mac OS X:\njava version \"1.8.0-ea\"\nJava(TM) SE Runtime Environment (build 1.8.0-ea-b65)\nJava HotSpot(TM) 64-Bit Server VM (build 25.0-b09, mixed mode)\n```\n\n以上主要参考 http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\n\n## 参考\n1. hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   \n2. org.apache.hadoop.yarn.api.records.ContainerExitStatus.java    \n3. org.apache.spark.executor.ExecutorExitCode.scala  \n4. http://journal.thobe.org/2013/02/jvms-and-kill-signals.html  \n5. org.apache.hadoop.yarn.server.nodemanager.ExitCode.java\n6. org.apache.spark.deploy.yarn.ApplicationMaster.scala  \n7. org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  \n","source":"_posts/2019-11-18-SparkOnYarn 任务退出码.md","raw":"---\nlayout:     post\ntitle:      Spark On Yarn 任务退出码\nsubtitle:   Spark On Yarn  ExitCode\ndate:       2019-11-18\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Yarn 2.7.3\n    - Spark 2.3.0\n    - Spark On Yarn\n---\n## 概述\n\nSparkOnYarn 程序的退出码种类比较繁多逻辑杂乱，主要包括：   \n\n1. Yarn 退出码      \n2. Spark 退出码    \n3. JVM 退出码   \n\n其中一些退出码可能是多个环节共同作用最终展现到日志上，e.g. SparkOnYarn 程序封装 driver/executor 的 ContainerLauchContext 时启动命令时加入\"-XX:OnOutOfMemoryError=kill %p\"，所以当程序 OOM 使用默认 signal(15) 终止 Container 脚本启动的 JVM，上报到 SparkOnYarn 日志的退出码是 143(128+15)。 \n\n<!-- more -->\n\n以下均基于 Hadoop2.7.3，Spark 2.3-SNAPSHOT，jdk1.8 \n\n## Yarn 退出码\n\nYarn container 的退出码分为两类: container-executor 和 container。container 是 Yarn 资源调度的基础，也是资源隔离的基础，在同一台服务器上运行的多个container 之间内存、CPU和权限等方面不应该相互影响(实际上内存和 CPU 没有做到绝对的资源隔离,仅仅监控内存/不管 CPU,在此不细说)。container-executor 负责使用 Yarn 参考 ContainerLaunchContext 上下文环境组装的脚本 初始化、启动和终止 Container，因此其错误码体系更像是操作系统的进程退出码。container-executor 共分两类: 通用的 DefaultContainerExecutor，所有的 Container 的用户均为启动 NodeManager 的用户；LinuxContainerExecutor，每个 Container 进程可以由不同用户启动，并支持 CGROUP 和 ACL。container 错误码体系适用于所有容器，是 Yarn 框架的一部分，应用程序错误码体系的补充。即 container-executor 错误码体系更靠近操作系统，container 错误码体系更靠近应用程序。  \n\n### container-executor\n\n仅列出 LinuxContainerExecutor 错误码体系  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|1|INVALID\\_ARGUMENT\\_NUMBER|1.启动脚本给定输入参数数量不符 2.未能成功初始化|\n|2|INVALID\\_USER\\_NAME|启动脚本所属用户不存在|\n|3| INVALID\\_COMMAND\\_PROVIDED |无法识别被提供的启动命令|\n|4|SUPER\\_USER\\_NOT\\_ALLOWED\\_TO\\_RUN\\_TASKS|未启用|\n|5|INVALID\\_NM\\_ROOT\\_DIRS|启动脚本参数定义的 NM root 目录不存在|\n|6|SETUID\\_OPER\\_FAILED|无法设置 UID 或 GID|\n|7|UNABLE\\_TO\\_EXECUTE\\_CONTAINER\\_SCRIPT|无法运行启动脚本|\n|8|UNABLE\\_TO\\_SIGNAL\\_CONTAINER|无法向指定容器发送信号|\n|9|INVALID\\_CONTAINER\\_PID|设置的启动命令 PID 不大于 0或不合法|\n|10| ERROR\\_RESOLVING\\_FILE\\_PATH |未启用|\n|11| RELATIVE\\_PATH\\_COMPONENTS\\_IN\\_FILE\\_PATH |未启用|\n|12| UNABLE\\_TO\\_STAT\\_FILE |未启用|\n|13| FILE\\_NOT\\_OWNED\\_BY\\_ROOT |未启用|\n|14| PREPARE\\_CONTAINER\\_DIRECTORIES\\_FAILED |未启用|\n|15| INITIALIZE\\_CONTAINER\\_FAILED |未启用|\n|16| PREPARE\\_CONTAINER\\_LOGS\\_FAILED |未启用|\n|17| INVALID\\_LOG\\_DIR |未启用|\n|18| OUT\\_OF\\_MEMORY |启动命令未能获得足够的内存|\n|19| INITIALIZE\\_DISTCACHEFILE\\_FAILED |未启用|\n|20| INITIALIZE\\_USER\\_FAILED |无法获取用户的 NM 目录|\n|21| UNABLE\\_TO\\_BUILD\\_PATH|启动脚本创建路径失败|\n|22| INVALID\\_CONTAINER\\_EXEC\\_PERMISSIONS |启动脚本没有设置正确的执行权限|\n|23| PREPARE\\_JOB\\_LOGS\\_FAILED |未启用|\n|24| INVALID\\_CONFIG\\_FILE |container-executor.cfg文件不存在或者权限不正确|\n|25| SETSID\\_OPER\\_FAILED|设置容器的 SID(sessionId?)失败|\n|26| WRITE\\_PIDFILE\\_FAILED|无法将 PID 写入 PID 文件|\n|27| WRITE\\_CGROUP\\_FAILED|无法写入CGROUP 信息(/sys/fs/cgroup ?)|\n|其他|UNKNOWN\\_ERROR|需要依据错误信息具体分析|\n\n以上主要参考 hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   \n\n### container\n\nLinuxContainerExecutor 和 DefaultContainerExecutor 通用的错误码体系，Yarn 框架的一部分，可作为应用程序错误码的补充。  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|0|SUCCESS|容器成功退出|\n|-1000|INVALID|容器退出码的初始值|\n|-100|ABORTED|容器被框架杀死,可能是 app 释放容器或 nm 失败丢失容器导致|\n|-101|DISKS\\_FAILED|NM 本地目录数量超过阈值|\n|-102|PREEMPTED|容器被抢占|\n|-103|KILLED\\_EXCEEDED\\_VMEM|虚拟内存超限被杀|\n|-104|KILLED\\_EXCEEDED\\_PMEM|物理内存超限被杀|\n|-105|KILLED\\_BY\\_APPMASTER|被 am 请求终止|\n|-106|KILLED\\_BY\\_RESOURCEMANAGER|被 rm 请求终止|\n|-107|KILLED\\_AFTER\\_APP\\_COMPLETION|app 完成后被杀死|\n\n以上主要参考 org.apache.hadoop.yarn.api.records.ContainerExitStatus.java  \n\n### Application  \n\n通用的 Yarn Application 状态码定义  \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n||UNDEFINED|APP 未完成时的状态码(初始值)|\n||SUCCEEDED|APP 成功完成|\n||FAILED|APP 失败|\n||KILLED|APP 被提交用户或者管理员用户终止|\n\n以上参考 org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  \n\n## Spark 退出码\n\n### Executor 退出码\n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|50|UNCAUGHT_EXCEPTION|触发了默认的异常处理器(?)|\n|51|UNCAUGHT\\_EXCEPTION\\_TWICE|触发了默认的异常处理器,并在记录异常时触发了新的异常(?)|\n|52|OOM|触发了默认的异常处理器,且异常是 OutOfMemoryError|\n|53|DISK\\_STORE\\_FAILED\\_TO\\_CREATE\\_DIR|DiskStore 未能创建本地临时目录|\n|54|EXTERNAL\\_BLOCK\\_STORE\\_FAILED\\_TO\\_INITIALIZE|ExternalBlockStore 未能初始化|\n|55|EXTERNAL\\_BLOCK\\_STORE\\_FAILED\\_TO\\_CREATE\\_DIR|ExternalBlockStore 未能创建本地临时目录|\n|56|HEARTBEAT_FAILURE|executor 向 driver 发送心跳信号失败次数超过了 spark.executor.heartbeat.maxFailures|\n\n以上主要参考 org.apache.spark.executor.ExecutorExitCode.scala  \n\n### ApplicationMaster 退出码\n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|0|EXIT_SUCCESS||\n|10|EXIT_UNCAUGHT_EXCEPTION|未捕获的异常(此列表之外的异常)退出|\n|11|EXIT_MAX_EXECUTOR_FAILURES|失败的 executor 数量超过目标值。目标值优先取 spark.yarn.max.executor.failures(默认无),未配置 且 开启动态资源管理时取spark.dynamicAllocation.maxExecutors 的 2 倍,其他情况则取 spark.executor.instances 的 2 倍|\n|12|EXIT_REPORTER_FAILURE|应用进度汇报线程(progress reporter)的失败次数超过spark.yarn.scheduler.reporterThread.maxFailures(默认 5)|\n|13|EXIT_SC_NOT_INITED|SparkContext 初始化时间超过 spark.yarn.am.waitTime(默认 100s)|\n|14|EXIT_SECURITY|未启用|\n|15|EXIT_EXCEPTION_USER_CLASS|用户MainClass 抛出异常|\n|16|EXIT_EARLY|在 APP 结束前调用了 ShutdownHook|\n\n以上参考 org.apache.spark.deploy.yarn.ApplicationMaster.scala  \n\n## JVM 退出码\n\n数值上超过 128 的退出码很有可能是由 Unix Signal 触发的程序关闭导致的，可以通过用退出码减去 128来计算 Unix Signal。例如最常见的 137 退出码是由 kill -9 强制杀死抛出的，143 退出码一般是由 kill 或 kill -15 杀死抛出的。 \n\n| 错误码  | 错误信息  | 描述 |\n|:------------- |:---------------|:-------------|\n|137|FORCE_KILLED|128+9,即是由 kill -9 命令强制终止,一般是虚拟或物理内存超限导致、也有可能是操作系统整体内存紧张杀死进程|\n|143|TERMINATED|128+15,即是由 kill 命令终止的,一般是 OOM/FGC 导致|\n|154|LOST|128+27,虚拟计时器过期(?)|\n\n```\nThen I ran the program in one terminal window (java Death; echo $?) while iterating through all kill signals (0-31) in another:\nkill -$SIGNAL $(jps | grep Death | cut -d\\  -f1)\n\nsignal\t    shutdown runs hook exit code    comment\ndefault (15)\tyes\tyes\t143\tSIGTERM is the default unix kill signal\n0\tno\t-\t-\t\n1 (SIGHUP)\tyes\tyes\t129\t\n2 (SIGINT)\tyes\tyes\t130\tSIGINT is the signal sent on ^C\n3 (SIGQUIT)\tno\t-\t-\tMakes the JVM dump threads / stack-traces\n4 (SIGILL)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6\n5\tyes\tno\t133\tMakes the JVM exit with \"Trace/BPT trap: 5\"\n6 (SIGABRT)\tyes\tno\t134\tMakes the JVM exit with \"Abort trap: 6\"\n7\tyes\tno\t135\tMakes the JVM exit with \"EMT trap: 7\"\n8 (SIGFPE)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6\n9 (SIGKILL)\tyes\tno\t137\tThe JVM is forcibly killed (exits with \"Killed: 9\")\n10 (SIGBUS)\tyes\tno\t134\tEmulates a \"Bus Error\"\n11 (SIGSEGV)\tyes\tno\t134\tEmulates a \"Segmentation fault\"\n12\tyes\tno\t140\tMakes the JVM exit with \"Bad system call: 12\"\n13\tno\t-\t-\t\n14\tyes\tno\t142\tMakes the JVM exit with \"Alarm clock: 14\"\n15 (SIGTERM)\tyes\tyes\t143\tThis is the default unix kill signal\n16\tno\t-\t-\t\n17\tno\t-\t145\tStops the application (sends it to the background), same as ^Z\n18\tno\t-\t146\tStops the application (sends it to the background), same as ^Z\n19\tno\t-\t-\t\n20\tno\t-\t-\t\n21\tno\t-\t149\tStops the application (sends it to the background), same as ^Z\n22\tno\t-\t150\tStops the application (sends it to the background), same as ^Z\n23\tno\t-\t-\t\n24\tyes\tno\t152\tMakes the JVM exit with \"Cputime limit exceeded: 24\"\n25\tno\t-\t-\t\n26\tyes\tno\t154\tMakes the JVM exit with \"Virtual timer expired: 26\"\n27\tyes\tno\t155\tMakes the JVM exit with \"Profiling timer expired: 27\"\n28\tno\t-\t-\t\n29\tno\t-\t-\t\n30\tyes\tno\t158\tMakes the JVM exit with \"User defined signal 1: 30\"\n31\tyes\tno\t134\tMakes the JVM exit on Segmentation fault\n\n\nThis list was compiled using (a quite old) Oracle Hotspot Java 8 EA on Mac OS X:\njava version \"1.8.0-ea\"\nJava(TM) SE Runtime Environment (build 1.8.0-ea-b65)\nJava HotSpot(TM) 64-Bit Server VM (build 25.0-b09, mixed mode)\n```\n\n以上主要参考 http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\n\n## 参考\n1. hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   \n2. org.apache.hadoop.yarn.api.records.ContainerExitStatus.java    \n3. org.apache.spark.executor.ExecutorExitCode.scala  \n4. http://journal.thobe.org/2013/02/jvms-and-kill-signals.html  \n5. org.apache.hadoop.yarn.server.nodemanager.ExitCode.java\n6. org.apache.spark.deploy.yarn.ApplicationMaster.scala  \n7. org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  \n","slug":"2019-11-18-SparkOnYarn 任务退出码","published":1,"updated":"2020-06-16T06:47:31.005Z","comments":1,"photos":[],"link":"","_id":"cke2rms4p0005y7c7a9cr1r05","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>SparkOnYarn 程序的退出码种类比较繁多逻辑杂乱，主要包括：   </p>\n<ol>\n<li>Yarn 退出码      </li>\n<li>Spark 退出码    </li>\n<li>JVM 退出码   </li>\n</ol>\n<p>其中一些退出码可能是多个环节共同作用最终展现到日志上，e.g. SparkOnYarn 程序封装 driver/executor 的 ContainerLauchContext 时启动命令时加入”-XX:OnOutOfMemoryError=kill %p”，所以当程序 OOM 使用默认 signal(15) 终止 Container 脚本启动的 JVM，上报到 SparkOnYarn 日志的退出码是 143(128+15)。 </p>\n<a id=\"more\"></a>\n\n<p>以下均基于 Hadoop2.7.3，Spark 2.3-SNAPSHOT，jdk1.8 </p>\n<h2 id=\"Yarn-退出码\"><a href=\"#Yarn-退出码\" class=\"headerlink\" title=\"Yarn 退出码\"></a>Yarn 退出码</h2><p>Yarn container 的退出码分为两类: container-executor 和 container。container 是 Yarn 资源调度的基础，也是资源隔离的基础，在同一台服务器上运行的多个container 之间内存、CPU和权限等方面不应该相互影响(实际上内存和 CPU 没有做到绝对的资源隔离,仅仅监控内存/不管 CPU,在此不细说)。container-executor 负责使用 Yarn 参考 ContainerLaunchContext 上下文环境组装的脚本 初始化、启动和终止 Container，因此其错误码体系更像是操作系统的进程退出码。container-executor 共分两类: 通用的 DefaultContainerExecutor，所有的 Container 的用户均为启动 NodeManager 的用户；LinuxContainerExecutor，每个 Container 进程可以由不同用户启动，并支持 CGROUP 和 ACL。container 错误码体系适用于所有容器，是 Yarn 框架的一部分，应用程序错误码体系的补充。即 container-executor 错误码体系更靠近操作系统，container 错误码体系更靠近应用程序。  </p>\n<h3 id=\"container-executor\"><a href=\"#container-executor\" class=\"headerlink\" title=\"container-executor\"></a>container-executor</h3><p>仅列出 LinuxContainerExecutor 错误码体系  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">INVALID_ARGUMENT_NUMBER</td>\n<td align=\"left\">1.启动脚本给定输入参数数量不符 2.未能成功初始化</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">INVALID_USER_NAME</td>\n<td align=\"left\">启动脚本所属用户不存在</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">INVALID_COMMAND_PROVIDED</td>\n<td align=\"left\">无法识别被提供的启动命令</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">SUPER_USER_NOT_ALLOWED_TO_RUN_TASKS</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">INVALID_NM_ROOT_DIRS</td>\n<td align=\"left\">启动脚本参数定义的 NM root 目录不存在</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">SETUID_OPER_FAILED</td>\n<td align=\"left\">无法设置 UID 或 GID</td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\">UNABLE_TO_EXECUTE_CONTAINER_SCRIPT</td>\n<td align=\"left\">无法运行启动脚本</td>\n</tr>\n<tr>\n<td align=\"left\">8</td>\n<td align=\"left\">UNABLE_TO_SIGNAL_CONTAINER</td>\n<td align=\"left\">无法向指定容器发送信号</td>\n</tr>\n<tr>\n<td align=\"left\">9</td>\n<td align=\"left\">INVALID_CONTAINER_PID</td>\n<td align=\"left\">设置的启动命令 PID 不大于 0或不合法</td>\n</tr>\n<tr>\n<td align=\"left\">10</td>\n<td align=\"left\">ERROR_RESOLVING_FILE_PATH</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">11</td>\n<td align=\"left\">RELATIVE_PATH_COMPONENTS_IN_FILE_PATH</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">12</td>\n<td align=\"left\">UNABLE_TO_STAT_FILE</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">13</td>\n<td align=\"left\">FILE_NOT_OWNED_BY_ROOT</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">14</td>\n<td align=\"left\">PREPARE_CONTAINER_DIRECTORIES_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">15</td>\n<td align=\"left\">INITIALIZE_CONTAINER_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">16</td>\n<td align=\"left\">PREPARE_CONTAINER_LOGS_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">17</td>\n<td align=\"left\">INVALID_LOG_DIR</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">18</td>\n<td align=\"left\">OUT_OF_MEMORY</td>\n<td align=\"left\">启动命令未能获得足够的内存</td>\n</tr>\n<tr>\n<td align=\"left\">19</td>\n<td align=\"left\">INITIALIZE_DISTCACHEFILE_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">20</td>\n<td align=\"left\">INITIALIZE_USER_FAILED</td>\n<td align=\"left\">无法获取用户的 NM 目录</td>\n</tr>\n<tr>\n<td align=\"left\">21</td>\n<td align=\"left\">UNABLE_TO_BUILD_PATH</td>\n<td align=\"left\">启动脚本创建路径失败</td>\n</tr>\n<tr>\n<td align=\"left\">22</td>\n<td align=\"left\">INVALID_CONTAINER_EXEC_PERMISSIONS</td>\n<td align=\"left\">启动脚本没有设置正确的执行权限</td>\n</tr>\n<tr>\n<td align=\"left\">23</td>\n<td align=\"left\">PREPARE_JOB_LOGS_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">24</td>\n<td align=\"left\">INVALID_CONFIG_FILE</td>\n<td align=\"left\">container-executor.cfg文件不存在或者权限不正确</td>\n</tr>\n<tr>\n<td align=\"left\">25</td>\n<td align=\"left\">SETSID_OPER_FAILED</td>\n<td align=\"left\">设置容器的 SID(sessionId?)失败</td>\n</tr>\n<tr>\n<td align=\"left\">26</td>\n<td align=\"left\">WRITE_PIDFILE_FAILED</td>\n<td align=\"left\">无法将 PID 写入 PID 文件</td>\n</tr>\n<tr>\n<td align=\"left\">27</td>\n<td align=\"left\">WRITE_CGROUP_FAILED</td>\n<td align=\"left\">无法写入CGROUP 信息(/sys/fs/cgroup ?)</td>\n</tr>\n<tr>\n<td align=\"left\">其他</td>\n<td align=\"left\">UNKNOWN_ERROR</td>\n<td align=\"left\">需要依据错误信息具体分析</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   </p>\n<h3 id=\"container\"><a href=\"#container\" class=\"headerlink\" title=\"container\"></a>container</h3><p>LinuxContainerExecutor 和 DefaultContainerExecutor 通用的错误码体系，Yarn 框架的一部分，可作为应用程序错误码的补充。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">0</td>\n<td align=\"left\">SUCCESS</td>\n<td align=\"left\">容器成功退出</td>\n</tr>\n<tr>\n<td align=\"left\">-1000</td>\n<td align=\"left\">INVALID</td>\n<td align=\"left\">容器退出码的初始值</td>\n</tr>\n<tr>\n<td align=\"left\">-100</td>\n<td align=\"left\">ABORTED</td>\n<td align=\"left\">容器被框架杀死,可能是 app 释放容器或 nm 失败丢失容器导致</td>\n</tr>\n<tr>\n<td align=\"left\">-101</td>\n<td align=\"left\">DISKS_FAILED</td>\n<td align=\"left\">NM 本地目录数量超过阈值</td>\n</tr>\n<tr>\n<td align=\"left\">-102</td>\n<td align=\"left\">PREEMPTED</td>\n<td align=\"left\">容器被抢占</td>\n</tr>\n<tr>\n<td align=\"left\">-103</td>\n<td align=\"left\">KILLED_EXCEEDED_VMEM</td>\n<td align=\"left\">虚拟内存超限被杀</td>\n</tr>\n<tr>\n<td align=\"left\">-104</td>\n<td align=\"left\">KILLED_EXCEEDED_PMEM</td>\n<td align=\"left\">物理内存超限被杀</td>\n</tr>\n<tr>\n<td align=\"left\">-105</td>\n<td align=\"left\">KILLED_BY_APPMASTER</td>\n<td align=\"left\">被 am 请求终止</td>\n</tr>\n<tr>\n<td align=\"left\">-106</td>\n<td align=\"left\">KILLED_BY_RESOURCEMANAGER</td>\n<td align=\"left\">被 rm 请求终止</td>\n</tr>\n<tr>\n<td align=\"left\">-107</td>\n<td align=\"left\">KILLED_AFTER_APP_COMPLETION</td>\n<td align=\"left\">app 完成后被杀死</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 org.apache.hadoop.yarn.api.records.ContainerExitStatus.java  </p>\n<h3 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h3><p>通用的 Yarn Application 状态码定义  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"></td>\n<td align=\"left\">UNDEFINED</td>\n<td align=\"left\">APP 未完成时的状态码(初始值)</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">SUCCEEDED</td>\n<td align=\"left\">APP 成功完成</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">FAILED</td>\n<td align=\"left\">APP 失败</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">KILLED</td>\n<td align=\"left\">APP 被提交用户或者管理员用户终止</td>\n</tr>\n</tbody></table>\n<p>以上参考 org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  </p>\n<h2 id=\"Spark-退出码\"><a href=\"#Spark-退出码\" class=\"headerlink\" title=\"Spark 退出码\"></a>Spark 退出码</h2><h3 id=\"Executor-退出码\"><a href=\"#Executor-退出码\" class=\"headerlink\" title=\"Executor 退出码\"></a>Executor 退出码</h3><table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">50</td>\n<td align=\"left\">UNCAUGHT_EXCEPTION</td>\n<td align=\"left\">触发了默认的异常处理器(?)</td>\n</tr>\n<tr>\n<td align=\"left\">51</td>\n<td align=\"left\">UNCAUGHT_EXCEPTION_TWICE</td>\n<td align=\"left\">触发了默认的异常处理器,并在记录异常时触发了新的异常(?)</td>\n</tr>\n<tr>\n<td align=\"left\">52</td>\n<td align=\"left\">OOM</td>\n<td align=\"left\">触发了默认的异常处理器,且异常是 OutOfMemoryError</td>\n</tr>\n<tr>\n<td align=\"left\">53</td>\n<td align=\"left\">DISK_STORE_FAILED_TO_CREATE_DIR</td>\n<td align=\"left\">DiskStore 未能创建本地临时目录</td>\n</tr>\n<tr>\n<td align=\"left\">54</td>\n<td align=\"left\">EXTERNAL_BLOCK_STORE_FAILED_TO_INITIALIZE</td>\n<td align=\"left\">ExternalBlockStore 未能初始化</td>\n</tr>\n<tr>\n<td align=\"left\">55</td>\n<td align=\"left\">EXTERNAL_BLOCK_STORE_FAILED_TO_CREATE_DIR</td>\n<td align=\"left\">ExternalBlockStore 未能创建本地临时目录</td>\n</tr>\n<tr>\n<td align=\"left\">56</td>\n<td align=\"left\">HEARTBEAT_FAILURE</td>\n<td align=\"left\">executor 向 driver 发送心跳信号失败次数超过了 spark.executor.heartbeat.maxFailures</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 org.apache.spark.executor.ExecutorExitCode.scala  </p>\n<h3 id=\"ApplicationMaster-退出码\"><a href=\"#ApplicationMaster-退出码\" class=\"headerlink\" title=\"ApplicationMaster 退出码\"></a>ApplicationMaster 退出码</h3><table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">0</td>\n<td align=\"left\">EXIT_SUCCESS</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">10</td>\n<td align=\"left\">EXIT_UNCAUGHT_EXCEPTION</td>\n<td align=\"left\">未捕获的异常(此列表之外的异常)退出</td>\n</tr>\n<tr>\n<td align=\"left\">11</td>\n<td align=\"left\">EXIT_MAX_EXECUTOR_FAILURES</td>\n<td align=\"left\">失败的 executor 数量超过目标值。目标值优先取 spark.yarn.max.executor.failures(默认无),未配置 且 开启动态资源管理时取spark.dynamicAllocation.maxExecutors 的 2 倍,其他情况则取 spark.executor.instances 的 2 倍</td>\n</tr>\n<tr>\n<td align=\"left\">12</td>\n<td align=\"left\">EXIT_REPORTER_FAILURE</td>\n<td align=\"left\">应用进度汇报线程(progress reporter)的失败次数超过spark.yarn.scheduler.reporterThread.maxFailures(默认 5)</td>\n</tr>\n<tr>\n<td align=\"left\">13</td>\n<td align=\"left\">EXIT_SC_NOT_INITED</td>\n<td align=\"left\">SparkContext 初始化时间超过 spark.yarn.am.waitTime(默认 100s)</td>\n</tr>\n<tr>\n<td align=\"left\">14</td>\n<td align=\"left\">EXIT_SECURITY</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">15</td>\n<td align=\"left\">EXIT_EXCEPTION_USER_CLASS</td>\n<td align=\"left\">用户MainClass 抛出异常</td>\n</tr>\n<tr>\n<td align=\"left\">16</td>\n<td align=\"left\">EXIT_EARLY</td>\n<td align=\"left\">在 APP 结束前调用了 ShutdownHook</td>\n</tr>\n</tbody></table>\n<p>以上参考 org.apache.spark.deploy.yarn.ApplicationMaster.scala  </p>\n<h2 id=\"JVM-退出码\"><a href=\"#JVM-退出码\" class=\"headerlink\" title=\"JVM 退出码\"></a>JVM 退出码</h2><p>数值上超过 128 的退出码很有可能是由 Unix Signal 触发的程序关闭导致的，可以通过用退出码减去 128来计算 Unix Signal。例如最常见的 137 退出码是由 kill -9 强制杀死抛出的，143 退出码一般是由 kill 或 kill -15 杀死抛出的。 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">137</td>\n<td align=\"left\">FORCE_KILLED</td>\n<td align=\"left\">128+9,即是由 kill -9 命令强制终止,一般是虚拟或物理内存超限导致、也有可能是操作系统整体内存紧张杀死进程</td>\n</tr>\n<tr>\n<td align=\"left\">143</td>\n<td align=\"left\">TERMINATED</td>\n<td align=\"left\">128+15,即是由 kill 命令终止的,一般是 OOM/FGC 导致</td>\n</tr>\n<tr>\n<td align=\"left\">154</td>\n<td align=\"left\">LOST</td>\n<td align=\"left\">128+27,虚拟计时器过期(?)</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then I ran the program in one terminal window (java Death; echo $?) while iterating through all kill signals (0-31) in another:</span><br><span class=\"line\">kill -$SIGNAL $(jps | grep Death | cut -d\\  -f1)</span><br><span class=\"line\"></span><br><span class=\"line\">signal\t    shutdown runs hook exit code    comment</span><br><span class=\"line\">default (15)\tyes\tyes\t143\tSIGTERM is the default unix kill signal</span><br><span class=\"line\">0\tno\t-\t-\t</span><br><span class=\"line\">1 (SIGHUP)\tyes\tyes\t129\t</span><br><span class=\"line\">2 (SIGINT)\tyes\tyes\t130\tSIGINT is the signal sent on ^C</span><br><span class=\"line\">3 (SIGQUIT)\tno\t-\t-\tMakes the JVM dump threads &#x2F; stack-traces</span><br><span class=\"line\">4 (SIGILL)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6</span><br><span class=\"line\">5\tyes\tno\t133\tMakes the JVM exit with &quot;Trace&#x2F;BPT trap: 5&quot;</span><br><span class=\"line\">6 (SIGABRT)\tyes\tno\t134\tMakes the JVM exit with &quot;Abort trap: 6&quot;</span><br><span class=\"line\">7\tyes\tno\t135\tMakes the JVM exit with &quot;EMT trap: 7&quot;</span><br><span class=\"line\">8 (SIGFPE)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6</span><br><span class=\"line\">9 (SIGKILL)\tyes\tno\t137\tThe JVM is forcibly killed (exits with &quot;Killed: 9&quot;)</span><br><span class=\"line\">10 (SIGBUS)\tyes\tno\t134\tEmulates a &quot;Bus Error&quot;</span><br><span class=\"line\">11 (SIGSEGV)\tyes\tno\t134\tEmulates a &quot;Segmentation fault&quot;</span><br><span class=\"line\">12\tyes\tno\t140\tMakes the JVM exit with &quot;Bad system call: 12&quot;</span><br><span class=\"line\">13\tno\t-\t-\t</span><br><span class=\"line\">14\tyes\tno\t142\tMakes the JVM exit with &quot;Alarm clock: 14&quot;</span><br><span class=\"line\">15 (SIGTERM)\tyes\tyes\t143\tThis is the default unix kill signal</span><br><span class=\"line\">16\tno\t-\t-\t</span><br><span class=\"line\">17\tno\t-\t145\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">18\tno\t-\t146\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">19\tno\t-\t-\t</span><br><span class=\"line\">20\tno\t-\t-\t</span><br><span class=\"line\">21\tno\t-\t149\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">22\tno\t-\t150\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">23\tno\t-\t-\t</span><br><span class=\"line\">24\tyes\tno\t152\tMakes the JVM exit with &quot;Cputime limit exceeded: 24&quot;</span><br><span class=\"line\">25\tno\t-\t-\t</span><br><span class=\"line\">26\tyes\tno\t154\tMakes the JVM exit with &quot;Virtual timer expired: 26&quot;</span><br><span class=\"line\">27\tyes\tno\t155\tMakes the JVM exit with &quot;Profiling timer expired: 27&quot;</span><br><span class=\"line\">28\tno\t-\t-\t</span><br><span class=\"line\">29\tno\t-\t-\t</span><br><span class=\"line\">30\tyes\tno\t158\tMakes the JVM exit with &quot;User defined signal 1: 30&quot;</span><br><span class=\"line\">31\tyes\tno\t134\tMakes the JVM exit on Segmentation fault</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">This list was compiled using (a quite old) Oracle Hotspot Java 8 EA on Mac OS X:</span><br><span class=\"line\">java version &quot;1.8.0-ea&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0-ea-b65)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.0-b09, mixed mode)</span><br></pre></td></tr></table></figure>\n\n<p>以上主要参考 <a href=\"http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\" target=\"_blank\" rel=\"noopener\">http://journal.thobe.org/2013/02/jvms-and-kill-signals.html</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li>hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   </li>\n<li>org.apache.hadoop.yarn.api.records.ContainerExitStatus.java    </li>\n<li>org.apache.spark.executor.ExecutorExitCode.scala  </li>\n<li><a href=\"http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\" target=\"_blank\" rel=\"noopener\">http://journal.thobe.org/2013/02/jvms-and-kill-signals.html</a>  </li>\n<li>org.apache.hadoop.yarn.server.nodemanager.ExitCode.java</li>\n<li>org.apache.spark.deploy.yarn.ApplicationMaster.scala  </li>\n<li>org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  </li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>SparkOnYarn 程序的退出码种类比较繁多逻辑杂乱，主要包括：   </p>\n<ol>\n<li>Yarn 退出码      </li>\n<li>Spark 退出码    </li>\n<li>JVM 退出码   </li>\n</ol>\n<p>其中一些退出码可能是多个环节共同作用最终展现到日志上，e.g. SparkOnYarn 程序封装 driver/executor 的 ContainerLauchContext 时启动命令时加入”-XX:OnOutOfMemoryError=kill %p”，所以当程序 OOM 使用默认 signal(15) 终止 Container 脚本启动的 JVM，上报到 SparkOnYarn 日志的退出码是 143(128+15)。 </p>","more":"<p>以下均基于 Hadoop2.7.3，Spark 2.3-SNAPSHOT，jdk1.8 </p>\n<h2 id=\"Yarn-退出码\"><a href=\"#Yarn-退出码\" class=\"headerlink\" title=\"Yarn 退出码\"></a>Yarn 退出码</h2><p>Yarn container 的退出码分为两类: container-executor 和 container。container 是 Yarn 资源调度的基础，也是资源隔离的基础，在同一台服务器上运行的多个container 之间内存、CPU和权限等方面不应该相互影响(实际上内存和 CPU 没有做到绝对的资源隔离,仅仅监控内存/不管 CPU,在此不细说)。container-executor 负责使用 Yarn 参考 ContainerLaunchContext 上下文环境组装的脚本 初始化、启动和终止 Container，因此其错误码体系更像是操作系统的进程退出码。container-executor 共分两类: 通用的 DefaultContainerExecutor，所有的 Container 的用户均为启动 NodeManager 的用户；LinuxContainerExecutor，每个 Container 进程可以由不同用户启动，并支持 CGROUP 和 ACL。container 错误码体系适用于所有容器，是 Yarn 框架的一部分，应用程序错误码体系的补充。即 container-executor 错误码体系更靠近操作系统，container 错误码体系更靠近应用程序。  </p>\n<h3 id=\"container-executor\"><a href=\"#container-executor\" class=\"headerlink\" title=\"container-executor\"></a>container-executor</h3><p>仅列出 LinuxContainerExecutor 错误码体系  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">INVALID_ARGUMENT_NUMBER</td>\n<td align=\"left\">1.启动脚本给定输入参数数量不符 2.未能成功初始化</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">INVALID_USER_NAME</td>\n<td align=\"left\">启动脚本所属用户不存在</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">INVALID_COMMAND_PROVIDED</td>\n<td align=\"left\">无法识别被提供的启动命令</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">SUPER_USER_NOT_ALLOWED_TO_RUN_TASKS</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">INVALID_NM_ROOT_DIRS</td>\n<td align=\"left\">启动脚本参数定义的 NM root 目录不存在</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">SETUID_OPER_FAILED</td>\n<td align=\"left\">无法设置 UID 或 GID</td>\n</tr>\n<tr>\n<td align=\"left\">7</td>\n<td align=\"left\">UNABLE_TO_EXECUTE_CONTAINER_SCRIPT</td>\n<td align=\"left\">无法运行启动脚本</td>\n</tr>\n<tr>\n<td align=\"left\">8</td>\n<td align=\"left\">UNABLE_TO_SIGNAL_CONTAINER</td>\n<td align=\"left\">无法向指定容器发送信号</td>\n</tr>\n<tr>\n<td align=\"left\">9</td>\n<td align=\"left\">INVALID_CONTAINER_PID</td>\n<td align=\"left\">设置的启动命令 PID 不大于 0或不合法</td>\n</tr>\n<tr>\n<td align=\"left\">10</td>\n<td align=\"left\">ERROR_RESOLVING_FILE_PATH</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">11</td>\n<td align=\"left\">RELATIVE_PATH_COMPONENTS_IN_FILE_PATH</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">12</td>\n<td align=\"left\">UNABLE_TO_STAT_FILE</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">13</td>\n<td align=\"left\">FILE_NOT_OWNED_BY_ROOT</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">14</td>\n<td align=\"left\">PREPARE_CONTAINER_DIRECTORIES_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">15</td>\n<td align=\"left\">INITIALIZE_CONTAINER_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">16</td>\n<td align=\"left\">PREPARE_CONTAINER_LOGS_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">17</td>\n<td align=\"left\">INVALID_LOG_DIR</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">18</td>\n<td align=\"left\">OUT_OF_MEMORY</td>\n<td align=\"left\">启动命令未能获得足够的内存</td>\n</tr>\n<tr>\n<td align=\"left\">19</td>\n<td align=\"left\">INITIALIZE_DISTCACHEFILE_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">20</td>\n<td align=\"left\">INITIALIZE_USER_FAILED</td>\n<td align=\"left\">无法获取用户的 NM 目录</td>\n</tr>\n<tr>\n<td align=\"left\">21</td>\n<td align=\"left\">UNABLE_TO_BUILD_PATH</td>\n<td align=\"left\">启动脚本创建路径失败</td>\n</tr>\n<tr>\n<td align=\"left\">22</td>\n<td align=\"left\">INVALID_CONTAINER_EXEC_PERMISSIONS</td>\n<td align=\"left\">启动脚本没有设置正确的执行权限</td>\n</tr>\n<tr>\n<td align=\"left\">23</td>\n<td align=\"left\">PREPARE_JOB_LOGS_FAILED</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">24</td>\n<td align=\"left\">INVALID_CONFIG_FILE</td>\n<td align=\"left\">container-executor.cfg文件不存在或者权限不正确</td>\n</tr>\n<tr>\n<td align=\"left\">25</td>\n<td align=\"left\">SETSID_OPER_FAILED</td>\n<td align=\"left\">设置容器的 SID(sessionId?)失败</td>\n</tr>\n<tr>\n<td align=\"left\">26</td>\n<td align=\"left\">WRITE_PIDFILE_FAILED</td>\n<td align=\"left\">无法将 PID 写入 PID 文件</td>\n</tr>\n<tr>\n<td align=\"left\">27</td>\n<td align=\"left\">WRITE_CGROUP_FAILED</td>\n<td align=\"left\">无法写入CGROUP 信息(/sys/fs/cgroup ?)</td>\n</tr>\n<tr>\n<td align=\"left\">其他</td>\n<td align=\"left\">UNKNOWN_ERROR</td>\n<td align=\"left\">需要依据错误信息具体分析</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   </p>\n<h3 id=\"container\"><a href=\"#container\" class=\"headerlink\" title=\"container\"></a>container</h3><p>LinuxContainerExecutor 和 DefaultContainerExecutor 通用的错误码体系，Yarn 框架的一部分，可作为应用程序错误码的补充。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">0</td>\n<td align=\"left\">SUCCESS</td>\n<td align=\"left\">容器成功退出</td>\n</tr>\n<tr>\n<td align=\"left\">-1000</td>\n<td align=\"left\">INVALID</td>\n<td align=\"left\">容器退出码的初始值</td>\n</tr>\n<tr>\n<td align=\"left\">-100</td>\n<td align=\"left\">ABORTED</td>\n<td align=\"left\">容器被框架杀死,可能是 app 释放容器或 nm 失败丢失容器导致</td>\n</tr>\n<tr>\n<td align=\"left\">-101</td>\n<td align=\"left\">DISKS_FAILED</td>\n<td align=\"left\">NM 本地目录数量超过阈值</td>\n</tr>\n<tr>\n<td align=\"left\">-102</td>\n<td align=\"left\">PREEMPTED</td>\n<td align=\"left\">容器被抢占</td>\n</tr>\n<tr>\n<td align=\"left\">-103</td>\n<td align=\"left\">KILLED_EXCEEDED_VMEM</td>\n<td align=\"left\">虚拟内存超限被杀</td>\n</tr>\n<tr>\n<td align=\"left\">-104</td>\n<td align=\"left\">KILLED_EXCEEDED_PMEM</td>\n<td align=\"left\">物理内存超限被杀</td>\n</tr>\n<tr>\n<td align=\"left\">-105</td>\n<td align=\"left\">KILLED_BY_APPMASTER</td>\n<td align=\"left\">被 am 请求终止</td>\n</tr>\n<tr>\n<td align=\"left\">-106</td>\n<td align=\"left\">KILLED_BY_RESOURCEMANAGER</td>\n<td align=\"left\">被 rm 请求终止</td>\n</tr>\n<tr>\n<td align=\"left\">-107</td>\n<td align=\"left\">KILLED_AFTER_APP_COMPLETION</td>\n<td align=\"left\">app 完成后被杀死</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 org.apache.hadoop.yarn.api.records.ContainerExitStatus.java  </p>\n<h3 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h3><p>通用的 Yarn Application 状态码定义  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"></td>\n<td align=\"left\">UNDEFINED</td>\n<td align=\"left\">APP 未完成时的状态码(初始值)</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">SUCCEEDED</td>\n<td align=\"left\">APP 成功完成</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">FAILED</td>\n<td align=\"left\">APP 失败</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">KILLED</td>\n<td align=\"left\">APP 被提交用户或者管理员用户终止</td>\n</tr>\n</tbody></table>\n<p>以上参考 org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  </p>\n<h2 id=\"Spark-退出码\"><a href=\"#Spark-退出码\" class=\"headerlink\" title=\"Spark 退出码\"></a>Spark 退出码</h2><h3 id=\"Executor-退出码\"><a href=\"#Executor-退出码\" class=\"headerlink\" title=\"Executor 退出码\"></a>Executor 退出码</h3><table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">50</td>\n<td align=\"left\">UNCAUGHT_EXCEPTION</td>\n<td align=\"left\">触发了默认的异常处理器(?)</td>\n</tr>\n<tr>\n<td align=\"left\">51</td>\n<td align=\"left\">UNCAUGHT_EXCEPTION_TWICE</td>\n<td align=\"left\">触发了默认的异常处理器,并在记录异常时触发了新的异常(?)</td>\n</tr>\n<tr>\n<td align=\"left\">52</td>\n<td align=\"left\">OOM</td>\n<td align=\"left\">触发了默认的异常处理器,且异常是 OutOfMemoryError</td>\n</tr>\n<tr>\n<td align=\"left\">53</td>\n<td align=\"left\">DISK_STORE_FAILED_TO_CREATE_DIR</td>\n<td align=\"left\">DiskStore 未能创建本地临时目录</td>\n</tr>\n<tr>\n<td align=\"left\">54</td>\n<td align=\"left\">EXTERNAL_BLOCK_STORE_FAILED_TO_INITIALIZE</td>\n<td align=\"left\">ExternalBlockStore 未能初始化</td>\n</tr>\n<tr>\n<td align=\"left\">55</td>\n<td align=\"left\">EXTERNAL_BLOCK_STORE_FAILED_TO_CREATE_DIR</td>\n<td align=\"left\">ExternalBlockStore 未能创建本地临时目录</td>\n</tr>\n<tr>\n<td align=\"left\">56</td>\n<td align=\"left\">HEARTBEAT_FAILURE</td>\n<td align=\"left\">executor 向 driver 发送心跳信号失败次数超过了 spark.executor.heartbeat.maxFailures</td>\n</tr>\n</tbody></table>\n<p>以上主要参考 org.apache.spark.executor.ExecutorExitCode.scala  </p>\n<h3 id=\"ApplicationMaster-退出码\"><a href=\"#ApplicationMaster-退出码\" class=\"headerlink\" title=\"ApplicationMaster 退出码\"></a>ApplicationMaster 退出码</h3><table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">0</td>\n<td align=\"left\">EXIT_SUCCESS</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">10</td>\n<td align=\"left\">EXIT_UNCAUGHT_EXCEPTION</td>\n<td align=\"left\">未捕获的异常(此列表之外的异常)退出</td>\n</tr>\n<tr>\n<td align=\"left\">11</td>\n<td align=\"left\">EXIT_MAX_EXECUTOR_FAILURES</td>\n<td align=\"left\">失败的 executor 数量超过目标值。目标值优先取 spark.yarn.max.executor.failures(默认无),未配置 且 开启动态资源管理时取spark.dynamicAllocation.maxExecutors 的 2 倍,其他情况则取 spark.executor.instances 的 2 倍</td>\n</tr>\n<tr>\n<td align=\"left\">12</td>\n<td align=\"left\">EXIT_REPORTER_FAILURE</td>\n<td align=\"left\">应用进度汇报线程(progress reporter)的失败次数超过spark.yarn.scheduler.reporterThread.maxFailures(默认 5)</td>\n</tr>\n<tr>\n<td align=\"left\">13</td>\n<td align=\"left\">EXIT_SC_NOT_INITED</td>\n<td align=\"left\">SparkContext 初始化时间超过 spark.yarn.am.waitTime(默认 100s)</td>\n</tr>\n<tr>\n<td align=\"left\">14</td>\n<td align=\"left\">EXIT_SECURITY</td>\n<td align=\"left\">未启用</td>\n</tr>\n<tr>\n<td align=\"left\">15</td>\n<td align=\"left\">EXIT_EXCEPTION_USER_CLASS</td>\n<td align=\"left\">用户MainClass 抛出异常</td>\n</tr>\n<tr>\n<td align=\"left\">16</td>\n<td align=\"left\">EXIT_EARLY</td>\n<td align=\"left\">在 APP 结束前调用了 ShutdownHook</td>\n</tr>\n</tbody></table>\n<p>以上参考 org.apache.spark.deploy.yarn.ApplicationMaster.scala  </p>\n<h2 id=\"JVM-退出码\"><a href=\"#JVM-退出码\" class=\"headerlink\" title=\"JVM 退出码\"></a>JVM 退出码</h2><p>数值上超过 128 的退出码很有可能是由 Unix Signal 触发的程序关闭导致的，可以通过用退出码减去 128来计算 Unix Signal。例如最常见的 137 退出码是由 kill -9 强制杀死抛出的，143 退出码一般是由 kill 或 kill -15 杀死抛出的。 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">错误码</th>\n<th align=\"left\">错误信息</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">137</td>\n<td align=\"left\">FORCE_KILLED</td>\n<td align=\"left\">128+9,即是由 kill -9 命令强制终止,一般是虚拟或物理内存超限导致、也有可能是操作系统整体内存紧张杀死进程</td>\n</tr>\n<tr>\n<td align=\"left\">143</td>\n<td align=\"left\">TERMINATED</td>\n<td align=\"left\">128+15,即是由 kill 命令终止的,一般是 OOM/FGC 导致</td>\n</tr>\n<tr>\n<td align=\"left\">154</td>\n<td align=\"left\">LOST</td>\n<td align=\"left\">128+27,虚拟计时器过期(?)</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Then I ran the program in one terminal window (java Death; echo $?) while iterating through all kill signals (0-31) in another:</span><br><span class=\"line\">kill -$SIGNAL $(jps | grep Death | cut -d\\  -f1)</span><br><span class=\"line\"></span><br><span class=\"line\">signal\t    shutdown runs hook exit code    comment</span><br><span class=\"line\">default (15)\tyes\tyes\t143\tSIGTERM is the default unix kill signal</span><br><span class=\"line\">0\tno\t-\t-\t</span><br><span class=\"line\">1 (SIGHUP)\tyes\tyes\t129\t</span><br><span class=\"line\">2 (SIGINT)\tyes\tyes\t130\tSIGINT is the signal sent on ^C</span><br><span class=\"line\">3 (SIGQUIT)\tno\t-\t-\tMakes the JVM dump threads &#x2F; stack-traces</span><br><span class=\"line\">4 (SIGILL)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6</span><br><span class=\"line\">5\tyes\tno\t133\tMakes the JVM exit with &quot;Trace&#x2F;BPT trap: 5&quot;</span><br><span class=\"line\">6 (SIGABRT)\tyes\tno\t134\tMakes the JVM exit with &quot;Abort trap: 6&quot;</span><br><span class=\"line\">7\tyes\tno\t135\tMakes the JVM exit with &quot;EMT trap: 7&quot;</span><br><span class=\"line\">8 (SIGFPE)\tyes\tno\t134\tMakes the JVM write a core dump and abort on trap 6</span><br><span class=\"line\">9 (SIGKILL)\tyes\tno\t137\tThe JVM is forcibly killed (exits with &quot;Killed: 9&quot;)</span><br><span class=\"line\">10 (SIGBUS)\tyes\tno\t134\tEmulates a &quot;Bus Error&quot;</span><br><span class=\"line\">11 (SIGSEGV)\tyes\tno\t134\tEmulates a &quot;Segmentation fault&quot;</span><br><span class=\"line\">12\tyes\tno\t140\tMakes the JVM exit with &quot;Bad system call: 12&quot;</span><br><span class=\"line\">13\tno\t-\t-\t</span><br><span class=\"line\">14\tyes\tno\t142\tMakes the JVM exit with &quot;Alarm clock: 14&quot;</span><br><span class=\"line\">15 (SIGTERM)\tyes\tyes\t143\tThis is the default unix kill signal</span><br><span class=\"line\">16\tno\t-\t-\t</span><br><span class=\"line\">17\tno\t-\t145\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">18\tno\t-\t146\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">19\tno\t-\t-\t</span><br><span class=\"line\">20\tno\t-\t-\t</span><br><span class=\"line\">21\tno\t-\t149\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">22\tno\t-\t150\tStops the application (sends it to the background), same as ^Z</span><br><span class=\"line\">23\tno\t-\t-\t</span><br><span class=\"line\">24\tyes\tno\t152\tMakes the JVM exit with &quot;Cputime limit exceeded: 24&quot;</span><br><span class=\"line\">25\tno\t-\t-\t</span><br><span class=\"line\">26\tyes\tno\t154\tMakes the JVM exit with &quot;Virtual timer expired: 26&quot;</span><br><span class=\"line\">27\tyes\tno\t155\tMakes the JVM exit with &quot;Profiling timer expired: 27&quot;</span><br><span class=\"line\">28\tno\t-\t-\t</span><br><span class=\"line\">29\tno\t-\t-\t</span><br><span class=\"line\">30\tyes\tno\t158\tMakes the JVM exit with &quot;User defined signal 1: 30&quot;</span><br><span class=\"line\">31\tyes\tno\t134\tMakes the JVM exit on Segmentation fault</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">This list was compiled using (a quite old) Oracle Hotspot Java 8 EA on Mac OS X:</span><br><span class=\"line\">java version &quot;1.8.0-ea&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0-ea-b65)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.0-b09, mixed mode)</span><br></pre></td></tr></table></figure>\n\n<p>以上主要参考 <a href=\"http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\" target=\"_blank\" rel=\"noopener\">http://journal.thobe.org/2013/02/jvms-and-kill-signals.html</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li>hadoop-project/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h   </li>\n<li>org.apache.hadoop.yarn.api.records.ContainerExitStatus.java    </li>\n<li>org.apache.spark.executor.ExecutorExitCode.scala  </li>\n<li><a href=\"http://journal.thobe.org/2013/02/jvms-and-kill-signals.html\" target=\"_blank\" rel=\"noopener\">http://journal.thobe.org/2013/02/jvms-and-kill-signals.html</a>  </li>\n<li>org.apache.hadoop.yarn.server.nodemanager.ExitCode.java</li>\n<li>org.apache.spark.deploy.yarn.ApplicationMaster.scala  </li>\n<li>org.apache.hadoop.yarn.api.records.FinalApplicationStatus.java  </li>\n</ol>"},{"layout":"post","title":"Yarn User Headroom","date":"2019-11-11T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\nResourceManager 尝试在 NM 上分配容器的过程中，主要考虑的限制条件如下:   \na)\t队列具有该节点的 access 权限,节点不在 APP 设置的黑名单内  \nb)\t节点上无保留且节点剩余空间大于最小容器大小(1G)  \nc)\t最高配额限制:如果分配该容器,叶子队列及其所有父队列均不能超过各自的最高配额。  \nd)\t用户配额限制(headroom):瞬时量,该容器大小不能超过 headroom 当前值。    \ne)\tAM 限制:不超过队列 AM 限制(AM资源和AM数量限制)  \n主要限制在 c)和 d)。c)是在在队列层面对队列可使用资源量做的限制; d) user headroom 是在用户层面对用户可使用资源量做的限制。只有在不违背上述条件时，容器才会在当前 NM 上分配或保留。\n\n<!-- more -->\n\n>\nuser headroom 的计算过程主要有三步,  \n第一步: 计算队列的配置容量 queueCapacity，即 max{capacity, required}        \n第二步: 计算队列的当前容量 currentCapacity，即分段函数 capacity 和 used + required   \n第三步: 计算用户限制 limit，即 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}  \n\n主要和两个配置有关:  \n1.  yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent(程序内对应 userLimit)  \n　表示队列内多用户均分资源的方式，默认为 100。此值可以在一个极小值与极大值之间动态变化,该配置决定极小值,极大值由队列内活跃用户数决定。e.g. 将此值配置为 30 时, 当队列内仅有1个用户时,该用户最多使用队列100%资源;2个用户,每个用户最多使用50%资源;3个用户,每个用户最多可以使用 33.3%资源;4 个及更多时 每个用户最多可以使用 30%资源。即 [0,max{0.3,1/activeUser}]，体现在源码中   \n　\n\n```\n　Resources.max(\n                    resourceCalculator, clusterResource,\n                    //当前容量/活跃用户数\n                    Resources.divideAndCeil(\n                        resourceCalculator, currentCapacity, activeUsers),\n                    //当前容量 * 用户限制/100\n                    Resources.divideAndCeil(\n                        resourceCalculator,\n                        Resources.multiplyAndRoundDown(\n                            currentCapacity, userLimit),\n                        100)\n                    );\n```\n2.  yarn.scheduler.capacity.<queue-path>.user-limit-factor(程序内对应 userLimitFactor)  \n　表示一个用户可以最大获取资源的能力,可以获取当前队列容量乘以该值的最大资源，默认为 1 以确保无论集群负载如何 单用户都不能使用超过队列容量的资源。体现在源码中  \n\n```\n　Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)\n```\n\n[LeafQueue.java]\n\n```\nprivate Resource computeUserLimit(FiCaSchedulerApp application,\n      Resource clusterResource, Resource required, User user,\n      Set<String> requestedLabels) {\n    Resource queueCapacity = Resource.newInstance(0, 0);\n    // What is our current capacity?\n    // * It is equal to the max(required, queue-capacity) if\n    //   we're running below capacity. The 'max' ensures that jobs in queues\n    //   with miniscule capacity (< 1 slot) make progress\n    // * If we're running over capacity, then its\n    //   (usedResources + required) (which extra resources we are allocating)\n    // 第一步: 依据标签计算队列的 capacity\n    if (requestedLabels != null && !requestedLabels.isEmpty()) {\n     String firstLabel = requestedLabels.iterator().next();\n      queueCapacity =\n          Resources\n              .max(resourceCalculator, clusterResource, queueCapacity,\n                  Resources.multiplyAndNormalizeUp(resourceCalculator,\n                      labelManager.getResourceByLabel(firstLabel,\n                          clusterResource),\n                      queueCapacities.getAbsoluteCapacity(firstLabel),\n                      minimumAllocation));\n    } else {\n      queueCapacity =\n          Resources.multiplyAndNormalizeUp(resourceCalculator, labelManager\n                .getResourceByLabel(CommonNodeLabelsManager.NO_LABEL, clusterResource),\n              queueCapacities.getAbsoluteCapacity(), minimumAllocation);\n    }\n    queueCapacity =\n        Resources.max(\n            resourceCalculator, clusterResource,\n            queueCapacity,\n            required);\n    // 第二步: 计算 currentCapacity\n    Resource currentCapacity =\n        Resources.lessThan(resourceCalculator, clusterResource,\n            queueUsage.getUsed(), queueCapacity) ?\n            queueCapacity : Resources.add(queueUsage.getUsed(), required);\n\n    // Never allow a single user to take more than the\n    // queue's configured capacity * user-limit-factor.\n    // Also, the queue's configured capacity should be higher than\n    // queue-hard-limit * ulMin\n\n    final int activeUsers = activeUsersManager.getNumActiveUsers();\n    // 第三步: 计算用户可使用的最高容量\n    Resource limit =\n        Resources.roundUp(\n            resourceCalculator,\n            Resources.min(\n                resourceCalculator, clusterResource,\n                Resources.max(\n                    resourceCalculator, clusterResource,\n                    //当前容量/活跃用户数\n                    Resources.divideAndCeil(\n                        resourceCalculator, currentCapacity, activeUsers),\n                    //当前容量 * 最高用户数/100\n                    Resources.divideAndCeil(\n                        resourceCalculator,\n                        Resources.multiplyAndRoundDown(\n                            currentCapacity, userLimit),\n                        100)\n                    ),\n                //当前容量 * 用户限制因子(倍数)\n                Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)\n                ),\n            minimumAllocation);\n    .....\n    user.setUserResourceLimit(limit);\n    return limit;\n  }\n```\n## 测试场景\n测试项: 用户提交应用时 user headroom 计算方式及能否平分集群资源。  \n用户限制因子 userLimitFactor = 100，queueCapacity * userLimitFactor >> clusterResource，即单用户资源上限很高  \n用户限制 userLimit = 30，即用户可以使用队列[0,max{0.3,1/activeUser}]资源  \n\n###  scene 1 单用户单队列单应用可以独占集群资源\n单用户时 user headroom 的变化 随着常规调度模块为应用不断分配资源(可用集群全部资源) 分为(分段函数)两个阶段:    \n1. 当 used < queueCapacity 时，limit = queueCapacity  \n2. 当 used >= queueCapacity 时，limit = used + required。上限是 min{queueCapacity * userLimitFactor, clusterResource}  \n\n单用户时 user headroom 变化曲线如下:  \n\n![](/img/pictures/yarn_headroom/headroom_1.jpeg)\n\n### scene 2 单用户双队列双应用未必平分集群资源 \n基于 scene 1,随后在另外一个队列(Queue\\_B)提交一个新的 APP(M 个 N 大小的容器)。该用户在 Queue\\_B 的 headroom 可分为两个阶段:  \n1. 当 used < queueCapacity 时,limit = queueCapacity。此时抢占调度模块 Queue\\_B 的 堆积(pending)ResourceRequest = min{limit-used, M * N}。抢占模块会负责将 overCapacity 队列的资源释放，Queue\\_B 队列可以有资源运行。即 Queue\\_B 可以运行容器,used 值逼近 queueCapacity，pendingResourceRequest 值逼近于 0(虽然此时 APP 还有很多容器没有分配)。  \n2.  这个阶段有两种可能，关键在于: 常规调度模块能否利用集群剩余资源为 Queue\\_B 再分配一个容器，使其 used > queueCapacity。  \n　　2a.不能分配一个容器。limit = queueCapacity，pendingResourceRequest = min{limit - used,M * N} 趋近于 0。pendingResourceRequest 会触发抢占调度(不考虑 0.2 的deadzone,超过 capacity * (1+deadzone)的 overCapacity 队列才会真正还回资源)。Queue\\_B 欠分配，最终理想容量为 used + pendingResourceRequest(因为很小,且初始 idealAssigned/guarantee 小,会先被分配)，pendingResourceRequest(min{limit-used,M * N}) 的资源量不足以启动一个容器(原因很多,尤其是大容器,抢占的资源分布在繁忙集群的多个节点上,每个节点都不足以启动一个容器；pendingResourceRequest 被 limit 削弱过,不足以启动一个容器)。此时 user 在 Queue_B 的 headroom 恒等于 queueCapacity。抢占也不能抢回足够资源。          \n　　2b. 能分配一个容器，使队列 used > queueCapacity。此时队列的 currentCapacity 值由 queueCapacity(常量)跃迁至 used+required，提高了 headroom 上限,进而提高了 pendingResourceRequest 上限，抢占调度依据 pendingResourceRequest 值计算集群各队列的理想容量，抢回了资源足以启动容器，又提升了 used。剩下限制 APP 资源量的条件只有 min{queueCapacity * userLimitFactor, maxCapacity,平分 clusterResource}了。\n\n单用户双队列双 APP 的 user headroom 变化曲线如下:  \n\n![](/img/pictures/yarn_headroom/headroom_2.jpeg)\n\n### scene 3 多用户多队列应用更难平分集群资源 \n 由上述源码可知,limit 取值为 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}    \n 其中 activeUser 为整个系统的活跃用户数，即多用户下 limit 取值比单用户更低，但是分段函数的跃迁规则是不变的(used > capacity?)，即系统既要为APP 分配容器，而且容器大小要使队列 used 资源量从 queueCapcity/activeUser 或者 queueCapacity * userLimit/100 跃迁至超过 queueCapacity。\n \n## 一点思考  \n1. 设置 userLimitFactor 很大时(超过 clusterResource),实现多用户平分集群资源是不现实的。第一个提交的大应用将会先独占集群资源，后续提交的应用借助抢占调度获得部分资源，但资源上界一般不会超过队列的容量，越来越难平分资源。但是如果缩小 userLimitFactor,使其不超过 clusterResource的某阈值，其他队列跃迁的可能性增加，通过抢占调度是可实现多用户平分资源。     \n2. user headroom 是为了解决在多用户环境下限制用户可使用资源量,为每个用户维护的瞬时状态量，通过 userLimit 和 userLimitFactor 保证了用户可使用资源的上界。  \n3. user headroom 的计算方式: min{queueCapacity * userLimitFactor, max{currentCapacity/activeUser, currentCapacity * userLimit/100}}         \n4. Yarn 的三大调度方式: 常规调度/抢占调度/预订调度 对于大容器都是不友好的。  \n\n \n","source":"_posts/2019-11-12-Yarn User Headroom.md","raw":"---\nlayout:     post\ntitle:      Yarn User Headroom\ndate:       2019-11-12\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Yarn 2.7.3\n    - 源码解析\n---\n\nResourceManager 尝试在 NM 上分配容器的过程中，主要考虑的限制条件如下:   \na)\t队列具有该节点的 access 权限,节点不在 APP 设置的黑名单内  \nb)\t节点上无保留且节点剩余空间大于最小容器大小(1G)  \nc)\t最高配额限制:如果分配该容器,叶子队列及其所有父队列均不能超过各自的最高配额。  \nd)\t用户配额限制(headroom):瞬时量,该容器大小不能超过 headroom 当前值。    \ne)\tAM 限制:不超过队列 AM 限制(AM资源和AM数量限制)  \n主要限制在 c)和 d)。c)是在在队列层面对队列可使用资源量做的限制; d) user headroom 是在用户层面对用户可使用资源量做的限制。只有在不违背上述条件时，容器才会在当前 NM 上分配或保留。\n\n<!-- more -->\n\n>\nuser headroom 的计算过程主要有三步,  \n第一步: 计算队列的配置容量 queueCapacity，即 max{capacity, required}        \n第二步: 计算队列的当前容量 currentCapacity，即分段函数 capacity 和 used + required   \n第三步: 计算用户限制 limit，即 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}  \n\n主要和两个配置有关:  \n1.  yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent(程序内对应 userLimit)  \n　表示队列内多用户均分资源的方式，默认为 100。此值可以在一个极小值与极大值之间动态变化,该配置决定极小值,极大值由队列内活跃用户数决定。e.g. 将此值配置为 30 时, 当队列内仅有1个用户时,该用户最多使用队列100%资源;2个用户,每个用户最多使用50%资源;3个用户,每个用户最多可以使用 33.3%资源;4 个及更多时 每个用户最多可以使用 30%资源。即 [0,max{0.3,1/activeUser}]，体现在源码中   \n　\n\n```\n　Resources.max(\n                    resourceCalculator, clusterResource,\n                    //当前容量/活跃用户数\n                    Resources.divideAndCeil(\n                        resourceCalculator, currentCapacity, activeUsers),\n                    //当前容量 * 用户限制/100\n                    Resources.divideAndCeil(\n                        resourceCalculator,\n                        Resources.multiplyAndRoundDown(\n                            currentCapacity, userLimit),\n                        100)\n                    );\n```\n2.  yarn.scheduler.capacity.<queue-path>.user-limit-factor(程序内对应 userLimitFactor)  \n　表示一个用户可以最大获取资源的能力,可以获取当前队列容量乘以该值的最大资源，默认为 1 以确保无论集群负载如何 单用户都不能使用超过队列容量的资源。体现在源码中  \n\n```\n　Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)\n```\n\n[LeafQueue.java]\n\n```\nprivate Resource computeUserLimit(FiCaSchedulerApp application,\n      Resource clusterResource, Resource required, User user,\n      Set<String> requestedLabels) {\n    Resource queueCapacity = Resource.newInstance(0, 0);\n    // What is our current capacity?\n    // * It is equal to the max(required, queue-capacity) if\n    //   we're running below capacity. The 'max' ensures that jobs in queues\n    //   with miniscule capacity (< 1 slot) make progress\n    // * If we're running over capacity, then its\n    //   (usedResources + required) (which extra resources we are allocating)\n    // 第一步: 依据标签计算队列的 capacity\n    if (requestedLabels != null && !requestedLabels.isEmpty()) {\n     String firstLabel = requestedLabels.iterator().next();\n      queueCapacity =\n          Resources\n              .max(resourceCalculator, clusterResource, queueCapacity,\n                  Resources.multiplyAndNormalizeUp(resourceCalculator,\n                      labelManager.getResourceByLabel(firstLabel,\n                          clusterResource),\n                      queueCapacities.getAbsoluteCapacity(firstLabel),\n                      minimumAllocation));\n    } else {\n      queueCapacity =\n          Resources.multiplyAndNormalizeUp(resourceCalculator, labelManager\n                .getResourceByLabel(CommonNodeLabelsManager.NO_LABEL, clusterResource),\n              queueCapacities.getAbsoluteCapacity(), minimumAllocation);\n    }\n    queueCapacity =\n        Resources.max(\n            resourceCalculator, clusterResource,\n            queueCapacity,\n            required);\n    // 第二步: 计算 currentCapacity\n    Resource currentCapacity =\n        Resources.lessThan(resourceCalculator, clusterResource,\n            queueUsage.getUsed(), queueCapacity) ?\n            queueCapacity : Resources.add(queueUsage.getUsed(), required);\n\n    // Never allow a single user to take more than the\n    // queue's configured capacity * user-limit-factor.\n    // Also, the queue's configured capacity should be higher than\n    // queue-hard-limit * ulMin\n\n    final int activeUsers = activeUsersManager.getNumActiveUsers();\n    // 第三步: 计算用户可使用的最高容量\n    Resource limit =\n        Resources.roundUp(\n            resourceCalculator,\n            Resources.min(\n                resourceCalculator, clusterResource,\n                Resources.max(\n                    resourceCalculator, clusterResource,\n                    //当前容量/活跃用户数\n                    Resources.divideAndCeil(\n                        resourceCalculator, currentCapacity, activeUsers),\n                    //当前容量 * 最高用户数/100\n                    Resources.divideAndCeil(\n                        resourceCalculator,\n                        Resources.multiplyAndRoundDown(\n                            currentCapacity, userLimit),\n                        100)\n                    ),\n                //当前容量 * 用户限制因子(倍数)\n                Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)\n                ),\n            minimumAllocation);\n    .....\n    user.setUserResourceLimit(limit);\n    return limit;\n  }\n```\n## 测试场景\n测试项: 用户提交应用时 user headroom 计算方式及能否平分集群资源。  \n用户限制因子 userLimitFactor = 100，queueCapacity * userLimitFactor >> clusterResource，即单用户资源上限很高  \n用户限制 userLimit = 30，即用户可以使用队列[0,max{0.3,1/activeUser}]资源  \n\n###  scene 1 单用户单队列单应用可以独占集群资源\n单用户时 user headroom 的变化 随着常规调度模块为应用不断分配资源(可用集群全部资源) 分为(分段函数)两个阶段:    \n1. 当 used < queueCapacity 时，limit = queueCapacity  \n2. 当 used >= queueCapacity 时，limit = used + required。上限是 min{queueCapacity * userLimitFactor, clusterResource}  \n\n单用户时 user headroom 变化曲线如下:  \n\n![](/img/pictures/yarn_headroom/headroom_1.jpeg)\n\n### scene 2 单用户双队列双应用未必平分集群资源 \n基于 scene 1,随后在另外一个队列(Queue\\_B)提交一个新的 APP(M 个 N 大小的容器)。该用户在 Queue\\_B 的 headroom 可分为两个阶段:  \n1. 当 used < queueCapacity 时,limit = queueCapacity。此时抢占调度模块 Queue\\_B 的 堆积(pending)ResourceRequest = min{limit-used, M * N}。抢占模块会负责将 overCapacity 队列的资源释放，Queue\\_B 队列可以有资源运行。即 Queue\\_B 可以运行容器,used 值逼近 queueCapacity，pendingResourceRequest 值逼近于 0(虽然此时 APP 还有很多容器没有分配)。  \n2.  这个阶段有两种可能，关键在于: 常规调度模块能否利用集群剩余资源为 Queue\\_B 再分配一个容器，使其 used > queueCapacity。  \n　　2a.不能分配一个容器。limit = queueCapacity，pendingResourceRequest = min{limit - used,M * N} 趋近于 0。pendingResourceRequest 会触发抢占调度(不考虑 0.2 的deadzone,超过 capacity * (1+deadzone)的 overCapacity 队列才会真正还回资源)。Queue\\_B 欠分配，最终理想容量为 used + pendingResourceRequest(因为很小,且初始 idealAssigned/guarantee 小,会先被分配)，pendingResourceRequest(min{limit-used,M * N}) 的资源量不足以启动一个容器(原因很多,尤其是大容器,抢占的资源分布在繁忙集群的多个节点上,每个节点都不足以启动一个容器；pendingResourceRequest 被 limit 削弱过,不足以启动一个容器)。此时 user 在 Queue_B 的 headroom 恒等于 queueCapacity。抢占也不能抢回足够资源。          \n　　2b. 能分配一个容器，使队列 used > queueCapacity。此时队列的 currentCapacity 值由 queueCapacity(常量)跃迁至 used+required，提高了 headroom 上限,进而提高了 pendingResourceRequest 上限，抢占调度依据 pendingResourceRequest 值计算集群各队列的理想容量，抢回了资源足以启动容器，又提升了 used。剩下限制 APP 资源量的条件只有 min{queueCapacity * userLimitFactor, maxCapacity,平分 clusterResource}了。\n\n单用户双队列双 APP 的 user headroom 变化曲线如下:  \n\n![](/img/pictures/yarn_headroom/headroom_2.jpeg)\n\n### scene 3 多用户多队列应用更难平分集群资源 \n 由上述源码可知,limit 取值为 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}    \n 其中 activeUser 为整个系统的活跃用户数，即多用户下 limit 取值比单用户更低，但是分段函数的跃迁规则是不变的(used > capacity?)，即系统既要为APP 分配容器，而且容器大小要使队列 used 资源量从 queueCapcity/activeUser 或者 queueCapacity * userLimit/100 跃迁至超过 queueCapacity。\n \n## 一点思考  \n1. 设置 userLimitFactor 很大时(超过 clusterResource),实现多用户平分集群资源是不现实的。第一个提交的大应用将会先独占集群资源，后续提交的应用借助抢占调度获得部分资源，但资源上界一般不会超过队列的容量，越来越难平分资源。但是如果缩小 userLimitFactor,使其不超过 clusterResource的某阈值，其他队列跃迁的可能性增加，通过抢占调度是可实现多用户平分资源。     \n2. user headroom 是为了解决在多用户环境下限制用户可使用资源量,为每个用户维护的瞬时状态量，通过 userLimit 和 userLimitFactor 保证了用户可使用资源的上界。  \n3. user headroom 的计算方式: min{queueCapacity * userLimitFactor, max{currentCapacity/activeUser, currentCapacity * userLimit/100}}         \n4. Yarn 的三大调度方式: 常规调度/抢占调度/预订调度 对于大容器都是不友好的。  \n\n \n","slug":"2019-11-12-Yarn User Headroom","published":1,"updated":"2020-06-16T06:41:34.143Z","comments":1,"photos":[],"link":"","_id":"cke2rms4t0006y7c7hita713l","content":"<p>ResourceManager 尝试在 NM 上分配容器的过程中，主要考虑的限制条件如下:<br>a)    队列具有该节点的 access 权限,节点不在 APP 设置的黑名单内<br>b)    节点上无保留且节点剩余空间大于最小容器大小(1G)<br>c)    最高配额限制:如果分配该容器,叶子队列及其所有父队列均不能超过各自的最高配额。<br>d)    用户配额限制(headroom):瞬时量,该容器大小不能超过 headroom 当前值。<br>e)    AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)<br>主要限制在 c)和 d)。c)是在在队列层面对队列可使用资源量做的限制; d) user headroom 是在用户层面对用户可使用资源量做的限制。只有在不违背上述条件时，容器才会在当前 NM 上分配或保留。</p>\n<a id=\"more\"></a>\n\n<blockquote>\n</blockquote>\n<p>user headroom 的计算过程主要有三步,<br>第一步: 计算队列的配置容量 queueCapacity，即 max{capacity, required}<br>第二步: 计算队列的当前容量 currentCapacity，即分段函数 capacity 和 used + required<br>第三步: 计算用户限制 limit，即 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}  </p>\n<p>主要和两个配置有关:  </p>\n<ol>\n<li>yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent(程序内对应 userLimit)<br>　表示队列内多用户均分资源的方式，默认为 100。此值可以在一个极小值与极大值之间动态变化,该配置决定极小值,极大值由队列内活跃用户数决定。e.g. 将此值配置为 30 时, 当队列内仅有1个用户时,该用户最多使用队列100%资源;2个用户,每个用户最多使用50%资源;3个用户,每个用户最多可以使用 33.3%资源;4 个及更多时 每个用户最多可以使用 30%资源。即 [0,max{0.3,1/activeUser}]，体现在源码中   \n　</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　Resources.max(</span><br><span class=\"line\">resourceCalculator, clusterResource,</span><br><span class=\"line\">&#x2F;&#x2F;当前容量&#x2F;活跃用户数</span><br><span class=\"line\">Resources.divideAndCeil(</span><br><span class=\"line\">    resourceCalculator, currentCapacity, activeUsers),</span><br><span class=\"line\">&#x2F;&#x2F;当前容量 * 用户限制&#x2F;100</span><br><span class=\"line\">Resources.divideAndCeil(</span><br><span class=\"line\">    resourceCalculator,</span><br><span class=\"line\">    Resources.multiplyAndRoundDown(</span><br><span class=\"line\">        currentCapacity, userLimit),</span><br><span class=\"line\">    100)</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>yarn.scheduler.capacity.<queue-path>.user-limit-factor(程序内对应 userLimitFactor)<br>　表示一个用户可以最大获取资源的能力,可以获取当前队列容量乘以该值的最大资源，默认为 1 以确保无论集群负载如何 单用户都不能使用超过队列容量的资源。体现在源码中  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)</span><br></pre></td></tr></table></figure>\n\n<p>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Resource computeUserLimit(FiCaSchedulerApp application,</span><br><span class=\"line\">      Resource clusterResource, Resource required, User user,</span><br><span class=\"line\">      Set&lt;String&gt; requestedLabels) &#123;</span><br><span class=\"line\">    Resource queueCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#x2F;&#x2F; What is our current capacity?</span><br><span class=\"line\">    &#x2F;&#x2F; * It is equal to the max(required, queue-capacity) if</span><br><span class=\"line\">    &#x2F;&#x2F;   we&#39;re running below capacity. The &#39;max&#39; ensures that jobs in queues</span><br><span class=\"line\">    &#x2F;&#x2F;   with miniscule capacity (&lt; 1 slot) make progress</span><br><span class=\"line\">    &#x2F;&#x2F; * If we&#39;re running over capacity, then its</span><br><span class=\"line\">    &#x2F;&#x2F;   (usedResources + required) (which extra resources we are allocating)</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 依据标签计算队列的 capacity</span><br><span class=\"line\">    if (requestedLabels !&#x3D; null &amp;&amp; !requestedLabels.isEmpty()) &#123;</span><br><span class=\"line\">     String firstLabel &#x3D; requestedLabels.iterator().next();</span><br><span class=\"line\">      queueCapacity &#x3D;</span><br><span class=\"line\">          Resources</span><br><span class=\"line\">              .max(resourceCalculator, clusterResource, queueCapacity,</span><br><span class=\"line\">                  Resources.multiplyAndNormalizeUp(resourceCalculator,</span><br><span class=\"line\">                      labelManager.getResourceByLabel(firstLabel,</span><br><span class=\"line\">                          clusterResource),</span><br><span class=\"line\">                      queueCapacities.getAbsoluteCapacity(firstLabel),</span><br><span class=\"line\">                      minimumAllocation));</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      queueCapacity &#x3D;</span><br><span class=\"line\">          Resources.multiplyAndNormalizeUp(resourceCalculator, labelManager</span><br><span class=\"line\">                .getResourceByLabel(CommonNodeLabelsManager.NO_LABEL, clusterResource),</span><br><span class=\"line\">              queueCapacities.getAbsoluteCapacity(), minimumAllocation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    queueCapacity &#x3D;</span><br><span class=\"line\">        Resources.max(</span><br><span class=\"line\">            resourceCalculator, clusterResource,</span><br><span class=\"line\">            queueCapacity,</span><br><span class=\"line\">            required);</span><br><span class=\"line\">    &#x2F;&#x2F; 第二步: 计算 currentCapacity</span><br><span class=\"line\">    Resource currentCapacity &#x3D;</span><br><span class=\"line\">        Resources.lessThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">            queueUsage.getUsed(), queueCapacity) ?</span><br><span class=\"line\">            queueCapacity : Resources.add(queueUsage.getUsed(), required);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Never allow a single user to take more than the</span><br><span class=\"line\">    &#x2F;&#x2F; queue&#39;s configured capacity * user-limit-factor.</span><br><span class=\"line\">    &#x2F;&#x2F; Also, the queue&#39;s configured capacity should be higher than</span><br><span class=\"line\">    &#x2F;&#x2F; queue-hard-limit * ulMin</span><br><span class=\"line\"></span><br><span class=\"line\">    final int activeUsers &#x3D; activeUsersManager.getNumActiveUsers();</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步: 计算用户可使用的最高容量</span><br><span class=\"line\">    Resource limit &#x3D;</span><br><span class=\"line\">        Resources.roundUp(</span><br><span class=\"line\">            resourceCalculator,</span><br><span class=\"line\">            Resources.min(</span><br><span class=\"line\">                resourceCalculator, clusterResource,</span><br><span class=\"line\">                Resources.max(</span><br><span class=\"line\">                    resourceCalculator, clusterResource,</span><br><span class=\"line\">                    &#x2F;&#x2F;当前容量&#x2F;活跃用户数</span><br><span class=\"line\">                    Resources.divideAndCeil(</span><br><span class=\"line\">                        resourceCalculator, currentCapacity, activeUsers),</span><br><span class=\"line\">                    &#x2F;&#x2F;当前容量 * 最高用户数&#x2F;100</span><br><span class=\"line\">                    Resources.divideAndCeil(</span><br><span class=\"line\">                        resourceCalculator,</span><br><span class=\"line\">                        Resources.multiplyAndRoundDown(</span><br><span class=\"line\">                            currentCapacity, userLimit),</span><br><span class=\"line\">                        100)</span><br><span class=\"line\">                    ),</span><br><span class=\"line\">                &#x2F;&#x2F;当前容量 * 用户限制因子(倍数)</span><br><span class=\"line\">                Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)</span><br><span class=\"line\">                ),</span><br><span class=\"line\">            minimumAllocation);</span><br><span class=\"line\">    .....</span><br><span class=\"line\">    user.setUserResourceLimit(limit);</span><br><span class=\"line\">    return limit;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试场景\"><a href=\"#测试场景\" class=\"headerlink\" title=\"测试场景\"></a>测试场景</h2><p>测试项: 用户提交应用时 user headroom 计算方式及能否平分集群资源。<br>用户限制因子 userLimitFactor = 100，queueCapacity * userLimitFactor &gt;&gt; clusterResource，即单用户资源上限很高<br>用户限制 userLimit = 30，即用户可以使用队列[0,max{0.3,1/activeUser}]资源  </p>\n<h3 id=\"scene-1-单用户单队列单应用可以独占集群资源\"><a href=\"#scene-1-单用户单队列单应用可以独占集群资源\" class=\"headerlink\" title=\"scene 1 单用户单队列单应用可以独占集群资源\"></a>scene 1 单用户单队列单应用可以独占集群资源</h3><p>单用户时 user headroom 的变化 随着常规调度模块为应用不断分配资源(可用集群全部资源) 分为(分段函数)两个阶段:    </p>\n<ol>\n<li>当 used &lt; queueCapacity 时，limit = queueCapacity  </li>\n<li>当 used &gt;= queueCapacity 时，limit = used + required。上限是 min{queueCapacity * userLimitFactor, clusterResource}  </li>\n</ol>\n<p>单用户时 user headroom 变化曲线如下:  </p>\n<p><img src=\"/img/pictures/yarn_headroom/headroom_1.jpeg\" alt=\"\"></p>\n<h3 id=\"scene-2-单用户双队列双应用未必平分集群资源\"><a href=\"#scene-2-单用户双队列双应用未必平分集群资源\" class=\"headerlink\" title=\"scene 2 单用户双队列双应用未必平分集群资源\"></a>scene 2 单用户双队列双应用未必平分集群资源</h3><p>基于 scene 1,随后在另外一个队列(Queue_B)提交一个新的 APP(M 个 N 大小的容器)。该用户在 Queue_B 的 headroom 可分为两个阶段:  </p>\n<ol>\n<li>当 used &lt; queueCapacity 时,limit = queueCapacity。此时抢占调度模块 Queue_B 的 堆积(pending)ResourceRequest = min{limit-used, M * N}。抢占模块会负责将 overCapacity 队列的资源释放，Queue_B 队列可以有资源运行。即 Queue_B 可以运行容器,used 值逼近 queueCapacity，pendingResourceRequest 值逼近于 0(虽然此时 APP 还有很多容器没有分配)。  </li>\n<li>这个阶段有两种可能，关键在于: 常规调度模块能否利用集群剩余资源为 Queue_B 再分配一个容器，使其 used &gt; queueCapacity。<br>　　2a.不能分配一个容器。limit = queueCapacity，pendingResourceRequest = min{limit - used,M * N} 趋近于 0。pendingResourceRequest 会触发抢占调度(不考虑 0.2 的deadzone,超过 capacity * (1+deadzone)的 overCapacity 队列才会真正还回资源)。Queue_B 欠分配，最终理想容量为 used + pendingResourceRequest(因为很小,且初始 idealAssigned/guarantee 小,会先被分配)，pendingResourceRequest(min{limit-used,M * N}) 的资源量不足以启动一个容器(原因很多,尤其是大容器,抢占的资源分布在繁忙集群的多个节点上,每个节点都不足以启动一个容器；pendingResourceRequest 被 limit 削弱过,不足以启动一个容器)。此时 user 在 Queue_B 的 headroom 恒等于 queueCapacity。抢占也不能抢回足够资源。<br>　　2b. 能分配一个容器，使队列 used &gt; queueCapacity。此时队列的 currentCapacity 值由 queueCapacity(常量)跃迁至 used+required，提高了 headroom 上限,进而提高了 pendingResourceRequest 上限，抢占调度依据 pendingResourceRequest 值计算集群各队列的理想容量，抢回了资源足以启动容器，又提升了 used。剩下限制 APP 资源量的条件只有 min{queueCapacity * userLimitFactor, maxCapacity,平分 clusterResource}了。</li>\n</ol>\n<p>单用户双队列双 APP 的 user headroom 变化曲线如下:  </p>\n<p><img src=\"/img/pictures/yarn_headroom/headroom_2.jpeg\" alt=\"\"></p>\n<h3 id=\"scene-3-多用户多队列应用更难平分集群资源\"><a href=\"#scene-3-多用户多队列应用更难平分集群资源\" class=\"headerlink\" title=\"scene 3 多用户多队列应用更难平分集群资源\"></a>scene 3 多用户多队列应用更难平分集群资源</h3><p> 由上述源码可知,limit 取值为 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}<br> 其中 activeUser 为整个系统的活跃用户数，即多用户下 limit 取值比单用户更低，但是分段函数的跃迁规则是不变的(used &gt; capacity?)，即系统既要为APP 分配容器，而且容器大小要使队列 used 资源量从 queueCapcity/activeUser 或者 queueCapacity * userLimit/100 跃迁至超过 queueCapacity。</p>\n<h2 id=\"一点思考\"><a href=\"#一点思考\" class=\"headerlink\" title=\"一点思考\"></a>一点思考</h2><ol>\n<li>设置 userLimitFactor 很大时(超过 clusterResource),实现多用户平分集群资源是不现实的。第一个提交的大应用将会先独占集群资源，后续提交的应用借助抢占调度获得部分资源，但资源上界一般不会超过队列的容量，越来越难平分资源。但是如果缩小 userLimitFactor,使其不超过 clusterResource的某阈值，其他队列跃迁的可能性增加，通过抢占调度是可实现多用户平分资源。     </li>\n<li>user headroom 是为了解决在多用户环境下限制用户可使用资源量,为每个用户维护的瞬时状态量，通过 userLimit 和 userLimitFactor 保证了用户可使用资源的上界。  </li>\n<li>user headroom 的计算方式: min{queueCapacity * userLimitFactor, max{currentCapacity/activeUser, currentCapacity * userLimit/100}}         </li>\n<li>Yarn 的三大调度方式: 常规调度/抢占调度/预订调度 对于大容器都是不友好的。  </li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>ResourceManager 尝试在 NM 上分配容器的过程中，主要考虑的限制条件如下:<br>a)    队列具有该节点的 access 权限,节点不在 APP 设置的黑名单内<br>b)    节点上无保留且节点剩余空间大于最小容器大小(1G)<br>c)    最高配额限制:如果分配该容器,叶子队列及其所有父队列均不能超过各自的最高配额。<br>d)    用户配额限制(headroom):瞬时量,该容器大小不能超过 headroom 当前值。<br>e)    AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)<br>主要限制在 c)和 d)。c)是在在队列层面对队列可使用资源量做的限制; d) user headroom 是在用户层面对用户可使用资源量做的限制。只有在不违背上述条件时，容器才会在当前 NM 上分配或保留。</p>","more":"<blockquote>\n</blockquote>\n<p>user headroom 的计算过程主要有三步,<br>第一步: 计算队列的配置容量 queueCapacity，即 max{capacity, required}<br>第二步: 计算队列的当前容量 currentCapacity，即分段函数 capacity 和 used + required<br>第三步: 计算用户限制 limit，即 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}  </p>\n<p>主要和两个配置有关:  </p>\n<ol>\n<li>yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent(程序内对应 userLimit)<br>　表示队列内多用户均分资源的方式，默认为 100。此值可以在一个极小值与极大值之间动态变化,该配置决定极小值,极大值由队列内活跃用户数决定。e.g. 将此值配置为 30 时, 当队列内仅有1个用户时,该用户最多使用队列100%资源;2个用户,每个用户最多使用50%资源;3个用户,每个用户最多可以使用 33.3%资源;4 个及更多时 每个用户最多可以使用 30%资源。即 [0,max{0.3,1/activeUser}]，体现在源码中   \n　</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　Resources.max(</span><br><span class=\"line\">resourceCalculator, clusterResource,</span><br><span class=\"line\">&#x2F;&#x2F;当前容量&#x2F;活跃用户数</span><br><span class=\"line\">Resources.divideAndCeil(</span><br><span class=\"line\">    resourceCalculator, currentCapacity, activeUsers),</span><br><span class=\"line\">&#x2F;&#x2F;当前容量 * 用户限制&#x2F;100</span><br><span class=\"line\">Resources.divideAndCeil(</span><br><span class=\"line\">    resourceCalculator,</span><br><span class=\"line\">    Resources.multiplyAndRoundDown(</span><br><span class=\"line\">        currentCapacity, userLimit),</span><br><span class=\"line\">    100)</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>yarn.scheduler.capacity.<queue-path>.user-limit-factor(程序内对应 userLimitFactor)<br>　表示一个用户可以最大获取资源的能力,可以获取当前队列容量乘以该值的最大资源，默认为 1 以确保无论集群负载如何 单用户都不能使用超过队列容量的资源。体现在源码中  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">　Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)</span><br></pre></td></tr></table></figure>\n\n<p>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Resource computeUserLimit(FiCaSchedulerApp application,</span><br><span class=\"line\">      Resource clusterResource, Resource required, User user,</span><br><span class=\"line\">      Set&lt;String&gt; requestedLabels) &#123;</span><br><span class=\"line\">    Resource queueCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#x2F;&#x2F; What is our current capacity?</span><br><span class=\"line\">    &#x2F;&#x2F; * It is equal to the max(required, queue-capacity) if</span><br><span class=\"line\">    &#x2F;&#x2F;   we&#39;re running below capacity. The &#39;max&#39; ensures that jobs in queues</span><br><span class=\"line\">    &#x2F;&#x2F;   with miniscule capacity (&lt; 1 slot) make progress</span><br><span class=\"line\">    &#x2F;&#x2F; * If we&#39;re running over capacity, then its</span><br><span class=\"line\">    &#x2F;&#x2F;   (usedResources + required) (which extra resources we are allocating)</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 依据标签计算队列的 capacity</span><br><span class=\"line\">    if (requestedLabels !&#x3D; null &amp;&amp; !requestedLabels.isEmpty()) &#123;</span><br><span class=\"line\">     String firstLabel &#x3D; requestedLabels.iterator().next();</span><br><span class=\"line\">      queueCapacity &#x3D;</span><br><span class=\"line\">          Resources</span><br><span class=\"line\">              .max(resourceCalculator, clusterResource, queueCapacity,</span><br><span class=\"line\">                  Resources.multiplyAndNormalizeUp(resourceCalculator,</span><br><span class=\"line\">                      labelManager.getResourceByLabel(firstLabel,</span><br><span class=\"line\">                          clusterResource),</span><br><span class=\"line\">                      queueCapacities.getAbsoluteCapacity(firstLabel),</span><br><span class=\"line\">                      minimumAllocation));</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      queueCapacity &#x3D;</span><br><span class=\"line\">          Resources.multiplyAndNormalizeUp(resourceCalculator, labelManager</span><br><span class=\"line\">                .getResourceByLabel(CommonNodeLabelsManager.NO_LABEL, clusterResource),</span><br><span class=\"line\">              queueCapacities.getAbsoluteCapacity(), minimumAllocation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    queueCapacity &#x3D;</span><br><span class=\"line\">        Resources.max(</span><br><span class=\"line\">            resourceCalculator, clusterResource,</span><br><span class=\"line\">            queueCapacity,</span><br><span class=\"line\">            required);</span><br><span class=\"line\">    &#x2F;&#x2F; 第二步: 计算 currentCapacity</span><br><span class=\"line\">    Resource currentCapacity &#x3D;</span><br><span class=\"line\">        Resources.lessThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">            queueUsage.getUsed(), queueCapacity) ?</span><br><span class=\"line\">            queueCapacity : Resources.add(queueUsage.getUsed(), required);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Never allow a single user to take more than the</span><br><span class=\"line\">    &#x2F;&#x2F; queue&#39;s configured capacity * user-limit-factor.</span><br><span class=\"line\">    &#x2F;&#x2F; Also, the queue&#39;s configured capacity should be higher than</span><br><span class=\"line\">    &#x2F;&#x2F; queue-hard-limit * ulMin</span><br><span class=\"line\"></span><br><span class=\"line\">    final int activeUsers &#x3D; activeUsersManager.getNumActiveUsers();</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步: 计算用户可使用的最高容量</span><br><span class=\"line\">    Resource limit &#x3D;</span><br><span class=\"line\">        Resources.roundUp(</span><br><span class=\"line\">            resourceCalculator,</span><br><span class=\"line\">            Resources.min(</span><br><span class=\"line\">                resourceCalculator, clusterResource,</span><br><span class=\"line\">                Resources.max(</span><br><span class=\"line\">                    resourceCalculator, clusterResource,</span><br><span class=\"line\">                    &#x2F;&#x2F;当前容量&#x2F;活跃用户数</span><br><span class=\"line\">                    Resources.divideAndCeil(</span><br><span class=\"line\">                        resourceCalculator, currentCapacity, activeUsers),</span><br><span class=\"line\">                    &#x2F;&#x2F;当前容量 * 最高用户数&#x2F;100</span><br><span class=\"line\">                    Resources.divideAndCeil(</span><br><span class=\"line\">                        resourceCalculator,</span><br><span class=\"line\">                        Resources.multiplyAndRoundDown(</span><br><span class=\"line\">                            currentCapacity, userLimit),</span><br><span class=\"line\">                        100)</span><br><span class=\"line\">                    ),</span><br><span class=\"line\">                &#x2F;&#x2F;当前容量 * 用户限制因子(倍数)</span><br><span class=\"line\">                Resources.multiplyAndRoundDown(queueCapacity, userLimitFactor)</span><br><span class=\"line\">                ),</span><br><span class=\"line\">            minimumAllocation);</span><br><span class=\"line\">    .....</span><br><span class=\"line\">    user.setUserResourceLimit(limit);</span><br><span class=\"line\">    return limit;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试场景\"><a href=\"#测试场景\" class=\"headerlink\" title=\"测试场景\"></a>测试场景</h2><p>测试项: 用户提交应用时 user headroom 计算方式及能否平分集群资源。<br>用户限制因子 userLimitFactor = 100，queueCapacity * userLimitFactor &gt;&gt; clusterResource，即单用户资源上限很高<br>用户限制 userLimit = 30，即用户可以使用队列[0,max{0.3,1/activeUser}]资源  </p>\n<h3 id=\"scene-1-单用户单队列单应用可以独占集群资源\"><a href=\"#scene-1-单用户单队列单应用可以独占集群资源\" class=\"headerlink\" title=\"scene 1 单用户单队列单应用可以独占集群资源\"></a>scene 1 单用户单队列单应用可以独占集群资源</h3><p>单用户时 user headroom 的变化 随着常规调度模块为应用不断分配资源(可用集群全部资源) 分为(分段函数)两个阶段:    </p>\n<ol>\n<li>当 used &lt; queueCapacity 时，limit = queueCapacity  </li>\n<li>当 used &gt;= queueCapacity 时，limit = used + required。上限是 min{queueCapacity * userLimitFactor, clusterResource}  </li>\n</ol>\n<p>单用户时 user headroom 变化曲线如下:  </p>\n<p><img src=\"/img/pictures/yarn_headroom/headroom_1.jpeg\" alt=\"\"></p>\n<h3 id=\"scene-2-单用户双队列双应用未必平分集群资源\"><a href=\"#scene-2-单用户双队列双应用未必平分集群资源\" class=\"headerlink\" title=\"scene 2 单用户双队列双应用未必平分集群资源\"></a>scene 2 单用户双队列双应用未必平分集群资源</h3><p>基于 scene 1,随后在另外一个队列(Queue_B)提交一个新的 APP(M 个 N 大小的容器)。该用户在 Queue_B 的 headroom 可分为两个阶段:  </p>\n<ol>\n<li>当 used &lt; queueCapacity 时,limit = queueCapacity。此时抢占调度模块 Queue_B 的 堆积(pending)ResourceRequest = min{limit-used, M * N}。抢占模块会负责将 overCapacity 队列的资源释放，Queue_B 队列可以有资源运行。即 Queue_B 可以运行容器,used 值逼近 queueCapacity，pendingResourceRequest 值逼近于 0(虽然此时 APP 还有很多容器没有分配)。  </li>\n<li>这个阶段有两种可能，关键在于: 常规调度模块能否利用集群剩余资源为 Queue_B 再分配一个容器，使其 used &gt; queueCapacity。<br>　　2a.不能分配一个容器。limit = queueCapacity，pendingResourceRequest = min{limit - used,M * N} 趋近于 0。pendingResourceRequest 会触发抢占调度(不考虑 0.2 的deadzone,超过 capacity * (1+deadzone)的 overCapacity 队列才会真正还回资源)。Queue_B 欠分配，最终理想容量为 used + pendingResourceRequest(因为很小,且初始 idealAssigned/guarantee 小,会先被分配)，pendingResourceRequest(min{limit-used,M * N}) 的资源量不足以启动一个容器(原因很多,尤其是大容器,抢占的资源分布在繁忙集群的多个节点上,每个节点都不足以启动一个容器；pendingResourceRequest 被 limit 削弱过,不足以启动一个容器)。此时 user 在 Queue_B 的 headroom 恒等于 queueCapacity。抢占也不能抢回足够资源。<br>　　2b. 能分配一个容器，使队列 used &gt; queueCapacity。此时队列的 currentCapacity 值由 queueCapacity(常量)跃迁至 used+required，提高了 headroom 上限,进而提高了 pendingResourceRequest 上限，抢占调度依据 pendingResourceRequest 值计算集群各队列的理想容量，抢回了资源足以启动容器，又提升了 used。剩下限制 APP 资源量的条件只有 min{queueCapacity * userLimitFactor, maxCapacity,平分 clusterResource}了。</li>\n</ol>\n<p>单用户双队列双 APP 的 user headroom 变化曲线如下:  </p>\n<p><img src=\"/img/pictures/yarn_headroom/headroom_2.jpeg\" alt=\"\"></p>\n<h3 id=\"scene-3-多用户多队列应用更难平分集群资源\"><a href=\"#scene-3-多用户多队列应用更难平分集群资源\" class=\"headerlink\" title=\"scene 3 多用户多队列应用更难平分集群资源\"></a>scene 3 多用户多队列应用更难平分集群资源</h3><p> 由上述源码可知,limit 取值为 min{queueCapacity * userLimitFactor, max{currentCapacty/activeUser, currentCapacity * userLimit/100}}<br> 其中 activeUser 为整个系统的活跃用户数，即多用户下 limit 取值比单用户更低，但是分段函数的跃迁规则是不变的(used &gt; capacity?)，即系统既要为APP 分配容器，而且容器大小要使队列 used 资源量从 queueCapcity/activeUser 或者 queueCapacity * userLimit/100 跃迁至超过 queueCapacity。</p>\n<h2 id=\"一点思考\"><a href=\"#一点思考\" class=\"headerlink\" title=\"一点思考\"></a>一点思考</h2><ol>\n<li>设置 userLimitFactor 很大时(超过 clusterResource),实现多用户平分集群资源是不现实的。第一个提交的大应用将会先独占集群资源，后续提交的应用借助抢占调度获得部分资源，但资源上界一般不会超过队列的容量，越来越难平分资源。但是如果缩小 userLimitFactor,使其不超过 clusterResource的某阈值，其他队列跃迁的可能性增加，通过抢占调度是可实现多用户平分资源。     </li>\n<li>user headroom 是为了解决在多用户环境下限制用户可使用资源量,为每个用户维护的瞬时状态量，通过 userLimit 和 userLimitFactor 保证了用户可使用资源的上界。  </li>\n<li>user headroom 的计算方式: min{queueCapacity * userLimitFactor, max{currentCapacity/activeUser, currentCapacity * userLimit/100}}         </li>\n<li>Yarn 的三大调度方式: 常规调度/抢占调度/预订调度 对于大容器都是不友好的。  </li>\n</ol>"},{"layout":"post","title":"Improved MapPartitions In Spark","subtitle":"Spark mapPartitions 算子取巧用法","date":"2019-11-21T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 概述\n\nmap 和 mapPartitions 是 Spark 中最常用的 ETL 算子，用于将数据映射为另一批数据。区别在于,  \n1.  数据粒度: map 输入和输出数据都是单条记录，严格的一对一关系。mapPartitions 输入和输出数据都是一个分区的数据迭代器, n 对 m 关系。  \n2.  功能: mapPartitions 功能远强于 map,其一 mapPartitions 可以在映射过程中做诸如过滤的额外操作，其二在需要创建重量级对象(e.g. 数据库连接)的场景下 mapPartitions 比 map 操作更合适。 \n3.  性能: 性能上无明显差异。一般将分区输出数据先存储在内存数据结构中,结束后转成迭代器形式。在内存中存储数据这种方式可能会导致 OOM，但是有方法避免，这是本文的价值点。     \n\n<!-- more -->\n\nforeach 和 foreachPartition 的关系/用法类似 map 和mapPartitions,区别在 foreach(Partition)是 action 算子而 map(Partitions)是 transformer 算子。  \n\n\n## mapPartitions 一般用法\n\nmapPartitions[V] 的输入参数是 f:(Iterator[U]=>Iterator[V]) 使RDD[U]映射为 RDD[V]  \n一般用法是在每个分区内维护一个内存数据结构 暂存输出数据，使用输入数据迭代器遍历输入数据处理得到输出数据加入到内存数据结构中，处理完毕后将内存数据结构转换为输出数据迭代器。   \n\n```\nval rdd:RDD[Int] = ?\nrdd.mapPartitions(it=>{\n      val buffer = new ArrayBuffer[Int](64)\n      while(it.hasNext){\n        val next = it.next()\n        if(???){\n          buffer.+=(doSomething(next))\n        }else{\n          // abort\n        }\n      }\n      buffer.iterator\n    })\n```\n优点在于使用简便，缺点在于内存数据结构在数据量大时容易 OOM。  \n\n## 场景 \n使用 Spark 向数据库(mongodb)中写入数据时有量级不小的错误，需要将存储失败的数据另存。  \n\n难点:   \n1. 数据库的并发度支持不高,且其他业务也需要占用部分连接。所以最终 RDD 的分区数不能太大，进而导致单分区内数据很大。     \n2.  单分区内数据很大，插入数据库的失败数据也很多，需要将失败数据转存到其他存储。大量失败数据维护在内存中容易导致 OOM。  \n\n## improved mapPartitions 1\n切入点在于 mapPartitions 参数是一个 迭代器向迭代器的映射函数   \n以下代码为伪代码，仅作为模板使用   \n\n```\n\n/**\n  * record by record 式的处理输入数据。\n  * @param srcDataIterator 输入数据迭代器\n  * @tparam T\n  */\nclass TransformSaveDataIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]{\n // 重量级对象最好在类内部维护，避免 driver->executor 序列化问题\n  lazy val connection = createOrGetFromPoll()\n  override def hasNext: Boolean = {\n    if(srcDataIterator.hasNext){\n      return true\n    }else{\n      closeResource()\n      return false\n    }\n  }\n\n  override def next(): (T, Boolean) = {\n    val next:T = srcDataIterator.next()\n    val result:Boolean = connectResource.doSave()  // try {doSomething;return true} catch {return false}\n    (next, result)\n  }\n\n  private def closeResource(): Unit ={\n    if(connectResource != null && connectResource.isActive()){\n      connectResource.commit()\n      connectResource.close()\n    }\n  }\n}\n```\n优点在于解决了 OOM 问题，缺点在于需要 record by record 式处理数据，效率低下对于支持 batch 的数据库而言未能充分利用   \n\n## improved mapPartitions 2\n切入点借鉴 Spark Streaming 的 mini-batch，将输入数据的迭代器 slice 为多个，每个 slice 整批插入数据库中以利用 batch 功能并同时保存 slice 迭代器和事务执行结果。记录下所有输入数据的数量，当返回数据数量不大于该值时 可返回数据。每个 slice 内迭代器数据迭代完之后，切换为下一个迭代器直到返回数据量等于输入数据总量。  \n以下代码为伪代码，仅作为模板使用 \n\n```\n\n/**\n  * buffer 式处理输入数据\n  * @param srcDataIterator 输入数据迭代器\n  * @tparam T\n  */\nclass TransformBufferedSaveIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]{\n  lazy val connection = createOrGetFromPoll()\n  val BUFFER_SIZE = 2048\n  lazy val buffer:ArrayBuffer[(Iterator[T],Boolean)] = new ArrayBuffer[(Iterator[T],Boolean)](128)\n  var dataCnt:Int = 0  // total input data count\n  var returnedCnt:Int = 0  // already returned data count\n  val trigger = new AtomicBoolean(true)\n  var bufferIterator:Iterator[(Iterator[T],Boolean)] = null\n  var curIterator:(Iterator[T],Boolean) = null\n\n  override def hasNext: Boolean = {\n    if(trigger.get()){\n      while(dataCnt < srcDataIterator.length){\n        val slice:Iterator[T] = srcDataIterator.slice(dataCnt, dataCnt+BUFFER_SIZE)\n        dataCnt +=slice.length\n        val result:Boolean = doSave(slice)\n        // new iterator or reverse iterator\n        val traversableSlice = slice.slice(0, BUFFER_SIZE)\n        buffer.+=:((traversableSlice,result))\n      }\n      closeResource()\n      bufferIterator = buffer.iterator\n      trigger.set(false)\n    }\n    returnedCnt < dataCnt\n  }\n\n  override def next(): (T, Boolean) = {\n    if(curIterator!=null && curIterator._1.hasNext){\n      dataCnt += 1\n      return (curIterator._1.next(),curIterator._2)\n    }else{\n     // bufferIterator 最后一个 iterator 不会调用 next(此时hasNext()=false)\n      curIterator = bufferIterator.next()\n      next()\n    }\n  }\n\n  def doSave(it:Iterator[T]): Boolean ={\n    try{\n      doSomething()\n      connectResource.commit()\n      buffer.clear()\n      return true\n    }catch{\n      case e:Exception =>{\n        return false\n      }\n    }\n  }\n  private def closeResource(): Unit ={\n    if(connectResource != null && connectResource.isActive()){\n      connectResource.commit()\n      connectResource.close()\n    }\n  }\n}\n```\n\n## SimpleTestCase\n分别使用 improved mapPartitions 1 和 improved mapPartitions 2 做测试样例\n\n```\nobject ImprovedMapPartition {\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder().appName(\"\").master(\"\").config(\"\",\"\").getOrCreate();\n    val sparkContext = spark.sparkContext\n    val rdd:RDD[Int] = sparkContext.parallelize(1 to 20,3)\n    // save record by record\n    val resultRDD = rdd.mapPartitions(it=>{\n      new TransformSaveDataIterator[Int](it)\n    })\n      resultRDD.cache()\n    resultRDD.filter(_._2 == false) // exception data\n      .saveAsTextFile(\"....\")\n\n    // save buffered records\n    print(\"finally save records: \"+ resultRDD.filter(_._2).count())\n    val resultRDD2 = rdd.mapPartitions(it=>{\n      new TransformBufferedSaveIterator[Int](it)\n    })\n      resultRDD2.cache()\n    resultRDD2.filter(_._2 == false)\n        .saveAsTextFile(\"....\")\n    print(\"finally save records: \"+ resultRDD.filter(_._2).count())\n\n    resultRDD.unpersist()\n    resultRDD2.unpersist()\n    spark.stop();\n  }\n}\n```\n\n","source":"_posts/2019-11-22-Improved MapPartitions In Spark.md","raw":"---\nlayout:     post\ntitle:      Improved MapPartitions In Spark\nsubtitle:   Spark mapPartitions 算子取巧用法  \ndate:       2019-11-22  \nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n---\n\n\n## 概述\n\nmap 和 mapPartitions 是 Spark 中最常用的 ETL 算子，用于将数据映射为另一批数据。区别在于,  \n1.  数据粒度: map 输入和输出数据都是单条记录，严格的一对一关系。mapPartitions 输入和输出数据都是一个分区的数据迭代器, n 对 m 关系。  \n2.  功能: mapPartitions 功能远强于 map,其一 mapPartitions 可以在映射过程中做诸如过滤的额外操作，其二在需要创建重量级对象(e.g. 数据库连接)的场景下 mapPartitions 比 map 操作更合适。 \n3.  性能: 性能上无明显差异。一般将分区输出数据先存储在内存数据结构中,结束后转成迭代器形式。在内存中存储数据这种方式可能会导致 OOM，但是有方法避免，这是本文的价值点。     \n\n<!-- more -->\n\nforeach 和 foreachPartition 的关系/用法类似 map 和mapPartitions,区别在 foreach(Partition)是 action 算子而 map(Partitions)是 transformer 算子。  \n\n\n## mapPartitions 一般用法\n\nmapPartitions[V] 的输入参数是 f:(Iterator[U]=>Iterator[V]) 使RDD[U]映射为 RDD[V]  \n一般用法是在每个分区内维护一个内存数据结构 暂存输出数据，使用输入数据迭代器遍历输入数据处理得到输出数据加入到内存数据结构中，处理完毕后将内存数据结构转换为输出数据迭代器。   \n\n```\nval rdd:RDD[Int] = ?\nrdd.mapPartitions(it=>{\n      val buffer = new ArrayBuffer[Int](64)\n      while(it.hasNext){\n        val next = it.next()\n        if(???){\n          buffer.+=(doSomething(next))\n        }else{\n          // abort\n        }\n      }\n      buffer.iterator\n    })\n```\n优点在于使用简便，缺点在于内存数据结构在数据量大时容易 OOM。  \n\n## 场景 \n使用 Spark 向数据库(mongodb)中写入数据时有量级不小的错误，需要将存储失败的数据另存。  \n\n难点:   \n1. 数据库的并发度支持不高,且其他业务也需要占用部分连接。所以最终 RDD 的分区数不能太大，进而导致单分区内数据很大。     \n2.  单分区内数据很大，插入数据库的失败数据也很多，需要将失败数据转存到其他存储。大量失败数据维护在内存中容易导致 OOM。  \n\n## improved mapPartitions 1\n切入点在于 mapPartitions 参数是一个 迭代器向迭代器的映射函数   \n以下代码为伪代码，仅作为模板使用   \n\n```\n\n/**\n  * record by record 式的处理输入数据。\n  * @param srcDataIterator 输入数据迭代器\n  * @tparam T\n  */\nclass TransformSaveDataIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]{\n // 重量级对象最好在类内部维护，避免 driver->executor 序列化问题\n  lazy val connection = createOrGetFromPoll()\n  override def hasNext: Boolean = {\n    if(srcDataIterator.hasNext){\n      return true\n    }else{\n      closeResource()\n      return false\n    }\n  }\n\n  override def next(): (T, Boolean) = {\n    val next:T = srcDataIterator.next()\n    val result:Boolean = connectResource.doSave()  // try {doSomething;return true} catch {return false}\n    (next, result)\n  }\n\n  private def closeResource(): Unit ={\n    if(connectResource != null && connectResource.isActive()){\n      connectResource.commit()\n      connectResource.close()\n    }\n  }\n}\n```\n优点在于解决了 OOM 问题，缺点在于需要 record by record 式处理数据，效率低下对于支持 batch 的数据库而言未能充分利用   \n\n## improved mapPartitions 2\n切入点借鉴 Spark Streaming 的 mini-batch，将输入数据的迭代器 slice 为多个，每个 slice 整批插入数据库中以利用 batch 功能并同时保存 slice 迭代器和事务执行结果。记录下所有输入数据的数量，当返回数据数量不大于该值时 可返回数据。每个 slice 内迭代器数据迭代完之后，切换为下一个迭代器直到返回数据量等于输入数据总量。  \n以下代码为伪代码，仅作为模板使用 \n\n```\n\n/**\n  * buffer 式处理输入数据\n  * @param srcDataIterator 输入数据迭代器\n  * @tparam T\n  */\nclass TransformBufferedSaveIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]{\n  lazy val connection = createOrGetFromPoll()\n  val BUFFER_SIZE = 2048\n  lazy val buffer:ArrayBuffer[(Iterator[T],Boolean)] = new ArrayBuffer[(Iterator[T],Boolean)](128)\n  var dataCnt:Int = 0  // total input data count\n  var returnedCnt:Int = 0  // already returned data count\n  val trigger = new AtomicBoolean(true)\n  var bufferIterator:Iterator[(Iterator[T],Boolean)] = null\n  var curIterator:(Iterator[T],Boolean) = null\n\n  override def hasNext: Boolean = {\n    if(trigger.get()){\n      while(dataCnt < srcDataIterator.length){\n        val slice:Iterator[T] = srcDataIterator.slice(dataCnt, dataCnt+BUFFER_SIZE)\n        dataCnt +=slice.length\n        val result:Boolean = doSave(slice)\n        // new iterator or reverse iterator\n        val traversableSlice = slice.slice(0, BUFFER_SIZE)\n        buffer.+=:((traversableSlice,result))\n      }\n      closeResource()\n      bufferIterator = buffer.iterator\n      trigger.set(false)\n    }\n    returnedCnt < dataCnt\n  }\n\n  override def next(): (T, Boolean) = {\n    if(curIterator!=null && curIterator._1.hasNext){\n      dataCnt += 1\n      return (curIterator._1.next(),curIterator._2)\n    }else{\n     // bufferIterator 最后一个 iterator 不会调用 next(此时hasNext()=false)\n      curIterator = bufferIterator.next()\n      next()\n    }\n  }\n\n  def doSave(it:Iterator[T]): Boolean ={\n    try{\n      doSomething()\n      connectResource.commit()\n      buffer.clear()\n      return true\n    }catch{\n      case e:Exception =>{\n        return false\n      }\n    }\n  }\n  private def closeResource(): Unit ={\n    if(connectResource != null && connectResource.isActive()){\n      connectResource.commit()\n      connectResource.close()\n    }\n  }\n}\n```\n\n## SimpleTestCase\n分别使用 improved mapPartitions 1 和 improved mapPartitions 2 做测试样例\n\n```\nobject ImprovedMapPartition {\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder().appName(\"\").master(\"\").config(\"\",\"\").getOrCreate();\n    val sparkContext = spark.sparkContext\n    val rdd:RDD[Int] = sparkContext.parallelize(1 to 20,3)\n    // save record by record\n    val resultRDD = rdd.mapPartitions(it=>{\n      new TransformSaveDataIterator[Int](it)\n    })\n      resultRDD.cache()\n    resultRDD.filter(_._2 == false) // exception data\n      .saveAsTextFile(\"....\")\n\n    // save buffered records\n    print(\"finally save records: \"+ resultRDD.filter(_._2).count())\n    val resultRDD2 = rdd.mapPartitions(it=>{\n      new TransformBufferedSaveIterator[Int](it)\n    })\n      resultRDD2.cache()\n    resultRDD2.filter(_._2 == false)\n        .saveAsTextFile(\"....\")\n    print(\"finally save records: \"+ resultRDD.filter(_._2).count())\n\n    resultRDD.unpersist()\n    resultRDD2.unpersist()\n    spark.stop();\n  }\n}\n```\n\n","slug":"2019-11-22-Improved MapPartitions In Spark","published":1,"updated":"2020-03-16T10:01:28.109Z","comments":1,"photos":[],"link":"","_id":"cke2rms4v0007y7c7c711eai2","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>map 和 mapPartitions 是 Spark 中最常用的 ETL 算子，用于将数据映射为另一批数据。区别在于,  </p>\n<ol>\n<li>数据粒度: map 输入和输出数据都是单条记录，严格的一对一关系。mapPartitions 输入和输出数据都是一个分区的数据迭代器, n 对 m 关系。  </li>\n<li>功能: mapPartitions 功能远强于 map,其一 mapPartitions 可以在映射过程中做诸如过滤的额外操作，其二在需要创建重量级对象(e.g. 数据库连接)的场景下 mapPartitions 比 map 操作更合适。 </li>\n<li>性能: 性能上无明显差异。一般将分区输出数据先存储在内存数据结构中,结束后转成迭代器形式。在内存中存储数据这种方式可能会导致 OOM，但是有方法避免，这是本文的价值点。     </li>\n</ol>\n<a id=\"more\"></a>\n\n<p>foreach 和 foreachPartition 的关系/用法类似 map 和mapPartitions,区别在 foreach(Partition)是 action 算子而 map(Partitions)是 transformer 算子。  </p>\n<h2 id=\"mapPartitions-一般用法\"><a href=\"#mapPartitions-一般用法\" class=\"headerlink\" title=\"mapPartitions 一般用法\"></a>mapPartitions 一般用法</h2><p>mapPartitions[V] 的输入参数是 f:(Iterator[U]=&gt;Iterator[V]) 使RDD[U]映射为 RDD[V]<br>一般用法是在每个分区内维护一个内存数据结构 暂存输出数据，使用输入数据迭代器遍历输入数据处理得到输出数据加入到内存数据结构中，处理完毕后将内存数据结构转换为输出数据迭代器。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val rdd:RDD[Int] &#x3D; ?</span><br><span class=\"line\">rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      val buffer &#x3D; new ArrayBuffer[Int](64)</span><br><span class=\"line\">      while(it.hasNext)&#123;</span><br><span class=\"line\">        val next &#x3D; it.next()</span><br><span class=\"line\">        if(???)&#123;</span><br><span class=\"line\">          buffer.+&#x3D;(doSomething(next))</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">          &#x2F;&#x2F; abort</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      buffer.iterator</span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>优点在于使用简便，缺点在于内存数据结构在数据量大时容易 OOM。  </p>\n<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><p>使用 Spark 向数据库(mongodb)中写入数据时有量级不小的错误，需要将存储失败的数据另存。  </p>\n<p>难点:   </p>\n<ol>\n<li>数据库的并发度支持不高,且其他业务也需要占用部分连接。所以最终 RDD 的分区数不能太大，进而导致单分区内数据很大。     </li>\n<li>单分区内数据很大，插入数据库的失败数据也很多，需要将失败数据转存到其他存储。大量失败数据维护在内存中容易导致 OOM。  </li>\n</ol>\n<h2 id=\"improved-mapPartitions-1\"><a href=\"#improved-mapPartitions-1\" class=\"headerlink\" title=\"improved mapPartitions 1\"></a>improved mapPartitions 1</h2><p>切入点在于 mapPartitions 参数是一个 迭代器向迭代器的映射函数<br>以下代码为伪代码，仅作为模板使用   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">  * record by record 式的处理输入数据。</span><br><span class=\"line\">  * @param srcDataIterator 输入数据迭代器</span><br><span class=\"line\">  * @tparam T</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\">class TransformSaveDataIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]&#123;</span><br><span class=\"line\"> &#x2F;&#x2F; 重量级对象最好在类内部维护，避免 driver-&gt;executor 序列化问题</span><br><span class=\"line\">  lazy val connection &#x3D; createOrGetFromPoll()</span><br><span class=\"line\">  override def hasNext: Boolean &#x3D; &#123;</span><br><span class=\"line\">    if(srcDataIterator.hasNext)&#123;</span><br><span class=\"line\">      return true</span><br><span class=\"line\">    &#125;else&#123;</span><br><span class=\"line\">      closeResource()</span><br><span class=\"line\">      return false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def next(): (T, Boolean) &#x3D; &#123;</span><br><span class=\"line\">    val next:T &#x3D; srcDataIterator.next()</span><br><span class=\"line\">    val result:Boolean &#x3D; connectResource.doSave()  &#x2F;&#x2F; try &#123;doSomething;return true&#125; catch &#123;return false&#125;</span><br><span class=\"line\">    (next, result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  private def closeResource(): Unit &#x3D;&#123;</span><br><span class=\"line\">    if(connectResource !&#x3D; null &amp;&amp; connectResource.isActive())&#123;</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      connectResource.close()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>优点在于解决了 OOM 问题，缺点在于需要 record by record 式处理数据，效率低下对于支持 batch 的数据库而言未能充分利用   </p>\n<h2 id=\"improved-mapPartitions-2\"><a href=\"#improved-mapPartitions-2\" class=\"headerlink\" title=\"improved mapPartitions 2\"></a>improved mapPartitions 2</h2><p>切入点借鉴 Spark Streaming 的 mini-batch，将输入数据的迭代器 slice 为多个，每个 slice 整批插入数据库中以利用 batch 功能并同时保存 slice 迭代器和事务执行结果。记录下所有输入数据的数量，当返回数据数量不大于该值时 可返回数据。每个 slice 内迭代器数据迭代完之后，切换为下一个迭代器直到返回数据量等于输入数据总量。<br>以下代码为伪代码，仅作为模板使用 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">  * buffer 式处理输入数据</span><br><span class=\"line\">  * @param srcDataIterator 输入数据迭代器</span><br><span class=\"line\">  * @tparam T</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\">class TransformBufferedSaveIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]&#123;</span><br><span class=\"line\">  lazy val connection &#x3D; createOrGetFromPoll()</span><br><span class=\"line\">  val BUFFER_SIZE &#x3D; 2048</span><br><span class=\"line\">  lazy val buffer:ArrayBuffer[(Iterator[T],Boolean)] &#x3D; new ArrayBuffer[(Iterator[T],Boolean)](128)</span><br><span class=\"line\">  var dataCnt:Int &#x3D; 0  &#x2F;&#x2F; total input data count</span><br><span class=\"line\">  var returnedCnt:Int &#x3D; 0  &#x2F;&#x2F; already returned data count</span><br><span class=\"line\">  val trigger &#x3D; new AtomicBoolean(true)</span><br><span class=\"line\">  var bufferIterator:Iterator[(Iterator[T],Boolean)] &#x3D; null</span><br><span class=\"line\">  var curIterator:(Iterator[T],Boolean) &#x3D; null</span><br><span class=\"line\"></span><br><span class=\"line\">  override def hasNext: Boolean &#x3D; &#123;</span><br><span class=\"line\">    if(trigger.get())&#123;</span><br><span class=\"line\">      while(dataCnt &lt; srcDataIterator.length)&#123;</span><br><span class=\"line\">        val slice:Iterator[T] &#x3D; srcDataIterator.slice(dataCnt, dataCnt+BUFFER_SIZE)</span><br><span class=\"line\">        dataCnt +&#x3D;slice.length</span><br><span class=\"line\">        val result:Boolean &#x3D; doSave(slice)</span><br><span class=\"line\">        &#x2F;&#x2F; new iterator or reverse iterator</span><br><span class=\"line\">        val traversableSlice &#x3D; slice.slice(0, BUFFER_SIZE)</span><br><span class=\"line\">        buffer.+&#x3D;:((traversableSlice,result))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      closeResource()</span><br><span class=\"line\">      bufferIterator &#x3D; buffer.iterator</span><br><span class=\"line\">      trigger.set(false)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    returnedCnt &lt; dataCnt</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def next(): (T, Boolean) &#x3D; &#123;</span><br><span class=\"line\">    if(curIterator!&#x3D;null &amp;&amp; curIterator._1.hasNext)&#123;</span><br><span class=\"line\">      dataCnt +&#x3D; 1</span><br><span class=\"line\">      return (curIterator._1.next(),curIterator._2)</span><br><span class=\"line\">    &#125;else&#123;</span><br><span class=\"line\">     &#x2F;&#x2F; bufferIterator 最后一个 iterator 不会调用 next(此时hasNext()&#x3D;false)</span><br><span class=\"line\">      curIterator &#x3D; bufferIterator.next()</span><br><span class=\"line\">      next()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def doSave(it:Iterator[T]): Boolean &#x3D;&#123;</span><br><span class=\"line\">    try&#123;</span><br><span class=\"line\">      doSomething()</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      buffer.clear()</span><br><span class=\"line\">      return true</span><br><span class=\"line\">    &#125;catch&#123;</span><br><span class=\"line\">      case e:Exception &#x3D;&gt;&#123;</span><br><span class=\"line\">        return false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def closeResource(): Unit &#x3D;&#123;</span><br><span class=\"line\">    if(connectResource !&#x3D; null &amp;&amp; connectResource.isActive())&#123;</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      connectResource.close()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"SimpleTestCase\"><a href=\"#SimpleTestCase\" class=\"headerlink\" title=\"SimpleTestCase\"></a>SimpleTestCase</h2><p>分别使用 improved mapPartitions 1 和 improved mapPartitions 2 做测试样例</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object ImprovedMapPartition &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class=\"line\">    val spark &#x3D; SparkSession.builder().appName(&quot;&quot;).master(&quot;&quot;).config(&quot;&quot;,&quot;&quot;).getOrCreate();</span><br><span class=\"line\">    val sparkContext &#x3D; spark.sparkContext</span><br><span class=\"line\">    val rdd:RDD[Int] &#x3D; sparkContext.parallelize(1 to 20,3)</span><br><span class=\"line\">    &#x2F;&#x2F; save record by record</span><br><span class=\"line\">    val resultRDD &#x3D; rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      new TransformSaveDataIterator[Int](it)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">      resultRDD.cache()</span><br><span class=\"line\">    resultRDD.filter(_._2 &#x3D;&#x3D; false) &#x2F;&#x2F; exception data</span><br><span class=\"line\">      .saveAsTextFile(&quot;....&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; save buffered records</span><br><span class=\"line\">    print(&quot;finally save records: &quot;+ resultRDD.filter(_._2).count())</span><br><span class=\"line\">    val resultRDD2 &#x3D; rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      new TransformBufferedSaveIterator[Int](it)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">      resultRDD2.cache()</span><br><span class=\"line\">    resultRDD2.filter(_._2 &#x3D;&#x3D; false)</span><br><span class=\"line\">        .saveAsTextFile(&quot;....&quot;)</span><br><span class=\"line\">    print(&quot;finally save records: &quot;+ resultRDD.filter(_._2).count())</span><br><span class=\"line\"></span><br><span class=\"line\">    resultRDD.unpersist()</span><br><span class=\"line\">    resultRDD2.unpersist()</span><br><span class=\"line\">    spark.stop();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>map 和 mapPartitions 是 Spark 中最常用的 ETL 算子，用于将数据映射为另一批数据。区别在于,  </p>\n<ol>\n<li>数据粒度: map 输入和输出数据都是单条记录，严格的一对一关系。mapPartitions 输入和输出数据都是一个分区的数据迭代器, n 对 m 关系。  </li>\n<li>功能: mapPartitions 功能远强于 map,其一 mapPartitions 可以在映射过程中做诸如过滤的额外操作，其二在需要创建重量级对象(e.g. 数据库连接)的场景下 mapPartitions 比 map 操作更合适。 </li>\n<li>性能: 性能上无明显差异。一般将分区输出数据先存储在内存数据结构中,结束后转成迭代器形式。在内存中存储数据这种方式可能会导致 OOM，但是有方法避免，这是本文的价值点。     </li>\n</ol>","more":"<p>foreach 和 foreachPartition 的关系/用法类似 map 和mapPartitions,区别在 foreach(Partition)是 action 算子而 map(Partitions)是 transformer 算子。  </p>\n<h2 id=\"mapPartitions-一般用法\"><a href=\"#mapPartitions-一般用法\" class=\"headerlink\" title=\"mapPartitions 一般用法\"></a>mapPartitions 一般用法</h2><p>mapPartitions[V] 的输入参数是 f:(Iterator[U]=&gt;Iterator[V]) 使RDD[U]映射为 RDD[V]<br>一般用法是在每个分区内维护一个内存数据结构 暂存输出数据，使用输入数据迭代器遍历输入数据处理得到输出数据加入到内存数据结构中，处理完毕后将内存数据结构转换为输出数据迭代器。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val rdd:RDD[Int] &#x3D; ?</span><br><span class=\"line\">rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      val buffer &#x3D; new ArrayBuffer[Int](64)</span><br><span class=\"line\">      while(it.hasNext)&#123;</span><br><span class=\"line\">        val next &#x3D; it.next()</span><br><span class=\"line\">        if(???)&#123;</span><br><span class=\"line\">          buffer.+&#x3D;(doSomething(next))</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">          &#x2F;&#x2F; abort</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      buffer.iterator</span><br><span class=\"line\">    &#125;)</span><br></pre></td></tr></table></figure>\n<p>优点在于使用简便，缺点在于内存数据结构在数据量大时容易 OOM。  </p>\n<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><p>使用 Spark 向数据库(mongodb)中写入数据时有量级不小的错误，需要将存储失败的数据另存。  </p>\n<p>难点:   </p>\n<ol>\n<li>数据库的并发度支持不高,且其他业务也需要占用部分连接。所以最终 RDD 的分区数不能太大，进而导致单分区内数据很大。     </li>\n<li>单分区内数据很大，插入数据库的失败数据也很多，需要将失败数据转存到其他存储。大量失败数据维护在内存中容易导致 OOM。  </li>\n</ol>\n<h2 id=\"improved-mapPartitions-1\"><a href=\"#improved-mapPartitions-1\" class=\"headerlink\" title=\"improved mapPartitions 1\"></a>improved mapPartitions 1</h2><p>切入点在于 mapPartitions 参数是一个 迭代器向迭代器的映射函数<br>以下代码为伪代码，仅作为模板使用   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">  * record by record 式的处理输入数据。</span><br><span class=\"line\">  * @param srcDataIterator 输入数据迭代器</span><br><span class=\"line\">  * @tparam T</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\">class TransformSaveDataIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]&#123;</span><br><span class=\"line\"> &#x2F;&#x2F; 重量级对象最好在类内部维护，避免 driver-&gt;executor 序列化问题</span><br><span class=\"line\">  lazy val connection &#x3D; createOrGetFromPoll()</span><br><span class=\"line\">  override def hasNext: Boolean &#x3D; &#123;</span><br><span class=\"line\">    if(srcDataIterator.hasNext)&#123;</span><br><span class=\"line\">      return true</span><br><span class=\"line\">    &#125;else&#123;</span><br><span class=\"line\">      closeResource()</span><br><span class=\"line\">      return false</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def next(): (T, Boolean) &#x3D; &#123;</span><br><span class=\"line\">    val next:T &#x3D; srcDataIterator.next()</span><br><span class=\"line\">    val result:Boolean &#x3D; connectResource.doSave()  &#x2F;&#x2F; try &#123;doSomething;return true&#125; catch &#123;return false&#125;</span><br><span class=\"line\">    (next, result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  private def closeResource(): Unit &#x3D;&#123;</span><br><span class=\"line\">    if(connectResource !&#x3D; null &amp;&amp; connectResource.isActive())&#123;</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      connectResource.close()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>优点在于解决了 OOM 问题，缺点在于需要 record by record 式处理数据，效率低下对于支持 batch 的数据库而言未能充分利用   </p>\n<h2 id=\"improved-mapPartitions-2\"><a href=\"#improved-mapPartitions-2\" class=\"headerlink\" title=\"improved mapPartitions 2\"></a>improved mapPartitions 2</h2><p>切入点借鉴 Spark Streaming 的 mini-batch，将输入数据的迭代器 slice 为多个，每个 slice 整批插入数据库中以利用 batch 功能并同时保存 slice 迭代器和事务执行结果。记录下所有输入数据的数量，当返回数据数量不大于该值时 可返回数据。每个 slice 内迭代器数据迭代完之后，切换为下一个迭代器直到返回数据量等于输入数据总量。<br>以下代码为伪代码，仅作为模板使用 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">  * buffer 式处理输入数据</span><br><span class=\"line\">  * @param srcDataIterator 输入数据迭代器</span><br><span class=\"line\">  * @tparam T</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\">class TransformBufferedSaveIterator[T](srcDataIterator:Iterator[T]) extends Iterator[(T,Boolean)]&#123;</span><br><span class=\"line\">  lazy val connection &#x3D; createOrGetFromPoll()</span><br><span class=\"line\">  val BUFFER_SIZE &#x3D; 2048</span><br><span class=\"line\">  lazy val buffer:ArrayBuffer[(Iterator[T],Boolean)] &#x3D; new ArrayBuffer[(Iterator[T],Boolean)](128)</span><br><span class=\"line\">  var dataCnt:Int &#x3D; 0  &#x2F;&#x2F; total input data count</span><br><span class=\"line\">  var returnedCnt:Int &#x3D; 0  &#x2F;&#x2F; already returned data count</span><br><span class=\"line\">  val trigger &#x3D; new AtomicBoolean(true)</span><br><span class=\"line\">  var bufferIterator:Iterator[(Iterator[T],Boolean)] &#x3D; null</span><br><span class=\"line\">  var curIterator:(Iterator[T],Boolean) &#x3D; null</span><br><span class=\"line\"></span><br><span class=\"line\">  override def hasNext: Boolean &#x3D; &#123;</span><br><span class=\"line\">    if(trigger.get())&#123;</span><br><span class=\"line\">      while(dataCnt &lt; srcDataIterator.length)&#123;</span><br><span class=\"line\">        val slice:Iterator[T] &#x3D; srcDataIterator.slice(dataCnt, dataCnt+BUFFER_SIZE)</span><br><span class=\"line\">        dataCnt +&#x3D;slice.length</span><br><span class=\"line\">        val result:Boolean &#x3D; doSave(slice)</span><br><span class=\"line\">        &#x2F;&#x2F; new iterator or reverse iterator</span><br><span class=\"line\">        val traversableSlice &#x3D; slice.slice(0, BUFFER_SIZE)</span><br><span class=\"line\">        buffer.+&#x3D;:((traversableSlice,result))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      closeResource()</span><br><span class=\"line\">      bufferIterator &#x3D; buffer.iterator</span><br><span class=\"line\">      trigger.set(false)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    returnedCnt &lt; dataCnt</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  override def next(): (T, Boolean) &#x3D; &#123;</span><br><span class=\"line\">    if(curIterator!&#x3D;null &amp;&amp; curIterator._1.hasNext)&#123;</span><br><span class=\"line\">      dataCnt +&#x3D; 1</span><br><span class=\"line\">      return (curIterator._1.next(),curIterator._2)</span><br><span class=\"line\">    &#125;else&#123;</span><br><span class=\"line\">     &#x2F;&#x2F; bufferIterator 最后一个 iterator 不会调用 next(此时hasNext()&#x3D;false)</span><br><span class=\"line\">      curIterator &#x3D; bufferIterator.next()</span><br><span class=\"line\">      next()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  def doSave(it:Iterator[T]): Boolean &#x3D;&#123;</span><br><span class=\"line\">    try&#123;</span><br><span class=\"line\">      doSomething()</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      buffer.clear()</span><br><span class=\"line\">      return true</span><br><span class=\"line\">    &#125;catch&#123;</span><br><span class=\"line\">      case e:Exception &#x3D;&gt;&#123;</span><br><span class=\"line\">        return false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private def closeResource(): Unit &#x3D;&#123;</span><br><span class=\"line\">    if(connectResource !&#x3D; null &amp;&amp; connectResource.isActive())&#123;</span><br><span class=\"line\">      connectResource.commit()</span><br><span class=\"line\">      connectResource.close()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"SimpleTestCase\"><a href=\"#SimpleTestCase\" class=\"headerlink\" title=\"SimpleTestCase\"></a>SimpleTestCase</h2><p>分别使用 improved mapPartitions 1 和 improved mapPartitions 2 做测试样例</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object ImprovedMapPartition &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class=\"line\">    val spark &#x3D; SparkSession.builder().appName(&quot;&quot;).master(&quot;&quot;).config(&quot;&quot;,&quot;&quot;).getOrCreate();</span><br><span class=\"line\">    val sparkContext &#x3D; spark.sparkContext</span><br><span class=\"line\">    val rdd:RDD[Int] &#x3D; sparkContext.parallelize(1 to 20,3)</span><br><span class=\"line\">    &#x2F;&#x2F; save record by record</span><br><span class=\"line\">    val resultRDD &#x3D; rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      new TransformSaveDataIterator[Int](it)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">      resultRDD.cache()</span><br><span class=\"line\">    resultRDD.filter(_._2 &#x3D;&#x3D; false) &#x2F;&#x2F; exception data</span><br><span class=\"line\">      .saveAsTextFile(&quot;....&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; save buffered records</span><br><span class=\"line\">    print(&quot;finally save records: &quot;+ resultRDD.filter(_._2).count())</span><br><span class=\"line\">    val resultRDD2 &#x3D; rdd.mapPartitions(it&#x3D;&gt;&#123;</span><br><span class=\"line\">      new TransformBufferedSaveIterator[Int](it)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">      resultRDD2.cache()</span><br><span class=\"line\">    resultRDD2.filter(_._2 &#x3D;&#x3D; false)</span><br><span class=\"line\">        .saveAsTextFile(&quot;....&quot;)</span><br><span class=\"line\">    print(&quot;finally save records: &quot;+ resultRDD.filter(_._2).count())</span><br><span class=\"line\"></span><br><span class=\"line\">    resultRDD.unpersist()</span><br><span class=\"line\">    resultRDD2.unpersist()</span><br><span class=\"line\">    spark.stop();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"layout":"post","title":"基于队列优先级的小集群大容器资源抢占模式","subtitle":"Preemption for large container in mini yarn cluster","date":"2019-11-24T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 抢占概述\n　　Yarn 使用树状层级队列组织方式来管理资源，所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源  \n　　层级队列中除 root 外每个队列可以设置其拥有父队列的最低配额和最高配额。最高配额是队列无论在任何情况下都不会超出的资源量，最低配额一般理解为在队列保底资源量以快速响应请求，实际上是用来决定集群资源配比权重。为了提高集群整体的资源利用率，Yarn 引入了抢占机制:  \n\n1. 在队列中无任务运行时，队列没有必要硬遵守保留最低配额，可以将资源借给负载较高的其他队列  \n2. 当借出资源的队列接收到了新提交的应用时，将出借的资源收回以满足本队列的资源需求  \n\n<!-- more -->\n \n　　其中第一点由常规调度器来实现: 队列的实际配额基于自身和其他队列的需求动态变化 介于 0与最高配额之间，当队列无任何 APP 时资源出借给繁忙的队列 实际资源量为 0，当队列负载较大时可能借用其他空闲队列的资源但不会超过其最高配额。第二点由抢占调度来实现: 出借资源的队列接收了新的 APP 之后，需要从其他超出最低配额的队列(超分配队列)中抢占(回)资源，用以启动APP。从超分配队列中抢占资源时，基于“最小化对已启动应用程序的影响”原则，选择杀死 Container 释放资源时 优先杀死最新启动 App 的最新分配的 Container，优先杀死保留的 Container，优先杀死非 AM 角色的 Container  \n\n## 现存问题\n场景: 假设集群内有两个队列:A 和 B  \n　　状态 1: A完全空闲，B 负载很高，借用了 A 的全部资源    \n　　状态 2: A新提交了 AppX，且 AppX 申请的每个 Container 都很大，e.g. 60G  \n　　状态 3: 抢占调度从B 队列杀死了一些 Container，释放了一些资源(默认单次抢占资源总量不超过集群总资源的10%)，但这些资源分布在n个节点上    \n　　状态 4: 当这 n 个节点向 RM 汇报心跳时，常规调度尝试对每个队列(有序，负载低的优先，A先于 B)的每个应用程序(有序，早提交的优先)进行分配资源，如果这n 个节点都不能启动“大 Container”，那么当 A 没有得到资源时对 B 尝试分配， B 内的 App是可以得到资源的(杀死的就是 B 内 App 的 Container)    \n　　状态 5: A没有得到资源，依旧处于欠分配状态;B 得到了资源，处于超分配状态。抢占调度会再次从 B 抢占资源，杀死一些 Container。即”状态 3”  \n\n造成的影响有四：    \n\n1. 在抢占调度层面，一直在状态 3->4->5->3...循环执行，处于“A 欠分配，B 超分配“->”抢占调度释放 B 的资源”->”A 不能使用资源，B 能使用则分配给 B”->”A 欠分配，B 超分配”循环  \n2. 在资源队列层面，A 队列一直没有得到资源，且一直触发抢占; B 队列在抢占调度释放资源，在常规调度得到 A 队列无法利用的资源  \n3. 在应用程序层面，对于 A中的大 Container App，一直没有得到资源，处于等待状态;对于 B 中的 App，其部分 Container 一直处于”被抢占”->”可启动”->”启动(成为最新容器)”->”被抢占”->”可启动”...的循环状态  \n4. 从 SLA 和资源利用率角度来说，A 中的 App 没有迅速响应，B 中的 App 被拖慢了速度。有一部分资源被反复无效调度  \n\n>\nTips:  \n1. 抢占调度是周期性执行的，默认 3 秒。  \n2. 抢占调度和常规调度是解耦的，常规调度负责为每个队列的每个 APP的每个 ResourceRequest 调度，抢占调度负责所有队列资源的平衡，不为某个具体的 App、具体的 Container 做调度    \n3. 抢占调度是”资源再平衡”的过程，空闲队列有了资源需求后，所有队列计算自身的理想容量，超分配的队列释放资源，欠分配的队列在常规调度模块优先分配资源  \n4. 集群内队列是平等的， A 队列可以抢 B， B 队列也可以抢 A。这对保证核心业务来说是不友好的，非核心业务可能会抢占核心业务的资源，拖慢核心业务  \n5. “最小化对已启动应用程序的影响”是抢占调度的核心原则，具体表现在:  \n　　a. 每轮总抢占:默认 10%，单次抢占总资源量最多不能超过集群总资源量的 10%  \n　　b. 忽略抢占阈值(deadzone,死区): 默认 0.2，超分配队列只有在 used>(1+0.2) * capacity 时，才会还回资源。e.g. capacity=100G， used=110G时则不在该队列抢占  \n　　c. 自然终止因子: 默认 20%，即使容器不被杀死，也有 95%概率在5 * 15s 内终止。所以每个超分配队列最多还回超分配部分的 20%。e.g. used=100G，ideal=40G，本轮抢占该队列最终归还量为(100-40) * 20%=12G\n　　d. 确定了最终归还量之后，则从队列所有 APP 中选择一些容器杀死，杀死容器的总容量大于等于最终归还量。选择容器时的规则是:优先选择最新启动的 APP最近启动的容器，优先选择保留的容器(未启动)，优先选择非 AM 角色的容器   \n　　故自然终止因子决定了该轮抢占立即能得到的资源量，每轮总抢占决定了该轮抢占最多能得到的资源量  \n6. 保留:常规调度为什么不在 n 个节点上做保留?  \n在节点上为容器做分配/保留的限制条件有:  \n　　a. 队列具有该节点的 access 权限，节点不在 APP 设置的黑名单内  \n　　b. 节点上无保留且节点剩余空间大于最小容器大小(yarn.scheduler.minimum-allocation-mb)    \n　　c. 最高配额限制: 如果分配该容器，叶子队列及其所有父队列均不能超过各自的最高配额  \n　　d. 用户配额限制: 用户配额上限(headroom) 取值为 min{当前容量 * user_limit_factor，max{当前容量/活跃用户数，当前容量/最高用户数}}，当前容量取决于最低配额、当前尝试的ResourceRequest 、 Used 关系。一个用户在任何情况下，使用的配额不能高于队列最低配额*user_limit_factor  \n　　e. AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)  \n主要限制在 c)和 d)。d)中 user_limit_factor 一般设置为 1，活跃用户数低于最高用户限制，则取值一般为当前容量/活跃用户数，当队列活跃用户越多，用户配额上限会越低  \n\n## 改进方案\n\n1. 建立业务优先级体系，为每个队列赋予业务优先级属性，优先级高的队列可以抢占低的队列，反之则不可。用以保证核心业务不被非核心业务抢占资源    \n2. 在计算每个队列应得的资源时，若队列预计得到的资源不足以满足队列中最小的 ResourceRequest，则抢占是无意义的，不为该队列做抢占，其预计得到的资源将被设置为 0，这些无法利用的资源将转给其他队列   \n按照对”大容器”的定义(e.g. 40G)，优先为”大容器”队列做抢占，然后为“普通容器“队列做抢占  \n3. 为”大容器”抢占时，在每个节点上选取待抢占队列的最新启动的 Application 的最新分配的部分容器释放且计算释放代价，最终选取释放代价最低的节点上的容器释放。若队列分配完”大容器” 或者待抢占量不高于”大容器” 定义，则后续将其作为”普通容器” 队列来处理  \n4. 为”普通容器”抢占时，选取待抢占队列中最新启动的 App 的最新 Container 开始释放    \n\n## 抢占流程\n\n![](/img/pictures/yarn_preempt/preemption_scheduler.png)\n\t \n1. 获取各队列使用状态  \n　　包括: 队列的业务优先级，最低配额(capacity)，最高配额，当前使用量(used)，当前堆积需求量(pending)，最低需求量，最高需求量，初始理想容量(min{used,capacity})  \n2. 按优先级计算每个队列的理想容量  \n　　首先将所有队列划分为空闲队列(used<=capacity)、超分配队列(used>capacity)两类，并统计出欠分配队列(used+pending>当前理想容量)。在保证弹性最低配额的条件下，将空闲队列的(capacity-used) 和超分配队列(used-capacity) 作为全局空闲量，按规则分配给欠分配队列:  \n　　a. 优先全局分配空闲资源给优先级高的欠分配队列，按优先级由高至低依次分配       \n　　b. 分配的资源量取决于 min{全局空闲量，需求量，(最高配额-当前理想容量)}，记为 increment   \n　　　即保证在不超过队列最高配额的情况下，从全局空闲量中优先满足高优先级队列的需求    \n　　c. 然后分配全局空闲资源给无优先级但配置有最低配额的欠分配队列，按照欠分配程度(当前理想配额/最低配额 值)由低到高依次分配    \n　　d. 分配的资源量取决于 min{全局空闲量 * 队列最低配额权重，需求量，(最高配额-当前理想容量)}，记为 increment     \n　　e. 最后分配全局空闲资源给无优先级无最配配额的欠分配队列，按照欠分配程度由低到高依次分配  \n　　f. 分配的资源量取决于 min{全局空闲量 * 队列平均权重，需求量，(最高配额-当前理想容量)}，记为 increment   \n　　g. 在全局空闲量为 0 或者没有欠分配队列时终止此环节    \n即: 抢占是一个再平衡的过程，资源从低优先级队列流向高优先级欠分配队列，从未配置最低配额的队列流向配置最低配额的队列，但是会保证有任务但被抢占资源队列的弹性最低配额(min{used+pending, capacity})  \n3. 判断各队列是否是有效抢占  \n　　判断各队列的 increment 值，若 increment < minResourceRequest，即新得到的资源不足以启动任何一个容器，在将其从欠分配队列中删除  \n4. 设置其 pending 为 0  \n　　依据 used + pending = 当前容量， pending 设为 0 之后，不再是欠分配队列。  \n5. 再计算  \n　　重新计算抢占依据 (e.g. 高优先级队列抢占的资源无法启动队列中最小的一个容器时，将此资源按优先级再分给其他队列)     \n\n\t此处的表述有歧义，但是画图过于冗余，故而放到了一个循环中，其实是三个循环。第一个循环: 优先分配有优先级配置的队列，并判断其是否是有效抢占，若是无效抢占，则把其 increment 还回到全局空闲配额中，供次级流程(后续的低优先级队列，无优先级队列)使用。第二个循环是: 分配无优先级但有最低配额的队列，判断其是否是有效抢占，若是无效抢占，则把 increment 还回到全局空闲配额中，供后续流程使用(后续无优先级有最低配额的欠分配队列，无优先级无最低配额的欠分配队列)。第三个循环是: 无优先级无最低配额的欠分配队列。第一个循环在队列分配完成后立即判断是否是有效抢占，第二三个循环需要在所属循环分配完成后按累计 increment 由高至低分别判断，若有无效抢占立即重算除其之外的所有的队列的理想配额。      \n6. 计算出所有队列的待抢占量，并依据最小需求将所有欠分配队列划分为”大容器”和”普通容器”，分别进行抢占调度  \n7. 对于每个”大容器”队列，循环(8->11)抢占，每次抢占量为 maxResourceRequest(足以启动队列中任何一个容器)。直到没有 NM 满足，或者待抢占量低于”大容器”定义    \n\n\t```\n伪代码\nMap<NodeId，QueueContainerDetail> info;\nFor(App: toPreemptedQueue)\n\taddContainer{runningContainer，reservedContainer} -> info(amContainers,nonAMContainers);\nMap<TempQueue,Resource> limit = listQueuePreemptionLimit(); \nFor( curQueue: allUnderReservedQueue) \n\tmaxRR=curQueue.getMaxResourceRequest \n\ttotalPreeption=curQueue.getTotalPreemption\n\twhile(totalPreeption>maxRR )\n\t\tminCostTime=Long.MaxValue\t\n\t\tMap<Queue，Container> containers，NodeId //在该节点上杀死这些容器为最小代价\n\t\tfor(nodeId: NodeId)\n\t\t\tcostTime = tryPreemptFromNode(node,limit,maxRR)\n\t\t\tif(costTime<minCostTime)\n\t\t\t\tminCostTime=costTime; containers,nodeId REFRESH->POINTER\n\t\tif(costTime ==Long.MaxValue)\n\t\t\tLOG.warn(\"no node meet the request in this queue, do left totalPreemption as common preemption\")\n\t        else\n\t\t\ttotalPreemption -= maxRR\n```\n\n8. 统计每个待抢占队列的(App，Container)->(nm) 信息  \n\n\t```\npublic void addContainer(TempQueue queue, RMContainer container){\n      ensureQueue(queue);\n      if(container.isAMContainer()){\n        amContainers.get(queue).add(container);\n      }else {\n        nonAMContainers.get(queue).add(container);\n      }\n    }\n\t```\n\n9. 尝试在每个 NM 上抢占，并计算抢占代价  \n　　尝试使用nonAMContainers 来满足 maxResourceRequest；并维护相关信息，计算抢占代价 costTime=∑（curTime-contianer.getStartTime）  \n限制条件是:  \n　　a. 每个队列不超出自身待释放的资源  \n　　b. 在该节点上所有超分配队列释放的资源和节点空闲资源 可以满足 maxRR  \nQueueAppInfo 的核心数据结构和逻辑:   \n[QueueContainerDetail.java]  \n\n\t```\nNodeId nodeId;Clock clock;Long curTime;Long costTime;\nMap<TempQueue, List<RMContainer>> nonAMContainers;\nMap<TempQueue, List<RMContainer>> amContainers;\n// 各队列 最终被抢占的 container 列表\nMap<TempQueue, List<RMContainer>> toBePreemptedContainers;\n// 各队列 最终被抢占的资源量\nMap<TempQueue, Resource> toBePreemptedResource;\n/**\n     * note: since to \"try preempt\",\n     * the maxResourceRequest and preemptedLimitOfQueue are copied.\n     * @param maxResourceRequest  待抢占量 e.g. 40G\n     * @param preemptedLimit queueLimit A->30G, B->20G 抢占总量限制\n     */\npublic long tryPreemptFromNode(Resource maxResourceRequest,\n                                   Map<TempQueue, Resource> preemptedLimit){\n      clearCache();\n      curTime=clock.getTime();\n      Resource toPreempt=Resources.clone(maxResourceRequest);\n      Iterator<Map.Entry<TempQueue, List<RMContainer>>> it =\n          nonAMContainers.entrySet().iterator();\n      while(Resources.greaterThan(rc, clusterResource,\n          toPreempt, Resources.none())&& it.hasNext()){\n        Map.Entry<TempQueue, List<RMContainer>> entry = it.next();\n        TempQueue currentQueue = entry.getKey();\n        Resource limit = Resources.clone(preemptedLimit.get(currentQueue));\n        if(Resources.greaterThan(rc, clusterResource,\n            limit, Resources.none()) == false || currentQueue.preemptionDisabled){\n          continue;\n        }\n        List<RMContainer> containerList = entry.getValue();\n        long sortStart = clock.getTime();\n        Collections.sort(containerList, COMPARATOR_CONTAINER);\n        long sortEnd = clock.getTime();\n        if(sortEnd - sortStart >= 100){\n          LOG.warn(\" Slow Sort nonAM Containers when trying preempt maxResourceRequest {} \" +\n                  \"at queue {} with queueLimit {} total {} container, token {}ms\",\n              toPreempt,\n              currentQueue.getQueueName(),\n              limit,\n              containerList.size(),\n              sortEnd-sortStart);\n        }\n        Iterator<RMContainer> iit=containerList.iterator();\n        while(Resources.greaterThan(rc, clusterResource, limit, Resources.none()) &&\n            Resources.greaterThan(rc, clusterResource, toPreempt, Resources.none()) &&\n            iit.hasNext()){\n          RMContainer i=iit.next();\n          if(isLabelContainer(i)){\n            continue;\n          }\n          toBePreemptedContainers.get(currentQueue).add(i);\n          Resources.addTo(toBePreemptedResource.get(currentQueue),\n              i.getContainer().getResource());\n          Resources.subtractFrom(limit, i.getContainer().getResource());\n          Resources.subtractFrom(toPreempt, i.getContainer().getResource());\n          costTime += (curTime - i.getCreationTime());\n        }\n      }\n      // 暂不抢占 AM,代价不可控 难维护\n      if(Resources.greaterThan(rc, clusterResource,\n          toPreempt, Resources.none())){\n        costTime = Long.MAX_VALUE;\n      }\n      return costTime;\n    }\n    public void preemptFromNode(){\n      for(Map.Entry<TempQueue, List<RMContainer>> entry :\n          toBePreemptedContainers.entrySet()){\n        nonAMContainers.get(entry.getKey()).removeAll(entry.getValue());\n        amContainers.get(entry.getKey()).removeAll(entry.getValue());\n      }\n    }\n\t```\n10. 选取抢占代价最低的节点，记录相应 container 集合    \n　　获取在所有 NM 中，costTime 最低的 Container 集合作为待抢占容器      \n11. 若当前队列的待抢占量小于”大容器”定义，则剩余资源作为”普通容器抢占”   \n12. 在抢占代价最低的节点上，释放了总资源量不低于 maxResourceRequest 的 Container 集合  \n13. 为所有”普通容器”做抢占:在待抢占的队列中杀死最新启动的容器(不存在于 步骤10 中) \n\n\t```\nResource toPreempted;\nFor(Queue : overReservedQueue)\n\t\tFor(App: Queue.getApps)\n\t\t\tif(toPreempted <= none) break;\n\t\t\tkillReservedContainer(App， toPreempted); \n\t\t\tkillAllocatedContainerByOrder(App， toPreempted);\n\t\t\trememberAM();\nif(toPreempted > none)\n\t\tkillAMByOrder();\n\t```\t\t\n\n## 优点缺点\n\n### 优点\n\n1. 在集群负载高的时候，可以保证优先级高的业务有资源执行;低优先级队列不能抢占高优先级队列的任务.(开源实现 不能保证)  \n2. 可以在节点上启动大容器 (开源实现 不能保证)  \n3. 可以避免无效抢占 (开源实现 不能保证)  \n\n### 缺点\n1. 集群负载高时，低优先级长时任务可能被无限拉长  \n2. 在节点上选取最新启动的容器抢占，而不是在全局选择最新启动的容器抢占，导致任务恢复无效工作量增加  \n3. 大容器调度时，增加了节点的资源利用率波动  \n\n## 抢占调度(开源抢占详解)\n抢占调度是周期性的，每周期抢占调度执行的策略分三步:  \n\n1.\t获取所有队列的快照  \n2.\t计算所有队列的理想容量，作为抢占依据  \n3.\t从超分配队列中，计算最终抢占量，并选取部分容器释放容量  \n其中最重要的是第2步，这一步决定着:队列是否释放资源及释放多少资源  \n\n### 获取所有队列的快照\n\n获取信息:  \n　　队列用量 used; 队列堆积需求 pending; 队列最低配额 capacity ;队列最高配额额 maxCapacity; 队列是否允许抢占 preemptionDisabled; 子队列 children   \n初始化信息:  \n　　理想容量 idealAssigned; 超出配额容量 toBePreempted; 最终被抢占容量 actuallyPreempted; 权重normalizedGrarantee\n\n### 计算所有队列的理想容量，作为抢占依据\n　　设置 root 队列的理想容量为 100%，递归向下按层计算 每个子队列的理想容量，子队列的子队列理想容量，直至叶子队列。每轮递归通用逻辑是:依据当前队列的理想容量和所有子队列快照信息，计算每个子队列的理想容量。  \na.  划分子队列类型  \n　　对于递归过程中，当前队列的所有子队列按照”是否配置了最低配额”划分为两类:第一类，配置最低配额的子队列集合;第二类，未配置最低配额的子队列集合。区别在于:第一类要优先于第二类得到资源，满足了第一类子队列需求之后如果还有空闲资源才会分配给第二类子队列；第一类子队列在分享当前队列的配额时，之间是以最低配额为权重的，第二类由于没有配置最低配额，之间是平均分配的  \nb. 优先分配 配置了最低配额的子队列  \n　　b1. 设定初始理想配额  \n　　　　这一过程的主要逻辑是: 为所有子队列设定动态初始理想配额，并计算出当前队列的空闲配额，记录欠分配子队列。  \n　　　　初始配额设定:   \n　　　　　　若 used<=capacity，则idealAssigned=used。将(capacity-used) 作为空闲资源     \n　　　　　　若 used>capacity，则ideaAssigned=capacity。将(used-capacity) 作为空闲资源拿出来再分配     \n　　　　计算当前队列的空闲配额:  \n　　　　　　将 上述两种空闲资源之和作为队列全局空闲资源，交由欠分配子队列 再分配  \n　　　　记录欠分配子队列:  \n　　　　　　若子队列 used+pending > idealAssigned，即需求量大于容量 为欠分配子队列。  \n　　b2. 再分配过程  \n　　　　目的: 按照 b1 记录的信息，将全局空闲资源，按照分配给所有的欠分配子队列。    \n　　　　方式: 以循环的方式，每轮循环为最欠分配的队列(most under served queue)分配资源。最欠分配的队列在得到资源后，欠分配程度排名可能会变化。因此这种分配方式会使各子队列资源量比值最终趋近于”最低配额”比值。”资源再平衡”的说法来源于此。  \n　　　　权重: 每个子队列在欠分配子队列集合中获取资源的权重是 capacity/∑capacity，存储在队列快照中的 normalizedGrarantee字段  \n　　　　变量: 全局空闲资源，欠分配子队列，子队列减少导致的分配权重增大  \n　　　　增量: 循环为最欠分配的队列分配资源，每次分配的增量为min{全局空闲资源 * 权重，需求量，最高配额-当前理想配额}  \n　　　　　　全局空闲资源 * 权重: 表示在该轮循环 能得到的资源  \n　　　　　　需求量:used+pending-idealAssigned，表示这些资源可以满足队列的需求  \n　　　　　　最高配额-当前理想配额: 表示在任何情况下，队列配额都不能超过最高配额  \n　　　　增量最终会增加到理想配额idealAssigned。在全局空闲资源为 0 或者无任何欠分配队列时，idealAssigned 会作为队列最终的理想配额  \n\n核心源码如下:\n\n```\n/**\n * orderedByNeed : 所有欠分配队列\n * unassigned: 全局空闲资源\n * /\nwhile (!orderedByNeed.isEmpty()\n   && Resources.greaterThan(rc，tot_guarant， unassigned，Resources.none())) {\n  Resource wQassigned = Resource.newInstance(0， 0);\n  / *\n   * 设置orderedByNeed中所有队列的权重\n   *  配置最低配额: capacity/sum(capacity)\n   * 未配置最低配额: 1/orderedByNeed.size()\n   * /\n  resetCapacity(rc， unassigned， orderedByNeed， ignoreGuarantee);\n  / **\n   * 最欠分配的队列( most under served queue )\n   * 比较 idealAssigned/capacity 值\n   */\n  Collection<TempQueue> underserved =\n      getMostUnderservedQueues(orderedByNeed， tqComparator);\n  for (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\n    TempQueue sub = i.next();\n    //全局空闲资源*权重\n    Resource wQavail = Resources.multiplyAndNormalizeUp(rc，\n        unassigned， sub.normalizedGuarantee， Resource.newInstance(1， 1));\n    //三元组{全局空闲资源*权重，used+pending-idealAssigned，maxCapacity-idealAssigned}\n    Resource wQidle = sub.offer(wQavail， rc， tot_guarant);\n    //增量\n    Resource wQdone = Resources.subtract(wQavail， wQidle);\n    if (Resources.greaterThan(rc， tot_guarant，\n          wQdone， Resources.none())) {\n      //如果这个队列在这轮得到了资源，那么下一轮也可能得到资源，所以加入到原集合中;直到没有得到资源\n      orderedByNeed.add(sub);\n    }\n    Resources.addTo(wQassigned， wQdone);\n  }\n  //修改全局空闲资源\n  Resources.subtractFrom(unassigned， wQassigned);\n}\n```\nc. 最后分配 未配置最低配额的子队列  \n　　如果在满足了”配置最低配额的子队列“的所有需求之后，还有剩余配额，则平均分配到所有”未配置最低配额的子队列”中。    \n　　除权重外，和 b 完全一致。  \n　　　　权重:子队列集合中每个子队列都会平均分配全局空闲资源(区别于 按最低配额做权重)  \nd. 规整队列待抢占量\n　　汇总所有叶子队列的超出配额(used-idealAssigned)之和，如果此值超出“每轮总抢占“(默认集群总资源的 10%)，则按比例减少每个叶子队列的待抢占量 以保证每轮抢占的总资源量不超过”每轮总抢占”。\n\n### 从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\n\n1. 判断超分配队列  \n　　若 used>capacity*(1+maxIgnoredOverCapacity)，则判断为超分配队列，该队列需要释放资源。  \n　　其中 maxIgnoredOverCapacity 表示 忽略抢占阈值，默认 0.2。 参考“Tips-5”  \n2. 计算最终抢占量  \n　　最终抢占量 actuallyPreempted=toBePreempted * naturalTerminationFactor。  \n　　其中naturalTerminationFactor表示 自然终止因子，默认 0.2。toBePreempt=used-idealAssigned。参考”Tips-5”  \n3. 选取容器  \n　　选取总大小不小于actuallyPreempted的一些容器，释放掉  \n　　选取规则如下:   \n　　　　优先选择队列中后启动的 APP  \n　　　　优先选择 APP 的保留容器  \n　　　　优先选择 APP 后启动的容器  \n　　如果以上普通容器全部选取完之后无法还回等量actuallyPreempted的资源，则开始选取AM 角色的容器，优先选取后启动的 AM容器  \n\n### 释放\n　　对于选取的、将要被抢占的所有容器，将<容器，时间>加入到一个全局的集合中追踪，考虑到自然终止的情况，如果集合中的容器存活时间超过 5 * 15s，则直接杀死。  \n\n\n\n\n\n","source":"_posts/2019-11-25-基于队列优先级的小集群大容器资源抢占方案.md","raw":"---\nlayout:     post\ntitle:      基于队列优先级的小集群大容器资源抢占模式\nsubtitle:   Preemption for large container in mini yarn cluster  \ndate:       2019-11-25  \nauthor:     jiulongzhu  \nheader-img: img/moon_night.jpg  \ncatalog: true  \ntags:  \n    - Yarn 2.7.3 \n    - 方案 \n---\n\n\n## 抢占概述\n　　Yarn 使用树状层级队列组织方式来管理资源，所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源  \n　　层级队列中除 root 外每个队列可以设置其拥有父队列的最低配额和最高配额。最高配额是队列无论在任何情况下都不会超出的资源量，最低配额一般理解为在队列保底资源量以快速响应请求，实际上是用来决定集群资源配比权重。为了提高集群整体的资源利用率，Yarn 引入了抢占机制:  \n\n1. 在队列中无任务运行时，队列没有必要硬遵守保留最低配额，可以将资源借给负载较高的其他队列  \n2. 当借出资源的队列接收到了新提交的应用时，将出借的资源收回以满足本队列的资源需求  \n\n<!-- more -->\n \n　　其中第一点由常规调度器来实现: 队列的实际配额基于自身和其他队列的需求动态变化 介于 0与最高配额之间，当队列无任何 APP 时资源出借给繁忙的队列 实际资源量为 0，当队列负载较大时可能借用其他空闲队列的资源但不会超过其最高配额。第二点由抢占调度来实现: 出借资源的队列接收了新的 APP 之后，需要从其他超出最低配额的队列(超分配队列)中抢占(回)资源，用以启动APP。从超分配队列中抢占资源时，基于“最小化对已启动应用程序的影响”原则，选择杀死 Container 释放资源时 优先杀死最新启动 App 的最新分配的 Container，优先杀死保留的 Container，优先杀死非 AM 角色的 Container  \n\n## 现存问题\n场景: 假设集群内有两个队列:A 和 B  \n　　状态 1: A完全空闲，B 负载很高，借用了 A 的全部资源    \n　　状态 2: A新提交了 AppX，且 AppX 申请的每个 Container 都很大，e.g. 60G  \n　　状态 3: 抢占调度从B 队列杀死了一些 Container，释放了一些资源(默认单次抢占资源总量不超过集群总资源的10%)，但这些资源分布在n个节点上    \n　　状态 4: 当这 n 个节点向 RM 汇报心跳时，常规调度尝试对每个队列(有序，负载低的优先，A先于 B)的每个应用程序(有序，早提交的优先)进行分配资源，如果这n 个节点都不能启动“大 Container”，那么当 A 没有得到资源时对 B 尝试分配， B 内的 App是可以得到资源的(杀死的就是 B 内 App 的 Container)    \n　　状态 5: A没有得到资源，依旧处于欠分配状态;B 得到了资源，处于超分配状态。抢占调度会再次从 B 抢占资源，杀死一些 Container。即”状态 3”  \n\n造成的影响有四：    \n\n1. 在抢占调度层面，一直在状态 3->4->5->3...循环执行，处于“A 欠分配，B 超分配“->”抢占调度释放 B 的资源”->”A 不能使用资源，B 能使用则分配给 B”->”A 欠分配，B 超分配”循环  \n2. 在资源队列层面，A 队列一直没有得到资源，且一直触发抢占; B 队列在抢占调度释放资源，在常规调度得到 A 队列无法利用的资源  \n3. 在应用程序层面，对于 A中的大 Container App，一直没有得到资源，处于等待状态;对于 B 中的 App，其部分 Container 一直处于”被抢占”->”可启动”->”启动(成为最新容器)”->”被抢占”->”可启动”...的循环状态  \n4. 从 SLA 和资源利用率角度来说，A 中的 App 没有迅速响应，B 中的 App 被拖慢了速度。有一部分资源被反复无效调度  \n\n>\nTips:  \n1. 抢占调度是周期性执行的，默认 3 秒。  \n2. 抢占调度和常规调度是解耦的，常规调度负责为每个队列的每个 APP的每个 ResourceRequest 调度，抢占调度负责所有队列资源的平衡，不为某个具体的 App、具体的 Container 做调度    \n3. 抢占调度是”资源再平衡”的过程，空闲队列有了资源需求后，所有队列计算自身的理想容量，超分配的队列释放资源，欠分配的队列在常规调度模块优先分配资源  \n4. 集群内队列是平等的， A 队列可以抢 B， B 队列也可以抢 A。这对保证核心业务来说是不友好的，非核心业务可能会抢占核心业务的资源，拖慢核心业务  \n5. “最小化对已启动应用程序的影响”是抢占调度的核心原则，具体表现在:  \n　　a. 每轮总抢占:默认 10%，单次抢占总资源量最多不能超过集群总资源量的 10%  \n　　b. 忽略抢占阈值(deadzone,死区): 默认 0.2，超分配队列只有在 used>(1+0.2) * capacity 时，才会还回资源。e.g. capacity=100G， used=110G时则不在该队列抢占  \n　　c. 自然终止因子: 默认 20%，即使容器不被杀死，也有 95%概率在5 * 15s 内终止。所以每个超分配队列最多还回超分配部分的 20%。e.g. used=100G，ideal=40G，本轮抢占该队列最终归还量为(100-40) * 20%=12G\n　　d. 确定了最终归还量之后，则从队列所有 APP 中选择一些容器杀死，杀死容器的总容量大于等于最终归还量。选择容器时的规则是:优先选择最新启动的 APP最近启动的容器，优先选择保留的容器(未启动)，优先选择非 AM 角色的容器   \n　　故自然终止因子决定了该轮抢占立即能得到的资源量，每轮总抢占决定了该轮抢占最多能得到的资源量  \n6. 保留:常规调度为什么不在 n 个节点上做保留?  \n在节点上为容器做分配/保留的限制条件有:  \n　　a. 队列具有该节点的 access 权限，节点不在 APP 设置的黑名单内  \n　　b. 节点上无保留且节点剩余空间大于最小容器大小(yarn.scheduler.minimum-allocation-mb)    \n　　c. 最高配额限制: 如果分配该容器，叶子队列及其所有父队列均不能超过各自的最高配额  \n　　d. 用户配额限制: 用户配额上限(headroom) 取值为 min{当前容量 * user_limit_factor，max{当前容量/活跃用户数，当前容量/最高用户数}}，当前容量取决于最低配额、当前尝试的ResourceRequest 、 Used 关系。一个用户在任何情况下，使用的配额不能高于队列最低配额*user_limit_factor  \n　　e. AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)  \n主要限制在 c)和 d)。d)中 user_limit_factor 一般设置为 1，活跃用户数低于最高用户限制，则取值一般为当前容量/活跃用户数，当队列活跃用户越多，用户配额上限会越低  \n\n## 改进方案\n\n1. 建立业务优先级体系，为每个队列赋予业务优先级属性，优先级高的队列可以抢占低的队列，反之则不可。用以保证核心业务不被非核心业务抢占资源    \n2. 在计算每个队列应得的资源时，若队列预计得到的资源不足以满足队列中最小的 ResourceRequest，则抢占是无意义的，不为该队列做抢占，其预计得到的资源将被设置为 0，这些无法利用的资源将转给其他队列   \n按照对”大容器”的定义(e.g. 40G)，优先为”大容器”队列做抢占，然后为“普通容器“队列做抢占  \n3. 为”大容器”抢占时，在每个节点上选取待抢占队列的最新启动的 Application 的最新分配的部分容器释放且计算释放代价，最终选取释放代价最低的节点上的容器释放。若队列分配完”大容器” 或者待抢占量不高于”大容器” 定义，则后续将其作为”普通容器” 队列来处理  \n4. 为”普通容器”抢占时，选取待抢占队列中最新启动的 App 的最新 Container 开始释放    \n\n## 抢占流程\n\n![](/img/pictures/yarn_preempt/preemption_scheduler.png)\n\t \n1. 获取各队列使用状态  \n　　包括: 队列的业务优先级，最低配额(capacity)，最高配额，当前使用量(used)，当前堆积需求量(pending)，最低需求量，最高需求量，初始理想容量(min{used,capacity})  \n2. 按优先级计算每个队列的理想容量  \n　　首先将所有队列划分为空闲队列(used<=capacity)、超分配队列(used>capacity)两类，并统计出欠分配队列(used+pending>当前理想容量)。在保证弹性最低配额的条件下，将空闲队列的(capacity-used) 和超分配队列(used-capacity) 作为全局空闲量，按规则分配给欠分配队列:  \n　　a. 优先全局分配空闲资源给优先级高的欠分配队列，按优先级由高至低依次分配       \n　　b. 分配的资源量取决于 min{全局空闲量，需求量，(最高配额-当前理想容量)}，记为 increment   \n　　　即保证在不超过队列最高配额的情况下，从全局空闲量中优先满足高优先级队列的需求    \n　　c. 然后分配全局空闲资源给无优先级但配置有最低配额的欠分配队列，按照欠分配程度(当前理想配额/最低配额 值)由低到高依次分配    \n　　d. 分配的资源量取决于 min{全局空闲量 * 队列最低配额权重，需求量，(最高配额-当前理想容量)}，记为 increment     \n　　e. 最后分配全局空闲资源给无优先级无最配配额的欠分配队列，按照欠分配程度由低到高依次分配  \n　　f. 分配的资源量取决于 min{全局空闲量 * 队列平均权重，需求量，(最高配额-当前理想容量)}，记为 increment   \n　　g. 在全局空闲量为 0 或者没有欠分配队列时终止此环节    \n即: 抢占是一个再平衡的过程，资源从低优先级队列流向高优先级欠分配队列，从未配置最低配额的队列流向配置最低配额的队列，但是会保证有任务但被抢占资源队列的弹性最低配额(min{used+pending, capacity})  \n3. 判断各队列是否是有效抢占  \n　　判断各队列的 increment 值，若 increment < minResourceRequest，即新得到的资源不足以启动任何一个容器，在将其从欠分配队列中删除  \n4. 设置其 pending 为 0  \n　　依据 used + pending = 当前容量， pending 设为 0 之后，不再是欠分配队列。  \n5. 再计算  \n　　重新计算抢占依据 (e.g. 高优先级队列抢占的资源无法启动队列中最小的一个容器时，将此资源按优先级再分给其他队列)     \n\n\t此处的表述有歧义，但是画图过于冗余，故而放到了一个循环中，其实是三个循环。第一个循环: 优先分配有优先级配置的队列，并判断其是否是有效抢占，若是无效抢占，则把其 increment 还回到全局空闲配额中，供次级流程(后续的低优先级队列，无优先级队列)使用。第二个循环是: 分配无优先级但有最低配额的队列，判断其是否是有效抢占，若是无效抢占，则把 increment 还回到全局空闲配额中，供后续流程使用(后续无优先级有最低配额的欠分配队列，无优先级无最低配额的欠分配队列)。第三个循环是: 无优先级无最低配额的欠分配队列。第一个循环在队列分配完成后立即判断是否是有效抢占，第二三个循环需要在所属循环分配完成后按累计 increment 由高至低分别判断，若有无效抢占立即重算除其之外的所有的队列的理想配额。      \n6. 计算出所有队列的待抢占量，并依据最小需求将所有欠分配队列划分为”大容器”和”普通容器”，分别进行抢占调度  \n7. 对于每个”大容器”队列，循环(8->11)抢占，每次抢占量为 maxResourceRequest(足以启动队列中任何一个容器)。直到没有 NM 满足，或者待抢占量低于”大容器”定义    \n\n\t```\n伪代码\nMap<NodeId，QueueContainerDetail> info;\nFor(App: toPreemptedQueue)\n\taddContainer{runningContainer，reservedContainer} -> info(amContainers,nonAMContainers);\nMap<TempQueue,Resource> limit = listQueuePreemptionLimit(); \nFor( curQueue: allUnderReservedQueue) \n\tmaxRR=curQueue.getMaxResourceRequest \n\ttotalPreeption=curQueue.getTotalPreemption\n\twhile(totalPreeption>maxRR )\n\t\tminCostTime=Long.MaxValue\t\n\t\tMap<Queue，Container> containers，NodeId //在该节点上杀死这些容器为最小代价\n\t\tfor(nodeId: NodeId)\n\t\t\tcostTime = tryPreemptFromNode(node,limit,maxRR)\n\t\t\tif(costTime<minCostTime)\n\t\t\t\tminCostTime=costTime; containers,nodeId REFRESH->POINTER\n\t\tif(costTime ==Long.MaxValue)\n\t\t\tLOG.warn(\"no node meet the request in this queue, do left totalPreemption as common preemption\")\n\t        else\n\t\t\ttotalPreemption -= maxRR\n```\n\n8. 统计每个待抢占队列的(App，Container)->(nm) 信息  \n\n\t```\npublic void addContainer(TempQueue queue, RMContainer container){\n      ensureQueue(queue);\n      if(container.isAMContainer()){\n        amContainers.get(queue).add(container);\n      }else {\n        nonAMContainers.get(queue).add(container);\n      }\n    }\n\t```\n\n9. 尝试在每个 NM 上抢占，并计算抢占代价  \n　　尝试使用nonAMContainers 来满足 maxResourceRequest；并维护相关信息，计算抢占代价 costTime=∑（curTime-contianer.getStartTime）  \n限制条件是:  \n　　a. 每个队列不超出自身待释放的资源  \n　　b. 在该节点上所有超分配队列释放的资源和节点空闲资源 可以满足 maxRR  \nQueueAppInfo 的核心数据结构和逻辑:   \n[QueueContainerDetail.java]  \n\n\t```\nNodeId nodeId;Clock clock;Long curTime;Long costTime;\nMap<TempQueue, List<RMContainer>> nonAMContainers;\nMap<TempQueue, List<RMContainer>> amContainers;\n// 各队列 最终被抢占的 container 列表\nMap<TempQueue, List<RMContainer>> toBePreemptedContainers;\n// 各队列 最终被抢占的资源量\nMap<TempQueue, Resource> toBePreemptedResource;\n/**\n     * note: since to \"try preempt\",\n     * the maxResourceRequest and preemptedLimitOfQueue are copied.\n     * @param maxResourceRequest  待抢占量 e.g. 40G\n     * @param preemptedLimit queueLimit A->30G, B->20G 抢占总量限制\n     */\npublic long tryPreemptFromNode(Resource maxResourceRequest,\n                                   Map<TempQueue, Resource> preemptedLimit){\n      clearCache();\n      curTime=clock.getTime();\n      Resource toPreempt=Resources.clone(maxResourceRequest);\n      Iterator<Map.Entry<TempQueue, List<RMContainer>>> it =\n          nonAMContainers.entrySet().iterator();\n      while(Resources.greaterThan(rc, clusterResource,\n          toPreempt, Resources.none())&& it.hasNext()){\n        Map.Entry<TempQueue, List<RMContainer>> entry = it.next();\n        TempQueue currentQueue = entry.getKey();\n        Resource limit = Resources.clone(preemptedLimit.get(currentQueue));\n        if(Resources.greaterThan(rc, clusterResource,\n            limit, Resources.none()) == false || currentQueue.preemptionDisabled){\n          continue;\n        }\n        List<RMContainer> containerList = entry.getValue();\n        long sortStart = clock.getTime();\n        Collections.sort(containerList, COMPARATOR_CONTAINER);\n        long sortEnd = clock.getTime();\n        if(sortEnd - sortStart >= 100){\n          LOG.warn(\" Slow Sort nonAM Containers when trying preempt maxResourceRequest {} \" +\n                  \"at queue {} with queueLimit {} total {} container, token {}ms\",\n              toPreempt,\n              currentQueue.getQueueName(),\n              limit,\n              containerList.size(),\n              sortEnd-sortStart);\n        }\n        Iterator<RMContainer> iit=containerList.iterator();\n        while(Resources.greaterThan(rc, clusterResource, limit, Resources.none()) &&\n            Resources.greaterThan(rc, clusterResource, toPreempt, Resources.none()) &&\n            iit.hasNext()){\n          RMContainer i=iit.next();\n          if(isLabelContainer(i)){\n            continue;\n          }\n          toBePreemptedContainers.get(currentQueue).add(i);\n          Resources.addTo(toBePreemptedResource.get(currentQueue),\n              i.getContainer().getResource());\n          Resources.subtractFrom(limit, i.getContainer().getResource());\n          Resources.subtractFrom(toPreempt, i.getContainer().getResource());\n          costTime += (curTime - i.getCreationTime());\n        }\n      }\n      // 暂不抢占 AM,代价不可控 难维护\n      if(Resources.greaterThan(rc, clusterResource,\n          toPreempt, Resources.none())){\n        costTime = Long.MAX_VALUE;\n      }\n      return costTime;\n    }\n    public void preemptFromNode(){\n      for(Map.Entry<TempQueue, List<RMContainer>> entry :\n          toBePreemptedContainers.entrySet()){\n        nonAMContainers.get(entry.getKey()).removeAll(entry.getValue());\n        amContainers.get(entry.getKey()).removeAll(entry.getValue());\n      }\n    }\n\t```\n10. 选取抢占代价最低的节点，记录相应 container 集合    \n　　获取在所有 NM 中，costTime 最低的 Container 集合作为待抢占容器      \n11. 若当前队列的待抢占量小于”大容器”定义，则剩余资源作为”普通容器抢占”   \n12. 在抢占代价最低的节点上，释放了总资源量不低于 maxResourceRequest 的 Container 集合  \n13. 为所有”普通容器”做抢占:在待抢占的队列中杀死最新启动的容器(不存在于 步骤10 中) \n\n\t```\nResource toPreempted;\nFor(Queue : overReservedQueue)\n\t\tFor(App: Queue.getApps)\n\t\t\tif(toPreempted <= none) break;\n\t\t\tkillReservedContainer(App， toPreempted); \n\t\t\tkillAllocatedContainerByOrder(App， toPreempted);\n\t\t\trememberAM();\nif(toPreempted > none)\n\t\tkillAMByOrder();\n\t```\t\t\n\n## 优点缺点\n\n### 优点\n\n1. 在集群负载高的时候，可以保证优先级高的业务有资源执行;低优先级队列不能抢占高优先级队列的任务.(开源实现 不能保证)  \n2. 可以在节点上启动大容器 (开源实现 不能保证)  \n3. 可以避免无效抢占 (开源实现 不能保证)  \n\n### 缺点\n1. 集群负载高时，低优先级长时任务可能被无限拉长  \n2. 在节点上选取最新启动的容器抢占，而不是在全局选择最新启动的容器抢占，导致任务恢复无效工作量增加  \n3. 大容器调度时，增加了节点的资源利用率波动  \n\n## 抢占调度(开源抢占详解)\n抢占调度是周期性的，每周期抢占调度执行的策略分三步:  \n\n1.\t获取所有队列的快照  \n2.\t计算所有队列的理想容量，作为抢占依据  \n3.\t从超分配队列中，计算最终抢占量，并选取部分容器释放容量  \n其中最重要的是第2步，这一步决定着:队列是否释放资源及释放多少资源  \n\n### 获取所有队列的快照\n\n获取信息:  \n　　队列用量 used; 队列堆积需求 pending; 队列最低配额 capacity ;队列最高配额额 maxCapacity; 队列是否允许抢占 preemptionDisabled; 子队列 children   \n初始化信息:  \n　　理想容量 idealAssigned; 超出配额容量 toBePreempted; 最终被抢占容量 actuallyPreempted; 权重normalizedGrarantee\n\n### 计算所有队列的理想容量，作为抢占依据\n　　设置 root 队列的理想容量为 100%，递归向下按层计算 每个子队列的理想容量，子队列的子队列理想容量，直至叶子队列。每轮递归通用逻辑是:依据当前队列的理想容量和所有子队列快照信息，计算每个子队列的理想容量。  \na.  划分子队列类型  \n　　对于递归过程中，当前队列的所有子队列按照”是否配置了最低配额”划分为两类:第一类，配置最低配额的子队列集合;第二类，未配置最低配额的子队列集合。区别在于:第一类要优先于第二类得到资源，满足了第一类子队列需求之后如果还有空闲资源才会分配给第二类子队列；第一类子队列在分享当前队列的配额时，之间是以最低配额为权重的，第二类由于没有配置最低配额，之间是平均分配的  \nb. 优先分配 配置了最低配额的子队列  \n　　b1. 设定初始理想配额  \n　　　　这一过程的主要逻辑是: 为所有子队列设定动态初始理想配额，并计算出当前队列的空闲配额，记录欠分配子队列。  \n　　　　初始配额设定:   \n　　　　　　若 used<=capacity，则idealAssigned=used。将(capacity-used) 作为空闲资源     \n　　　　　　若 used>capacity，则ideaAssigned=capacity。将(used-capacity) 作为空闲资源拿出来再分配     \n　　　　计算当前队列的空闲配额:  \n　　　　　　将 上述两种空闲资源之和作为队列全局空闲资源，交由欠分配子队列 再分配  \n　　　　记录欠分配子队列:  \n　　　　　　若子队列 used+pending > idealAssigned，即需求量大于容量 为欠分配子队列。  \n　　b2. 再分配过程  \n　　　　目的: 按照 b1 记录的信息，将全局空闲资源，按照分配给所有的欠分配子队列。    \n　　　　方式: 以循环的方式，每轮循环为最欠分配的队列(most under served queue)分配资源。最欠分配的队列在得到资源后，欠分配程度排名可能会变化。因此这种分配方式会使各子队列资源量比值最终趋近于”最低配额”比值。”资源再平衡”的说法来源于此。  \n　　　　权重: 每个子队列在欠分配子队列集合中获取资源的权重是 capacity/∑capacity，存储在队列快照中的 normalizedGrarantee字段  \n　　　　变量: 全局空闲资源，欠分配子队列，子队列减少导致的分配权重增大  \n　　　　增量: 循环为最欠分配的队列分配资源，每次分配的增量为min{全局空闲资源 * 权重，需求量，最高配额-当前理想配额}  \n　　　　　　全局空闲资源 * 权重: 表示在该轮循环 能得到的资源  \n　　　　　　需求量:used+pending-idealAssigned，表示这些资源可以满足队列的需求  \n　　　　　　最高配额-当前理想配额: 表示在任何情况下，队列配额都不能超过最高配额  \n　　　　增量最终会增加到理想配额idealAssigned。在全局空闲资源为 0 或者无任何欠分配队列时，idealAssigned 会作为队列最终的理想配额  \n\n核心源码如下:\n\n```\n/**\n * orderedByNeed : 所有欠分配队列\n * unassigned: 全局空闲资源\n * /\nwhile (!orderedByNeed.isEmpty()\n   && Resources.greaterThan(rc，tot_guarant， unassigned，Resources.none())) {\n  Resource wQassigned = Resource.newInstance(0， 0);\n  / *\n   * 设置orderedByNeed中所有队列的权重\n   *  配置最低配额: capacity/sum(capacity)\n   * 未配置最低配额: 1/orderedByNeed.size()\n   * /\n  resetCapacity(rc， unassigned， orderedByNeed， ignoreGuarantee);\n  / **\n   * 最欠分配的队列( most under served queue )\n   * 比较 idealAssigned/capacity 值\n   */\n  Collection<TempQueue> underserved =\n      getMostUnderservedQueues(orderedByNeed， tqComparator);\n  for (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\n    TempQueue sub = i.next();\n    //全局空闲资源*权重\n    Resource wQavail = Resources.multiplyAndNormalizeUp(rc，\n        unassigned， sub.normalizedGuarantee， Resource.newInstance(1， 1));\n    //三元组{全局空闲资源*权重，used+pending-idealAssigned，maxCapacity-idealAssigned}\n    Resource wQidle = sub.offer(wQavail， rc， tot_guarant);\n    //增量\n    Resource wQdone = Resources.subtract(wQavail， wQidle);\n    if (Resources.greaterThan(rc， tot_guarant，\n          wQdone， Resources.none())) {\n      //如果这个队列在这轮得到了资源，那么下一轮也可能得到资源，所以加入到原集合中;直到没有得到资源\n      orderedByNeed.add(sub);\n    }\n    Resources.addTo(wQassigned， wQdone);\n  }\n  //修改全局空闲资源\n  Resources.subtractFrom(unassigned， wQassigned);\n}\n```\nc. 最后分配 未配置最低配额的子队列  \n　　如果在满足了”配置最低配额的子队列“的所有需求之后，还有剩余配额，则平均分配到所有”未配置最低配额的子队列”中。    \n　　除权重外，和 b 完全一致。  \n　　　　权重:子队列集合中每个子队列都会平均分配全局空闲资源(区别于 按最低配额做权重)  \nd. 规整队列待抢占量\n　　汇总所有叶子队列的超出配额(used-idealAssigned)之和，如果此值超出“每轮总抢占“(默认集群总资源的 10%)，则按比例减少每个叶子队列的待抢占量 以保证每轮抢占的总资源量不超过”每轮总抢占”。\n\n### 从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\n\n1. 判断超分配队列  \n　　若 used>capacity*(1+maxIgnoredOverCapacity)，则判断为超分配队列，该队列需要释放资源。  \n　　其中 maxIgnoredOverCapacity 表示 忽略抢占阈值，默认 0.2。 参考“Tips-5”  \n2. 计算最终抢占量  \n　　最终抢占量 actuallyPreempted=toBePreempted * naturalTerminationFactor。  \n　　其中naturalTerminationFactor表示 自然终止因子，默认 0.2。toBePreempt=used-idealAssigned。参考”Tips-5”  \n3. 选取容器  \n　　选取总大小不小于actuallyPreempted的一些容器，释放掉  \n　　选取规则如下:   \n　　　　优先选择队列中后启动的 APP  \n　　　　优先选择 APP 的保留容器  \n　　　　优先选择 APP 后启动的容器  \n　　如果以上普通容器全部选取完之后无法还回等量actuallyPreempted的资源，则开始选取AM 角色的容器，优先选取后启动的 AM容器  \n\n### 释放\n　　对于选取的、将要被抢占的所有容器，将<容器，时间>加入到一个全局的集合中追踪，考虑到自然终止的情况，如果集合中的容器存活时间超过 5 * 15s，则直接杀死。  \n\n\n\n\n\n","slug":"2019-11-25-基于队列优先级的小集群大容器资源抢占方案","published":1,"updated":"2020-07-03T11:20:03.369Z","comments":1,"photos":[],"link":"","_id":"cke2rms4y0009y7c74qy2emgl","content":"<h2 id=\"抢占概述\"><a href=\"#抢占概述\" class=\"headerlink\" title=\"抢占概述\"></a>抢占概述</h2><p>　　Yarn 使用树状层级队列组织方式来管理资源，所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源<br>　　层级队列中除 root 外每个队列可以设置其拥有父队列的最低配额和最高配额。最高配额是队列无论在任何情况下都不会超出的资源量，最低配额一般理解为在队列保底资源量以快速响应请求，实际上是用来决定集群资源配比权重。为了提高集群整体的资源利用率，Yarn 引入了抢占机制:  </p>\n<ol>\n<li>在队列中无任务运行时，队列没有必要硬遵守保留最低配额，可以将资源借给负载较高的其他队列  </li>\n<li>当借出资源的队列接收到了新提交的应用时，将出借的资源收回以满足本队列的资源需求  </li>\n</ol>\n<a id=\"more\"></a>\n\n<p>　　其中第一点由常规调度器来实现: 队列的实际配额基于自身和其他队列的需求动态变化 介于 0与最高配额之间，当队列无任何 APP 时资源出借给繁忙的队列 实际资源量为 0，当队列负载较大时可能借用其他空闲队列的资源但不会超过其最高配额。第二点由抢占调度来实现: 出借资源的队列接收了新的 APP 之后，需要从其他超出最低配额的队列(超分配队列)中抢占(回)资源，用以启动APP。从超分配队列中抢占资源时，基于“最小化对已启动应用程序的影响”原则，选择杀死 Container 释放资源时 优先杀死最新启动 App 的最新分配的 Container，优先杀死保留的 Container，优先杀死非 AM 角色的 Container  </p>\n<h2 id=\"现存问题\"><a href=\"#现存问题\" class=\"headerlink\" title=\"现存问题\"></a>现存问题</h2><p>场景: 假设集群内有两个队列:A 和 B<br>　　状态 1: A完全空闲，B 负载很高，借用了 A 的全部资源<br>　　状态 2: A新提交了 AppX，且 AppX 申请的每个 Container 都很大，e.g. 60G<br>　　状态 3: 抢占调度从B 队列杀死了一些 Container，释放了一些资源(默认单次抢占资源总量不超过集群总资源的10%)，但这些资源分布在n个节点上<br>　　状态 4: 当这 n 个节点向 RM 汇报心跳时，常规调度尝试对每个队列(有序，负载低的优先，A先于 B)的每个应用程序(有序，早提交的优先)进行分配资源，如果这n 个节点都不能启动“大 Container”，那么当 A 没有得到资源时对 B 尝试分配， B 内的 App是可以得到资源的(杀死的就是 B 内 App 的 Container)<br>　　状态 5: A没有得到资源，依旧处于欠分配状态;B 得到了资源，处于超分配状态。抢占调度会再次从 B 抢占资源，杀死一些 Container。即”状态 3”  </p>\n<p>造成的影响有四：    </p>\n<ol>\n<li>在抢占调度层面，一直在状态 3-&gt;4-&gt;5-&gt;3…循环执行，处于“A 欠分配，B 超分配“-&gt;”抢占调度释放 B 的资源”-&gt;”A 不能使用资源，B 能使用则分配给 B”-&gt;”A 欠分配，B 超分配”循环  </li>\n<li>在资源队列层面，A 队列一直没有得到资源，且一直触发抢占; B 队列在抢占调度释放资源，在常规调度得到 A 队列无法利用的资源  </li>\n<li>在应用程序层面，对于 A中的大 Container App，一直没有得到资源，处于等待状态;对于 B 中的 App，其部分 Container 一直处于”被抢占”-&gt;”可启动”-&gt;”启动(成为最新容器)”-&gt;”被抢占”-&gt;”可启动”…的循环状态  </li>\n<li>从 SLA 和资源利用率角度来说，A 中的 App 没有迅速响应，B 中的 App 被拖慢了速度。有一部分资源被反复无效调度  </li>\n</ol>\n<blockquote>\n</blockquote>\n<p>Tips:  </p>\n<ol>\n<li>抢占调度是周期性执行的，默认 3 秒。  </li>\n<li>抢占调度和常规调度是解耦的，常规调度负责为每个队列的每个 APP的每个 ResourceRequest 调度，抢占调度负责所有队列资源的平衡，不为某个具体的 App、具体的 Container 做调度    </li>\n<li>抢占调度是”资源再平衡”的过程，空闲队列有了资源需求后，所有队列计算自身的理想容量，超分配的队列释放资源，欠分配的队列在常规调度模块优先分配资源  </li>\n<li>集群内队列是平等的， A 队列可以抢 B， B 队列也可以抢 A。这对保证核心业务来说是不友好的，非核心业务可能会抢占核心业务的资源，拖慢核心业务  </li>\n<li>“最小化对已启动应用程序的影响”是抢占调度的核心原则，具体表现在:<br>　　a. 每轮总抢占:默认 10%，单次抢占总资源量最多不能超过集群总资源量的 10%<br>　　b. 忽略抢占阈值(deadzone,死区): 默认 0.2，超分配队列只有在 used&gt;(1+0.2) * capacity 时，才会还回资源。e.g. capacity=100G， used=110G时则不在该队列抢占<br>　　c. 自然终止因子: 默认 20%，即使容器不被杀死，也有 95%概率在5 * 15s 内终止。所以每个超分配队列最多还回超分配部分的 20%。e.g. used=100G，ideal=40G，本轮抢占该队列最终归还量为(100-40) * 20%=12G\n　　d. 确定了最终归还量之后，则从队列所有 APP 中选择一些容器杀死，杀死容器的总容量大于等于最终归还量。选择容器时的规则是:优先选择最新启动的 APP最近启动的容器，优先选择保留的容器(未启动)，优先选择非 AM 角色的容器<br>　　故自然终止因子决定了该轮抢占立即能得到的资源量，每轮总抢占决定了该轮抢占最多能得到的资源量  </li>\n<li>保留:常规调度为什么不在 n 个节点上做保留?<br>在节点上为容器做分配/保留的限制条件有:<br>　　a. 队列具有该节点的 access 权限，节点不在 APP 设置的黑名单内<br>　　b. 节点上无保留且节点剩余空间大于最小容器大小(yarn.scheduler.minimum-allocation-mb)<br>　　c. 最高配额限制: 如果分配该容器，叶子队列及其所有父队列均不能超过各自的最高配额<br>　　d. 用户配额限制: 用户配额上限(headroom) 取值为 min{当前容量 * user_limit_factor，max{当前容量/活跃用户数，当前容量/最高用户数}}，当前容量取决于最低配额、当前尝试的ResourceRequest 、 Used 关系。一个用户在任何情况下，使用的配额不能高于队列最低配额*user_limit_factor<br>　　e. AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)<br>主要限制在 c)和 d)。d)中 user_limit_factor 一般设置为 1，活跃用户数低于最高用户限制，则取值一般为当前容量/活跃用户数，当队列活跃用户越多，用户配额上限会越低  </li>\n</ol>\n<h2 id=\"改进方案\"><a href=\"#改进方案\" class=\"headerlink\" title=\"改进方案\"></a>改进方案</h2><ol>\n<li>建立业务优先级体系，为每个队列赋予业务优先级属性，优先级高的队列可以抢占低的队列，反之则不可。用以保证核心业务不被非核心业务抢占资源    </li>\n<li>在计算每个队列应得的资源时，若队列预计得到的资源不足以满足队列中最小的 ResourceRequest，则抢占是无意义的，不为该队列做抢占，其预计得到的资源将被设置为 0，这些无法利用的资源将转给其他队列<br>按照对”大容器”的定义(e.g. 40G)，优先为”大容器”队列做抢占，然后为“普通容器“队列做抢占  </li>\n<li>为”大容器”抢占时，在每个节点上选取待抢占队列的最新启动的 Application 的最新分配的部分容器释放且计算释放代价，最终选取释放代价最低的节点上的容器释放。若队列分配完”大容器” 或者待抢占量不高于”大容器” 定义，则后续将其作为”普通容器” 队列来处理  </li>\n<li>为”普通容器”抢占时，选取待抢占队列中最新启动的 App 的最新 Container 开始释放    </li>\n</ol>\n<h2 id=\"抢占流程\"><a href=\"#抢占流程\" class=\"headerlink\" title=\"抢占流程\"></a>抢占流程</h2><p><img src=\"/img/pictures/yarn_preempt/preemption_scheduler.png\" alt=\"\"></p>\n<ol>\n<li><p>获取各队列使用状态<br>　　包括: 队列的业务优先级，最低配额(capacity)，最高配额，当前使用量(used)，当前堆积需求量(pending)，最低需求量，最高需求量，初始理想容量(min{used,capacity})  </p>\n</li>\n<li><p>按优先级计算每个队列的理想容量<br>　　首先将所有队列划分为空闲队列(used&lt;=capacity)、超分配队列(used&gt;capacity)两类，并统计出欠分配队列(used+pending&gt;当前理想容量)。在保证弹性最低配额的条件下，将空闲队列的(capacity-used) 和超分配队列(used-capacity) 作为全局空闲量，按规则分配给欠分配队列:<br>　　a. 优先全局分配空闲资源给优先级高的欠分配队列，按优先级由高至低依次分配<br>　　b. 分配的资源量取决于 min{全局空闲量，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　　即保证在不超过队列最高配额的情况下，从全局空闲量中优先满足高优先级队列的需求<br>　　c. 然后分配全局空闲资源给无优先级但配置有最低配额的欠分配队列，按照欠分配程度(当前理想配额/最低配额 值)由低到高依次分配<br>　　d. 分配的资源量取决于 min{全局空闲量 * 队列最低配额权重，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　e. 最后分配全局空闲资源给无优先级无最配配额的欠分配队列，按照欠分配程度由低到高依次分配<br>　　f. 分配的资源量取决于 min{全局空闲量 * 队列平均权重，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　g. 在全局空闲量为 0 或者没有欠分配队列时终止此环节<br>即: 抢占是一个再平衡的过程，资源从低优先级队列流向高优先级欠分配队列，从未配置最低配额的队列流向配置最低配额的队列，但是会保证有任务但被抢占资源队列的弹性最低配额(min{used+pending, capacity})  </p>\n</li>\n<li><p>判断各队列是否是有效抢占<br>　　判断各队列的 increment 值，若 increment &lt; minResourceRequest，即新得到的资源不足以启动任何一个容器，在将其从欠分配队列中删除  </p>\n</li>\n<li><p>设置其 pending 为 0<br>　　依据 used + pending = 当前容量， pending 设为 0 之后，不再是欠分配队列。  </p>\n</li>\n<li><p>再计算<br>　　重新计算抢占依据 (e.g. 高优先级队列抢占的资源无法启动队列中最小的一个容器时，将此资源按优先级再分给其他队列)     </p>\n<p> 此处的表述有歧义，但是画图过于冗余，故而放到了一个循环中，其实是三个循环。第一个循环: 优先分配有优先级配置的队列，并判断其是否是有效抢占，若是无效抢占，则把其 increment 还回到全局空闲配额中，供次级流程(后续的低优先级队列，无优先级队列)使用。第二个循环是: 分配无优先级但有最低配额的队列，判断其是否是有效抢占，若是无效抢占，则把 increment 还回到全局空闲配额中，供后续流程使用(后续无优先级有最低配额的欠分配队列，无优先级无最低配额的欠分配队列)。第三个循环是: 无优先级无最低配额的欠分配队列。第一个循环在队列分配完成后立即判断是否是有效抢占，第二三个循环需要在所属循环分配完成后按累计 increment 由高至低分别判断，若有无效抢占立即重算除其之外的所有的队列的理想配额。      </p>\n</li>\n<li><p>计算出所有队列的待抢占量，并依据最小需求将所有欠分配队列划分为”大容器”和”普通容器”，分别进行抢占调度  </p>\n</li>\n<li><p>对于每个”大容器”队列，循环(8-&gt;11)抢占，每次抢占量为 maxResourceRequest(足以启动队列中任何一个容器)。直到没有 NM 满足，或者待抢占量低于”大容器”定义    </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">伪代码</span><br><span class=\"line\">Map&lt;NodeId，QueueContainerDetail&gt; info;</span><br><span class=\"line\">For(App: toPreemptedQueue)</span><br><span class=\"line\">\taddContainer&#123;runningContainer，reservedContainer&#125; -&gt; info(amContainers,nonAMContainers);</span><br><span class=\"line\">Map&lt;TempQueue,Resource&gt; limit &#x3D; listQueuePreemptionLimit(); </span><br><span class=\"line\">For( curQueue: allUnderReservedQueue) </span><br><span class=\"line\">\tmaxRR&#x3D;curQueue.getMaxResourceRequest </span><br><span class=\"line\">\ttotalPreeption&#x3D;curQueue.getTotalPreemption</span><br><span class=\"line\">\twhile(totalPreeption&gt;maxRR )</span><br><span class=\"line\">\t\tminCostTime&#x3D;Long.MaxValue\t</span><br><span class=\"line\">\t\tMap&lt;Queue，Container&gt; containers，NodeId &#x2F;&#x2F;在该节点上杀死这些容器为最小代价</span><br><span class=\"line\">\t\tfor(nodeId: NodeId)</span><br><span class=\"line\">\t\t\tcostTime &#x3D; tryPreemptFromNode(node,limit,maxRR)</span><br><span class=\"line\">\t\t\tif(costTime&lt;minCostTime)</span><br><span class=\"line\">\t\t\t\tminCostTime&#x3D;costTime; containers,nodeId REFRESH-&gt;POINTER</span><br><span class=\"line\">\t\tif(costTime &#x3D;&#x3D;Long.MaxValue)</span><br><span class=\"line\">\t\t\tLOG.warn(&quot;no node meet the request in this queue, do left totalPreemption as common preemption&quot;)</span><br><span class=\"line\">\t        else</span><br><span class=\"line\">\t\t\ttotalPreemption -&#x3D; maxRR</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>统计每个待抢占队列的(App，Container)-&gt;(nm) 信息  </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void addContainer(TempQueue queue, RMContainer container)&#123;</span><br><span class=\"line\">      ensureQueue(queue);</span><br><span class=\"line\">      if(container.isAMContainer())&#123;</span><br><span class=\"line\">        amContainers.get(queue).add(container);</span><br><span class=\"line\">      &#125;else &#123;</span><br><span class=\"line\">        nonAMContainers.get(queue).add(container);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>尝试在每个 NM 上抢占，并计算抢占代价<br>　　尝试使用nonAMContainers 来满足 maxResourceRequest；并维护相关信息，计算抢占代价 costTime=∑（curTime-contianer.getStartTime）<br>限制条件是:<br>　　a. 每个队列不超出自身待释放的资源<br>　　b. 在该节点上所有超分配队列释放的资源和节点空闲资源 可以满足 maxRR<br>QueueAppInfo 的核心数据结构和逻辑:<br>[QueueContainerDetail.java]  </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NodeId nodeId;Clock clock;Long curTime;Long costTime;</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; nonAMContainers;</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; amContainers;</span><br><span class=\"line\">&#x2F;&#x2F; 各队列 最终被抢占的 container 列表</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; toBePreemptedContainers;</span><br><span class=\"line\">&#x2F;&#x2F; 各队列 最终被抢占的资源量</span><br><span class=\"line\">Map&lt;TempQueue, Resource&gt; toBePreemptedResource;</span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">     * note: since to &quot;try preempt&quot;,</span><br><span class=\"line\">     * the maxResourceRequest and preemptedLimitOfQueue are copied.</span><br><span class=\"line\">     * @param maxResourceRequest  待抢占量 e.g. 40G</span><br><span class=\"line\">     * @param preemptedLimit queueLimit A-&gt;30G, B-&gt;20G 抢占总量限制</span><br><span class=\"line\">     *&#x2F;</span><br><span class=\"line\">public long tryPreemptFromNode(Resource maxResourceRequest,</span><br><span class=\"line\">                                   Map&lt;TempQueue, Resource&gt; preemptedLimit)&#123;</span><br><span class=\"line\">      clearCache();</span><br><span class=\"line\">      curTime&#x3D;clock.getTime();</span><br><span class=\"line\">      Resource toPreempt&#x3D;Resources.clone(maxResourceRequest);</span><br><span class=\"line\">      Iterator&lt;Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt;&gt; it &#x3D;</span><br><span class=\"line\">          nonAMContainers.entrySet().iterator();</span><br><span class=\"line\">      while(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">          toPreempt, Resources.none())&amp;&amp; it.hasNext())&#123;</span><br><span class=\"line\">        Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt; entry &#x3D; it.next();</span><br><span class=\"line\">        TempQueue currentQueue &#x3D; entry.getKey();</span><br><span class=\"line\">        Resource limit &#x3D; Resources.clone(preemptedLimit.get(currentQueue));</span><br><span class=\"line\">        if(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">            limit, Resources.none()) &#x3D;&#x3D; false || currentQueue.preemptionDisabled)&#123;</span><br><span class=\"line\">          continue;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        List&lt;RMContainer&gt; containerList &#x3D; entry.getValue();</span><br><span class=\"line\">        long sortStart &#x3D; clock.getTime();</span><br><span class=\"line\">        Collections.sort(containerList, COMPARATOR_CONTAINER);</span><br><span class=\"line\">        long sortEnd &#x3D; clock.getTime();</span><br><span class=\"line\">        if(sortEnd - sortStart &gt;&#x3D; 100)&#123;</span><br><span class=\"line\">          LOG.warn(&quot; Slow Sort nonAM Containers when trying preempt maxResourceRequest &#123;&#125; &quot; +</span><br><span class=\"line\">                  &quot;at queue &#123;&#125; with queueLimit &#123;&#125; total &#123;&#125; container, token &#123;&#125;ms&quot;,</span><br><span class=\"line\">              toPreempt,</span><br><span class=\"line\">              currentQueue.getQueueName(),</span><br><span class=\"line\">              limit,</span><br><span class=\"line\">              containerList.size(),</span><br><span class=\"line\">              sortEnd-sortStart);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Iterator&lt;RMContainer&gt; iit&#x3D;containerList.iterator();</span><br><span class=\"line\">        while(Resources.greaterThan(rc, clusterResource, limit, Resources.none()) &amp;&amp;</span><br><span class=\"line\">            Resources.greaterThan(rc, clusterResource, toPreempt, Resources.none()) &amp;&amp;</span><br><span class=\"line\">            iit.hasNext())&#123;</span><br><span class=\"line\">          RMContainer i&#x3D;iit.next();</span><br><span class=\"line\">          if(isLabelContainer(i))&#123;</span><br><span class=\"line\">            continue;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          toBePreemptedContainers.get(currentQueue).add(i);</span><br><span class=\"line\">          Resources.addTo(toBePreemptedResource.get(currentQueue),</span><br><span class=\"line\">              i.getContainer().getResource());</span><br><span class=\"line\">          Resources.subtractFrom(limit, i.getContainer().getResource());</span><br><span class=\"line\">          Resources.subtractFrom(toPreempt, i.getContainer().getResource());</span><br><span class=\"line\">          costTime +&#x3D; (curTime - i.getCreationTime());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 暂不抢占 AM,代价不可控 难维护</span><br><span class=\"line\">      if(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">          toPreempt, Resources.none()))&#123;</span><br><span class=\"line\">        costTime &#x3D; Long.MAX_VALUE;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return costTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public void preemptFromNode()&#123;</span><br><span class=\"line\">      for(Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt; entry :</span><br><span class=\"line\">          toBePreemptedContainers.entrySet())&#123;</span><br><span class=\"line\">        nonAMContainers.get(entry.getKey()).removeAll(entry.getValue());</span><br><span class=\"line\">        amContainers.get(entry.getKey()).removeAll(entry.getValue());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>选取抢占代价最低的节点，记录相应 container 集合<br>　　获取在所有 NM 中，costTime 最低的 Container 集合作为待抢占容器      </p>\n</li>\n<li><p>若当前队列的待抢占量小于”大容器”定义，则剩余资源作为”普通容器抢占”   </p>\n</li>\n<li><p>在抢占代价最低的节点上，释放了总资源量不低于 maxResourceRequest 的 Container 集合  </p>\n</li>\n<li><p>为所有”普通容器”做抢占:在待抢占的队列中杀死最新启动的容器(不存在于 步骤10 中) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Resource toPreempted;</span><br><span class=\"line\">For(Queue : overReservedQueue)</span><br><span class=\"line\">\t\tFor(App: Queue.getApps)</span><br><span class=\"line\">\t\t\tif(toPreempted &lt;&#x3D; none) break;</span><br><span class=\"line\">\t\t\tkillReservedContainer(App， toPreempted); </span><br><span class=\"line\">\t\t\tkillAllocatedContainerByOrder(App， toPreempted);</span><br><span class=\"line\">\t\t\trememberAM();</span><br><span class=\"line\">if(toPreempted &gt; none)</span><br><span class=\"line\">\t\tkillAMByOrder();</span><br><span class=\"line\">\t&#96;&#96;&#96;\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">## 优点缺点</span><br><span class=\"line\"></span><br><span class=\"line\">### 优点</span><br><span class=\"line\"></span><br><span class=\"line\">1. 在集群负载高的时候，可以保证优先级高的业务有资源执行;低优先级队列不能抢占高优先级队列的任务.(开源实现 不能保证)  </span><br><span class=\"line\">2. 可以在节点上启动大容器 (开源实现 不能保证)  </span><br><span class=\"line\">3. 可以避免无效抢占 (开源实现 不能保证)  </span><br><span class=\"line\"></span><br><span class=\"line\">### 缺点</span><br><span class=\"line\">1. 集群负载高时，低优先级长时任务可能被无限拉长  </span><br><span class=\"line\">2. 在节点上选取最新启动的容器抢占，而不是在全局选择最新启动的容器抢占，导致任务恢复无效工作量增加  </span><br><span class=\"line\">3. 大容器调度时，增加了节点的资源利用率波动  </span><br><span class=\"line\"></span><br><span class=\"line\">## 抢占调度(开源抢占详解)</span><br><span class=\"line\">抢占调度是周期性的，每周期抢占调度执行的策略分三步:  </span><br><span class=\"line\"></span><br><span class=\"line\">1.\t获取所有队列的快照  </span><br><span class=\"line\">2.\t计算所有队列的理想容量，作为抢占依据  </span><br><span class=\"line\">3.\t从超分配队列中，计算最终抢占量，并选取部分容器释放容量  </span><br><span class=\"line\">其中最重要的是第2步，这一步决定着:队列是否释放资源及释放多少资源  </span><br><span class=\"line\"></span><br><span class=\"line\">### 获取所有队列的快照</span><br><span class=\"line\"></span><br><span class=\"line\">获取信息:  </span><br><span class=\"line\">　　队列用量 used; 队列堆积需求 pending; 队列最低配额 capacity ;队列最高配额额 maxCapacity; 队列是否允许抢占 preemptionDisabled; 子队列 children   </span><br><span class=\"line\">初始化信息:  </span><br><span class=\"line\">　　理想容量 idealAssigned; 超出配额容量 toBePreempted; 最终被抢占容量 actuallyPreempted; 权重normalizedGrarantee</span><br><span class=\"line\"></span><br><span class=\"line\">### 计算所有队列的理想容量，作为抢占依据</span><br><span class=\"line\">　　设置 root 队列的理想容量为 100%，递归向下按层计算 每个子队列的理想容量，子队列的子队列理想容量，直至叶子队列。每轮递归通用逻辑是:依据当前队列的理想容量和所有子队列快照信息，计算每个子队列的理想容量。  </span><br><span class=\"line\">a.  划分子队列类型  </span><br><span class=\"line\">　　对于递归过程中，当前队列的所有子队列按照”是否配置了最低配额”划分为两类:第一类，配置最低配额的子队列集合;第二类，未配置最低配额的子队列集合。区别在于:第一类要优先于第二类得到资源，满足了第一类子队列需求之后如果还有空闲资源才会分配给第二类子队列；第一类子队列在分享当前队列的配额时，之间是以最低配额为权重的，第二类由于没有配置最低配额，之间是平均分配的  </span><br><span class=\"line\">b. 优先分配 配置了最低配额的子队列  </span><br><span class=\"line\">　　b1. 设定初始理想配额  </span><br><span class=\"line\">　　　　这一过程的主要逻辑是: 为所有子队列设定动态初始理想配额，并计算出当前队列的空闲配额，记录欠分配子队列。  </span><br><span class=\"line\">　　　　初始配额设定:   </span><br><span class=\"line\">　　　　　　若 used&lt;&#x3D;capacity，则idealAssigned&#x3D;used。将(capacity-used) 作为空闲资源     </span><br><span class=\"line\">　　　　　　若 used&gt;capacity，则ideaAssigned&#x3D;capacity。将(used-capacity) 作为空闲资源拿出来再分配     </span><br><span class=\"line\">　　　　计算当前队列的空闲配额:  </span><br><span class=\"line\">　　　　　　将 上述两种空闲资源之和作为队列全局空闲资源，交由欠分配子队列 再分配  </span><br><span class=\"line\">　　　　记录欠分配子队列:  </span><br><span class=\"line\">　　　　　　若子队列 used+pending &gt; idealAssigned，即需求量大于容量 为欠分配子队列。  </span><br><span class=\"line\">　　b2. 再分配过程  </span><br><span class=\"line\">　　　　目的: 按照 b1 记录的信息，将全局空闲资源，按照分配给所有的欠分配子队列。    </span><br><span class=\"line\">　　　　方式: 以循环的方式，每轮循环为最欠分配的队列(most under served queue)分配资源。最欠分配的队列在得到资源后，欠分配程度排名可能会变化。因此这种分配方式会使各子队列资源量比值最终趋近于”最低配额”比值。”资源再平衡”的说法来源于此。  </span><br><span class=\"line\">　　　　权重: 每个子队列在欠分配子队列集合中获取资源的权重是 capacity&#x2F;∑capacity，存储在队列快照中的 normalizedGrarantee字段  </span><br><span class=\"line\">　　　　变量: 全局空闲资源，欠分配子队列，子队列减少导致的分配权重增大  </span><br><span class=\"line\">　　　　增量: 循环为最欠分配的队列分配资源，每次分配的增量为min&#123;全局空闲资源 * 权重，需求量，最高配额-当前理想配额&#125;  </span><br><span class=\"line\">　　　　　　全局空闲资源 * 权重: 表示在该轮循环 能得到的资源  </span><br><span class=\"line\">　　　　　　需求量:used+pending-idealAssigned，表示这些资源可以满足队列的需求  </span><br><span class=\"line\">　　　　　　最高配额-当前理想配额: 表示在任何情况下，队列配额都不能超过最高配额  </span><br><span class=\"line\">　　　　增量最终会增加到理想配额idealAssigned。在全局空闲资源为 0 或者无任何欠分配队列时，idealAssigned 会作为队列最终的理想配额  </span><br><span class=\"line\"></span><br><span class=\"line\">核心源码如下:</span><br></pre></td></tr></table></figure>\n<p>/**</p>\n<ul>\n<li>orderedByNeed : 所有欠分配队列</li>\n<li>unassigned: 全局空闲资源</li>\n<li>/\nwhile (!orderedByNeed.isEmpty()\n&amp;&amp; Resources.greaterThan(rc，tot_guarant， unassigned，Resources.none())) {\nResource wQassigned = Resource.newInstance(0， 0);\n/ *</li>\n<li>设置orderedByNeed中所有队列的权重</li>\n<li>配置最低配额: capacity/sum(capacity)</li>\n<li>未配置最低配额: 1/orderedByNeed.size()</li>\n<li>/\nresetCapacity(rc， unassigned， orderedByNeed， ignoreGuarantee);\n/ **</li>\n<li>最欠分配的队列( most under served queue )</li>\n<li>比较 idealAssigned/capacity 值</li>\n<li>/\nCollection<TempQueue> underserved =\n getMostUnderservedQueues(orderedByNeed， tqComparator);\nfor (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\nTempQueue sub = i.next();\n//全局空闲资源<em>权重\nResource wQavail = Resources.multiplyAndNormalizeUp(rc，\n   unassigned， sub.normalizedGuarantee， Resource.newInstance(1， 1));\n//三元组{全局空闲资源</em>权重，used+pending-idealAssigned，maxCapacity-idealAssigned}\nResource wQidle = sub.offer(wQavail， rc， tot_guarant);\n//增量\nResource wQdone = Resources.subtract(wQavail， wQidle);\nif (Resources.greaterThan(rc， tot_guarant，<pre><code>wQdone， Resources.none())) {</code></pre> //如果这个队列在这轮得到了资源，那么下一轮也可能得到资源，所以加入到原集合中;直到没有得到资源\n orderedByNeed.add(sub);\n}\nResources.addTo(wQassigned， wQdone);\n}\n//修改全局空闲资源\nResources.subtractFrom(unassigned， wQassigned);\n}<pre><code>c. 最后分配 未配置最低配额的子队列  \n　　如果在满足了”配置最低配额的子队列“的所有需求之后，还有剩余配额，则平均分配到所有”未配置最低配额的子队列”中。    \n　　除权重外，和 b 完全一致。  \n　　　　权重:子队列集合中每个子队列都会平均分配全局空闲资源(区别于 按最低配额做权重)  \nd. 规整队列待抢占量\n　　汇总所有叶子队列的超出配额(used-idealAssigned)之和，如果此值超出“每轮总抢占“(默认集群总资源的 10%)，则按比例减少每个叶子队列的待抢占量 以保证每轮抢占的总资源量不超过”每轮总抢占”。\n</code></pre></li>\n</ul>\n</li>\n</ol>\n<h3 id=\"从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\"><a href=\"#从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\" class=\"headerlink\" title=\"从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\"></a>从超分配队列中，计算最终待抢占量，并选取部分容器释放容量</h3><ol>\n<li>判断超分配队列<br>　　若 used&gt;capacity*(1+maxIgnoredOverCapacity)，则判断为超分配队列，该队列需要释放资源。<br>　　其中 maxIgnoredOverCapacity 表示 忽略抢占阈值，默认 0.2。 参考“Tips-5”  </li>\n<li>计算最终抢占量<br>　　最终抢占量 actuallyPreempted=toBePreempted * naturalTerminationFactor。<br>　　其中naturalTerminationFactor表示 自然终止因子，默认 0.2。toBePreempt=used-idealAssigned。参考”Tips-5”  </li>\n<li>选取容器<br>　　选取总大小不小于actuallyPreempted的一些容器，释放掉<br>　　选取规则如下:<br>　　　　优先选择队列中后启动的 APP<br>　　　　优先选择 APP 的保留容器<br>　　　　优先选择 APP 后启动的容器<br>　　如果以上普通容器全部选取完之后无法还回等量actuallyPreempted的资源，则开始选取AM 角色的容器，优先选取后启动的 AM容器  </li>\n</ol>\n<h3 id=\"释放\"><a href=\"#释放\" class=\"headerlink\" title=\"释放\"></a>释放</h3><p>　　对于选取的、将要被抢占的所有容器，将&lt;容器，时间&gt;加入到一个全局的集合中追踪，考虑到自然终止的情况，如果集合中的容器存活时间超过 5 * 15s，则直接杀死。  </p>\n","site":{"data":{}},"excerpt":"<h2 id=\"抢占概述\"><a href=\"#抢占概述\" class=\"headerlink\" title=\"抢占概述\"></a>抢占概述</h2><p>　　Yarn 使用树状层级队列组织方式来管理资源，所有 NodeManager 持有的资源聚集作为资源根队列 root 来代表集群中所有可用资源<br>　　层级队列中除 root 外每个队列可以设置其拥有父队列的最低配额和最高配额。最高配额是队列无论在任何情况下都不会超出的资源量，最低配额一般理解为在队列保底资源量以快速响应请求，实际上是用来决定集群资源配比权重。为了提高集群整体的资源利用率，Yarn 引入了抢占机制:  </p>\n<ol>\n<li>在队列中无任务运行时，队列没有必要硬遵守保留最低配额，可以将资源借给负载较高的其他队列  </li>\n<li>当借出资源的队列接收到了新提交的应用时，将出借的资源收回以满足本队列的资源需求  </li>\n</ol>","more":"<p>　　其中第一点由常规调度器来实现: 队列的实际配额基于自身和其他队列的需求动态变化 介于 0与最高配额之间，当队列无任何 APP 时资源出借给繁忙的队列 实际资源量为 0，当队列负载较大时可能借用其他空闲队列的资源但不会超过其最高配额。第二点由抢占调度来实现: 出借资源的队列接收了新的 APP 之后，需要从其他超出最低配额的队列(超分配队列)中抢占(回)资源，用以启动APP。从超分配队列中抢占资源时，基于“最小化对已启动应用程序的影响”原则，选择杀死 Container 释放资源时 优先杀死最新启动 App 的最新分配的 Container，优先杀死保留的 Container，优先杀死非 AM 角色的 Container  </p>\n<h2 id=\"现存问题\"><a href=\"#现存问题\" class=\"headerlink\" title=\"现存问题\"></a>现存问题</h2><p>场景: 假设集群内有两个队列:A 和 B<br>　　状态 1: A完全空闲，B 负载很高，借用了 A 的全部资源<br>　　状态 2: A新提交了 AppX，且 AppX 申请的每个 Container 都很大，e.g. 60G<br>　　状态 3: 抢占调度从B 队列杀死了一些 Container，释放了一些资源(默认单次抢占资源总量不超过集群总资源的10%)，但这些资源分布在n个节点上<br>　　状态 4: 当这 n 个节点向 RM 汇报心跳时，常规调度尝试对每个队列(有序，负载低的优先，A先于 B)的每个应用程序(有序，早提交的优先)进行分配资源，如果这n 个节点都不能启动“大 Container”，那么当 A 没有得到资源时对 B 尝试分配， B 内的 App是可以得到资源的(杀死的就是 B 内 App 的 Container)<br>　　状态 5: A没有得到资源，依旧处于欠分配状态;B 得到了资源，处于超分配状态。抢占调度会再次从 B 抢占资源，杀死一些 Container。即”状态 3”  </p>\n<p>造成的影响有四：    </p>\n<ol>\n<li>在抢占调度层面，一直在状态 3-&gt;4-&gt;5-&gt;3…循环执行，处于“A 欠分配，B 超分配“-&gt;”抢占调度释放 B 的资源”-&gt;”A 不能使用资源，B 能使用则分配给 B”-&gt;”A 欠分配，B 超分配”循环  </li>\n<li>在资源队列层面，A 队列一直没有得到资源，且一直触发抢占; B 队列在抢占调度释放资源，在常规调度得到 A 队列无法利用的资源  </li>\n<li>在应用程序层面，对于 A中的大 Container App，一直没有得到资源，处于等待状态;对于 B 中的 App，其部分 Container 一直处于”被抢占”-&gt;”可启动”-&gt;”启动(成为最新容器)”-&gt;”被抢占”-&gt;”可启动”…的循环状态  </li>\n<li>从 SLA 和资源利用率角度来说，A 中的 App 没有迅速响应，B 中的 App 被拖慢了速度。有一部分资源被反复无效调度  </li>\n</ol>\n<blockquote>\n</blockquote>\n<p>Tips:  </p>\n<ol>\n<li>抢占调度是周期性执行的，默认 3 秒。  </li>\n<li>抢占调度和常规调度是解耦的，常规调度负责为每个队列的每个 APP的每个 ResourceRequest 调度，抢占调度负责所有队列资源的平衡，不为某个具体的 App、具体的 Container 做调度    </li>\n<li>抢占调度是”资源再平衡”的过程，空闲队列有了资源需求后，所有队列计算自身的理想容量，超分配的队列释放资源，欠分配的队列在常规调度模块优先分配资源  </li>\n<li>集群内队列是平等的， A 队列可以抢 B， B 队列也可以抢 A。这对保证核心业务来说是不友好的，非核心业务可能会抢占核心业务的资源，拖慢核心业务  </li>\n<li>“最小化对已启动应用程序的影响”是抢占调度的核心原则，具体表现在:<br>　　a. 每轮总抢占:默认 10%，单次抢占总资源量最多不能超过集群总资源量的 10%<br>　　b. 忽略抢占阈值(deadzone,死区): 默认 0.2，超分配队列只有在 used&gt;(1+0.2) * capacity 时，才会还回资源。e.g. capacity=100G， used=110G时则不在该队列抢占<br>　　c. 自然终止因子: 默认 20%，即使容器不被杀死，也有 95%概率在5 * 15s 内终止。所以每个超分配队列最多还回超分配部分的 20%。e.g. used=100G，ideal=40G，本轮抢占该队列最终归还量为(100-40) * 20%=12G\n　　d. 确定了最终归还量之后，则从队列所有 APP 中选择一些容器杀死，杀死容器的总容量大于等于最终归还量。选择容器时的规则是:优先选择最新启动的 APP最近启动的容器，优先选择保留的容器(未启动)，优先选择非 AM 角色的容器<br>　　故自然终止因子决定了该轮抢占立即能得到的资源量，每轮总抢占决定了该轮抢占最多能得到的资源量  </li>\n<li>保留:常规调度为什么不在 n 个节点上做保留?<br>在节点上为容器做分配/保留的限制条件有:<br>　　a. 队列具有该节点的 access 权限，节点不在 APP 设置的黑名单内<br>　　b. 节点上无保留且节点剩余空间大于最小容器大小(yarn.scheduler.minimum-allocation-mb)<br>　　c. 最高配额限制: 如果分配该容器，叶子队列及其所有父队列均不能超过各自的最高配额<br>　　d. 用户配额限制: 用户配额上限(headroom) 取值为 min{当前容量 * user_limit_factor，max{当前容量/活跃用户数，当前容量/最高用户数}}，当前容量取决于最低配额、当前尝试的ResourceRequest 、 Used 关系。一个用户在任何情况下，使用的配额不能高于队列最低配额*user_limit_factor<br>　　e. AM 限制:不超过队列 AM 限制(AM资源和AM数量限制)<br>主要限制在 c)和 d)。d)中 user_limit_factor 一般设置为 1，活跃用户数低于最高用户限制，则取值一般为当前容量/活跃用户数，当队列活跃用户越多，用户配额上限会越低  </li>\n</ol>\n<h2 id=\"改进方案\"><a href=\"#改进方案\" class=\"headerlink\" title=\"改进方案\"></a>改进方案</h2><ol>\n<li>建立业务优先级体系，为每个队列赋予业务优先级属性，优先级高的队列可以抢占低的队列，反之则不可。用以保证核心业务不被非核心业务抢占资源    </li>\n<li>在计算每个队列应得的资源时，若队列预计得到的资源不足以满足队列中最小的 ResourceRequest，则抢占是无意义的，不为该队列做抢占，其预计得到的资源将被设置为 0，这些无法利用的资源将转给其他队列<br>按照对”大容器”的定义(e.g. 40G)，优先为”大容器”队列做抢占，然后为“普通容器“队列做抢占  </li>\n<li>为”大容器”抢占时，在每个节点上选取待抢占队列的最新启动的 Application 的最新分配的部分容器释放且计算释放代价，最终选取释放代价最低的节点上的容器释放。若队列分配完”大容器” 或者待抢占量不高于”大容器” 定义，则后续将其作为”普通容器” 队列来处理  </li>\n<li>为”普通容器”抢占时，选取待抢占队列中最新启动的 App 的最新 Container 开始释放    </li>\n</ol>\n<h2 id=\"抢占流程\"><a href=\"#抢占流程\" class=\"headerlink\" title=\"抢占流程\"></a>抢占流程</h2><p><img src=\"/img/pictures/yarn_preempt/preemption_scheduler.png\" alt=\"\"></p>\n<ol>\n<li><p>获取各队列使用状态<br>　　包括: 队列的业务优先级，最低配额(capacity)，最高配额，当前使用量(used)，当前堆积需求量(pending)，最低需求量，最高需求量，初始理想容量(min{used,capacity})  </p>\n</li>\n<li><p>按优先级计算每个队列的理想容量<br>　　首先将所有队列划分为空闲队列(used&lt;=capacity)、超分配队列(used&gt;capacity)两类，并统计出欠分配队列(used+pending&gt;当前理想容量)。在保证弹性最低配额的条件下，将空闲队列的(capacity-used) 和超分配队列(used-capacity) 作为全局空闲量，按规则分配给欠分配队列:<br>　　a. 优先全局分配空闲资源给优先级高的欠分配队列，按优先级由高至低依次分配<br>　　b. 分配的资源量取决于 min{全局空闲量，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　　即保证在不超过队列最高配额的情况下，从全局空闲量中优先满足高优先级队列的需求<br>　　c. 然后分配全局空闲资源给无优先级但配置有最低配额的欠分配队列，按照欠分配程度(当前理想配额/最低配额 值)由低到高依次分配<br>　　d. 分配的资源量取决于 min{全局空闲量 * 队列最低配额权重，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　e. 最后分配全局空闲资源给无优先级无最配配额的欠分配队列，按照欠分配程度由低到高依次分配<br>　　f. 分配的资源量取决于 min{全局空闲量 * 队列平均权重，需求量，(最高配额-当前理想容量)}，记为 increment<br>　　g. 在全局空闲量为 0 或者没有欠分配队列时终止此环节<br>即: 抢占是一个再平衡的过程，资源从低优先级队列流向高优先级欠分配队列，从未配置最低配额的队列流向配置最低配额的队列，但是会保证有任务但被抢占资源队列的弹性最低配额(min{used+pending, capacity})  </p>\n</li>\n<li><p>判断各队列是否是有效抢占<br>　　判断各队列的 increment 值，若 increment &lt; minResourceRequest，即新得到的资源不足以启动任何一个容器，在将其从欠分配队列中删除  </p>\n</li>\n<li><p>设置其 pending 为 0<br>　　依据 used + pending = 当前容量， pending 设为 0 之后，不再是欠分配队列。  </p>\n</li>\n<li><p>再计算<br>　　重新计算抢占依据 (e.g. 高优先级队列抢占的资源无法启动队列中最小的一个容器时，将此资源按优先级再分给其他队列)     </p>\n<p> 此处的表述有歧义，但是画图过于冗余，故而放到了一个循环中，其实是三个循环。第一个循环: 优先分配有优先级配置的队列，并判断其是否是有效抢占，若是无效抢占，则把其 increment 还回到全局空闲配额中，供次级流程(后续的低优先级队列，无优先级队列)使用。第二个循环是: 分配无优先级但有最低配额的队列，判断其是否是有效抢占，若是无效抢占，则把 increment 还回到全局空闲配额中，供后续流程使用(后续无优先级有最低配额的欠分配队列，无优先级无最低配额的欠分配队列)。第三个循环是: 无优先级无最低配额的欠分配队列。第一个循环在队列分配完成后立即判断是否是有效抢占，第二三个循环需要在所属循环分配完成后按累计 increment 由高至低分别判断，若有无效抢占立即重算除其之外的所有的队列的理想配额。      </p>\n</li>\n<li><p>计算出所有队列的待抢占量，并依据最小需求将所有欠分配队列划分为”大容器”和”普通容器”，分别进行抢占调度  </p>\n</li>\n<li><p>对于每个”大容器”队列，循环(8-&gt;11)抢占，每次抢占量为 maxResourceRequest(足以启动队列中任何一个容器)。直到没有 NM 满足，或者待抢占量低于”大容器”定义    </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">伪代码</span><br><span class=\"line\">Map&lt;NodeId，QueueContainerDetail&gt; info;</span><br><span class=\"line\">For(App: toPreemptedQueue)</span><br><span class=\"line\">\taddContainer&#123;runningContainer，reservedContainer&#125; -&gt; info(amContainers,nonAMContainers);</span><br><span class=\"line\">Map&lt;TempQueue,Resource&gt; limit &#x3D; listQueuePreemptionLimit(); </span><br><span class=\"line\">For( curQueue: allUnderReservedQueue) </span><br><span class=\"line\">\tmaxRR&#x3D;curQueue.getMaxResourceRequest </span><br><span class=\"line\">\ttotalPreeption&#x3D;curQueue.getTotalPreemption</span><br><span class=\"line\">\twhile(totalPreeption&gt;maxRR )</span><br><span class=\"line\">\t\tminCostTime&#x3D;Long.MaxValue\t</span><br><span class=\"line\">\t\tMap&lt;Queue，Container&gt; containers，NodeId &#x2F;&#x2F;在该节点上杀死这些容器为最小代价</span><br><span class=\"line\">\t\tfor(nodeId: NodeId)</span><br><span class=\"line\">\t\t\tcostTime &#x3D; tryPreemptFromNode(node,limit,maxRR)</span><br><span class=\"line\">\t\t\tif(costTime&lt;minCostTime)</span><br><span class=\"line\">\t\t\t\tminCostTime&#x3D;costTime; containers,nodeId REFRESH-&gt;POINTER</span><br><span class=\"line\">\t\tif(costTime &#x3D;&#x3D;Long.MaxValue)</span><br><span class=\"line\">\t\t\tLOG.warn(&quot;no node meet the request in this queue, do left totalPreemption as common preemption&quot;)</span><br><span class=\"line\">\t        else</span><br><span class=\"line\">\t\t\ttotalPreemption -&#x3D; maxRR</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>统计每个待抢占队列的(App，Container)-&gt;(nm) 信息  </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void addContainer(TempQueue queue, RMContainer container)&#123;</span><br><span class=\"line\">      ensureQueue(queue);</span><br><span class=\"line\">      if(container.isAMContainer())&#123;</span><br><span class=\"line\">        amContainers.get(queue).add(container);</span><br><span class=\"line\">      &#125;else &#123;</span><br><span class=\"line\">        nonAMContainers.get(queue).add(container);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>尝试在每个 NM 上抢占，并计算抢占代价<br>　　尝试使用nonAMContainers 来满足 maxResourceRequest；并维护相关信息，计算抢占代价 costTime=∑（curTime-contianer.getStartTime）<br>限制条件是:<br>　　a. 每个队列不超出自身待释放的资源<br>　　b. 在该节点上所有超分配队列释放的资源和节点空闲资源 可以满足 maxRR<br>QueueAppInfo 的核心数据结构和逻辑:<br>[QueueContainerDetail.java]  </p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">NodeId nodeId;Clock clock;Long curTime;Long costTime;</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; nonAMContainers;</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; amContainers;</span><br><span class=\"line\">&#x2F;&#x2F; 各队列 最终被抢占的 container 列表</span><br><span class=\"line\">Map&lt;TempQueue, List&lt;RMContainer&gt;&gt; toBePreemptedContainers;</span><br><span class=\"line\">&#x2F;&#x2F; 各队列 最终被抢占的资源量</span><br><span class=\"line\">Map&lt;TempQueue, Resource&gt; toBePreemptedResource;</span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\">     * note: since to &quot;try preempt&quot;,</span><br><span class=\"line\">     * the maxResourceRequest and preemptedLimitOfQueue are copied.</span><br><span class=\"line\">     * @param maxResourceRequest  待抢占量 e.g. 40G</span><br><span class=\"line\">     * @param preemptedLimit queueLimit A-&gt;30G, B-&gt;20G 抢占总量限制</span><br><span class=\"line\">     *&#x2F;</span><br><span class=\"line\">public long tryPreemptFromNode(Resource maxResourceRequest,</span><br><span class=\"line\">                                   Map&lt;TempQueue, Resource&gt; preemptedLimit)&#123;</span><br><span class=\"line\">      clearCache();</span><br><span class=\"line\">      curTime&#x3D;clock.getTime();</span><br><span class=\"line\">      Resource toPreempt&#x3D;Resources.clone(maxResourceRequest);</span><br><span class=\"line\">      Iterator&lt;Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt;&gt; it &#x3D;</span><br><span class=\"line\">          nonAMContainers.entrySet().iterator();</span><br><span class=\"line\">      while(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">          toPreempt, Resources.none())&amp;&amp; it.hasNext())&#123;</span><br><span class=\"line\">        Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt; entry &#x3D; it.next();</span><br><span class=\"line\">        TempQueue currentQueue &#x3D; entry.getKey();</span><br><span class=\"line\">        Resource limit &#x3D; Resources.clone(preemptedLimit.get(currentQueue));</span><br><span class=\"line\">        if(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">            limit, Resources.none()) &#x3D;&#x3D; false || currentQueue.preemptionDisabled)&#123;</span><br><span class=\"line\">          continue;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        List&lt;RMContainer&gt; containerList &#x3D; entry.getValue();</span><br><span class=\"line\">        long sortStart &#x3D; clock.getTime();</span><br><span class=\"line\">        Collections.sort(containerList, COMPARATOR_CONTAINER);</span><br><span class=\"line\">        long sortEnd &#x3D; clock.getTime();</span><br><span class=\"line\">        if(sortEnd - sortStart &gt;&#x3D; 100)&#123;</span><br><span class=\"line\">          LOG.warn(&quot; Slow Sort nonAM Containers when trying preempt maxResourceRequest &#123;&#125; &quot; +</span><br><span class=\"line\">                  &quot;at queue &#123;&#125; with queueLimit &#123;&#125; total &#123;&#125; container, token &#123;&#125;ms&quot;,</span><br><span class=\"line\">              toPreempt,</span><br><span class=\"line\">              currentQueue.getQueueName(),</span><br><span class=\"line\">              limit,</span><br><span class=\"line\">              containerList.size(),</span><br><span class=\"line\">              sortEnd-sortStart);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        Iterator&lt;RMContainer&gt; iit&#x3D;containerList.iterator();</span><br><span class=\"line\">        while(Resources.greaterThan(rc, clusterResource, limit, Resources.none()) &amp;&amp;</span><br><span class=\"line\">            Resources.greaterThan(rc, clusterResource, toPreempt, Resources.none()) &amp;&amp;</span><br><span class=\"line\">            iit.hasNext())&#123;</span><br><span class=\"line\">          RMContainer i&#x3D;iit.next();</span><br><span class=\"line\">          if(isLabelContainer(i))&#123;</span><br><span class=\"line\">            continue;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          toBePreemptedContainers.get(currentQueue).add(i);</span><br><span class=\"line\">          Resources.addTo(toBePreemptedResource.get(currentQueue),</span><br><span class=\"line\">              i.getContainer().getResource());</span><br><span class=\"line\">          Resources.subtractFrom(limit, i.getContainer().getResource());</span><br><span class=\"line\">          Resources.subtractFrom(toPreempt, i.getContainer().getResource());</span><br><span class=\"line\">          costTime +&#x3D; (curTime - i.getCreationTime());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 暂不抢占 AM,代价不可控 难维护</span><br><span class=\"line\">      if(Resources.greaterThan(rc, clusterResource,</span><br><span class=\"line\">          toPreempt, Resources.none()))&#123;</span><br><span class=\"line\">        costTime &#x3D; Long.MAX_VALUE;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return costTime;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public void preemptFromNode()&#123;</span><br><span class=\"line\">      for(Map.Entry&lt;TempQueue, List&lt;RMContainer&gt;&gt; entry :</span><br><span class=\"line\">          toBePreemptedContainers.entrySet())&#123;</span><br><span class=\"line\">        nonAMContainers.get(entry.getKey()).removeAll(entry.getValue());</span><br><span class=\"line\">        amContainers.get(entry.getKey()).removeAll(entry.getValue());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>选取抢占代价最低的节点，记录相应 container 集合<br>　　获取在所有 NM 中，costTime 最低的 Container 集合作为待抢占容器      </p>\n</li>\n<li><p>若当前队列的待抢占量小于”大容器”定义，则剩余资源作为”普通容器抢占”   </p>\n</li>\n<li><p>在抢占代价最低的节点上，释放了总资源量不低于 maxResourceRequest 的 Container 集合  </p>\n</li>\n<li><p>为所有”普通容器”做抢占:在待抢占的队列中杀死最新启动的容器(不存在于 步骤10 中) </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Resource toPreempted;</span><br><span class=\"line\">For(Queue : overReservedQueue)</span><br><span class=\"line\">\t\tFor(App: Queue.getApps)</span><br><span class=\"line\">\t\t\tif(toPreempted &lt;&#x3D; none) break;</span><br><span class=\"line\">\t\t\tkillReservedContainer(App， toPreempted); </span><br><span class=\"line\">\t\t\tkillAllocatedContainerByOrder(App， toPreempted);</span><br><span class=\"line\">\t\t\trememberAM();</span><br><span class=\"line\">if(toPreempted &gt; none)</span><br><span class=\"line\">\t\tkillAMByOrder();</span><br><span class=\"line\">\t&#96;&#96;&#96;\t\t</span><br><span class=\"line\"></span><br><span class=\"line\">## 优点缺点</span><br><span class=\"line\"></span><br><span class=\"line\">### 优点</span><br><span class=\"line\"></span><br><span class=\"line\">1. 在集群负载高的时候，可以保证优先级高的业务有资源执行;低优先级队列不能抢占高优先级队列的任务.(开源实现 不能保证)  </span><br><span class=\"line\">2. 可以在节点上启动大容器 (开源实现 不能保证)  </span><br><span class=\"line\">3. 可以避免无效抢占 (开源实现 不能保证)  </span><br><span class=\"line\"></span><br><span class=\"line\">### 缺点</span><br><span class=\"line\">1. 集群负载高时，低优先级长时任务可能被无限拉长  </span><br><span class=\"line\">2. 在节点上选取最新启动的容器抢占，而不是在全局选择最新启动的容器抢占，导致任务恢复无效工作量增加  </span><br><span class=\"line\">3. 大容器调度时，增加了节点的资源利用率波动  </span><br><span class=\"line\"></span><br><span class=\"line\">## 抢占调度(开源抢占详解)</span><br><span class=\"line\">抢占调度是周期性的，每周期抢占调度执行的策略分三步:  </span><br><span class=\"line\"></span><br><span class=\"line\">1.\t获取所有队列的快照  </span><br><span class=\"line\">2.\t计算所有队列的理想容量，作为抢占依据  </span><br><span class=\"line\">3.\t从超分配队列中，计算最终抢占量，并选取部分容器释放容量  </span><br><span class=\"line\">其中最重要的是第2步，这一步决定着:队列是否释放资源及释放多少资源  </span><br><span class=\"line\"></span><br><span class=\"line\">### 获取所有队列的快照</span><br><span class=\"line\"></span><br><span class=\"line\">获取信息:  </span><br><span class=\"line\">　　队列用量 used; 队列堆积需求 pending; 队列最低配额 capacity ;队列最高配额额 maxCapacity; 队列是否允许抢占 preemptionDisabled; 子队列 children   </span><br><span class=\"line\">初始化信息:  </span><br><span class=\"line\">　　理想容量 idealAssigned; 超出配额容量 toBePreempted; 最终被抢占容量 actuallyPreempted; 权重normalizedGrarantee</span><br><span class=\"line\"></span><br><span class=\"line\">### 计算所有队列的理想容量，作为抢占依据</span><br><span class=\"line\">　　设置 root 队列的理想容量为 100%，递归向下按层计算 每个子队列的理想容量，子队列的子队列理想容量，直至叶子队列。每轮递归通用逻辑是:依据当前队列的理想容量和所有子队列快照信息，计算每个子队列的理想容量。  </span><br><span class=\"line\">a.  划分子队列类型  </span><br><span class=\"line\">　　对于递归过程中，当前队列的所有子队列按照”是否配置了最低配额”划分为两类:第一类，配置最低配额的子队列集合;第二类，未配置最低配额的子队列集合。区别在于:第一类要优先于第二类得到资源，满足了第一类子队列需求之后如果还有空闲资源才会分配给第二类子队列；第一类子队列在分享当前队列的配额时，之间是以最低配额为权重的，第二类由于没有配置最低配额，之间是平均分配的  </span><br><span class=\"line\">b. 优先分配 配置了最低配额的子队列  </span><br><span class=\"line\">　　b1. 设定初始理想配额  </span><br><span class=\"line\">　　　　这一过程的主要逻辑是: 为所有子队列设定动态初始理想配额，并计算出当前队列的空闲配额，记录欠分配子队列。  </span><br><span class=\"line\">　　　　初始配额设定:   </span><br><span class=\"line\">　　　　　　若 used&lt;&#x3D;capacity，则idealAssigned&#x3D;used。将(capacity-used) 作为空闲资源     </span><br><span class=\"line\">　　　　　　若 used&gt;capacity，则ideaAssigned&#x3D;capacity。将(used-capacity) 作为空闲资源拿出来再分配     </span><br><span class=\"line\">　　　　计算当前队列的空闲配额:  </span><br><span class=\"line\">　　　　　　将 上述两种空闲资源之和作为队列全局空闲资源，交由欠分配子队列 再分配  </span><br><span class=\"line\">　　　　记录欠分配子队列:  </span><br><span class=\"line\">　　　　　　若子队列 used+pending &gt; idealAssigned，即需求量大于容量 为欠分配子队列。  </span><br><span class=\"line\">　　b2. 再分配过程  </span><br><span class=\"line\">　　　　目的: 按照 b1 记录的信息，将全局空闲资源，按照分配给所有的欠分配子队列。    </span><br><span class=\"line\">　　　　方式: 以循环的方式，每轮循环为最欠分配的队列(most under served queue)分配资源。最欠分配的队列在得到资源后，欠分配程度排名可能会变化。因此这种分配方式会使各子队列资源量比值最终趋近于”最低配额”比值。”资源再平衡”的说法来源于此。  </span><br><span class=\"line\">　　　　权重: 每个子队列在欠分配子队列集合中获取资源的权重是 capacity&#x2F;∑capacity，存储在队列快照中的 normalizedGrarantee字段  </span><br><span class=\"line\">　　　　变量: 全局空闲资源，欠分配子队列，子队列减少导致的分配权重增大  </span><br><span class=\"line\">　　　　增量: 循环为最欠分配的队列分配资源，每次分配的增量为min&#123;全局空闲资源 * 权重，需求量，最高配额-当前理想配额&#125;  </span><br><span class=\"line\">　　　　　　全局空闲资源 * 权重: 表示在该轮循环 能得到的资源  </span><br><span class=\"line\">　　　　　　需求量:used+pending-idealAssigned，表示这些资源可以满足队列的需求  </span><br><span class=\"line\">　　　　　　最高配额-当前理想配额: 表示在任何情况下，队列配额都不能超过最高配额  </span><br><span class=\"line\">　　　　增量最终会增加到理想配额idealAssigned。在全局空闲资源为 0 或者无任何欠分配队列时，idealAssigned 会作为队列最终的理想配额  </span><br><span class=\"line\"></span><br><span class=\"line\">核心源码如下:</span><br></pre></td></tr></table></figure>\n<p>/**</p>\n<ul>\n<li>orderedByNeed : 所有欠分配队列</li>\n<li>unassigned: 全局空闲资源</li>\n<li>/\nwhile (!orderedByNeed.isEmpty()\n&amp;&amp; Resources.greaterThan(rc，tot_guarant， unassigned，Resources.none())) {\nResource wQassigned = Resource.newInstance(0， 0);\n/ *</li>\n<li>设置orderedByNeed中所有队列的权重</li>\n<li>配置最低配额: capacity/sum(capacity)</li>\n<li>未配置最低配额: 1/orderedByNeed.size()</li>\n<li>/\nresetCapacity(rc， unassigned， orderedByNeed， ignoreGuarantee);\n/ **</li>\n<li>最欠分配的队列( most under served queue )</li>\n<li>比较 idealAssigned/capacity 值</li>\n<li>/\nCollection<TempQueue> underserved =\n getMostUnderservedQueues(orderedByNeed， tqComparator);\nfor (Iterator<TempQueue> i = underserved.iterator(); i.hasNext();) {\nTempQueue sub = i.next();\n//全局空闲资源<em>权重\nResource wQavail = Resources.multiplyAndNormalizeUp(rc，\n   unassigned， sub.normalizedGuarantee， Resource.newInstance(1， 1));\n//三元组{全局空闲资源</em>权重，used+pending-idealAssigned，maxCapacity-idealAssigned}\nResource wQidle = sub.offer(wQavail， rc， tot_guarant);\n//增量\nResource wQdone = Resources.subtract(wQavail， wQidle);\nif (Resources.greaterThan(rc， tot_guarant，<pre><code>wQdone， Resources.none())) {</code></pre> //如果这个队列在这轮得到了资源，那么下一轮也可能得到资源，所以加入到原集合中;直到没有得到资源\n orderedByNeed.add(sub);\n}\nResources.addTo(wQassigned， wQdone);\n}\n//修改全局空闲资源\nResources.subtractFrom(unassigned， wQassigned);\n}<pre><code>c. 最后分配 未配置最低配额的子队列  \n　　如果在满足了”配置最低配额的子队列“的所有需求之后，还有剩余配额，则平均分配到所有”未配置最低配额的子队列”中。    \n　　除权重外，和 b 完全一致。  \n　　　　权重:子队列集合中每个子队列都会平均分配全局空闲资源(区别于 按最低配额做权重)  \nd. 规整队列待抢占量\n　　汇总所有叶子队列的超出配额(used-idealAssigned)之和，如果此值超出“每轮总抢占“(默认集群总资源的 10%)，则按比例减少每个叶子队列的待抢占量 以保证每轮抢占的总资源量不超过”每轮总抢占”。\n</code></pre></li>\n</ul>\n</li>\n</ol>\n<h3 id=\"从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\"><a href=\"#从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\" class=\"headerlink\" title=\"从超分配队列中，计算最终待抢占量，并选取部分容器释放容量\"></a>从超分配队列中，计算最终待抢占量，并选取部分容器释放容量</h3><ol>\n<li>判断超分配队列<br>　　若 used&gt;capacity*(1+maxIgnoredOverCapacity)，则判断为超分配队列，该队列需要释放资源。<br>　　其中 maxIgnoredOverCapacity 表示 忽略抢占阈值，默认 0.2。 参考“Tips-5”  </li>\n<li>计算最终抢占量<br>　　最终抢占量 actuallyPreempted=toBePreempted * naturalTerminationFactor。<br>　　其中naturalTerminationFactor表示 自然终止因子，默认 0.2。toBePreempt=used-idealAssigned。参考”Tips-5”  </li>\n<li>选取容器<br>　　选取总大小不小于actuallyPreempted的一些容器，释放掉<br>　　选取规则如下:<br>　　　　优先选择队列中后启动的 APP<br>　　　　优先选择 APP 的保留容器<br>　　　　优先选择 APP 后启动的容器<br>　　如果以上普通容器全部选取完之后无法还回等量actuallyPreempted的资源，则开始选取AM 角色的容器，优先选取后启动的 AM容器  </li>\n</ol>\n<h3 id=\"释放\"><a href=\"#释放\" class=\"headerlink\" title=\"释放\"></a>释放</h3><p>　　对于选取的、将要被抢占的所有容器，将&lt;容器，时间&gt;加入到一个全局的集合中追踪，考虑到自然终止的情况，如果集合中的容器存活时间超过 5 * 15s，则直接杀死。  </p>"},{"layout":"post","title":"SparkSQL源码解析(1):从 SQL 到 UnResolvedLogicPlan","subtitle":"SparkSQL 源码解析","date":"2019-11-27T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n## 概览\nParser模块 SparkSqlParser 持有的 SparkSqlAstBuilder 遍历 ANTLR 生成的词法/句法解析器解析成的语法树节点转换成相应的 LogicPlan节点，此时的 LogicPlan 节点仅仅从原始 SQL 文本中解析出来，不包含[表|列|函数]信息，因而称之为未解析的逻辑算子树 UnresolvedLogicPlan。\n\n<!-- more -->\n\n## 源码解析\n\nSparkSession#sql(sqlText) 首先使用 sessionState 持有的 SparkSqlParser#parsePlan(sqlText) 将 sqlText 解析为逻辑计划，然后由 SparkSession 和逻辑计划来构建 DataFrame   \n[SparkSession.scala]\n\n```\n def sql(sqlText: String): DataFrame = {\n    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))\n  }\n```\nSparkSqlParser#parsePlan 继承自 AbstractSqlParser，SparkSqlParser 主要有两个作用: \n\n1. 自定义 SparkSqlAstBuilder 遍历 原始 sql 解析来的语法树每个节点 解析映射为UnResolvedLogicPlan。这是核心功能    \n2.  变量替换，替换`${var}`, `${system:var}` and `${env:var}`     \n\n[SparkSqlParser.scala]\n\n```\nclass SparkSqlParser(conf: SQLConf) extends AbstractSqlParser {\n  val astBuilder = new SparkSqlAstBuilder(conf)\n  private val substitutor = new VariableSubstitution(conf)\n  protected override def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n    super.parse(substitutor.substitute(command))(toResult)\n  }\n}\n```\n使用词法/语法规则解析原始 sql 为语法树的逻辑在 AbstractSqlParser#parse中，\n[AbstractSqlParser.scala]\n\n```\n  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n   // 第一步 : 使用子类的 parse方法来解析原始 sql 为语法树\n   // 第二步: 使用子类定义的AstBuilder通过观察者模式访问语法树,转成 LogicPlan.\n    astBuilder.visitSingleStatement(parser.singleStatement()) match {\n      case plan: LogicalPlan => plan\n      case _ =>\n        val position = Origin(None, None)\n        throw new ParseException(Option(sqlText), \"Unsupported SQL statement\", position, position)\n    }\n    \n```\n第一步: 使用 ANTLR 编译 SqlBase.g4 文件定义的词法/句法来解析原始 sql 为语法树  \n[AbstractSqlParser.scala]\n\n```\n  protected def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n   // 词法分析器\n    val lexer = new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n    // 使用 SparkSQL 的词法错误流来替换 antlr 的\n    lexer.removeErrorListeners()\n    lexer.addErrorListener(ParseErrorListener)\n    val tokenStream = new CommonTokenStream(lexer)\n    // 语法分析器\n    val parser = new SqlBaseParser(tokenStream)\n    parser.addParseListener(PostProcessor)\n    // 使用 SparkSQL 的语法错误流来替换 antlr 的\n    parser.removeErrorListeners()\n    parser.addErrorListener(ParseErrorListener)\n    try {\n      try {\n       //SLL 和 LL 是 ANTLR对冲突和歧义的两种处理模式。SLL 速度快而功能弱,LL 相反\n        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n        toResult(parser)\n      }\n      catch {\n        case e: ParseCancellationException =>\n          tokenStream.seek(0) \n          parser.reset()\n          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n          toResult(parser)\n      }\n    }\n    catch {\n      ...\n    }\n  }\n}\n```\n第二步: 使用 SparkSqlAstBuilder 来遍历语法树，生成为逻辑算子树   \n调用栈   \n-> SparkSqlAstBuilder.visitSingleStatement   \n　->AstBuilder.visitSingleStatement   \n　　->AbstractParseTreeVisitor.visit   \n　　　->SingleStatementContext.accept    \n　　　　->SparkSqlAstBuilder.visitChildren \n  　　　　     \n引用SqlBase.g4 文件的查询规范  \n[SqlBase.g4]\n\n```\nquerySpecification\n    : (((SELECT kind=TRANSFORM '(' namedExpressionSeq ')'\n        | kind=MAP namedExpressionSeq\n        | kind=REDUCE namedExpressionSeq))\n       inRowFormat=rowFormat?\n       (RECORDWRITER recordWriter=STRING)?\n       USING script=STRING\n       (AS (identifierSeq | colTypeList | ('(' (identifierSeq | colTypeList) ')')))?\n       outRowFormat=rowFormat?\n       (RECORDREADER recordReader=STRING)?\n       fromClause?\n       (WHERE where=booleanExpression)?)\n    | ((kind=SELECT (hints+=hint)* setQuantifier? namedExpressionSeq fromClause?\n       | fromClause (kind=SELECT setQuantifier? namedExpressionSeq)?)\n       lateralView*\n       (WHERE where=booleanExpression)?\n       aggregation?\n       (HAVING having=booleanExpression)?\n       windows?)\n    ;\n```\n引用一个最常用的查询语句的语法算子->逻辑计划算子的转换过程，最终得到的是一个逻辑计划对象       \n\n[SparkSqlAstBuilder.scala]  \n\n```\n  override def visitQuerySpecification(\n      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n      // 第一步: 解析 from 语句，如果有多个表则转为内关联关系  \n    val from = OneRowRelation.optional(ctx.fromClause) {\n      visitFromClause(ctx.fromClause)\n    }\n    // 第二步: 基于 from 的语境来解析 select 语句关键词\n    withQuerySpecification(ctx, from)\n  }\n \n  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n    val from = ctx.relation.asScala.foldLeft(null: LogicalPlan) { (left, relation) =>\n      val right = plan(relation.relationPrimary)\n      val join = right.optionalMap(left)(Join(_, _, Inner, None))\n      // 做关联\n      withJoinRelations(join, relation)\n    }\n    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n  }\n  \n  /**\n   * Add a query specification to a logical plan. The query specification is the core of the logical\n   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n   *\n   * Note that query hints are ignored (both by the parser and the builder).\n   */\n  private def withQuerySpecification(\n      ctx: QuerySpecificationContext,\n      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n    import ctx._\n\n    // WHERE\n    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n      Filter(expression(ctx), plan)\n    }\n    // Expressions.\n    val expressions = Option(namedExpressionSeq).toSeq\n      .flatMap(_.namedExpression.asScala)\n      .map(typedVisit[Expression])\n    // Create either a transform or a regular query.\n    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n    specType match {\n      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n        // Transform\n        // Add where.\n        val withFilter = relation.optionalMap(where)(filter)\n\n        // Create the attributes.\n        val (attributes, schemaLess) = if (colTypeList != null) {\n          // Typed return columns.\n          (createSchema(colTypeList).toAttributes, false)\n        } else if (identifierSeq != null) {\n          // Untyped return columns.\n          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n            AttributeReference(name, StringType, nullable = true)()\n          }\n          (attrs, false)\n        } else {\n          (Seq(AttributeReference(\"key\", StringType)(),\n            AttributeReference(\"value\", StringType)()), true)\n        }\n        // 一元关系节点  \n        ScriptTransformation(\n          expressions,\n          string(script),\n          attributes,\n          withFilter,\n          withScriptIOSchema(\n            ctx, inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n\n      case SqlBaseParser.SELECT =>\n        // Regular select\n\t// 解析 lateral 视图(lateral view UDTF as...),where 语句,groupBy,having,distinct 等\n        // Add lateral views.\n        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n        // Add where.\n        val withFilter = withLateralView.optionalMap(where)(filter)\n        // Add aggregation or a project.\n        val namedExpressions = expressions.map {\n          case e: NamedExpression => e\n          case e: Expression => UnresolvedAlias(e)\n        }\n        val withProject = if (aggregation != null) {\n          withAggregation(aggregation, namedExpressions, withFilter)\n        } else if (namedExpressions.nonEmpty) {\n          Project(namedExpressions, withFilter)\n        } else {\n          withFilter\n        }\n        // Having\n        val withHaving = withProject.optional(having) {\n          // Note that we add a cast to non-predicate expressions. If the expression itself is\n          // already boolean, the optimizer will get rid of the unnecessary cast.\n          val predicate = expression(having) match {\n            case p: Predicate => p\n            case e => Cast(e, BooleanType)\n          }\n          Filter(predicate, withProject)\n        }\n        // Distinct\n        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n          Distinct(withHaving)\n        } else {\n          withHaving\n        }\n        // Window\n        val withWindow = withDistinct.optionalMap(windows)(withWindows)\n        // Hint\n        hints.asScala.foldRight(withWindow)(withHints)\n    }\n  }\n```\n\n## TODO\n\n由 SparkSqlAstBuilder.scala 部分方法的返回值可知：Parser 模块的 SparkSqlParser 和 SparkSqlAstBuilder 将原始的 SQL 文本转换成了 UnResolved(Relation|Attribute|Funcation)等，但是从原始 SQL 文本解析来的Relation|Attribute|Funcation 是否存在如何解析调用均需要绑定和验证，e.g. SQL 文本中的表是 Hive Orc 表还是 JSON 文件，Funcation 代表了哪个函数 builtin 函数还是 UDF，Attribute 来自哪个基础表....  这些工作就是 Analyzer 模块的工作内容   \n\n>\nvisitTable 方法的返回值是 UnresovedRelation  \nvisitStar 方法的返回值是 UnresolvedStar  \nvisitColumnReference 方法的返回值是 UnresolvedRegex 或 UnresolvedAttribute  \nvisitFunctionCall 方法的返回值是 UnresolvedWindowExpression 或 WindowExpression  \n....\n\n\n\n\n\n","source":"_posts/2019-11-28-SparkSQL 源码解析(1):从 SQL 到 UnResolvedLogicPlan.md","raw":"---\nlayout:     post\ntitle:      SparkSQL源码解析(1):从 SQL 到 UnResolvedLogicPlan\nsubtitle:   SparkSQL 源码解析\ndate:       2019-11-28\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0  \n    - Spark SQL  \n    - 源码解析  \n---\n\n## 概览\nParser模块 SparkSqlParser 持有的 SparkSqlAstBuilder 遍历 ANTLR 生成的词法/句法解析器解析成的语法树节点转换成相应的 LogicPlan节点，此时的 LogicPlan 节点仅仅从原始 SQL 文本中解析出来，不包含[表|列|函数]信息，因而称之为未解析的逻辑算子树 UnresolvedLogicPlan。\n\n<!-- more -->\n\n## 源码解析\n\nSparkSession#sql(sqlText) 首先使用 sessionState 持有的 SparkSqlParser#parsePlan(sqlText) 将 sqlText 解析为逻辑计划，然后由 SparkSession 和逻辑计划来构建 DataFrame   \n[SparkSession.scala]\n\n```\n def sql(sqlText: String): DataFrame = {\n    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))\n  }\n```\nSparkSqlParser#parsePlan 继承自 AbstractSqlParser，SparkSqlParser 主要有两个作用: \n\n1. 自定义 SparkSqlAstBuilder 遍历 原始 sql 解析来的语法树每个节点 解析映射为UnResolvedLogicPlan。这是核心功能    \n2.  变量替换，替换`${var}`, `${system:var}` and `${env:var}`     \n\n[SparkSqlParser.scala]\n\n```\nclass SparkSqlParser(conf: SQLConf) extends AbstractSqlParser {\n  val astBuilder = new SparkSqlAstBuilder(conf)\n  private val substitutor = new VariableSubstitution(conf)\n  protected override def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n    super.parse(substitutor.substitute(command))(toResult)\n  }\n}\n```\n使用词法/语法规则解析原始 sql 为语法树的逻辑在 AbstractSqlParser#parse中，\n[AbstractSqlParser.scala]\n\n```\n  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n   // 第一步 : 使用子类的 parse方法来解析原始 sql 为语法树\n   // 第二步: 使用子类定义的AstBuilder通过观察者模式访问语法树,转成 LogicPlan.\n    astBuilder.visitSingleStatement(parser.singleStatement()) match {\n      case plan: LogicalPlan => plan\n      case _ =>\n        val position = Origin(None, None)\n        throw new ParseException(Option(sqlText), \"Unsupported SQL statement\", position, position)\n    }\n    \n```\n第一步: 使用 ANTLR 编译 SqlBase.g4 文件定义的词法/句法来解析原始 sql 为语法树  \n[AbstractSqlParser.scala]\n\n```\n  protected def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n   // 词法分析器\n    val lexer = new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n    // 使用 SparkSQL 的词法错误流来替换 antlr 的\n    lexer.removeErrorListeners()\n    lexer.addErrorListener(ParseErrorListener)\n    val tokenStream = new CommonTokenStream(lexer)\n    // 语法分析器\n    val parser = new SqlBaseParser(tokenStream)\n    parser.addParseListener(PostProcessor)\n    // 使用 SparkSQL 的语法错误流来替换 antlr 的\n    parser.removeErrorListeners()\n    parser.addErrorListener(ParseErrorListener)\n    try {\n      try {\n       //SLL 和 LL 是 ANTLR对冲突和歧义的两种处理模式。SLL 速度快而功能弱,LL 相反\n        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n        toResult(parser)\n      }\n      catch {\n        case e: ParseCancellationException =>\n          tokenStream.seek(0) \n          parser.reset()\n          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n          toResult(parser)\n      }\n    }\n    catch {\n      ...\n    }\n  }\n}\n```\n第二步: 使用 SparkSqlAstBuilder 来遍历语法树，生成为逻辑算子树   \n调用栈   \n-> SparkSqlAstBuilder.visitSingleStatement   \n　->AstBuilder.visitSingleStatement   \n　　->AbstractParseTreeVisitor.visit   \n　　　->SingleStatementContext.accept    \n　　　　->SparkSqlAstBuilder.visitChildren \n  　　　　     \n引用SqlBase.g4 文件的查询规范  \n[SqlBase.g4]\n\n```\nquerySpecification\n    : (((SELECT kind=TRANSFORM '(' namedExpressionSeq ')'\n        | kind=MAP namedExpressionSeq\n        | kind=REDUCE namedExpressionSeq))\n       inRowFormat=rowFormat?\n       (RECORDWRITER recordWriter=STRING)?\n       USING script=STRING\n       (AS (identifierSeq | colTypeList | ('(' (identifierSeq | colTypeList) ')')))?\n       outRowFormat=rowFormat?\n       (RECORDREADER recordReader=STRING)?\n       fromClause?\n       (WHERE where=booleanExpression)?)\n    | ((kind=SELECT (hints+=hint)* setQuantifier? namedExpressionSeq fromClause?\n       | fromClause (kind=SELECT setQuantifier? namedExpressionSeq)?)\n       lateralView*\n       (WHERE where=booleanExpression)?\n       aggregation?\n       (HAVING having=booleanExpression)?\n       windows?)\n    ;\n```\n引用一个最常用的查询语句的语法算子->逻辑计划算子的转换过程，最终得到的是一个逻辑计划对象       \n\n[SparkSqlAstBuilder.scala]  \n\n```\n  override def visitQuerySpecification(\n      ctx: QuerySpecificationContext): LogicalPlan = withOrigin(ctx) {\n      // 第一步: 解析 from 语句，如果有多个表则转为内关联关系  \n    val from = OneRowRelation.optional(ctx.fromClause) {\n      visitFromClause(ctx.fromClause)\n    }\n    // 第二步: 基于 from 的语境来解析 select 语句关键词\n    withQuerySpecification(ctx, from)\n  }\n \n  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) {\n    val from = ctx.relation.asScala.foldLeft(null: LogicalPlan) { (left, relation) =>\n      val right = plan(relation.relationPrimary)\n      val join = right.optionalMap(left)(Join(_, _, Inner, None))\n      // 做关联\n      withJoinRelations(join, relation)\n    }\n    ctx.lateralView.asScala.foldLeft(from)(withGenerate)\n  }\n  \n  /**\n   * Add a query specification to a logical plan. The query specification is the core of the logical\n   * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM/MAP/REDUCE),\n   * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.\n   *\n   * Note that query hints are ignored (both by the parser and the builder).\n   */\n  private def withQuerySpecification(\n      ctx: QuerySpecificationContext,\n      relation: LogicalPlan): LogicalPlan = withOrigin(ctx) {\n    import ctx._\n\n    // WHERE\n    def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan = {\n      Filter(expression(ctx), plan)\n    }\n    // Expressions.\n    val expressions = Option(namedExpressionSeq).toSeq\n      .flatMap(_.namedExpression.asScala)\n      .map(typedVisit[Expression])\n    // Create either a transform or a regular query.\n    val specType = Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)\n    specType match {\n      case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM =>\n        // Transform\n        // Add where.\n        val withFilter = relation.optionalMap(where)(filter)\n\n        // Create the attributes.\n        val (attributes, schemaLess) = if (colTypeList != null) {\n          // Typed return columns.\n          (createSchema(colTypeList).toAttributes, false)\n        } else if (identifierSeq != null) {\n          // Untyped return columns.\n          val attrs = visitIdentifierSeq(identifierSeq).map { name =>\n            AttributeReference(name, StringType, nullable = true)()\n          }\n          (attrs, false)\n        } else {\n          (Seq(AttributeReference(\"key\", StringType)(),\n            AttributeReference(\"value\", StringType)()), true)\n        }\n        // 一元关系节点  \n        ScriptTransformation(\n          expressions,\n          string(script),\n          attributes,\n          withFilter,\n          withScriptIOSchema(\n            ctx, inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))\n\n      case SqlBaseParser.SELECT =>\n        // Regular select\n\t// 解析 lateral 视图(lateral view UDTF as...),where 语句,groupBy,having,distinct 等\n        // Add lateral views.\n        val withLateralView = ctx.lateralView.asScala.foldLeft(relation)(withGenerate)\n        // Add where.\n        val withFilter = withLateralView.optionalMap(where)(filter)\n        // Add aggregation or a project.\n        val namedExpressions = expressions.map {\n          case e: NamedExpression => e\n          case e: Expression => UnresolvedAlias(e)\n        }\n        val withProject = if (aggregation != null) {\n          withAggregation(aggregation, namedExpressions, withFilter)\n        } else if (namedExpressions.nonEmpty) {\n          Project(namedExpressions, withFilter)\n        } else {\n          withFilter\n        }\n        // Having\n        val withHaving = withProject.optional(having) {\n          // Note that we add a cast to non-predicate expressions. If the expression itself is\n          // already boolean, the optimizer will get rid of the unnecessary cast.\n          val predicate = expression(having) match {\n            case p: Predicate => p\n            case e => Cast(e, BooleanType)\n          }\n          Filter(predicate, withProject)\n        }\n        // Distinct\n        val withDistinct = if (setQuantifier() != null && setQuantifier().DISTINCT() != null) {\n          Distinct(withHaving)\n        } else {\n          withHaving\n        }\n        // Window\n        val withWindow = withDistinct.optionalMap(windows)(withWindows)\n        // Hint\n        hints.asScala.foldRight(withWindow)(withHints)\n    }\n  }\n```\n\n## TODO\n\n由 SparkSqlAstBuilder.scala 部分方法的返回值可知：Parser 模块的 SparkSqlParser 和 SparkSqlAstBuilder 将原始的 SQL 文本转换成了 UnResolved(Relation|Attribute|Funcation)等，但是从原始 SQL 文本解析来的Relation|Attribute|Funcation 是否存在如何解析调用均需要绑定和验证，e.g. SQL 文本中的表是 Hive Orc 表还是 JSON 文件，Funcation 代表了哪个函数 builtin 函数还是 UDF，Attribute 来自哪个基础表....  这些工作就是 Analyzer 模块的工作内容   \n\n>\nvisitTable 方法的返回值是 UnresovedRelation  \nvisitStar 方法的返回值是 UnresolvedStar  \nvisitColumnReference 方法的返回值是 UnresolvedRegex 或 UnresolvedAttribute  \nvisitFunctionCall 方法的返回值是 UnresolvedWindowExpression 或 WindowExpression  \n....\n\n\n\n\n\n","slug":"2019-11-28-SparkSQL 源码解析(1):从 SQL 到 UnResolvedLogicPlan","published":1,"updated":"2020-06-16T06:42:32.975Z","comments":1,"photos":[],"link":"","_id":"cke2rms52000by7c7d3fq86hg","content":"<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>Parser模块 SparkSqlParser 持有的 SparkSqlAstBuilder 遍历 ANTLR 生成的词法/句法解析器解析成的语法树节点转换成相应的 LogicPlan节点，此时的 LogicPlan 节点仅仅从原始 SQL 文本中解析出来，不包含[表|列|函数]信息，因而称之为未解析的逻辑算子树 UnresolvedLogicPlan。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>SparkSession#sql(sqlText) 首先使用 sessionState 持有的 SparkSqlParser#parsePlan(sqlText) 将 sqlText 解析为逻辑计划，然后由 SparkSession 和逻辑计划来构建 DataFrame<br>[SparkSession.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def sql(sqlText: String): DataFrame &#x3D; &#123;</span><br><span class=\"line\">   Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>SparkSqlParser#parsePlan 继承自 AbstractSqlParser，SparkSqlParser 主要有两个作用: </p>\n<ol>\n<li>自定义 SparkSqlAstBuilder 遍历 原始 sql 解析来的语法树每个节点 解析映射为UnResolvedLogicPlan。这是核心功能    </li>\n<li>变量替换，替换<code>${var}</code>, <code>${system:var}</code> and <code>${env:var}</code>     </li>\n</ol>\n<p>[SparkSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SparkSqlParser(conf: SQLConf) extends AbstractSqlParser &#123;</span><br><span class=\"line\">  val astBuilder &#x3D; new SparkSqlAstBuilder(conf)</span><br><span class=\"line\">  private val substitutor &#x3D; new VariableSubstitution(conf)</span><br><span class=\"line\">  protected override def parse[T](command: String)(toResult: SqlBaseParser &#x3D;&gt; T): T &#x3D; &#123;</span><br><span class=\"line\">    super.parse(substitutor.substitute(command))(toResult)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用词法/语法规则解析原始 sql 为语法树的逻辑在 AbstractSqlParser#parse中，\n[AbstractSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def parsePlan(sqlText: String): LogicalPlan &#x3D; parse(sqlText) &#123; parser &#x3D;&gt;</span><br><span class=\"line\"> &#x2F;&#x2F; 第一步 : 使用子类的 parse方法来解析原始 sql 为语法树</span><br><span class=\"line\"> &#x2F;&#x2F; 第二步: 使用子类定义的AstBuilder通过观察者模式访问语法树,转成 LogicPlan.</span><br><span class=\"line\">  astBuilder.visitSingleStatement(parser.singleStatement()) match &#123;</span><br><span class=\"line\">    case plan: LogicalPlan &#x3D;&gt; plan</span><br><span class=\"line\">    case _ &#x3D;&gt;</span><br><span class=\"line\">      val position &#x3D; Origin(None, None)</span><br><span class=\"line\">      throw new ParseException(Option(sqlText), &quot;Unsupported SQL statement&quot;, position, position)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第一步: 使用 ANTLR 编译 SqlBase.g4 文件定义的词法/句法来解析原始 sql 为语法树<br>[AbstractSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  protected def parse[T](command: String)(toResult: SqlBaseParser &#x3D;&gt; T): T &#x3D; &#123;</span><br><span class=\"line\">   &#x2F;&#x2F; 词法分析器</span><br><span class=\"line\">    val lexer &#x3D; new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command)))</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 SparkSQL 的词法错误流来替换 antlr 的</span><br><span class=\"line\">    lexer.removeErrorListeners()</span><br><span class=\"line\">    lexer.addErrorListener(ParseErrorListener)</span><br><span class=\"line\">    val tokenStream &#x3D; new CommonTokenStream(lexer)</span><br><span class=\"line\">    &#x2F;&#x2F; 语法分析器</span><br><span class=\"line\">    val parser &#x3D; new SqlBaseParser(tokenStream)</span><br><span class=\"line\">    parser.addParseListener(PostProcessor)</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 SparkSQL 的语法错误流来替换 antlr 的</span><br><span class=\"line\">    parser.removeErrorListeners()</span><br><span class=\"line\">    parser.addErrorListener(ParseErrorListener)</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">       &#x2F;&#x2F;SLL 和 LL 是 ANTLR对冲突和歧义的两种处理模式。SLL 速度快而功能弱,LL 相反</span><br><span class=\"line\">        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)</span><br><span class=\"line\">        toResult(parser)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      catch &#123;</span><br><span class=\"line\">        case e: ParseCancellationException &#x3D;&gt;</span><br><span class=\"line\">          tokenStream.seek(0) </span><br><span class=\"line\">          parser.reset()</span><br><span class=\"line\">          parser.getInterpreter.setPredictionMode(PredictionMode.LL)</span><br><span class=\"line\">          toResult(parser)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    catch &#123;</span><br><span class=\"line\">      ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>第二步: 使用 SparkSqlAstBuilder 来遍历语法树，生成为逻辑算子树<br>调用栈<br>-&gt; SparkSqlAstBuilder.visitSingleStatement<br>　-&gt;AstBuilder.visitSingleStatement<br>　　-&gt;AbstractParseTreeVisitor.visit<br>　　　-&gt;SingleStatementContext.accept<br>　　　　-&gt;SparkSqlAstBuilder.visitChildren \n  　　　　<br>引用SqlBase.g4 文件的查询规范<br>[SqlBase.g4]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">querySpecification</span><br><span class=\"line\">    : (((SELECT kind&#x3D;TRANSFORM &#39;(&#39; namedExpressionSeq &#39;)&#39;</span><br><span class=\"line\">        | kind&#x3D;MAP namedExpressionSeq</span><br><span class=\"line\">        | kind&#x3D;REDUCE namedExpressionSeq))</span><br><span class=\"line\">       inRowFormat&#x3D;rowFormat?</span><br><span class=\"line\">       (RECORDWRITER recordWriter&#x3D;STRING)?</span><br><span class=\"line\">       USING script&#x3D;STRING</span><br><span class=\"line\">       (AS (identifierSeq | colTypeList | (&#39;(&#39; (identifierSeq | colTypeList) &#39;)&#39;)))?</span><br><span class=\"line\">       outRowFormat&#x3D;rowFormat?</span><br><span class=\"line\">       (RECORDREADER recordReader&#x3D;STRING)?</span><br><span class=\"line\">       fromClause?</span><br><span class=\"line\">       (WHERE where&#x3D;booleanExpression)?)</span><br><span class=\"line\">    | ((kind&#x3D;SELECT (hints+&#x3D;hint)* setQuantifier? namedExpressionSeq fromClause?</span><br><span class=\"line\">       | fromClause (kind&#x3D;SELECT setQuantifier? namedExpressionSeq)?)</span><br><span class=\"line\">       lateralView*</span><br><span class=\"line\">       (WHERE where&#x3D;booleanExpression)?</span><br><span class=\"line\">       aggregation?</span><br><span class=\"line\">       (HAVING having&#x3D;booleanExpression)?</span><br><span class=\"line\">       windows?)</span><br><span class=\"line\">    ;</span><br></pre></td></tr></table></figure>\n<p>引用一个最常用的查询语句的语法算子-&gt;逻辑计划算子的转换过程，最终得到的是一个逻辑计划对象       </p>\n<p>[SparkSqlAstBuilder.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> override def visitQuerySpecification(</span><br><span class=\"line\">     ctx: QuerySpecificationContext): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 第一步: 解析 from 语句，如果有多个表则转为内关联关系  </span><br><span class=\"line\">   val from &#x3D; OneRowRelation.optional(ctx.fromClause) &#123;</span><br><span class=\"line\">     visitFromClause(ctx.fromClause)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F; 第二步: 基于 from 的语境来解析 select 语句关键词</span><br><span class=\"line\">   withQuerySpecification(ctx, from)</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> override def visitFromClause(ctx: FromClauseContext): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">   val from &#x3D; ctx.relation.asScala.foldLeft(null: LogicalPlan) &#123; (left, relation) &#x3D;&gt;</span><br><span class=\"line\">     val right &#x3D; plan(relation.relationPrimary)</span><br><span class=\"line\">     val join &#x3D; right.optionalMap(left)(Join(_, _, Inner, None))</span><br><span class=\"line\">     &#x2F;&#x2F; 做关联</span><br><span class=\"line\">     withJoinRelations(join, relation)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   ctx.lateralView.asScala.foldLeft(from)(withGenerate)</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"> &#x2F;**</span><br><span class=\"line\">  * Add a query specification to a logical plan. The query specification is the core of the logical</span><br><span class=\"line\">  * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM&#x2F;MAP&#x2F;REDUCE),</span><br><span class=\"line\">  * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * Note that query hints are ignored (both by the parser and the builder).</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\"> private def withQuerySpecification(</span><br><span class=\"line\">     ctx: QuerySpecificationContext,</span><br><span class=\"line\">     relation: LogicalPlan): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">   import ctx._</span><br><span class=\"line\"></span><br><span class=\"line\">   &#x2F;&#x2F; WHERE</span><br><span class=\"line\">   def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">     Filter(expression(ctx), plan)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F; Expressions.</span><br><span class=\"line\">   val expressions &#x3D; Option(namedExpressionSeq).toSeq</span><br><span class=\"line\">     .flatMap(_.namedExpression.asScala)</span><br><span class=\"line\">     .map(typedVisit[Expression])</span><br><span class=\"line\">   &#x2F;&#x2F; Create either a transform or a regular query.</span><br><span class=\"line\">   val specType &#x3D; Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)</span><br><span class=\"line\">   specType match &#123;</span><br><span class=\"line\">     case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Transform</span><br><span class=\"line\">       &#x2F;&#x2F; Add where.</span><br><span class=\"line\">       val withFilter &#x3D; relation.optionalMap(where)(filter)</span><br><span class=\"line\"></span><br><span class=\"line\">       &#x2F;&#x2F; Create the attributes.</span><br><span class=\"line\">       val (attributes, schemaLess) &#x3D; if (colTypeList !&#x3D; null) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Typed return columns.</span><br><span class=\"line\">         (createSchema(colTypeList).toAttributes, false)</span><br><span class=\"line\">       &#125; else if (identifierSeq !&#x3D; null) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Untyped return columns.</span><br><span class=\"line\">         val attrs &#x3D; visitIdentifierSeq(identifierSeq).map &#123; name &#x3D;&gt;</span><br><span class=\"line\">           AttributeReference(name, StringType, nullable &#x3D; true)()</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         (attrs, false)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         (Seq(AttributeReference(&quot;key&quot;, StringType)(),</span><br><span class=\"line\">           AttributeReference(&quot;value&quot;, StringType)()), true)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; 一元关系节点  </span><br><span class=\"line\">       ScriptTransformation(</span><br><span class=\"line\">         expressions,</span><br><span class=\"line\">         string(script),</span><br><span class=\"line\">         attributes,</span><br><span class=\"line\">         withFilter,</span><br><span class=\"line\">         withScriptIOSchema(</span><br><span class=\"line\">           ctx, inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))</span><br><span class=\"line\"></span><br><span class=\"line\">     case SqlBaseParser.SELECT &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Regular select</span><br><span class=\"line\">&#x2F;&#x2F; 解析 lateral 视图(lateral view UDTF as...),where 语句,groupBy,having,distinct 等</span><br><span class=\"line\">       &#x2F;&#x2F; Add lateral views.</span><br><span class=\"line\">       val withLateralView &#x3D; ctx.lateralView.asScala.foldLeft(relation)(withGenerate)</span><br><span class=\"line\">       &#x2F;&#x2F; Add where.</span><br><span class=\"line\">       val withFilter &#x3D; withLateralView.optionalMap(where)(filter)</span><br><span class=\"line\">       &#x2F;&#x2F; Add aggregation or a project.</span><br><span class=\"line\">       val namedExpressions &#x3D; expressions.map &#123;</span><br><span class=\"line\">         case e: NamedExpression &#x3D;&gt; e</span><br><span class=\"line\">         case e: Expression &#x3D;&gt; UnresolvedAlias(e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       val withProject &#x3D; if (aggregation !&#x3D; null) &#123;</span><br><span class=\"line\">         withAggregation(aggregation, namedExpressions, withFilter)</span><br><span class=\"line\">       &#125; else if (namedExpressions.nonEmpty) &#123;</span><br><span class=\"line\">         Project(namedExpressions, withFilter)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         withFilter</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Having</span><br><span class=\"line\">       val withHaving &#x3D; withProject.optional(having) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Note that we add a cast to non-predicate expressions. If the expression itself is</span><br><span class=\"line\">         &#x2F;&#x2F; already boolean, the optimizer will get rid of the unnecessary cast.</span><br><span class=\"line\">         val predicate &#x3D; expression(having) match &#123;</span><br><span class=\"line\">           case p: Predicate &#x3D;&gt; p</span><br><span class=\"line\">           case e &#x3D;&gt; Cast(e, BooleanType)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         Filter(predicate, withProject)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Distinct</span><br><span class=\"line\">       val withDistinct &#x3D; if (setQuantifier() !&#x3D; null &amp;&amp; setQuantifier().DISTINCT() !&#x3D; null) &#123;</span><br><span class=\"line\">         Distinct(withHaving)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         withHaving</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Window</span><br><span class=\"line\">       val withWindow &#x3D; withDistinct.optionalMap(windows)(withWindows)</span><br><span class=\"line\">       &#x2F;&#x2F; Hint</span><br><span class=\"line\">       hints.asScala.foldRight(withWindow)(withHints)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><p>由 SparkSqlAstBuilder.scala 部分方法的返回值可知：Parser 模块的 SparkSqlParser 和 SparkSqlAstBuilder 将原始的 SQL 文本转换成了 UnResolved(Relation|Attribute|Funcation)等，但是从原始 SQL 文本解析来的Relation|Attribute|Funcation 是否存在如何解析调用均需要绑定和验证，e.g. SQL 文本中的表是 Hive Orc 表还是 JSON 文件，Funcation 代表了哪个函数 builtin 函数还是 UDF，Attribute 来自哪个基础表….  这些工作就是 Analyzer 模块的工作内容   </p>\n<blockquote>\n</blockquote>\n<p>visitTable 方法的返回值是 UnresovedRelation<br>visitStar 方法的返回值是 UnresolvedStar<br>visitColumnReference 方法的返回值是 UnresolvedRegex 或 UnresolvedAttribute<br>visitFunctionCall 方法的返回值是 UnresolvedWindowExpression 或 WindowExpression<br>….</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>Parser模块 SparkSqlParser 持有的 SparkSqlAstBuilder 遍历 ANTLR 生成的词法/句法解析器解析成的语法树节点转换成相应的 LogicPlan节点，此时的 LogicPlan 节点仅仅从原始 SQL 文本中解析出来，不包含[表|列|函数]信息，因而称之为未解析的逻辑算子树 UnresolvedLogicPlan。</p>","more":"<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>SparkSession#sql(sqlText) 首先使用 sessionState 持有的 SparkSqlParser#parsePlan(sqlText) 将 sqlText 解析为逻辑计划，然后由 SparkSession 和逻辑计划来构建 DataFrame<br>[SparkSession.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def sql(sqlText: String): DataFrame &#x3D; &#123;</span><br><span class=\"line\">   Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>SparkSqlParser#parsePlan 继承自 AbstractSqlParser，SparkSqlParser 主要有两个作用: </p>\n<ol>\n<li>自定义 SparkSqlAstBuilder 遍历 原始 sql 解析来的语法树每个节点 解析映射为UnResolvedLogicPlan。这是核心功能    </li>\n<li>变量替换，替换<code>${var}</code>, <code>${system:var}</code> and <code>${env:var}</code>     </li>\n</ol>\n<p>[SparkSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class SparkSqlParser(conf: SQLConf) extends AbstractSqlParser &#123;</span><br><span class=\"line\">  val astBuilder &#x3D; new SparkSqlAstBuilder(conf)</span><br><span class=\"line\">  private val substitutor &#x3D; new VariableSubstitution(conf)</span><br><span class=\"line\">  protected override def parse[T](command: String)(toResult: SqlBaseParser &#x3D;&gt; T): T &#x3D; &#123;</span><br><span class=\"line\">    super.parse(substitutor.substitute(command))(toResult)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用词法/语法规则解析原始 sql 为语法树的逻辑在 AbstractSqlParser#parse中，\n[AbstractSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def parsePlan(sqlText: String): LogicalPlan &#x3D; parse(sqlText) &#123; parser &#x3D;&gt;</span><br><span class=\"line\"> &#x2F;&#x2F; 第一步 : 使用子类的 parse方法来解析原始 sql 为语法树</span><br><span class=\"line\"> &#x2F;&#x2F; 第二步: 使用子类定义的AstBuilder通过观察者模式访问语法树,转成 LogicPlan.</span><br><span class=\"line\">  astBuilder.visitSingleStatement(parser.singleStatement()) match &#123;</span><br><span class=\"line\">    case plan: LogicalPlan &#x3D;&gt; plan</span><br><span class=\"line\">    case _ &#x3D;&gt;</span><br><span class=\"line\">      val position &#x3D; Origin(None, None)</span><br><span class=\"line\">      throw new ParseException(Option(sqlText), &quot;Unsupported SQL statement&quot;, position, position)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第一步: 使用 ANTLR 编译 SqlBase.g4 文件定义的词法/句法来解析原始 sql 为语法树<br>[AbstractSqlParser.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  protected def parse[T](command: String)(toResult: SqlBaseParser &#x3D;&gt; T): T &#x3D; &#123;</span><br><span class=\"line\">   &#x2F;&#x2F; 词法分析器</span><br><span class=\"line\">    val lexer &#x3D; new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command)))</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 SparkSQL 的词法错误流来替换 antlr 的</span><br><span class=\"line\">    lexer.removeErrorListeners()</span><br><span class=\"line\">    lexer.addErrorListener(ParseErrorListener)</span><br><span class=\"line\">    val tokenStream &#x3D; new CommonTokenStream(lexer)</span><br><span class=\"line\">    &#x2F;&#x2F; 语法分析器</span><br><span class=\"line\">    val parser &#x3D; new SqlBaseParser(tokenStream)</span><br><span class=\"line\">    parser.addParseListener(PostProcessor)</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 SparkSQL 的语法错误流来替换 antlr 的</span><br><span class=\"line\">    parser.removeErrorListeners()</span><br><span class=\"line\">    parser.addErrorListener(ParseErrorListener)</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">       &#x2F;&#x2F;SLL 和 LL 是 ANTLR对冲突和歧义的两种处理模式。SLL 速度快而功能弱,LL 相反</span><br><span class=\"line\">        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)</span><br><span class=\"line\">        toResult(parser)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      catch &#123;</span><br><span class=\"line\">        case e: ParseCancellationException &#x3D;&gt;</span><br><span class=\"line\">          tokenStream.seek(0) </span><br><span class=\"line\">          parser.reset()</span><br><span class=\"line\">          parser.getInterpreter.setPredictionMode(PredictionMode.LL)</span><br><span class=\"line\">          toResult(parser)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    catch &#123;</span><br><span class=\"line\">      ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>第二步: 使用 SparkSqlAstBuilder 来遍历语法树，生成为逻辑算子树<br>调用栈<br>-&gt; SparkSqlAstBuilder.visitSingleStatement<br>　-&gt;AstBuilder.visitSingleStatement<br>　　-&gt;AbstractParseTreeVisitor.visit<br>　　　-&gt;SingleStatementContext.accept<br>　　　　-&gt;SparkSqlAstBuilder.visitChildren \n  　　　　<br>引用SqlBase.g4 文件的查询规范<br>[SqlBase.g4]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">querySpecification</span><br><span class=\"line\">    : (((SELECT kind&#x3D;TRANSFORM &#39;(&#39; namedExpressionSeq &#39;)&#39;</span><br><span class=\"line\">        | kind&#x3D;MAP namedExpressionSeq</span><br><span class=\"line\">        | kind&#x3D;REDUCE namedExpressionSeq))</span><br><span class=\"line\">       inRowFormat&#x3D;rowFormat?</span><br><span class=\"line\">       (RECORDWRITER recordWriter&#x3D;STRING)?</span><br><span class=\"line\">       USING script&#x3D;STRING</span><br><span class=\"line\">       (AS (identifierSeq | colTypeList | (&#39;(&#39; (identifierSeq | colTypeList) &#39;)&#39;)))?</span><br><span class=\"line\">       outRowFormat&#x3D;rowFormat?</span><br><span class=\"line\">       (RECORDREADER recordReader&#x3D;STRING)?</span><br><span class=\"line\">       fromClause?</span><br><span class=\"line\">       (WHERE where&#x3D;booleanExpression)?)</span><br><span class=\"line\">    | ((kind&#x3D;SELECT (hints+&#x3D;hint)* setQuantifier? namedExpressionSeq fromClause?</span><br><span class=\"line\">       | fromClause (kind&#x3D;SELECT setQuantifier? namedExpressionSeq)?)</span><br><span class=\"line\">       lateralView*</span><br><span class=\"line\">       (WHERE where&#x3D;booleanExpression)?</span><br><span class=\"line\">       aggregation?</span><br><span class=\"line\">       (HAVING having&#x3D;booleanExpression)?</span><br><span class=\"line\">       windows?)</span><br><span class=\"line\">    ;</span><br></pre></td></tr></table></figure>\n<p>引用一个最常用的查询语句的语法算子-&gt;逻辑计划算子的转换过程，最终得到的是一个逻辑计划对象       </p>\n<p>[SparkSqlAstBuilder.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> override def visitQuerySpecification(</span><br><span class=\"line\">     ctx: QuerySpecificationContext): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 第一步: 解析 from 语句，如果有多个表则转为内关联关系  </span><br><span class=\"line\">   val from &#x3D; OneRowRelation.optional(ctx.fromClause) &#123;</span><br><span class=\"line\">     visitFromClause(ctx.fromClause)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F; 第二步: 基于 from 的语境来解析 select 语句关键词</span><br><span class=\"line\">   withQuerySpecification(ctx, from)</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> override def visitFromClause(ctx: FromClauseContext): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">   val from &#x3D; ctx.relation.asScala.foldLeft(null: LogicalPlan) &#123; (left, relation) &#x3D;&gt;</span><br><span class=\"line\">     val right &#x3D; plan(relation.relationPrimary)</span><br><span class=\"line\">     val join &#x3D; right.optionalMap(left)(Join(_, _, Inner, None))</span><br><span class=\"line\">     &#x2F;&#x2F; 做关联</span><br><span class=\"line\">     withJoinRelations(join, relation)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   ctx.lateralView.asScala.foldLeft(from)(withGenerate)</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"> &#x2F;**</span><br><span class=\"line\">  * Add a query specification to a logical plan. The query specification is the core of the logical</span><br><span class=\"line\">  * plan, this is where sourcing (FROM clause), transforming (SELECT TRANSFORM&#x2F;MAP&#x2F;REDUCE),</span><br><span class=\"line\">  * projection (SELECT), aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place.</span><br><span class=\"line\">  *</span><br><span class=\"line\">  * Note that query hints are ignored (both by the parser and the builder).</span><br><span class=\"line\">  *&#x2F;</span><br><span class=\"line\"> private def withQuerySpecification(</span><br><span class=\"line\">     ctx: QuerySpecificationContext,</span><br><span class=\"line\">     relation: LogicalPlan): LogicalPlan &#x3D; withOrigin(ctx) &#123;</span><br><span class=\"line\">   import ctx._</span><br><span class=\"line\"></span><br><span class=\"line\">   &#x2F;&#x2F; WHERE</span><br><span class=\"line\">   def filter(ctx: BooleanExpressionContext, plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">     Filter(expression(ctx), plan)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F; Expressions.</span><br><span class=\"line\">   val expressions &#x3D; Option(namedExpressionSeq).toSeq</span><br><span class=\"line\">     .flatMap(_.namedExpression.asScala)</span><br><span class=\"line\">     .map(typedVisit[Expression])</span><br><span class=\"line\">   &#x2F;&#x2F; Create either a transform or a regular query.</span><br><span class=\"line\">   val specType &#x3D; Option(kind).map(_.getType).getOrElse(SqlBaseParser.SELECT)</span><br><span class=\"line\">   specType match &#123;</span><br><span class=\"line\">     case SqlBaseParser.MAP | SqlBaseParser.REDUCE | SqlBaseParser.TRANSFORM &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Transform</span><br><span class=\"line\">       &#x2F;&#x2F; Add where.</span><br><span class=\"line\">       val withFilter &#x3D; relation.optionalMap(where)(filter)</span><br><span class=\"line\"></span><br><span class=\"line\">       &#x2F;&#x2F; Create the attributes.</span><br><span class=\"line\">       val (attributes, schemaLess) &#x3D; if (colTypeList !&#x3D; null) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Typed return columns.</span><br><span class=\"line\">         (createSchema(colTypeList).toAttributes, false)</span><br><span class=\"line\">       &#125; else if (identifierSeq !&#x3D; null) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Untyped return columns.</span><br><span class=\"line\">         val attrs &#x3D; visitIdentifierSeq(identifierSeq).map &#123; name &#x3D;&gt;</span><br><span class=\"line\">           AttributeReference(name, StringType, nullable &#x3D; true)()</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         (attrs, false)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         (Seq(AttributeReference(&quot;key&quot;, StringType)(),</span><br><span class=\"line\">           AttributeReference(&quot;value&quot;, StringType)()), true)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; 一元关系节点  </span><br><span class=\"line\">       ScriptTransformation(</span><br><span class=\"line\">         expressions,</span><br><span class=\"line\">         string(script),</span><br><span class=\"line\">         attributes,</span><br><span class=\"line\">         withFilter,</span><br><span class=\"line\">         withScriptIOSchema(</span><br><span class=\"line\">           ctx, inRowFormat, recordWriter, outRowFormat, recordReader, schemaLess))</span><br><span class=\"line\"></span><br><span class=\"line\">     case SqlBaseParser.SELECT &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Regular select</span><br><span class=\"line\">&#x2F;&#x2F; 解析 lateral 视图(lateral view UDTF as...),where 语句,groupBy,having,distinct 等</span><br><span class=\"line\">       &#x2F;&#x2F; Add lateral views.</span><br><span class=\"line\">       val withLateralView &#x3D; ctx.lateralView.asScala.foldLeft(relation)(withGenerate)</span><br><span class=\"line\">       &#x2F;&#x2F; Add where.</span><br><span class=\"line\">       val withFilter &#x3D; withLateralView.optionalMap(where)(filter)</span><br><span class=\"line\">       &#x2F;&#x2F; Add aggregation or a project.</span><br><span class=\"line\">       val namedExpressions &#x3D; expressions.map &#123;</span><br><span class=\"line\">         case e: NamedExpression &#x3D;&gt; e</span><br><span class=\"line\">         case e: Expression &#x3D;&gt; UnresolvedAlias(e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       val withProject &#x3D; if (aggregation !&#x3D; null) &#123;</span><br><span class=\"line\">         withAggregation(aggregation, namedExpressions, withFilter)</span><br><span class=\"line\">       &#125; else if (namedExpressions.nonEmpty) &#123;</span><br><span class=\"line\">         Project(namedExpressions, withFilter)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         withFilter</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Having</span><br><span class=\"line\">       val withHaving &#x3D; withProject.optional(having) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; Note that we add a cast to non-predicate expressions. If the expression itself is</span><br><span class=\"line\">         &#x2F;&#x2F; already boolean, the optimizer will get rid of the unnecessary cast.</span><br><span class=\"line\">         val predicate &#x3D; expression(having) match &#123;</span><br><span class=\"line\">           case p: Predicate &#x3D;&gt; p</span><br><span class=\"line\">           case e &#x3D;&gt; Cast(e, BooleanType)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         Filter(predicate, withProject)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Distinct</span><br><span class=\"line\">       val withDistinct &#x3D; if (setQuantifier() !&#x3D; null &amp;&amp; setQuantifier().DISTINCT() !&#x3D; null) &#123;</span><br><span class=\"line\">         Distinct(withHaving)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         withHaving</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; Window</span><br><span class=\"line\">       val withWindow &#x3D; withDistinct.optionalMap(windows)(withWindows)</span><br><span class=\"line\">       &#x2F;&#x2F; Hint</span><br><span class=\"line\">       hints.asScala.foldRight(withWindow)(withHints)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><p>由 SparkSqlAstBuilder.scala 部分方法的返回值可知：Parser 模块的 SparkSqlParser 和 SparkSqlAstBuilder 将原始的 SQL 文本转换成了 UnResolved(Relation|Attribute|Funcation)等，但是从原始 SQL 文本解析来的Relation|Attribute|Funcation 是否存在如何解析调用均需要绑定和验证，e.g. SQL 文本中的表是 Hive Orc 表还是 JSON 文件，Funcation 代表了哪个函数 builtin 函数还是 UDF，Attribute 来自哪个基础表….  这些工作就是 Analyzer 模块的工作内容   </p>\n<blockquote>\n</blockquote>\n<p>visitTable 方法的返回值是 UnresovedRelation<br>visitStar 方法的返回值是 UnresolvedStar<br>visitColumnReference 方法的返回值是 UnresolvedRegex 或 UnresolvedAttribute<br>visitFunctionCall 方法的返回值是 UnresolvedWindowExpression 或 WindowExpression<br>….</p>"},{"layout":"post","title":"SparkSQL源码解析(2):从 UnsolvedLogicPlan 到 ResolvedLogicPlan","subtitle":"SparkSQL 源码解析","date":"2019-11-28T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n## 概述\nParser 模块负责将原始 SQL 文本解析成 UnResolvedLogicPlan，Anayzer 模块负责将 UnResolvedLogicPlan(Relation|Attribute|Function) 通过 SessionCatalog 信息绑定到真正可操作的实体上(File|Column|MainClass)，即 SolvedLogicPlan，并校验其是否合法。   \n\n<!-- more -->\n\n则关键点在于:   \n\n1. SessionCatalog 存在目的        \n2. 如何绑定，将 UnresolvedLogicPlan 转换为 ResolvedLogicPlan    \n3. 如何校验 ResolvedLogicPlan 合法  \n\n1.Analyzer 对 UnresolvedLogicPlan应用各类规则(Rule),并对各个 UnresolvedLogicPlan节点绑定 CataLog 信息,将 UnresolvedLogicPlan中的UnresolvedRelation 和 UnresolvedAttribute解析成有类型(Typed)的对象,生成解析后的逻辑算子树 SolvedLogicPlan.  \n2.对 UnresolvedLogicPlan 的大部分操作,如绑定解析优化等,主要方法都是基于规则(Rule)的,然后通过模式匹配对UnresolvedLogicPlan进行相应处理.  \n3.Catalog信息在Analyzer的构造函数中.  \n\n## 源码解析  \n\n调用栈  \n->SparkSession#sql  \n　->DataSet#ofRows  \n　　->QueryExecution#assertAnalyzed   \n　　　->QueryExecution#analyzed (lazy)  \n　　　　->Analyzer#executeAndCheck  \n[Anayzer.scala]\n\n```\ndef executeAndCheck(plan: LogicalPlan): LogicalPlan = {\n    // 第一步: 绑定\n    val analyzed = execute(plan)\n    try {\n      // 第二步: 校验\n      checkAnalysis(analyzed)\n      // 将 LogicPlan 的 AnalysisBarrier 装箱去掉(如果存在)\n      EliminateBarriers(analyzed)\n    } catch {\n      case e: AnalysisException =>\n        val ae = new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed))\n        ae.setStackTrace(e.getStackTrace)\n        throw ae\n    }\n  }\n```\n### SessionCatalog\n\nSessionCatalog 是 SparkSession 索引维护的信息库，SessionCatalog 一方面可以管理 SparkSession 创建的临时视图和 UDF，一方面可以作为外部元数据库的代理以便 SparkSession 获取外部源数据库信息(e.g. HiveSessionCatalog 操作 Hive metastore)。    \n\n|成员变量 | 数据类型|功能 | Catalog 类型|\n|:------------- |:---------------|:---------------|:---------------|\n|tableRelationCache|Cache[QualifiedTableName, LogicalPlan]|表全限定名与逻辑计划的映射|SessionCatalog|\n|tempViews|HashMap[String, LogicalPlan]|临时视图名与逻辑计划的映射|SessionCatalog|\n|globalTempViewManager|GlobalTempViewManager|全局视图管理|SessionCatalog|\n|externalCatalog|ExternalCatalog|外部元数据库的代理,从获得连接到库表分区函数等信息获取接口|SessionCatalog|\n|functionRegistry|FunctionRegistry|函数(builtin,UDF)操作代理|SessionCatalog|\n|parser|ParserInterface|SQL 文本解析 Parser |SessionCatalog |\n|functionResourceLoader|FunctionResourceLoader|加载函数主类资源加载器|SessionCatalog|\n|metastoreCatalog|HiveMetastoreCatalog|HiveExternalCatalog 的旧接口,deprecated|HiveSessionCatalog|\n\n引用一个通过表名绑定逻辑计划的函数: 通过表所属数据库名在全局视图，外部元数据库和临时视图内搜索以绑定逻辑计划  \n[SessionCatalog.scala]\n\n```\ndef lookupRelation(name: TableIdentifier): LogicalPlan = {\n    synchronized {\n      val db = formatDatabaseName(name.database.getOrElse(currentDb))\n      val table = formatTableName(name.table)\n      // 全局视图\n      if (db == globalTempViewManager.database) {\n        globalTempViewManager.get(table).map { viewDef =>\n          SubqueryAlias(table, viewDef)\n        }.getOrElse(throw new NoSuchTableException(db, table))\n      } else if (name.database.isDefined || !tempViews.contains(table)) {\n      // 外部元数据库\n        val metadata = externalCatalog.getTable(db, table)\n        if (metadata.tableType == CatalogTableType.VIEW) {\n          val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))\n          val child = View(\n            desc = metadata,\n            output = metadata.schema.toAttributes,\n            child = parser.parsePlan(viewText))\n          SubqueryAlias(table, child)\n        } else {\n          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))\n        }\n      } else {\n      // 临时视图\n        SubqueryAlias(table, tempViews(table))\n      }\n    }\n  }\n```\n\n### 绑定\n\nAnayzer 模块对 UnResolvedLogicPlan 应用多批规则库，其中主要是 Resolution(解析)规则库和 Substitution(转换)规则库，直到每批规则库达到了指定的迭代次数或者规则库无法再优化逻辑计划为止。  \n\n调用栈  \nAnalyzer#executeAndCheck  \n　->Analyzer#execute  \n　　->Analyzer#executeSameContext  \n　　　->RuleExecutor#execute  \n[RuleExecutor.scala]\n\n```\n  /**\n   * 规则批次间和批次内都是串行执行\n   * 逻辑计划在此处被 Analyzed 之后就替换了(result/curPlan和返回值)\n   */\n  def execute(plan: TreeType): TreeType = {\n    var curPlan = plan\n    batches.foreach { batch =>\n      val batchStartPlan = curPlan\n      var iteration = 1\n      var lastPlan = curPlan\t//tempPlan\n      var continue = true\n      // foldLeft 依次执行规则 batch 中的所有规则,直到达到了规则的最大执行次数或者逻辑算子树不再变化\n      while (continue) {\n        curPlan = batch.rules.foldLeft(curPlan) { \n          case (plan, rule) =>\n          // 在逻辑计划上应用规则处理\n            val result = rule(plan) //rule.apply(plan)\n            if (!result.fastEquals(plan)) {\n              logTrace(\n                s\"\"\"\n                  |=== Applying Rule ${rule.ruleName} ===\n                  |${sideBySide(plan.treeString, result.treeString).mkString(\"\\n\")}\n                \"\"\".stripMargin)\n            }\n            queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime)\n            queryExecutionMetrics.incNumExecution(rule.ruleName)\n            ....\n             result\n        }\n        iteration += 1\n         // 达到了该规则批策略的最大迭代次数\n        if (iteration > batch.strategy.maxIterations) {\n          ...\n          continue = false\n        }\n        // 这批规则已经不能再优化逻辑计划了\n        if (curPlan.fastEquals(lastPlan)) {\n          logTrace(\n            s\"Fixed point reached for batch ${batch.name} after ${iteration - 1} iterations.\")\n          continue = false\n        }\n        lastPlan = curPlan\n      }\n      ...\n    }\n    curPlan\n  }\n```\n重点是其中的 rule(plan)方法，即 Rule#apply(plan)。各规则子类依据自身的功能解析转换 TreeNode，包括绑定 UnResolvedLogicPlan 为 ResolvedLogicPlan   \n [Rule.scala]\n  \n```\nabstract class Rule[TreeType <: TreeNode[_]] extends Logging {\n  val ruleName: String = {\n    val className = getClass.getName\n    if (className endsWith \"$\") className.dropRight(1) else className\n  }\n  def apply(plan: TreeType): TreeType\n}\n```\n规则的具体功能列表参考\"附录：规则功能列表\"   \n\n### 校验\n\n调用栈  \nAnalyze#executeAndCheck  \n　->CheckAndAnalysis#checkAnalysis  \n\n检查 ResolvedLogicPlan 中不合乎语法规范的错误，使用后序遍历(先子节点后当前节点)的方式尽可能抛出最先导致失败的错误      \n[CheckAndAnaysis.scala]  \n\n```\ndef checkAnalysis(plan: LogicalPlan): Unit = {\n    plan.foreachUp {\n      case u: UnresolvedRelation =>\n        u.failAnalysis(s\"Table or view not found: ${u.tableIdentifier}\")\n\n      case operator: LogicalPlan =>\n        operator transformExpressionsUp {\n          case a: Attribute if !a.resolved =>\n            val from = operator.inputSet.map(_.qualifiedName).mkString(\", \")\n            a.failAnalysis(s\"cannot resolve '${a.sql}' given input columns: [$from]\")\n\n          case e: Expression if e.checkInputDataTypes().isFailure =>\n            e.checkInputDataTypes() match {\n              case TypeCheckResult.TypeCheckFailure(message) =>\n                e.failAnalysis(\n                  s\"cannot resolve '${e.sql}' due to data type mismatch: $message\")\n            }\n\n          case c: Cast if !c.resolved =>\n            failAnalysis(\n              s\"invalid cast from ${c.child.dataType.simpleString} to ${c.dataType.simpleString}\")\n\n          case g: Grouping =>\n            failAnalysis(\"grouping() can only be used with GroupingSets/Cube/Rollup\")\n          case g: GroupingID =>\n            failAnalysis(\"grouping_id() can only be used with GroupingSets/Cube/Rollup\")\n\n          case w @ WindowExpression(AggregateExpression(_, _, true, _), _) =>\n            failAnalysis(s\"Distinct window functions are not supported: $w\")\n\n          case w @ WindowExpression(_: OffsetWindowFunction,\n            WindowSpecDefinition(_, order, frame: SpecifiedWindowFrame))\n             if order.isEmpty || !frame.isOffset =>\n            failAnalysis(\"An offset window function can only be evaluated in an ordered \" +\n              s\"row-based window frame with a single offset: $w\")\n\n          case w @ WindowExpression(e, s) =>\n            // Only allow window functions with an aggregate expression or an offset window\n            // function.\n            e match {\n              case _: AggregateExpression | _: OffsetWindowFunction | _: AggregateWindowFunction =>\n                w\n              case _ =>\n                failAnalysis(s\"Expression '$e' not supported within a window function.\")\n            }\n\n          case s: SubqueryExpression =>\n            checkSubqueryExpression(operator, s)\n            s\n        }\n\n        operator match {\n          case etw: EventTimeWatermark =>\n            etw.eventTime.dataType match {\n              case s: StructType\n                if s.find(_.name == \"end\").map(_.dataType) == Some(TimestampType) =>\n              case _: TimestampType =>\n              case _ =>\n                failAnalysis(\n                  s\"Event time must be defined on a window or a timestamp, but \" +\n                  s\"${etw.eventTime.name} is of type ${etw.eventTime.dataType.simpleString}\")\n            }\n          case f: Filter if f.condition.dataType != BooleanType =>\n            failAnalysis(\n              s\"filter expression '${f.condition.sql}' \" +\n                s\"of type ${f.condition.dataType.simpleString} is not a boolean.\")\n\n          case Filter(condition, _) if hasNullAwarePredicateWithinNot(condition) =>\n            failAnalysis(\"Null-aware predicate sub-queries cannot be used in nested \" +\n              s\"conditions: $condition\")\n\n          case j @ Join(_, _, _, Some(condition)) if condition.dataType != BooleanType =>\n            failAnalysis(\n              s\"join condition '${condition.sql}' \" +\n                s\"of type ${condition.dataType.simpleString} is not a boolean.\")\n\t..........\n\t// 自定义的规则检查在内置规则检查之后才能开始\n    extendedCheckRules.foreach(_(plan))\n    plan.foreachUp {\n      case AnalysisBarrier(child) if !child.resolved => checkAnalysis(child)\n      case o if !o.resolved => failAnalysis(s\"unresolved operator ${o.simpleString}\")\n      case _ =>\n    }\n  }\n```\n\n## TODO\n\nUnResolvedLogicPlan 在绑定了 Catalog 之后可以转换为 RDD 模式来执行了，但是由于提交的 SQL 质量参差不齐，按照 ResolvedLogicPlan 按部就班的执行会导致代价/效率差距很大 且 要求用户对执行引擎的执行模式很了解并熟悉 SQL 优化手段才能写出效率高执行快的 SQL。所以为了尽可能忽略用户的代码质量，对SQL 优化的熟悉程度，SparkSQL 都需要以很高的效率执行，SparkSQL 在后续阶段需要对 UnResolvedLogicPlan 进行优化，即 Rule-Based Optimizer，也称为 RBO。    \n\n## 附录：规则功能列表\n\n规则库 Batch: 在 UnResolvedLogicPlan 上应用规则库 rules 的执行策略 strategy。 \n[Batch.scala]  \n\n```\ncase class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)\n```\nStrategy 表示规则批的最大迭代次数，有两个子类: Once 表明只需应用一次规则库即可；FixedPoint 表示最大可以应用规则库 maxIterations 次，如果应用中途无法再优化逻辑计划则跳出。   \n\n```\nabstract class Strategy { def maxIterations: Int }\ncase object Once extends Strategy { val maxIterations = 1 }\ncase class FixedPoint(maxIterations: Int) extends Strategy\n```\n规则 Rule 的子类有很多,在 Analyze.scala 中使用的所有规则如下  \n[Analyzer.scala]\n\n```\n  lazy val batches: Seq[Batch] = Seq(\n    Batch(\"Hints\", fixedPoint,\n      new ResolveHints.ResolveBroadcastHints(conf),\n      ResolveHints.RemoveAllHints),\n    Batch(\"Simple Sanity Check\", Once,\n      LookupFunctions),\n    Batch(\"Substitution\", fixedPoint,\n      CTESubstitution,\n      WindowsSubstitution,\n      EliminateUnions,\n      new SubstituteUnresolvedOrdinals(conf)),\n    Batch(\"Resolution\", fixedPoint,\n      ResolveTableValuedFunctions ::\n      ResolveRelations ::\n      ResolveReferences ::\n      ResolveCreateNamedStruct ::\n      ResolveDeserializer ::\n      ResolveNewInstance ::\n      ResolveUpCast ::\n      ResolveGroupingAnalytics ::\n      ResolvePivot ::\n      ResolveOrdinalInOrderByAndGroupBy ::\n      ResolveAggAliasInGroupBy ::\n      ResolveMissingReferences ::\n      ExtractGenerator ::\n      ResolveGenerate ::\n      ResolveFunctions ::\n      ResolveAliases ::\n      ResolveSubquery ::\n      ResolveSubqueryColumnAliases ::\n      ResolveWindowOrder ::\n      ResolveWindowFrame ::\n      ResolveNaturalAndUsingJoin ::\n      ExtractWindowExpressions ::\n      GlobalAggregates ::\n      ResolveAggregateFunctions ::\n      TimeWindowing ::\n      ResolveInlineTables(conf) ::\n      ResolveTimeZone(conf) ::\n      TypeCoercion.typeCoercionRules(conf) ++\n      extendedResolutionRules : _*),\n    Batch(\"Post-Hoc Resolution\", Once, postHocResolutionRules: _*),\n    Batch(\"View\", Once,\n      AliasViewChild(conf)),\n    Batch(\"Nondeterministic\", Once,\n      PullOutNondeterministic),\n    Batch(\"UDF\", Once,\n      HandleNullInputsForUDF),\n    Batch(\"FixNullability\", Once,\n      FixNullability),\n    Batch(\"Subquery\", Once,\n      UpdateOuterReferences),\n    Batch(\"Cleanup\", fixedPoint,\n      CleanupAliases)\n  )\n```\n\n其中最重要的规则库是Substitution(替换)和Resolution(解析绑定)。这些规则库之间及规则库之内都是有序的，使用规则时也是串行执行的。打乱了顺序可能有转换不充分等未知的错误  \n下表主要参照 org.apache.spark.sql.catalyst.parser.SqlBase.g4 文件和源码，带问号的为不确定的。\n\n|  规则批次   |规则  | 功能| 备注|\n|  ----  | ----  |----  |----  |\n| Hints |  ResolveBroadcastHints|广播|mapjoin|\n| Hints | RemoveAllHints |删除无效标识符||\n|Simple Sanity Check| LookupFunctions|函数存在性检验||\n| Substitution |CTESubstitution|with,合并计划|with a as something select * from a |\n| Substitution |WindowsSubstitution|窗口函数|row_number() over (partitioned by a sort by b desc) |\n| Substitution |EliminateUnions|union的计划只有一个时删除 union||\n| Resolution |ResolveTableValuedFunctions|解析可作为表的函数|range函数|\n| Resolution |ResolveRelations|表绑定catalog中的逻辑计划|对应createXXXView接口|\n| Resolution |ResolveReferences|展开星号绑定列|select a.* from|\n| Resolution |ResolveCreateNamedStruct|解析结构体构造方法|?,没见过hql中使用结构体|\n| Resolution |ResolveDeserializer|解析反序列化类|decoder|\n| Resolution |ResolveNewInstance|解析创建实例|encoder|\n| Resolution |ResolveUpCast|解析类型转换|cast,在丢失精度时抛异常|\n| Resolution |ResolveGroupingAnalytics|解析rollup多维度分析|group by a,b,c with rollup 等价于 group by a,b,c grouping sets((a,b,c),(a,b),(a),()) |\n| Resolution |ResolvePivot|行转列|pivot 接口|\n| Resolution |ResolveOrdinalInOrderByAndGroupBy|解析order/sort/group by语句的下标数字||\n| Resolution |ResolveAggAliasInGroupBy|解析聚合时的表达式|group by case when then x else y end|\n| Resolution |ResolveMissingReferences|解析在排序时不存在的列,加上但隐藏该列|select a from ... order by b|\n| Resolution |ExtractGenerator|解析UDTF生成器|select explode(xx) from ..|\n| Resolution |ResolveGenerate|?|?|\n| Resolution |ResolveFunctions|解析函数为表达式||\n| Resolution |ResolveAliases|解析生成别名表达式|?|\n| Resolution |ResolveSubquery|解析子查询||\n| Resolution |ResolveSubqueryColumnAliases|解析子查询列别名||\n| Resolution |ResolveWindowOrder|解析窗口函数中的排序| over partitioned by .. order by..|\n| Resolution |ResolveWindowFrame|解析检验窗口函数||\n| Resolution |ResolveNaturalAndUsingJoin|通过输出列解析自然连接||\n| Resolution |ExtractWindowExpressions|提取窗口函数表达式||\n| Resolution |GlobalAggregates|解析全局聚合|select max(a) from tbl|\n| Resolution |ResolveAggregateFunctions|解析不在聚合中的聚合函数|having/order by |\n| Resolution |TimeWindowing|解析滑动时间窗口||\n| Resolution |ResolveInlineTables|解析内联表为LocalRelation| select * fom values(..),(..) as (columns)|\n| Resolution |ResolveTimeZone|解析时区表达式||\n| Resolution |TypeCoercion.typeCoercionRules|强制转换为兼容类型|在比较和 union 时使用,不损失精度|\n| Resolution |extendedResolutionRules|拓展规则,空集合|?|\n|Post-Hoc Resolution|postHocResolutionRules|Resolution 规则后执行的规则.空集合||\n|View|AliasViewChild|视图的分析规则||\n|Nondeterministic|PullOutNondeterministic|提取非确定性表达式,放到child 中|?|\n|UDF|HandleNullInputsForUDF|对UDF增加基本数据类型null处理(空输入则空输出)||\n|FixNullability|FixNullability|通过 child 字段的 Nullablity 修复父逻辑计划字段的 Nullablity||\n|Subquery|UpdateOuterReferences|聚合表达式下推|?|\n|Cleanup|CleanupAliases|删除不需要的别名||\n\n### with \n\nwith a  as ... select * from a 语法是 SQL 语句中的一个特殊 case，with 子句打乱了 SQL 解析的通用模式，无法从上至下从左至右进行解析，只能将整个语句解析为两个相对独立的逻辑算子树，然后通过别名将 with 语句的逻辑算子树加入到主体逻辑算子树的下面  \n\n[WindowsSubstitution.scala] \n\n```\n  object WindowsSubstitution extends Rule[LogicalPlan] {\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      //使用多层 match case 来匹配拿到 window 函数,\n      case WithWindowDefinition(windowDefinitions, child) =>\n        child.transform {\n          case p => p.transformExpressions {\n            case UnresolvedWindowExpression(c, WindowSpecReference(windowName)) =>\n              val errorMessage =\n                s\"Window specification $windowName is not defined in the WINDOW clause.\"\n              val windowSpecDefinition =\n                windowDefinitions.getOrElse(windowName, failAnalysis(errorMessage))\n              WindowExpression(c, windowSpecDefinition)\n          }\n        }\n    } \n```\n### relation \n\n绑定关系  \n[ResolveRelations.scala]\n\n```\n  object ResolveRelations extends Rule[LogicalPlan] {\n   // 后序遍历尝试绑定每个匹配的节点到 ResolvedLogicPlan\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      case i @ InsertIntoTable(u: UnresolvedRelation, parts, child, _, _) if child.resolved =>\n       // lookupTableFromCatalog(u) 从全局视图>外接元数据库>临时视图中绑定实体  \n       // EliminateSubqueryAliases 去掉别名将子查询查询计划剥离出来\n        EliminateSubqueryAliases(lookupTableFromCatalog(u)) match {\n          case v: View =>\n            u.failAnalysis(s\"Inserting into a view is not allowed. View: ${v.desc.identifier}.\")\n          case other => i.copy(table = other)\n        }\n      case u: UnresolvedRelation => resolveRelation(u)\t\n    }\n    先看第一个 case,如果是 insert into 句法的话,借助 catalog的<tableName,LogicPlan>缓存绑定表为已解析过的逻辑计划\n    private def lookupTableFromCatalog(\n        u: UnresolvedRelation,\n        defaultDatabase: Option[String] = None): LogicalPlan = {\n      val tableIdentWithDb = u.tableIdentifier.copy(\n        database = u.tableIdentifier.database.orElse(defaultDatabase))\n      try {\n        catalog.lookupRelation(tableIdentWithDb)\n      } catch {\n\t\t....        \n          }\n    }\n  }\n [SessionCatalog.scala] \n  def lookupRelation(name: TableIdentifier): LogicalPlan = {\n    synchronized {\n      val db = formatDatabaseName(name.database.getOrElse(currentDb))\n      val table = formatTableName(name.table)\n      //从全局临时视图,外接元数据库,临时视图中查找该表的逻辑计划\n      if (db == globalTempViewManager.database) {\n        globalTempViewManager.get(table).map { viewDef =>\n          SubqueryAlias(table, viewDef)\n        }.getOrElse(throw new NoSuchTableException(db, table))\n      } else if (name.database.isDefined || !tempViews.contains(table)) {\n        val metadata = externalCatalog.getTable(db, table)\n        if (metadata.tableType == CatalogTableType.VIEW) {\n          val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))\n          val child = View(\n            desc = metadata,\n            output = metadata.schema.toAttributes,\n            child = parser.parsePlan(viewText))\n          SubqueryAlias(table, child)\n        } else {\n          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))\n        }\n      } else {\n        SubqueryAlias(table, tempViews(table))\n      }\n    }\n  }\n  再看ResolveRelations.apply方法的第二个 case u:UnresolvedRelation => resolveRelation(u),所以首先会进入下面方法的第一个 case, 即如果不是直接在文件上运行(from json.$path)的话,先从 Catalog 中绑定逻辑计划\n  [ResolveRelation.scala]\n    def resolveRelation(plan: LogicalPlan): LogicalPlan = plan match {\n      case u: UnresolvedRelation if !isRunningDirectlyOnFiles(u.tableIdentifier) =>\n        val defaultDatabase = AnalysisContext.get.defaultDatabase\n        val foundRelation = lookupTableFromCatalog(u, defaultDatabase)\n        resolveRelation(foundRelation)\n      case view @ View(desc, _, child) if !child.resolved =>\n        // Resolve all the UnresolvedRelations and Views in the child.\n        val newChild = AnalysisContext.withAnalysisContext(desc.viewDefaultDatabase) {\n          if (AnalysisContext.get.nestedViewDepth > conf.maxNestedViewDepth) {\n            view.failAnalysis(s\"The depth of view ${view.desc.identifier} exceeds the maximum \" +\n              s\"view resolution depth (${conf.maxNestedViewDepth}). Analysis is aborted to \" +\n              s\"avoid errors. Increase the value of ${SQLConf.MAX_NESTED_VIEW_DEPTH.key} to work \" +\n              \"around this.\")\n          }\n          executeSameContext(child)\n        }\n        view.copy(child = newChild)\n      case p @ SubqueryAlias(_, view: View) =>\n        val newChild = resolveRelation(view)\n        p.copy(child = newChild)\n      case _ => plan\n    }\n```\n\n### LookupFunctions \n\n函数存在性校验  \n[LookupFuncations.scala]\n\n```\nobject LookupFunctions extends Rule[LogicalPlan] {\n    override def apply(plan: LogicalPlan): LogicalPlan = plan.transformAllExpressions {\n     // 递归检测使用的函数在 catalog 中是否存在\n      case f: UnresolvedFunction if !catalog.functionExists(f.name) =>\n        withPosition(f) {\n          throw new NoSuchFunctionException(f.name.database.getOrElse(\"default\"), f.name.funcName)\n        }\n    }\n  }\n```\n\n[SessionCatalog.scala]\n```\ndef functionExists(name: FunctionIdentifier): Boolean = {\n    val db = formatDatabaseName(name.database.getOrElse(getCurrentDatabase))\n    requireDbExists(db)\n    // 使用 FuncationRegistry 和外接元数据库检测函数是否存在\n    functionRegistry.functionExists(name) ||\n      externalCatalog.functionExists(db, name.funcName)\n  }\n```\n\n### ResolveFunction  \n\n将函数替换为具体的表达式  \n[ResolveFunction.scala]\n\n```\nobject ResolveFunctions extends Rule[LogicalPlan] {\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      case q: LogicalPlan =>\n        q transformExpressions {\n          case u if !u.childrenResolved => u // Skip until children are resolved.\n          case u: UnresolvedAttribute if resolver(u.name, VirtualColumn.hiveGroupingIdName) =>\n            withPosition(u) {\n              Alias(GroupingID(Nil), VirtualColumn.hiveGroupingIdName)()\n            }\n          case u @ UnresolvedGenerator(name, children) =>\n            withPosition(u) {\n              catalog.lookupFunction(name, children) match {\n                case generator: Generator => generator\n                case other =>\n                  failAnalysis(s\"$name is expected to be a generator. However, \" +\n                    s\"its class is ${other.getClass.getCanonicalName}, which is not a generator.\")\n              }\n            }\n          case u @ UnresolvedFunction(funcId, children, isDistinct) =>\n            withPosition(u) {\n              // 使用 catalog 寻找函数\n              catalog.lookupFunction(funcId, children) match {\n                case wf: AggregateWindowFunction =>\n                  if (isDistinct) {\n                    failAnalysis(s\"${wf.prettyName} does not support the modifier DISTINCT\")\n                  } else {\n                    wf\n                  }\n                case agg: AggregateFunction => AggregateExpression(agg, Complete, isDistinct)\n                case other =>\n                  if (isDistinct) {\n                    failAnalysis(s\"${other.prettyName} does not support the modifier DISTINCT\")\n                  } else {\n                    other\n                  }\n              }\n            }\n        }\n    }\n  }\n```\n使用 SessionCatalog 优先从 FunctionRegistry 中寻找，然后从外接元数据库寻找并加载资源注册到 FunctionRegistry，有些像双亲委派模型，保证 builtin 的函数不被 UDF覆盖。   \n[SessionCatalog.scala]  \n  \n```\ndef lookupFunction(\n      name: FunctionIdentifier,\n      children: Seq[Expression]): Expression = synchronized {\n    if (name.database.isEmpty && functionRegistry.functionExists(name)) {\n      // This function has been already loaded into the function registry.\n      return functionRegistry.lookupFunction(name, children)\n    }\n\n    // If the name itself is not qualified, add the current database to it.\n    val database = formatDatabaseName(name.database.getOrElse(getCurrentDatabase))\n    val qualifiedName = name.copy(database = Some(database))\n\n    if (functionRegistry.functionExists(qualifiedName)) {\n      return functionRegistry.lookupFunction(qualifiedName, children)\n    }\n    val catalogFunction = try {\n      externalCatalog.getFunction(database, name.funcName)\n    } catch {\n      case _: AnalysisException => failFunctionLookup(name)\n      case _: NoSuchPermanentFunctionException => failFunctionLookup(name)\n    }\n    loadFunctionResources(catalogFunction.resources)\n    registerFunction(catalogFunction.copy(identifier = qualifiedName), overrideIfExists = false)\n    // Now, we need to create the Expression.\n    functionRegistry.lookupFunction(qualifiedName, children)\n  }\n```\n","source":"_posts/2019-11-29-SparkSQL 源码解析(2):从 UnresolvedLogicPlan 到 ResolvedLogicPlan.md","raw":"---\nlayout:     post\ntitle:     SparkSQL源码解析(2):从 UnsolvedLogicPlan 到 ResolvedLogicPlan\nsubtitle:   SparkSQL 源码解析\ndate:       2019-11-29\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0  \n    - Spark SQL  \n    - 源码解析  \n---\n\n## 概述\nParser 模块负责将原始 SQL 文本解析成 UnResolvedLogicPlan，Anayzer 模块负责将 UnResolvedLogicPlan(Relation|Attribute|Function) 通过 SessionCatalog 信息绑定到真正可操作的实体上(File|Column|MainClass)，即 SolvedLogicPlan，并校验其是否合法。   \n\n<!-- more -->\n\n则关键点在于:   \n\n1. SessionCatalog 存在目的        \n2. 如何绑定，将 UnresolvedLogicPlan 转换为 ResolvedLogicPlan    \n3. 如何校验 ResolvedLogicPlan 合法  \n\n1.Analyzer 对 UnresolvedLogicPlan应用各类规则(Rule),并对各个 UnresolvedLogicPlan节点绑定 CataLog 信息,将 UnresolvedLogicPlan中的UnresolvedRelation 和 UnresolvedAttribute解析成有类型(Typed)的对象,生成解析后的逻辑算子树 SolvedLogicPlan.  \n2.对 UnresolvedLogicPlan 的大部分操作,如绑定解析优化等,主要方法都是基于规则(Rule)的,然后通过模式匹配对UnresolvedLogicPlan进行相应处理.  \n3.Catalog信息在Analyzer的构造函数中.  \n\n## 源码解析  \n\n调用栈  \n->SparkSession#sql  \n　->DataSet#ofRows  \n　　->QueryExecution#assertAnalyzed   \n　　　->QueryExecution#analyzed (lazy)  \n　　　　->Analyzer#executeAndCheck  \n[Anayzer.scala]\n\n```\ndef executeAndCheck(plan: LogicalPlan): LogicalPlan = {\n    // 第一步: 绑定\n    val analyzed = execute(plan)\n    try {\n      // 第二步: 校验\n      checkAnalysis(analyzed)\n      // 将 LogicPlan 的 AnalysisBarrier 装箱去掉(如果存在)\n      EliminateBarriers(analyzed)\n    } catch {\n      case e: AnalysisException =>\n        val ae = new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed))\n        ae.setStackTrace(e.getStackTrace)\n        throw ae\n    }\n  }\n```\n### SessionCatalog\n\nSessionCatalog 是 SparkSession 索引维护的信息库，SessionCatalog 一方面可以管理 SparkSession 创建的临时视图和 UDF，一方面可以作为外部元数据库的代理以便 SparkSession 获取外部源数据库信息(e.g. HiveSessionCatalog 操作 Hive metastore)。    \n\n|成员变量 | 数据类型|功能 | Catalog 类型|\n|:------------- |:---------------|:---------------|:---------------|\n|tableRelationCache|Cache[QualifiedTableName, LogicalPlan]|表全限定名与逻辑计划的映射|SessionCatalog|\n|tempViews|HashMap[String, LogicalPlan]|临时视图名与逻辑计划的映射|SessionCatalog|\n|globalTempViewManager|GlobalTempViewManager|全局视图管理|SessionCatalog|\n|externalCatalog|ExternalCatalog|外部元数据库的代理,从获得连接到库表分区函数等信息获取接口|SessionCatalog|\n|functionRegistry|FunctionRegistry|函数(builtin,UDF)操作代理|SessionCatalog|\n|parser|ParserInterface|SQL 文本解析 Parser |SessionCatalog |\n|functionResourceLoader|FunctionResourceLoader|加载函数主类资源加载器|SessionCatalog|\n|metastoreCatalog|HiveMetastoreCatalog|HiveExternalCatalog 的旧接口,deprecated|HiveSessionCatalog|\n\n引用一个通过表名绑定逻辑计划的函数: 通过表所属数据库名在全局视图，外部元数据库和临时视图内搜索以绑定逻辑计划  \n[SessionCatalog.scala]\n\n```\ndef lookupRelation(name: TableIdentifier): LogicalPlan = {\n    synchronized {\n      val db = formatDatabaseName(name.database.getOrElse(currentDb))\n      val table = formatTableName(name.table)\n      // 全局视图\n      if (db == globalTempViewManager.database) {\n        globalTempViewManager.get(table).map { viewDef =>\n          SubqueryAlias(table, viewDef)\n        }.getOrElse(throw new NoSuchTableException(db, table))\n      } else if (name.database.isDefined || !tempViews.contains(table)) {\n      // 外部元数据库\n        val metadata = externalCatalog.getTable(db, table)\n        if (metadata.tableType == CatalogTableType.VIEW) {\n          val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))\n          val child = View(\n            desc = metadata,\n            output = metadata.schema.toAttributes,\n            child = parser.parsePlan(viewText))\n          SubqueryAlias(table, child)\n        } else {\n          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))\n        }\n      } else {\n      // 临时视图\n        SubqueryAlias(table, tempViews(table))\n      }\n    }\n  }\n```\n\n### 绑定\n\nAnayzer 模块对 UnResolvedLogicPlan 应用多批规则库，其中主要是 Resolution(解析)规则库和 Substitution(转换)规则库，直到每批规则库达到了指定的迭代次数或者规则库无法再优化逻辑计划为止。  \n\n调用栈  \nAnalyzer#executeAndCheck  \n　->Analyzer#execute  \n　　->Analyzer#executeSameContext  \n　　　->RuleExecutor#execute  \n[RuleExecutor.scala]\n\n```\n  /**\n   * 规则批次间和批次内都是串行执行\n   * 逻辑计划在此处被 Analyzed 之后就替换了(result/curPlan和返回值)\n   */\n  def execute(plan: TreeType): TreeType = {\n    var curPlan = plan\n    batches.foreach { batch =>\n      val batchStartPlan = curPlan\n      var iteration = 1\n      var lastPlan = curPlan\t//tempPlan\n      var continue = true\n      // foldLeft 依次执行规则 batch 中的所有规则,直到达到了规则的最大执行次数或者逻辑算子树不再变化\n      while (continue) {\n        curPlan = batch.rules.foldLeft(curPlan) { \n          case (plan, rule) =>\n          // 在逻辑计划上应用规则处理\n            val result = rule(plan) //rule.apply(plan)\n            if (!result.fastEquals(plan)) {\n              logTrace(\n                s\"\"\"\n                  |=== Applying Rule ${rule.ruleName} ===\n                  |${sideBySide(plan.treeString, result.treeString).mkString(\"\\n\")}\n                \"\"\".stripMargin)\n            }\n            queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime)\n            queryExecutionMetrics.incNumExecution(rule.ruleName)\n            ....\n             result\n        }\n        iteration += 1\n         // 达到了该规则批策略的最大迭代次数\n        if (iteration > batch.strategy.maxIterations) {\n          ...\n          continue = false\n        }\n        // 这批规则已经不能再优化逻辑计划了\n        if (curPlan.fastEquals(lastPlan)) {\n          logTrace(\n            s\"Fixed point reached for batch ${batch.name} after ${iteration - 1} iterations.\")\n          continue = false\n        }\n        lastPlan = curPlan\n      }\n      ...\n    }\n    curPlan\n  }\n```\n重点是其中的 rule(plan)方法，即 Rule#apply(plan)。各规则子类依据自身的功能解析转换 TreeNode，包括绑定 UnResolvedLogicPlan 为 ResolvedLogicPlan   \n [Rule.scala]\n  \n```\nabstract class Rule[TreeType <: TreeNode[_]] extends Logging {\n  val ruleName: String = {\n    val className = getClass.getName\n    if (className endsWith \"$\") className.dropRight(1) else className\n  }\n  def apply(plan: TreeType): TreeType\n}\n```\n规则的具体功能列表参考\"附录：规则功能列表\"   \n\n### 校验\n\n调用栈  \nAnalyze#executeAndCheck  \n　->CheckAndAnalysis#checkAnalysis  \n\n检查 ResolvedLogicPlan 中不合乎语法规范的错误，使用后序遍历(先子节点后当前节点)的方式尽可能抛出最先导致失败的错误      \n[CheckAndAnaysis.scala]  \n\n```\ndef checkAnalysis(plan: LogicalPlan): Unit = {\n    plan.foreachUp {\n      case u: UnresolvedRelation =>\n        u.failAnalysis(s\"Table or view not found: ${u.tableIdentifier}\")\n\n      case operator: LogicalPlan =>\n        operator transformExpressionsUp {\n          case a: Attribute if !a.resolved =>\n            val from = operator.inputSet.map(_.qualifiedName).mkString(\", \")\n            a.failAnalysis(s\"cannot resolve '${a.sql}' given input columns: [$from]\")\n\n          case e: Expression if e.checkInputDataTypes().isFailure =>\n            e.checkInputDataTypes() match {\n              case TypeCheckResult.TypeCheckFailure(message) =>\n                e.failAnalysis(\n                  s\"cannot resolve '${e.sql}' due to data type mismatch: $message\")\n            }\n\n          case c: Cast if !c.resolved =>\n            failAnalysis(\n              s\"invalid cast from ${c.child.dataType.simpleString} to ${c.dataType.simpleString}\")\n\n          case g: Grouping =>\n            failAnalysis(\"grouping() can only be used with GroupingSets/Cube/Rollup\")\n          case g: GroupingID =>\n            failAnalysis(\"grouping_id() can only be used with GroupingSets/Cube/Rollup\")\n\n          case w @ WindowExpression(AggregateExpression(_, _, true, _), _) =>\n            failAnalysis(s\"Distinct window functions are not supported: $w\")\n\n          case w @ WindowExpression(_: OffsetWindowFunction,\n            WindowSpecDefinition(_, order, frame: SpecifiedWindowFrame))\n             if order.isEmpty || !frame.isOffset =>\n            failAnalysis(\"An offset window function can only be evaluated in an ordered \" +\n              s\"row-based window frame with a single offset: $w\")\n\n          case w @ WindowExpression(e, s) =>\n            // Only allow window functions with an aggregate expression or an offset window\n            // function.\n            e match {\n              case _: AggregateExpression | _: OffsetWindowFunction | _: AggregateWindowFunction =>\n                w\n              case _ =>\n                failAnalysis(s\"Expression '$e' not supported within a window function.\")\n            }\n\n          case s: SubqueryExpression =>\n            checkSubqueryExpression(operator, s)\n            s\n        }\n\n        operator match {\n          case etw: EventTimeWatermark =>\n            etw.eventTime.dataType match {\n              case s: StructType\n                if s.find(_.name == \"end\").map(_.dataType) == Some(TimestampType) =>\n              case _: TimestampType =>\n              case _ =>\n                failAnalysis(\n                  s\"Event time must be defined on a window or a timestamp, but \" +\n                  s\"${etw.eventTime.name} is of type ${etw.eventTime.dataType.simpleString}\")\n            }\n          case f: Filter if f.condition.dataType != BooleanType =>\n            failAnalysis(\n              s\"filter expression '${f.condition.sql}' \" +\n                s\"of type ${f.condition.dataType.simpleString} is not a boolean.\")\n\n          case Filter(condition, _) if hasNullAwarePredicateWithinNot(condition) =>\n            failAnalysis(\"Null-aware predicate sub-queries cannot be used in nested \" +\n              s\"conditions: $condition\")\n\n          case j @ Join(_, _, _, Some(condition)) if condition.dataType != BooleanType =>\n            failAnalysis(\n              s\"join condition '${condition.sql}' \" +\n                s\"of type ${condition.dataType.simpleString} is not a boolean.\")\n\t..........\n\t// 自定义的规则检查在内置规则检查之后才能开始\n    extendedCheckRules.foreach(_(plan))\n    plan.foreachUp {\n      case AnalysisBarrier(child) if !child.resolved => checkAnalysis(child)\n      case o if !o.resolved => failAnalysis(s\"unresolved operator ${o.simpleString}\")\n      case _ =>\n    }\n  }\n```\n\n## TODO\n\nUnResolvedLogicPlan 在绑定了 Catalog 之后可以转换为 RDD 模式来执行了，但是由于提交的 SQL 质量参差不齐，按照 ResolvedLogicPlan 按部就班的执行会导致代价/效率差距很大 且 要求用户对执行引擎的执行模式很了解并熟悉 SQL 优化手段才能写出效率高执行快的 SQL。所以为了尽可能忽略用户的代码质量，对SQL 优化的熟悉程度，SparkSQL 都需要以很高的效率执行，SparkSQL 在后续阶段需要对 UnResolvedLogicPlan 进行优化，即 Rule-Based Optimizer，也称为 RBO。    \n\n## 附录：规则功能列表\n\n规则库 Batch: 在 UnResolvedLogicPlan 上应用规则库 rules 的执行策略 strategy。 \n[Batch.scala]  \n\n```\ncase class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)\n```\nStrategy 表示规则批的最大迭代次数，有两个子类: Once 表明只需应用一次规则库即可；FixedPoint 表示最大可以应用规则库 maxIterations 次，如果应用中途无法再优化逻辑计划则跳出。   \n\n```\nabstract class Strategy { def maxIterations: Int }\ncase object Once extends Strategy { val maxIterations = 1 }\ncase class FixedPoint(maxIterations: Int) extends Strategy\n```\n规则 Rule 的子类有很多,在 Analyze.scala 中使用的所有规则如下  \n[Analyzer.scala]\n\n```\n  lazy val batches: Seq[Batch] = Seq(\n    Batch(\"Hints\", fixedPoint,\n      new ResolveHints.ResolveBroadcastHints(conf),\n      ResolveHints.RemoveAllHints),\n    Batch(\"Simple Sanity Check\", Once,\n      LookupFunctions),\n    Batch(\"Substitution\", fixedPoint,\n      CTESubstitution,\n      WindowsSubstitution,\n      EliminateUnions,\n      new SubstituteUnresolvedOrdinals(conf)),\n    Batch(\"Resolution\", fixedPoint,\n      ResolveTableValuedFunctions ::\n      ResolveRelations ::\n      ResolveReferences ::\n      ResolveCreateNamedStruct ::\n      ResolveDeserializer ::\n      ResolveNewInstance ::\n      ResolveUpCast ::\n      ResolveGroupingAnalytics ::\n      ResolvePivot ::\n      ResolveOrdinalInOrderByAndGroupBy ::\n      ResolveAggAliasInGroupBy ::\n      ResolveMissingReferences ::\n      ExtractGenerator ::\n      ResolveGenerate ::\n      ResolveFunctions ::\n      ResolveAliases ::\n      ResolveSubquery ::\n      ResolveSubqueryColumnAliases ::\n      ResolveWindowOrder ::\n      ResolveWindowFrame ::\n      ResolveNaturalAndUsingJoin ::\n      ExtractWindowExpressions ::\n      GlobalAggregates ::\n      ResolveAggregateFunctions ::\n      TimeWindowing ::\n      ResolveInlineTables(conf) ::\n      ResolveTimeZone(conf) ::\n      TypeCoercion.typeCoercionRules(conf) ++\n      extendedResolutionRules : _*),\n    Batch(\"Post-Hoc Resolution\", Once, postHocResolutionRules: _*),\n    Batch(\"View\", Once,\n      AliasViewChild(conf)),\n    Batch(\"Nondeterministic\", Once,\n      PullOutNondeterministic),\n    Batch(\"UDF\", Once,\n      HandleNullInputsForUDF),\n    Batch(\"FixNullability\", Once,\n      FixNullability),\n    Batch(\"Subquery\", Once,\n      UpdateOuterReferences),\n    Batch(\"Cleanup\", fixedPoint,\n      CleanupAliases)\n  )\n```\n\n其中最重要的规则库是Substitution(替换)和Resolution(解析绑定)。这些规则库之间及规则库之内都是有序的，使用规则时也是串行执行的。打乱了顺序可能有转换不充分等未知的错误  \n下表主要参照 org.apache.spark.sql.catalyst.parser.SqlBase.g4 文件和源码，带问号的为不确定的。\n\n|  规则批次   |规则  | 功能| 备注|\n|  ----  | ----  |----  |----  |\n| Hints |  ResolveBroadcastHints|广播|mapjoin|\n| Hints | RemoveAllHints |删除无效标识符||\n|Simple Sanity Check| LookupFunctions|函数存在性检验||\n| Substitution |CTESubstitution|with,合并计划|with a as something select * from a |\n| Substitution |WindowsSubstitution|窗口函数|row_number() over (partitioned by a sort by b desc) |\n| Substitution |EliminateUnions|union的计划只有一个时删除 union||\n| Resolution |ResolveTableValuedFunctions|解析可作为表的函数|range函数|\n| Resolution |ResolveRelations|表绑定catalog中的逻辑计划|对应createXXXView接口|\n| Resolution |ResolveReferences|展开星号绑定列|select a.* from|\n| Resolution |ResolveCreateNamedStruct|解析结构体构造方法|?,没见过hql中使用结构体|\n| Resolution |ResolveDeserializer|解析反序列化类|decoder|\n| Resolution |ResolveNewInstance|解析创建实例|encoder|\n| Resolution |ResolveUpCast|解析类型转换|cast,在丢失精度时抛异常|\n| Resolution |ResolveGroupingAnalytics|解析rollup多维度分析|group by a,b,c with rollup 等价于 group by a,b,c grouping sets((a,b,c),(a,b),(a),()) |\n| Resolution |ResolvePivot|行转列|pivot 接口|\n| Resolution |ResolveOrdinalInOrderByAndGroupBy|解析order/sort/group by语句的下标数字||\n| Resolution |ResolveAggAliasInGroupBy|解析聚合时的表达式|group by case when then x else y end|\n| Resolution |ResolveMissingReferences|解析在排序时不存在的列,加上但隐藏该列|select a from ... order by b|\n| Resolution |ExtractGenerator|解析UDTF生成器|select explode(xx) from ..|\n| Resolution |ResolveGenerate|?|?|\n| Resolution |ResolveFunctions|解析函数为表达式||\n| Resolution |ResolveAliases|解析生成别名表达式|?|\n| Resolution |ResolveSubquery|解析子查询||\n| Resolution |ResolveSubqueryColumnAliases|解析子查询列别名||\n| Resolution |ResolveWindowOrder|解析窗口函数中的排序| over partitioned by .. order by..|\n| Resolution |ResolveWindowFrame|解析检验窗口函数||\n| Resolution |ResolveNaturalAndUsingJoin|通过输出列解析自然连接||\n| Resolution |ExtractWindowExpressions|提取窗口函数表达式||\n| Resolution |GlobalAggregates|解析全局聚合|select max(a) from tbl|\n| Resolution |ResolveAggregateFunctions|解析不在聚合中的聚合函数|having/order by |\n| Resolution |TimeWindowing|解析滑动时间窗口||\n| Resolution |ResolveInlineTables|解析内联表为LocalRelation| select * fom values(..),(..) as (columns)|\n| Resolution |ResolveTimeZone|解析时区表达式||\n| Resolution |TypeCoercion.typeCoercionRules|强制转换为兼容类型|在比较和 union 时使用,不损失精度|\n| Resolution |extendedResolutionRules|拓展规则,空集合|?|\n|Post-Hoc Resolution|postHocResolutionRules|Resolution 规则后执行的规则.空集合||\n|View|AliasViewChild|视图的分析规则||\n|Nondeterministic|PullOutNondeterministic|提取非确定性表达式,放到child 中|?|\n|UDF|HandleNullInputsForUDF|对UDF增加基本数据类型null处理(空输入则空输出)||\n|FixNullability|FixNullability|通过 child 字段的 Nullablity 修复父逻辑计划字段的 Nullablity||\n|Subquery|UpdateOuterReferences|聚合表达式下推|?|\n|Cleanup|CleanupAliases|删除不需要的别名||\n\n### with \n\nwith a  as ... select * from a 语法是 SQL 语句中的一个特殊 case，with 子句打乱了 SQL 解析的通用模式，无法从上至下从左至右进行解析，只能将整个语句解析为两个相对独立的逻辑算子树，然后通过别名将 with 语句的逻辑算子树加入到主体逻辑算子树的下面  \n\n[WindowsSubstitution.scala] \n\n```\n  object WindowsSubstitution extends Rule[LogicalPlan] {\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      //使用多层 match case 来匹配拿到 window 函数,\n      case WithWindowDefinition(windowDefinitions, child) =>\n        child.transform {\n          case p => p.transformExpressions {\n            case UnresolvedWindowExpression(c, WindowSpecReference(windowName)) =>\n              val errorMessage =\n                s\"Window specification $windowName is not defined in the WINDOW clause.\"\n              val windowSpecDefinition =\n                windowDefinitions.getOrElse(windowName, failAnalysis(errorMessage))\n              WindowExpression(c, windowSpecDefinition)\n          }\n        }\n    } \n```\n### relation \n\n绑定关系  \n[ResolveRelations.scala]\n\n```\n  object ResolveRelations extends Rule[LogicalPlan] {\n   // 后序遍历尝试绑定每个匹配的节点到 ResolvedLogicPlan\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      case i @ InsertIntoTable(u: UnresolvedRelation, parts, child, _, _) if child.resolved =>\n       // lookupTableFromCatalog(u) 从全局视图>外接元数据库>临时视图中绑定实体  \n       // EliminateSubqueryAliases 去掉别名将子查询查询计划剥离出来\n        EliminateSubqueryAliases(lookupTableFromCatalog(u)) match {\n          case v: View =>\n            u.failAnalysis(s\"Inserting into a view is not allowed. View: ${v.desc.identifier}.\")\n          case other => i.copy(table = other)\n        }\n      case u: UnresolvedRelation => resolveRelation(u)\t\n    }\n    先看第一个 case,如果是 insert into 句法的话,借助 catalog的<tableName,LogicPlan>缓存绑定表为已解析过的逻辑计划\n    private def lookupTableFromCatalog(\n        u: UnresolvedRelation,\n        defaultDatabase: Option[String] = None): LogicalPlan = {\n      val tableIdentWithDb = u.tableIdentifier.copy(\n        database = u.tableIdentifier.database.orElse(defaultDatabase))\n      try {\n        catalog.lookupRelation(tableIdentWithDb)\n      } catch {\n\t\t....        \n          }\n    }\n  }\n [SessionCatalog.scala] \n  def lookupRelation(name: TableIdentifier): LogicalPlan = {\n    synchronized {\n      val db = formatDatabaseName(name.database.getOrElse(currentDb))\n      val table = formatTableName(name.table)\n      //从全局临时视图,外接元数据库,临时视图中查找该表的逻辑计划\n      if (db == globalTempViewManager.database) {\n        globalTempViewManager.get(table).map { viewDef =>\n          SubqueryAlias(table, viewDef)\n        }.getOrElse(throw new NoSuchTableException(db, table))\n      } else if (name.database.isDefined || !tempViews.contains(table)) {\n        val metadata = externalCatalog.getTable(db, table)\n        if (metadata.tableType == CatalogTableType.VIEW) {\n          val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))\n          val child = View(\n            desc = metadata,\n            output = metadata.schema.toAttributes,\n            child = parser.parsePlan(viewText))\n          SubqueryAlias(table, child)\n        } else {\n          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))\n        }\n      } else {\n        SubqueryAlias(table, tempViews(table))\n      }\n    }\n  }\n  再看ResolveRelations.apply方法的第二个 case u:UnresolvedRelation => resolveRelation(u),所以首先会进入下面方法的第一个 case, 即如果不是直接在文件上运行(from json.$path)的话,先从 Catalog 中绑定逻辑计划\n  [ResolveRelation.scala]\n    def resolveRelation(plan: LogicalPlan): LogicalPlan = plan match {\n      case u: UnresolvedRelation if !isRunningDirectlyOnFiles(u.tableIdentifier) =>\n        val defaultDatabase = AnalysisContext.get.defaultDatabase\n        val foundRelation = lookupTableFromCatalog(u, defaultDatabase)\n        resolveRelation(foundRelation)\n      case view @ View(desc, _, child) if !child.resolved =>\n        // Resolve all the UnresolvedRelations and Views in the child.\n        val newChild = AnalysisContext.withAnalysisContext(desc.viewDefaultDatabase) {\n          if (AnalysisContext.get.nestedViewDepth > conf.maxNestedViewDepth) {\n            view.failAnalysis(s\"The depth of view ${view.desc.identifier} exceeds the maximum \" +\n              s\"view resolution depth (${conf.maxNestedViewDepth}). Analysis is aborted to \" +\n              s\"avoid errors. Increase the value of ${SQLConf.MAX_NESTED_VIEW_DEPTH.key} to work \" +\n              \"around this.\")\n          }\n          executeSameContext(child)\n        }\n        view.copy(child = newChild)\n      case p @ SubqueryAlias(_, view: View) =>\n        val newChild = resolveRelation(view)\n        p.copy(child = newChild)\n      case _ => plan\n    }\n```\n\n### LookupFunctions \n\n函数存在性校验  \n[LookupFuncations.scala]\n\n```\nobject LookupFunctions extends Rule[LogicalPlan] {\n    override def apply(plan: LogicalPlan): LogicalPlan = plan.transformAllExpressions {\n     // 递归检测使用的函数在 catalog 中是否存在\n      case f: UnresolvedFunction if !catalog.functionExists(f.name) =>\n        withPosition(f) {\n          throw new NoSuchFunctionException(f.name.database.getOrElse(\"default\"), f.name.funcName)\n        }\n    }\n  }\n```\n\n[SessionCatalog.scala]\n```\ndef functionExists(name: FunctionIdentifier): Boolean = {\n    val db = formatDatabaseName(name.database.getOrElse(getCurrentDatabase))\n    requireDbExists(db)\n    // 使用 FuncationRegistry 和外接元数据库检测函数是否存在\n    functionRegistry.functionExists(name) ||\n      externalCatalog.functionExists(db, name.funcName)\n  }\n```\n\n### ResolveFunction  \n\n将函数替换为具体的表达式  \n[ResolveFunction.scala]\n\n```\nobject ResolveFunctions extends Rule[LogicalPlan] {\n    def apply(plan: LogicalPlan): LogicalPlan = plan.transformUp {\n      case q: LogicalPlan =>\n        q transformExpressions {\n          case u if !u.childrenResolved => u // Skip until children are resolved.\n          case u: UnresolvedAttribute if resolver(u.name, VirtualColumn.hiveGroupingIdName) =>\n            withPosition(u) {\n              Alias(GroupingID(Nil), VirtualColumn.hiveGroupingIdName)()\n            }\n          case u @ UnresolvedGenerator(name, children) =>\n            withPosition(u) {\n              catalog.lookupFunction(name, children) match {\n                case generator: Generator => generator\n                case other =>\n                  failAnalysis(s\"$name is expected to be a generator. However, \" +\n                    s\"its class is ${other.getClass.getCanonicalName}, which is not a generator.\")\n              }\n            }\n          case u @ UnresolvedFunction(funcId, children, isDistinct) =>\n            withPosition(u) {\n              // 使用 catalog 寻找函数\n              catalog.lookupFunction(funcId, children) match {\n                case wf: AggregateWindowFunction =>\n                  if (isDistinct) {\n                    failAnalysis(s\"${wf.prettyName} does not support the modifier DISTINCT\")\n                  } else {\n                    wf\n                  }\n                case agg: AggregateFunction => AggregateExpression(agg, Complete, isDistinct)\n                case other =>\n                  if (isDistinct) {\n                    failAnalysis(s\"${other.prettyName} does not support the modifier DISTINCT\")\n                  } else {\n                    other\n                  }\n              }\n            }\n        }\n    }\n  }\n```\n使用 SessionCatalog 优先从 FunctionRegistry 中寻找，然后从外接元数据库寻找并加载资源注册到 FunctionRegistry，有些像双亲委派模型，保证 builtin 的函数不被 UDF覆盖。   \n[SessionCatalog.scala]  \n  \n```\ndef lookupFunction(\n      name: FunctionIdentifier,\n      children: Seq[Expression]): Expression = synchronized {\n    if (name.database.isEmpty && functionRegistry.functionExists(name)) {\n      // This function has been already loaded into the function registry.\n      return functionRegistry.lookupFunction(name, children)\n    }\n\n    // If the name itself is not qualified, add the current database to it.\n    val database = formatDatabaseName(name.database.getOrElse(getCurrentDatabase))\n    val qualifiedName = name.copy(database = Some(database))\n\n    if (functionRegistry.functionExists(qualifiedName)) {\n      return functionRegistry.lookupFunction(qualifiedName, children)\n    }\n    val catalogFunction = try {\n      externalCatalog.getFunction(database, name.funcName)\n    } catch {\n      case _: AnalysisException => failFunctionLookup(name)\n      case _: NoSuchPermanentFunctionException => failFunctionLookup(name)\n    }\n    loadFunctionResources(catalogFunction.resources)\n    registerFunction(catalogFunction.copy(identifier = qualifiedName), overrideIfExists = false)\n    // Now, we need to create the Expression.\n    functionRegistry.lookupFunction(qualifiedName, children)\n  }\n```\n","slug":"2019-11-29-SparkSQL 源码解析(2):从 UnresolvedLogicPlan 到 ResolvedLogicPlan","published":1,"updated":"2020-03-16T10:01:28.110Z","comments":1,"photos":[],"link":"","_id":"cke2rms59000dy7c703as3cw0","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Parser 模块负责将原始 SQL 文本解析成 UnResolvedLogicPlan，Anayzer 模块负责将 UnResolvedLogicPlan(Relation|Attribute|Function) 通过 SessionCatalog 信息绑定到真正可操作的实体上(File|Column|MainClass)，即 SolvedLogicPlan，并校验其是否合法。   </p>\n<a id=\"more\"></a>\n\n<p>则关键点在于:   </p>\n<ol>\n<li>SessionCatalog 存在目的        </li>\n<li>如何绑定，将 UnresolvedLogicPlan 转换为 ResolvedLogicPlan    </li>\n<li>如何校验 ResolvedLogicPlan 合法  </li>\n</ol>\n<p>1.Analyzer 对 UnresolvedLogicPlan应用各类规则(Rule),并对各个 UnresolvedLogicPlan节点绑定 CataLog 信息,将 UnresolvedLogicPlan中的UnresolvedRelation 和 UnresolvedAttribute解析成有类型(Typed)的对象,生成解析后的逻辑算子树 SolvedLogicPlan.<br>2.对 UnresolvedLogicPlan 的大部分操作,如绑定解析优化等,主要方法都是基于规则(Rule)的,然后通过模式匹配对UnresolvedLogicPlan进行相应处理.<br>3.Catalog信息在Analyzer的构造函数中.  </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>调用栈<br>-&gt;SparkSession#sql<br>　-&gt;DataSet#ofRows<br>　　-&gt;QueryExecution#assertAnalyzed<br>　　　-&gt;QueryExecution#analyzed (lazy)<br>　　　　-&gt;Analyzer#executeAndCheck<br>[Anayzer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def executeAndCheck(plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 绑定</span><br><span class=\"line\">    val analyzed &#x3D; execute(plan)</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 第二步: 校验</span><br><span class=\"line\">      checkAnalysis(analyzed)</span><br><span class=\"line\">      &#x2F;&#x2F; 将 LogicPlan 的 AnalysisBarrier 装箱去掉(如果存在)</span><br><span class=\"line\">      EliminateBarriers(analyzed)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">      case e: AnalysisException &#x3D;&gt;</span><br><span class=\"line\">        val ae &#x3D; new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed))</span><br><span class=\"line\">        ae.setStackTrace(e.getStackTrace)</span><br><span class=\"line\">        throw ae</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"SessionCatalog\"><a href=\"#SessionCatalog\" class=\"headerlink\" title=\"SessionCatalog\"></a>SessionCatalog</h3><p>SessionCatalog 是 SparkSession 索引维护的信息库，SessionCatalog 一方面可以管理 SparkSession 创建的临时视图和 UDF，一方面可以作为外部元数据库的代理以便 SparkSession 获取外部源数据库信息(e.g. HiveSessionCatalog 操作 Hive metastore)。    </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">成员变量</th>\n<th align=\"left\">数据类型</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">Catalog 类型</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">tableRelationCache</td>\n<td align=\"left\">Cache[QualifiedTableName, LogicalPlan]</td>\n<td align=\"left\">表全限定名与逻辑计划的映射</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">tempViews</td>\n<td align=\"left\">HashMap[String, LogicalPlan]</td>\n<td align=\"left\">临时视图名与逻辑计划的映射</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">globalTempViewManager</td>\n<td align=\"left\">GlobalTempViewManager</td>\n<td align=\"left\">全局视图管理</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">externalCatalog</td>\n<td align=\"left\">ExternalCatalog</td>\n<td align=\"left\">外部元数据库的代理,从获得连接到库表分区函数等信息获取接口</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">functionRegistry</td>\n<td align=\"left\">FunctionRegistry</td>\n<td align=\"left\">函数(builtin,UDF)操作代理</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">parser</td>\n<td align=\"left\">ParserInterface</td>\n<td align=\"left\">SQL 文本解析 Parser</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">functionResourceLoader</td>\n<td align=\"left\">FunctionResourceLoader</td>\n<td align=\"left\">加载函数主类资源加载器</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">metastoreCatalog</td>\n<td align=\"left\">HiveMetastoreCatalog</td>\n<td align=\"left\">HiveExternalCatalog 的旧接口,deprecated</td>\n<td align=\"left\">HiveSessionCatalog</td>\n</tr>\n</tbody></table>\n<p>引用一个通过表名绑定逻辑计划的函数: 通过表所属数据库名在全局视图，外部元数据库和临时视图内搜索以绑定逻辑计划<br>[SessionCatalog.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def lookupRelation(name: TableIdentifier): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    synchronized &#123;</span><br><span class=\"line\">      val db &#x3D; formatDatabaseName(name.database.getOrElse(currentDb))</span><br><span class=\"line\">      val table &#x3D; formatTableName(name.table)</span><br><span class=\"line\">      &#x2F;&#x2F; 全局视图</span><br><span class=\"line\">      if (db &#x3D;&#x3D; globalTempViewManager.database) &#123;</span><br><span class=\"line\">        globalTempViewManager.get(table).map &#123; viewDef &#x3D;&gt;</span><br><span class=\"line\">          SubqueryAlias(table, viewDef)</span><br><span class=\"line\">        &#125;.getOrElse(throw new NoSuchTableException(db, table))</span><br><span class=\"line\">      &#125; else if (name.database.isDefined || !tempViews.contains(table)) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 外部元数据库</span><br><span class=\"line\">        val metadata &#x3D; externalCatalog.getTable(db, table)</span><br><span class=\"line\">        if (metadata.tableType &#x3D;&#x3D; CatalogTableType.VIEW) &#123;</span><br><span class=\"line\">          val viewText &#x3D; metadata.viewText.getOrElse(sys.error(&quot;Invalid view without text.&quot;))</span><br><span class=\"line\">          val child &#x3D; View(</span><br><span class=\"line\">            desc &#x3D; metadata,</span><br><span class=\"line\">            output &#x3D; metadata.schema.toAttributes,</span><br><span class=\"line\">            child &#x3D; parser.parsePlan(viewText))</span><br><span class=\"line\">          SubqueryAlias(table, child)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 临时视图</span><br><span class=\"line\">        SubqueryAlias(table, tempViews(table))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"绑定\"><a href=\"#绑定\" class=\"headerlink\" title=\"绑定\"></a>绑定</h3><p>Anayzer 模块对 UnResolvedLogicPlan 应用多批规则库，其中主要是 Resolution(解析)规则库和 Substitution(转换)规则库，直到每批规则库达到了指定的迭代次数或者规则库无法再优化逻辑计划为止。  </p>\n<p>调用栈<br>Analyzer#executeAndCheck<br>　-&gt;Analyzer#execute<br>　　-&gt;Analyzer#executeSameContext<br>　　　-&gt;RuleExecutor#execute<br>[RuleExecutor.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 规则批次间和批次内都是串行执行</span><br><span class=\"line\"> * 逻辑计划在此处被 Analyzed 之后就替换了(result&#x2F;curPlan和返回值)</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">def execute(plan: TreeType): TreeType &#x3D; &#123;</span><br><span class=\"line\">  var curPlan &#x3D; plan</span><br><span class=\"line\">  batches.foreach &#123; batch &#x3D;&gt;</span><br><span class=\"line\">    val batchStartPlan &#x3D; curPlan</span><br><span class=\"line\">    var iteration &#x3D; 1</span><br><span class=\"line\">    var lastPlan &#x3D; curPlan\t&#x2F;&#x2F;tempPlan</span><br><span class=\"line\">    var continue &#x3D; true</span><br><span class=\"line\">    &#x2F;&#x2F; foldLeft 依次执行规则 batch 中的所有规则,直到达到了规则的最大执行次数或者逻辑算子树不再变化</span><br><span class=\"line\">    while (continue) &#123;</span><br><span class=\"line\">      curPlan &#x3D; batch.rules.foldLeft(curPlan) &#123; </span><br><span class=\"line\">        case (plan, rule) &#x3D;&gt;</span><br><span class=\"line\">        &#x2F;&#x2F; 在逻辑计划上应用规则处理</span><br><span class=\"line\">          val result &#x3D; rule(plan) &#x2F;&#x2F;rule.apply(plan)</span><br><span class=\"line\">          if (!result.fastEquals(plan)) &#123;</span><br><span class=\"line\">            logTrace(</span><br><span class=\"line\">              s&quot;&quot;&quot;</span><br><span class=\"line\">                |&#x3D;&#x3D;&#x3D; Applying Rule $&#123;rule.ruleName&#125; &#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">                |$&#123;sideBySide(plan.treeString, result.treeString).mkString(&quot;\\n&quot;)&#125;</span><br><span class=\"line\">              &quot;&quot;&quot;.stripMargin)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime)</span><br><span class=\"line\">          queryExecutionMetrics.incNumExecution(rule.ruleName)</span><br><span class=\"line\">          ....</span><br><span class=\"line\">           result</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      iteration +&#x3D; 1</span><br><span class=\"line\">       &#x2F;&#x2F; 达到了该规则批策略的最大迭代次数</span><br><span class=\"line\">      if (iteration &gt; batch.strategy.maxIterations) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        continue &#x3D; false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 这批规则已经不能再优化逻辑计划了</span><br><span class=\"line\">      if (curPlan.fastEquals(lastPlan)) &#123;</span><br><span class=\"line\">        logTrace(</span><br><span class=\"line\">          s&quot;Fixed point reached for batch $&#123;batch.name&#125; after $&#123;iteration - 1&#125; iterations.&quot;)</span><br><span class=\"line\">        continue &#x3D; false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      lastPlan &#x3D; curPlan</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  curPlan</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>重点是其中的 rule(plan)方法，即 Rule#apply(plan)。各规则子类依据自身的功能解析转换 TreeNode，包括绑定 UnResolvedLogicPlan 为 ResolvedLogicPlan<br> [Rule.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging &#123;</span><br><span class=\"line\">  val ruleName: String &#x3D; &#123;</span><br><span class=\"line\">    val className &#x3D; getClass.getName</span><br><span class=\"line\">    if (className endsWith &quot;$&quot;) className.dropRight(1) else className</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  def apply(plan: TreeType): TreeType</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>规则的具体功能列表参考”附录：规则功能列表”   </p>\n<h3 id=\"校验\"><a href=\"#校验\" class=\"headerlink\" title=\"校验\"></a>校验</h3><p>调用栈<br>Analyze#executeAndCheck<br>　-&gt;CheckAndAnalysis#checkAnalysis  </p>\n<p>检查 ResolvedLogicPlan 中不合乎语法规范的错误，使用后序遍历(先子节点后当前节点)的方式尽可能抛出最先导致失败的错误<br>[CheckAndAnaysis.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def checkAnalysis(plan: LogicalPlan): Unit &#x3D; &#123;</span><br><span class=\"line\">    plan.foreachUp &#123;</span><br><span class=\"line\">      case u: UnresolvedRelation &#x3D;&gt;</span><br><span class=\"line\">        u.failAnalysis(s&quot;Table or view not found: $&#123;u.tableIdentifier&#125;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">      case operator: LogicalPlan &#x3D;&gt;</span><br><span class=\"line\">        operator transformExpressionsUp &#123;</span><br><span class=\"line\">          case a: Attribute if !a.resolved &#x3D;&gt;</span><br><span class=\"line\">            val from &#x3D; operator.inputSet.map(_.qualifiedName).mkString(&quot;, &quot;)</span><br><span class=\"line\">            a.failAnalysis(s&quot;cannot resolve &#39;$&#123;a.sql&#125;&#39; given input columns: [$from]&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case e: Expression if e.checkInputDataTypes().isFailure &#x3D;&gt;</span><br><span class=\"line\">            e.checkInputDataTypes() match &#123;</span><br><span class=\"line\">              case TypeCheckResult.TypeCheckFailure(message) &#x3D;&gt;</span><br><span class=\"line\">                e.failAnalysis(</span><br><span class=\"line\">                  s&quot;cannot resolve &#39;$&#123;e.sql&#125;&#39; due to data type mismatch: $message&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          case c: Cast if !c.resolved &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;invalid cast from $&#123;c.child.dataType.simpleString&#125; to $&#123;c.dataType.simpleString&#125;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case g: Grouping &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;grouping() can only be used with GroupingSets&#x2F;Cube&#x2F;Rollup&quot;)</span><br><span class=\"line\">          case g: GroupingID &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;grouping_id() can only be used with GroupingSets&#x2F;Cube&#x2F;Rollup&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(AggregateExpression(_, _, true, _), _) &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(s&quot;Distinct window functions are not supported: $w&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(_: OffsetWindowFunction,</span><br><span class=\"line\">            WindowSpecDefinition(_, order, frame: SpecifiedWindowFrame))</span><br><span class=\"line\">             if order.isEmpty || !frame.isOffset &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;An offset window function can only be evaluated in an ordered &quot; +</span><br><span class=\"line\">              s&quot;row-based window frame with a single offset: $w&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(e, s) &#x3D;&gt;</span><br><span class=\"line\">            &#x2F;&#x2F; Only allow window functions with an aggregate expression or an offset window</span><br><span class=\"line\">            &#x2F;&#x2F; function.</span><br><span class=\"line\">            e match &#123;</span><br><span class=\"line\">              case _: AggregateExpression | _: OffsetWindowFunction | _: AggregateWindowFunction &#x3D;&gt;</span><br><span class=\"line\">                w</span><br><span class=\"line\">              case _ &#x3D;&gt;</span><br><span class=\"line\">                failAnalysis(s&quot;Expression &#39;$e&#39; not supported within a window function.&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          case s: SubqueryExpression &#x3D;&gt;</span><br><span class=\"line\">            checkSubqueryExpression(operator, s)</span><br><span class=\"line\">            s</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        operator match &#123;</span><br><span class=\"line\">          case etw: EventTimeWatermark &#x3D;&gt;</span><br><span class=\"line\">            etw.eventTime.dataType match &#123;</span><br><span class=\"line\">              case s: StructType</span><br><span class=\"line\">                if s.find(_.name &#x3D;&#x3D; &quot;end&quot;).map(_.dataType) &#x3D;&#x3D; Some(TimestampType) &#x3D;&gt;</span><br><span class=\"line\">              case _: TimestampType &#x3D;&gt;</span><br><span class=\"line\">              case _ &#x3D;&gt;</span><br><span class=\"line\">                failAnalysis(</span><br><span class=\"line\">                  s&quot;Event time must be defined on a window or a timestamp, but &quot; +</span><br><span class=\"line\">                  s&quot;$&#123;etw.eventTime.name&#125; is of type $&#123;etw.eventTime.dataType.simpleString&#125;&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case f: Filter if f.condition.dataType !&#x3D; BooleanType &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;filter expression &#39;$&#123;f.condition.sql&#125;&#39; &quot; +</span><br><span class=\"line\">                s&quot;of type $&#123;f.condition.dataType.simpleString&#125; is not a boolean.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case Filter(condition, _) if hasNullAwarePredicateWithinNot(condition) &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;Null-aware predicate sub-queries cannot be used in nested &quot; +</span><br><span class=\"line\">              s&quot;conditions: $condition&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case j @ Join(_, _, _, Some(condition)) if condition.dataType !&#x3D; BooleanType &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;join condition &#39;$&#123;condition.sql&#125;&#39; &quot; +</span><br><span class=\"line\">                s&quot;of type $&#123;condition.dataType.simpleString&#125; is not a boolean.&quot;)</span><br><span class=\"line\">\t..........</span><br><span class=\"line\">\t&#x2F;&#x2F; 自定义的规则检查在内置规则检查之后才能开始</span><br><span class=\"line\">    extendedCheckRules.foreach(_(plan))</span><br><span class=\"line\">    plan.foreachUp &#123;</span><br><span class=\"line\">      case AnalysisBarrier(child) if !child.resolved &#x3D;&gt; checkAnalysis(child)</span><br><span class=\"line\">      case o if !o.resolved &#x3D;&gt; failAnalysis(s&quot;unresolved operator $&#123;o.simpleString&#125;&quot;)</span><br><span class=\"line\">      case _ &#x3D;&gt;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><p>UnResolvedLogicPlan 在绑定了 Catalog 之后可以转换为 RDD 模式来执行了，但是由于提交的 SQL 质量参差不齐，按照 ResolvedLogicPlan 按部就班的执行会导致代价/效率差距很大 且 要求用户对执行引擎的执行模式很了解并熟悉 SQL 优化手段才能写出效率高执行快的 SQL。所以为了尽可能忽略用户的代码质量，对SQL 优化的熟悉程度，SparkSQL 都需要以很高的效率执行，SparkSQL 在后续阶段需要对 UnResolvedLogicPlan 进行优化，即 Rule-Based Optimizer，也称为 RBO。    </p>\n<h2 id=\"附录：规则功能列表\"><a href=\"#附录：规则功能列表\" class=\"headerlink\" title=\"附录：规则功能列表\"></a>附录：规则功能列表</h2><p>规则库 Batch: 在 UnResolvedLogicPlan 上应用规则库 rules 的执行策略 strategy。 \n[Batch.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)</span><br></pre></td></tr></table></figure>\n<p>Strategy 表示规则批的最大迭代次数，有两个子类: Once 表明只需应用一次规则库即可；FixedPoint 表示最大可以应用规则库 maxIterations 次，如果应用中途无法再优化逻辑计划则跳出。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">abstract class Strategy &#123; def maxIterations: Int &#125;</span><br><span class=\"line\">case object Once extends Strategy &#123; val maxIterations &#x3D; 1 &#125;</span><br><span class=\"line\">case class FixedPoint(maxIterations: Int) extends Strategy</span><br></pre></td></tr></table></figure>\n<p>规则 Rule 的子类有很多,在 Analyze.scala 中使用的所有规则如下<br>[Analyzer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lazy val batches: Seq[Batch] &#x3D; Seq(</span><br><span class=\"line\">  Batch(&quot;Hints&quot;, fixedPoint,</span><br><span class=\"line\">    new ResolveHints.ResolveBroadcastHints(conf),</span><br><span class=\"line\">    ResolveHints.RemoveAllHints),</span><br><span class=\"line\">  Batch(&quot;Simple Sanity Check&quot;, Once,</span><br><span class=\"line\">    LookupFunctions),</span><br><span class=\"line\">  Batch(&quot;Substitution&quot;, fixedPoint,</span><br><span class=\"line\">    CTESubstitution,</span><br><span class=\"line\">    WindowsSubstitution,</span><br><span class=\"line\">    EliminateUnions,</span><br><span class=\"line\">    new SubstituteUnresolvedOrdinals(conf)),</span><br><span class=\"line\">  Batch(&quot;Resolution&quot;, fixedPoint,</span><br><span class=\"line\">    ResolveTableValuedFunctions ::</span><br><span class=\"line\">    ResolveRelations ::</span><br><span class=\"line\">    ResolveReferences ::</span><br><span class=\"line\">    ResolveCreateNamedStruct ::</span><br><span class=\"line\">    ResolveDeserializer ::</span><br><span class=\"line\">    ResolveNewInstance ::</span><br><span class=\"line\">    ResolveUpCast ::</span><br><span class=\"line\">    ResolveGroupingAnalytics ::</span><br><span class=\"line\">    ResolvePivot ::</span><br><span class=\"line\">    ResolveOrdinalInOrderByAndGroupBy ::</span><br><span class=\"line\">    ResolveAggAliasInGroupBy ::</span><br><span class=\"line\">    ResolveMissingReferences ::</span><br><span class=\"line\">    ExtractGenerator ::</span><br><span class=\"line\">    ResolveGenerate ::</span><br><span class=\"line\">    ResolveFunctions ::</span><br><span class=\"line\">    ResolveAliases ::</span><br><span class=\"line\">    ResolveSubquery ::</span><br><span class=\"line\">    ResolveSubqueryColumnAliases ::</span><br><span class=\"line\">    ResolveWindowOrder ::</span><br><span class=\"line\">    ResolveWindowFrame ::</span><br><span class=\"line\">    ResolveNaturalAndUsingJoin ::</span><br><span class=\"line\">    ExtractWindowExpressions ::</span><br><span class=\"line\">    GlobalAggregates ::</span><br><span class=\"line\">    ResolveAggregateFunctions ::</span><br><span class=\"line\">    TimeWindowing ::</span><br><span class=\"line\">    ResolveInlineTables(conf) ::</span><br><span class=\"line\">    ResolveTimeZone(conf) ::</span><br><span class=\"line\">    TypeCoercion.typeCoercionRules(conf) ++</span><br><span class=\"line\">    extendedResolutionRules : _*),</span><br><span class=\"line\">  Batch(&quot;Post-Hoc Resolution&quot;, Once, postHocResolutionRules: _*),</span><br><span class=\"line\">  Batch(&quot;View&quot;, Once,</span><br><span class=\"line\">    AliasViewChild(conf)),</span><br><span class=\"line\">  Batch(&quot;Nondeterministic&quot;, Once,</span><br><span class=\"line\">    PullOutNondeterministic),</span><br><span class=\"line\">  Batch(&quot;UDF&quot;, Once,</span><br><span class=\"line\">    HandleNullInputsForUDF),</span><br><span class=\"line\">  Batch(&quot;FixNullability&quot;, Once,</span><br><span class=\"line\">    FixNullability),</span><br><span class=\"line\">  Batch(&quot;Subquery&quot;, Once,</span><br><span class=\"line\">    UpdateOuterReferences),</span><br><span class=\"line\">  Batch(&quot;Cleanup&quot;, fixedPoint,</span><br><span class=\"line\">    CleanupAliases)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<p>其中最重要的规则库是Substitution(替换)和Resolution(解析绑定)。这些规则库之间及规则库之内都是有序的，使用规则时也是串行执行的。打乱了顺序可能有转换不充分等未知的错误<br>下表主要参照 org.apache.spark.sql.catalyst.parser.SqlBase.g4 文件和源码，带问号的为不确定的。</p>\n<table>\n<thead>\n<tr>\n<th>规则批次</th>\n<th>规则</th>\n<th>功能</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hints</td>\n<td>ResolveBroadcastHints</td>\n<td>广播</td>\n<td>mapjoin</td>\n</tr>\n<tr>\n<td>Hints</td>\n<td>RemoveAllHints</td>\n<td>删除无效标识符</td>\n<td></td>\n</tr>\n<tr>\n<td>Simple Sanity Check</td>\n<td>LookupFunctions</td>\n<td>函数存在性检验</td>\n<td></td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>CTESubstitution</td>\n<td>with,合并计划</td>\n<td>with a as something select * from a</td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>WindowsSubstitution</td>\n<td>窗口函数</td>\n<td>row_number() over (partitioned by a sort by b desc)</td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>EliminateUnions</td>\n<td>union的计划只有一个时删除 union</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveTableValuedFunctions</td>\n<td>解析可作为表的函数</td>\n<td>range函数</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveRelations</td>\n<td>表绑定catalog中的逻辑计划</td>\n<td>对应createXXXView接口</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveReferences</td>\n<td>展开星号绑定列</td>\n<td>select a.* from</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveCreateNamedStruct</td>\n<td>解析结构体构造方法</td>\n<td>?,没见过hql中使用结构体</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveDeserializer</td>\n<td>解析反序列化类</td>\n<td>decoder</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveNewInstance</td>\n<td>解析创建实例</td>\n<td>encoder</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveUpCast</td>\n<td>解析类型转换</td>\n<td>cast,在丢失精度时抛异常</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveGroupingAnalytics</td>\n<td>解析rollup多维度分析</td>\n<td>group by a,b,c with rollup 等价于 group by a,b,c grouping sets((a,b,c),(a,b),(a),())</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolvePivot</td>\n<td>行转列</td>\n<td>pivot 接口</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveOrdinalInOrderByAndGroupBy</td>\n<td>解析order/sort/group by语句的下标数字</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAggAliasInGroupBy</td>\n<td>解析聚合时的表达式</td>\n<td>group by case when then x else y end</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveMissingReferences</td>\n<td>解析在排序时不存在的列,加上但隐藏该列</td>\n<td>select a from … order by b</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ExtractGenerator</td>\n<td>解析UDTF生成器</td>\n<td>select explode(xx) from ..</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveGenerate</td>\n<td>?</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveFunctions</td>\n<td>解析函数为表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAliases</td>\n<td>解析生成别名表达式</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveSubquery</td>\n<td>解析子查询</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveSubqueryColumnAliases</td>\n<td>解析子查询列别名</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveWindowOrder</td>\n<td>解析窗口函数中的排序</td>\n<td>over partitioned by .. order by..</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveWindowFrame</td>\n<td>解析检验窗口函数</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveNaturalAndUsingJoin</td>\n<td>通过输出列解析自然连接</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ExtractWindowExpressions</td>\n<td>提取窗口函数表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>GlobalAggregates</td>\n<td>解析全局聚合</td>\n<td>select max(a) from tbl</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAggregateFunctions</td>\n<td>解析不在聚合中的聚合函数</td>\n<td>having/order by</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>TimeWindowing</td>\n<td>解析滑动时间窗口</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveInlineTables</td>\n<td>解析内联表为LocalRelation</td>\n<td>select * fom values(..),(..) as (columns)</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveTimeZone</td>\n<td>解析时区表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>TypeCoercion.typeCoercionRules</td>\n<td>强制转换为兼容类型</td>\n<td>在比较和 union 时使用,不损失精度</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>extendedResolutionRules</td>\n<td>拓展规则,空集合</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Post-Hoc Resolution</td>\n<td>postHocResolutionRules</td>\n<td>Resolution 规则后执行的规则.空集合</td>\n<td></td>\n</tr>\n<tr>\n<td>View</td>\n<td>AliasViewChild</td>\n<td>视图的分析规则</td>\n<td></td>\n</tr>\n<tr>\n<td>Nondeterministic</td>\n<td>PullOutNondeterministic</td>\n<td>提取非确定性表达式,放到child 中</td>\n<td>?</td>\n</tr>\n<tr>\n<td>UDF</td>\n<td>HandleNullInputsForUDF</td>\n<td>对UDF增加基本数据类型null处理(空输入则空输出)</td>\n<td></td>\n</tr>\n<tr>\n<td>FixNullability</td>\n<td>FixNullability</td>\n<td>通过 child 字段的 Nullablity 修复父逻辑计划字段的 Nullablity</td>\n<td></td>\n</tr>\n<tr>\n<td>Subquery</td>\n<td>UpdateOuterReferences</td>\n<td>聚合表达式下推</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Cleanup</td>\n<td>CleanupAliases</td>\n<td>删除不需要的别名</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3 id=\"with\"><a href=\"#with\" class=\"headerlink\" title=\"with\"></a>with</h3><p>with a  as … select * from a 语法是 SQL 语句中的一个特殊 case，with 子句打乱了 SQL 解析的通用模式，无法从上至下从左至右进行解析，只能将整个语句解析为两个相对独立的逻辑算子树，然后通过别名将 with 语句的逻辑算子树加入到主体逻辑算子树的下面  </p>\n<p>[WindowsSubstitution.scala] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object WindowsSubstitution extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">  def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;使用多层 match case 来匹配拿到 window 函数,</span><br><span class=\"line\">    case WithWindowDefinition(windowDefinitions, child) &#x3D;&gt;</span><br><span class=\"line\">      child.transform &#123;</span><br><span class=\"line\">        case p &#x3D;&gt; p.transformExpressions &#123;</span><br><span class=\"line\">          case UnresolvedWindowExpression(c, WindowSpecReference(windowName)) &#x3D;&gt;</span><br><span class=\"line\">            val errorMessage &#x3D;</span><br><span class=\"line\">              s&quot;Window specification $windowName is not defined in the WINDOW clause.&quot;</span><br><span class=\"line\">            val windowSpecDefinition &#x3D;</span><br><span class=\"line\">              windowDefinitions.getOrElse(windowName, failAnalysis(errorMessage))</span><br><span class=\"line\">            WindowExpression(c, windowSpecDefinition)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"relation\"><a href=\"#relation\" class=\"headerlink\" title=\"relation\"></a>relation</h3><p>绑定关系<br>[ResolveRelations.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> object ResolveRelations extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">  &#x2F;&#x2F; 后序遍历尝试绑定每个匹配的节点到 ResolvedLogicPlan</span><br><span class=\"line\">   def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">     case i @ InsertIntoTable(u: UnresolvedRelation, parts, child, _, _) if child.resolved &#x3D;&gt;</span><br><span class=\"line\">      &#x2F;&#x2F; lookupTableFromCatalog(u) 从全局视图&gt;外接元数据库&gt;临时视图中绑定实体  </span><br><span class=\"line\">      &#x2F;&#x2F; EliminateSubqueryAliases 去掉别名将子查询查询计划剥离出来</span><br><span class=\"line\">       EliminateSubqueryAliases(lookupTableFromCatalog(u)) match &#123;</span><br><span class=\"line\">         case v: View &#x3D;&gt;</span><br><span class=\"line\">           u.failAnalysis(s&quot;Inserting into a view is not allowed. View: $&#123;v.desc.identifier&#125;.&quot;)</span><br><span class=\"line\">         case other &#x3D;&gt; i.copy(table &#x3D; other)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     case u: UnresolvedRelation &#x3D;&gt; resolveRelation(u)\t</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   先看第一个 case,如果是 insert into 句法的话,借助 catalog的&lt;tableName,LogicPlan&gt;缓存绑定表为已解析过的逻辑计划</span><br><span class=\"line\">   private def lookupTableFromCatalog(</span><br><span class=\"line\">       u: UnresolvedRelation,</span><br><span class=\"line\">       defaultDatabase: Option[String] &#x3D; None): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">     val tableIdentWithDb &#x3D; u.tableIdentifier.copy(</span><br><span class=\"line\">       database &#x3D; u.tableIdentifier.database.orElse(defaultDatabase))</span><br><span class=\"line\">     try &#123;</span><br><span class=\"line\">       catalog.lookupRelation(tableIdentWithDb)</span><br><span class=\"line\">     &#125; catch &#123;</span><br><span class=\"line\">\t....        </span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">[SessionCatalog.scala] </span><br><span class=\"line\"> def lookupRelation(name: TableIdentifier): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">   synchronized &#123;</span><br><span class=\"line\">     val db &#x3D; formatDatabaseName(name.database.getOrElse(currentDb))</span><br><span class=\"line\">     val table &#x3D; formatTableName(name.table)</span><br><span class=\"line\">     &#x2F;&#x2F;从全局临时视图,外接元数据库,临时视图中查找该表的逻辑计划</span><br><span class=\"line\">     if (db &#x3D;&#x3D; globalTempViewManager.database) &#123;</span><br><span class=\"line\">       globalTempViewManager.get(table).map &#123; viewDef &#x3D;&gt;</span><br><span class=\"line\">         SubqueryAlias(table, viewDef)</span><br><span class=\"line\">       &#125;.getOrElse(throw new NoSuchTableException(db, table))</span><br><span class=\"line\">     &#125; else if (name.database.isDefined || !tempViews.contains(table)) &#123;</span><br><span class=\"line\">       val metadata &#x3D; externalCatalog.getTable(db, table)</span><br><span class=\"line\">       if (metadata.tableType &#x3D;&#x3D; CatalogTableType.VIEW) &#123;</span><br><span class=\"line\">         val viewText &#x3D; metadata.viewText.getOrElse(sys.error(&quot;Invalid view without text.&quot;))</span><br><span class=\"line\">         val child &#x3D; View(</span><br><span class=\"line\">           desc &#x3D; metadata,</span><br><span class=\"line\">           output &#x3D; metadata.schema.toAttributes,</span><br><span class=\"line\">           child &#x3D; parser.parsePlan(viewText))</span><br><span class=\"line\">         SubqueryAlias(table, child)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         SubqueryAlias(table, UnresolvedCatalogRelation(metadata))</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       SubqueryAlias(table, tempViews(table))</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> 再看ResolveRelations.apply方法的第二个 case u:UnresolvedRelation &#x3D;&gt; resolveRelation(u),所以首先会进入下面方法的第一个 case, 即如果不是直接在文件上运行(from json.$path)的话,先从 Catalog 中绑定逻辑计划</span><br><span class=\"line\"> [ResolveRelation.scala]</span><br><span class=\"line\">   def resolveRelation(plan: LogicalPlan): LogicalPlan &#x3D; plan match &#123;</span><br><span class=\"line\">     case u: UnresolvedRelation if !isRunningDirectlyOnFiles(u.tableIdentifier) &#x3D;&gt;</span><br><span class=\"line\">       val defaultDatabase &#x3D; AnalysisContext.get.defaultDatabase</span><br><span class=\"line\">       val foundRelation &#x3D; lookupTableFromCatalog(u, defaultDatabase)</span><br><span class=\"line\">       resolveRelation(foundRelation)</span><br><span class=\"line\">     case view @ View(desc, _, child) if !child.resolved &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Resolve all the UnresolvedRelations and Views in the child.</span><br><span class=\"line\">       val newChild &#x3D; AnalysisContext.withAnalysisContext(desc.viewDefaultDatabase) &#123;</span><br><span class=\"line\">         if (AnalysisContext.get.nestedViewDepth &gt; conf.maxNestedViewDepth) &#123;</span><br><span class=\"line\">           view.failAnalysis(s&quot;The depth of view $&#123;view.desc.identifier&#125; exceeds the maximum &quot; +</span><br><span class=\"line\">             s&quot;view resolution depth ($&#123;conf.maxNestedViewDepth&#125;). Analysis is aborted to &quot; +</span><br><span class=\"line\">             s&quot;avoid errors. Increase the value of $&#123;SQLConf.MAX_NESTED_VIEW_DEPTH.key&#125; to work &quot; +</span><br><span class=\"line\">             &quot;around this.&quot;)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         executeSameContext(child)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       view.copy(child &#x3D; newChild)</span><br><span class=\"line\">     case p @ SubqueryAlias(_, view: View) &#x3D;&gt;</span><br><span class=\"line\">       val newChild &#x3D; resolveRelation(view)</span><br><span class=\"line\">       p.copy(child &#x3D; newChild)</span><br><span class=\"line\">     case _ &#x3D;&gt; plan</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"LookupFunctions\"><a href=\"#LookupFunctions\" class=\"headerlink\" title=\"LookupFunctions\"></a>LookupFunctions</h3><p>函数存在性校验<br>[LookupFuncations.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object LookupFunctions extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">    override def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformAllExpressions &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 递归检测使用的函数在 catalog 中是否存在</span><br><span class=\"line\">      case f: UnresolvedFunction if !catalog.functionExists(f.name) &#x3D;&gt;</span><br><span class=\"line\">        withPosition(f) &#123;</span><br><span class=\"line\">          throw new NoSuchFunctionException(f.name.database.getOrElse(&quot;default&quot;), f.name.funcName)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[SessionCatalog.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def functionExists(name: FunctionIdentifier): Boolean &#x3D; &#123;</span><br><span class=\"line\">    val db &#x3D; formatDatabaseName(name.database.getOrElse(getCurrentDatabase))</span><br><span class=\"line\">    requireDbExists(db)</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 FuncationRegistry 和外接元数据库检测函数是否存在</span><br><span class=\"line\">    functionRegistry.functionExists(name) ||</span><br><span class=\"line\">      externalCatalog.functionExists(db, name.funcName)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ResolveFunction\"><a href=\"#ResolveFunction\" class=\"headerlink\" title=\"ResolveFunction\"></a>ResolveFunction</h3><p>将函数替换为具体的表达式<br>[ResolveFunction.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object ResolveFunctions extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">    def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">      case q: LogicalPlan &#x3D;&gt;</span><br><span class=\"line\">        q transformExpressions &#123;</span><br><span class=\"line\">          case u if !u.childrenResolved &#x3D;&gt; u &#x2F;&#x2F; Skip until children are resolved.</span><br><span class=\"line\">          case u: UnresolvedAttribute if resolver(u.name, VirtualColumn.hiveGroupingIdName) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              Alias(GroupingID(Nil), VirtualColumn.hiveGroupingIdName)()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case u @ UnresolvedGenerator(name, children) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              catalog.lookupFunction(name, children) match &#123;</span><br><span class=\"line\">                case generator: Generator &#x3D;&gt; generator</span><br><span class=\"line\">                case other &#x3D;&gt;</span><br><span class=\"line\">                  failAnalysis(s&quot;$name is expected to be a generator. However, &quot; +</span><br><span class=\"line\">                    s&quot;its class is $&#123;other.getClass.getCanonicalName&#125;, which is not a generator.&quot;)</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case u @ UnresolvedFunction(funcId, children, isDistinct) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              &#x2F;&#x2F; 使用 catalog 寻找函数</span><br><span class=\"line\">              catalog.lookupFunction(funcId, children) match &#123;</span><br><span class=\"line\">                case wf: AggregateWindowFunction &#x3D;&gt;</span><br><span class=\"line\">                  if (isDistinct) &#123;</span><br><span class=\"line\">                    failAnalysis(s&quot;$&#123;wf.prettyName&#125; does not support the modifier DISTINCT&quot;)</span><br><span class=\"line\">                  &#125; else &#123;</span><br><span class=\"line\">                    wf</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">                case agg: AggregateFunction &#x3D;&gt; AggregateExpression(agg, Complete, isDistinct)</span><br><span class=\"line\">                case other &#x3D;&gt;</span><br><span class=\"line\">                  if (isDistinct) &#123;</span><br><span class=\"line\">                    failAnalysis(s&quot;$&#123;other.prettyName&#125; does not support the modifier DISTINCT&quot;)</span><br><span class=\"line\">                  &#125; else &#123;</span><br><span class=\"line\">                    other</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>使用 SessionCatalog 优先从 FunctionRegistry 中寻找，然后从外接元数据库寻找并加载资源注册到 FunctionRegistry，有些像双亲委派模型，保证 builtin 的函数不被 UDF覆盖。<br>[SessionCatalog.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def lookupFunction(</span><br><span class=\"line\">      name: FunctionIdentifier,</span><br><span class=\"line\">      children: Seq[Expression]): Expression &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (name.database.isEmpty &amp;&amp; functionRegistry.functionExists(name)) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; This function has been already loaded into the function registry.</span><br><span class=\"line\">      return functionRegistry.lookupFunction(name, children)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; If the name itself is not qualified, add the current database to it.</span><br><span class=\"line\">    val database &#x3D; formatDatabaseName(name.database.getOrElse(getCurrentDatabase))</span><br><span class=\"line\">    val qualifiedName &#x3D; name.copy(database &#x3D; Some(database))</span><br><span class=\"line\"></span><br><span class=\"line\">    if (functionRegistry.functionExists(qualifiedName)) &#123;</span><br><span class=\"line\">      return functionRegistry.lookupFunction(qualifiedName, children)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val catalogFunction &#x3D; try &#123;</span><br><span class=\"line\">      externalCatalog.getFunction(database, name.funcName)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">      case _: AnalysisException &#x3D;&gt; failFunctionLookup(name)</span><br><span class=\"line\">      case _: NoSuchPermanentFunctionException &#x3D;&gt; failFunctionLookup(name)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    loadFunctionResources(catalogFunction.resources)</span><br><span class=\"line\">    registerFunction(catalogFunction.copy(identifier &#x3D; qualifiedName), overrideIfExists &#x3D; false)</span><br><span class=\"line\">    &#x2F;&#x2F; Now, we need to create the Expression.</span><br><span class=\"line\">    functionRegistry.lookupFunction(qualifiedName, children)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Parser 模块负责将原始 SQL 文本解析成 UnResolvedLogicPlan，Anayzer 模块负责将 UnResolvedLogicPlan(Relation|Attribute|Function) 通过 SessionCatalog 信息绑定到真正可操作的实体上(File|Column|MainClass)，即 SolvedLogicPlan，并校验其是否合法。   </p>","more":"<p>则关键点在于:   </p>\n<ol>\n<li>SessionCatalog 存在目的        </li>\n<li>如何绑定，将 UnresolvedLogicPlan 转换为 ResolvedLogicPlan    </li>\n<li>如何校验 ResolvedLogicPlan 合法  </li>\n</ol>\n<p>1.Analyzer 对 UnresolvedLogicPlan应用各类规则(Rule),并对各个 UnresolvedLogicPlan节点绑定 CataLog 信息,将 UnresolvedLogicPlan中的UnresolvedRelation 和 UnresolvedAttribute解析成有类型(Typed)的对象,生成解析后的逻辑算子树 SolvedLogicPlan.<br>2.对 UnresolvedLogicPlan 的大部分操作,如绑定解析优化等,主要方法都是基于规则(Rule)的,然后通过模式匹配对UnresolvedLogicPlan进行相应处理.<br>3.Catalog信息在Analyzer的构造函数中.  </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>调用栈<br>-&gt;SparkSession#sql<br>　-&gt;DataSet#ofRows<br>　　-&gt;QueryExecution#assertAnalyzed<br>　　　-&gt;QueryExecution#analyzed (lazy)<br>　　　　-&gt;Analyzer#executeAndCheck<br>[Anayzer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def executeAndCheck(plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 绑定</span><br><span class=\"line\">    val analyzed &#x3D; execute(plan)</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 第二步: 校验</span><br><span class=\"line\">      checkAnalysis(analyzed)</span><br><span class=\"line\">      &#x2F;&#x2F; 将 LogicPlan 的 AnalysisBarrier 装箱去掉(如果存在)</span><br><span class=\"line\">      EliminateBarriers(analyzed)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">      case e: AnalysisException &#x3D;&gt;</span><br><span class=\"line\">        val ae &#x3D; new AnalysisException(e.message, e.line, e.startPosition, Option(analyzed))</span><br><span class=\"line\">        ae.setStackTrace(e.getStackTrace)</span><br><span class=\"line\">        throw ae</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"SessionCatalog\"><a href=\"#SessionCatalog\" class=\"headerlink\" title=\"SessionCatalog\"></a>SessionCatalog</h3><p>SessionCatalog 是 SparkSession 索引维护的信息库，SessionCatalog 一方面可以管理 SparkSession 创建的临时视图和 UDF，一方面可以作为外部元数据库的代理以便 SparkSession 获取外部源数据库信息(e.g. HiveSessionCatalog 操作 Hive metastore)。    </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">成员变量</th>\n<th align=\"left\">数据类型</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">Catalog 类型</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">tableRelationCache</td>\n<td align=\"left\">Cache[QualifiedTableName, LogicalPlan]</td>\n<td align=\"left\">表全限定名与逻辑计划的映射</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">tempViews</td>\n<td align=\"left\">HashMap[String, LogicalPlan]</td>\n<td align=\"left\">临时视图名与逻辑计划的映射</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">globalTempViewManager</td>\n<td align=\"left\">GlobalTempViewManager</td>\n<td align=\"left\">全局视图管理</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">externalCatalog</td>\n<td align=\"left\">ExternalCatalog</td>\n<td align=\"left\">外部元数据库的代理,从获得连接到库表分区函数等信息获取接口</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">functionRegistry</td>\n<td align=\"left\">FunctionRegistry</td>\n<td align=\"left\">函数(builtin,UDF)操作代理</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">parser</td>\n<td align=\"left\">ParserInterface</td>\n<td align=\"left\">SQL 文本解析 Parser</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">functionResourceLoader</td>\n<td align=\"left\">FunctionResourceLoader</td>\n<td align=\"left\">加载函数主类资源加载器</td>\n<td align=\"left\">SessionCatalog</td>\n</tr>\n<tr>\n<td align=\"left\">metastoreCatalog</td>\n<td align=\"left\">HiveMetastoreCatalog</td>\n<td align=\"left\">HiveExternalCatalog 的旧接口,deprecated</td>\n<td align=\"left\">HiveSessionCatalog</td>\n</tr>\n</tbody></table>\n<p>引用一个通过表名绑定逻辑计划的函数: 通过表所属数据库名在全局视图，外部元数据库和临时视图内搜索以绑定逻辑计划<br>[SessionCatalog.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def lookupRelation(name: TableIdentifier): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    synchronized &#123;</span><br><span class=\"line\">      val db &#x3D; formatDatabaseName(name.database.getOrElse(currentDb))</span><br><span class=\"line\">      val table &#x3D; formatTableName(name.table)</span><br><span class=\"line\">      &#x2F;&#x2F; 全局视图</span><br><span class=\"line\">      if (db &#x3D;&#x3D; globalTempViewManager.database) &#123;</span><br><span class=\"line\">        globalTempViewManager.get(table).map &#123; viewDef &#x3D;&gt;</span><br><span class=\"line\">          SubqueryAlias(table, viewDef)</span><br><span class=\"line\">        &#125;.getOrElse(throw new NoSuchTableException(db, table))</span><br><span class=\"line\">      &#125; else if (name.database.isDefined || !tempViews.contains(table)) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 外部元数据库</span><br><span class=\"line\">        val metadata &#x3D; externalCatalog.getTable(db, table)</span><br><span class=\"line\">        if (metadata.tableType &#x3D;&#x3D; CatalogTableType.VIEW) &#123;</span><br><span class=\"line\">          val viewText &#x3D; metadata.viewText.getOrElse(sys.error(&quot;Invalid view without text.&quot;))</span><br><span class=\"line\">          val child &#x3D; View(</span><br><span class=\"line\">            desc &#x3D; metadata,</span><br><span class=\"line\">            output &#x3D; metadata.schema.toAttributes,</span><br><span class=\"line\">            child &#x3D; parser.parsePlan(viewText))</span><br><span class=\"line\">          SubqueryAlias(table, child)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          SubqueryAlias(table, UnresolvedCatalogRelation(metadata))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 临时视图</span><br><span class=\"line\">        SubqueryAlias(table, tempViews(table))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"绑定\"><a href=\"#绑定\" class=\"headerlink\" title=\"绑定\"></a>绑定</h3><p>Anayzer 模块对 UnResolvedLogicPlan 应用多批规则库，其中主要是 Resolution(解析)规则库和 Substitution(转换)规则库，直到每批规则库达到了指定的迭代次数或者规则库无法再优化逻辑计划为止。  </p>\n<p>调用栈<br>Analyzer#executeAndCheck<br>　-&gt;Analyzer#execute<br>　　-&gt;Analyzer#executeSameContext<br>　　　-&gt;RuleExecutor#execute<br>[RuleExecutor.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 规则批次间和批次内都是串行执行</span><br><span class=\"line\"> * 逻辑计划在此处被 Analyzed 之后就替换了(result&#x2F;curPlan和返回值)</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">def execute(plan: TreeType): TreeType &#x3D; &#123;</span><br><span class=\"line\">  var curPlan &#x3D; plan</span><br><span class=\"line\">  batches.foreach &#123; batch &#x3D;&gt;</span><br><span class=\"line\">    val batchStartPlan &#x3D; curPlan</span><br><span class=\"line\">    var iteration &#x3D; 1</span><br><span class=\"line\">    var lastPlan &#x3D; curPlan\t&#x2F;&#x2F;tempPlan</span><br><span class=\"line\">    var continue &#x3D; true</span><br><span class=\"line\">    &#x2F;&#x2F; foldLeft 依次执行规则 batch 中的所有规则,直到达到了规则的最大执行次数或者逻辑算子树不再变化</span><br><span class=\"line\">    while (continue) &#123;</span><br><span class=\"line\">      curPlan &#x3D; batch.rules.foldLeft(curPlan) &#123; </span><br><span class=\"line\">        case (plan, rule) &#x3D;&gt;</span><br><span class=\"line\">        &#x2F;&#x2F; 在逻辑计划上应用规则处理</span><br><span class=\"line\">          val result &#x3D; rule(plan) &#x2F;&#x2F;rule.apply(plan)</span><br><span class=\"line\">          if (!result.fastEquals(plan)) &#123;</span><br><span class=\"line\">            logTrace(</span><br><span class=\"line\">              s&quot;&quot;&quot;</span><br><span class=\"line\">                |&#x3D;&#x3D;&#x3D; Applying Rule $&#123;rule.ruleName&#125; &#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">                |$&#123;sideBySide(plan.treeString, result.treeString).mkString(&quot;\\n&quot;)&#125;</span><br><span class=\"line\">              &quot;&quot;&quot;.stripMargin)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime)</span><br><span class=\"line\">          queryExecutionMetrics.incNumExecution(rule.ruleName)</span><br><span class=\"line\">          ....</span><br><span class=\"line\">           result</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      iteration +&#x3D; 1</span><br><span class=\"line\">       &#x2F;&#x2F; 达到了该规则批策略的最大迭代次数</span><br><span class=\"line\">      if (iteration &gt; batch.strategy.maxIterations) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        continue &#x3D; false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 这批规则已经不能再优化逻辑计划了</span><br><span class=\"line\">      if (curPlan.fastEquals(lastPlan)) &#123;</span><br><span class=\"line\">        logTrace(</span><br><span class=\"line\">          s&quot;Fixed point reached for batch $&#123;batch.name&#125; after $&#123;iteration - 1&#125; iterations.&quot;)</span><br><span class=\"line\">        continue &#x3D; false</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      lastPlan &#x3D; curPlan</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  curPlan</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>重点是其中的 rule(plan)方法，即 Rule#apply(plan)。各规则子类依据自身的功能解析转换 TreeNode，包括绑定 UnResolvedLogicPlan 为 ResolvedLogicPlan<br> [Rule.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">abstract class Rule[TreeType &lt;: TreeNode[_]] extends Logging &#123;</span><br><span class=\"line\">  val ruleName: String &#x3D; &#123;</span><br><span class=\"line\">    val className &#x3D; getClass.getName</span><br><span class=\"line\">    if (className endsWith &quot;$&quot;) className.dropRight(1) else className</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  def apply(plan: TreeType): TreeType</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>规则的具体功能列表参考”附录：规则功能列表”   </p>\n<h3 id=\"校验\"><a href=\"#校验\" class=\"headerlink\" title=\"校验\"></a>校验</h3><p>调用栈<br>Analyze#executeAndCheck<br>　-&gt;CheckAndAnalysis#checkAnalysis  </p>\n<p>检查 ResolvedLogicPlan 中不合乎语法规范的错误，使用后序遍历(先子节点后当前节点)的方式尽可能抛出最先导致失败的错误<br>[CheckAndAnaysis.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def checkAnalysis(plan: LogicalPlan): Unit &#x3D; &#123;</span><br><span class=\"line\">    plan.foreachUp &#123;</span><br><span class=\"line\">      case u: UnresolvedRelation &#x3D;&gt;</span><br><span class=\"line\">        u.failAnalysis(s&quot;Table or view not found: $&#123;u.tableIdentifier&#125;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">      case operator: LogicalPlan &#x3D;&gt;</span><br><span class=\"line\">        operator transformExpressionsUp &#123;</span><br><span class=\"line\">          case a: Attribute if !a.resolved &#x3D;&gt;</span><br><span class=\"line\">            val from &#x3D; operator.inputSet.map(_.qualifiedName).mkString(&quot;, &quot;)</span><br><span class=\"line\">            a.failAnalysis(s&quot;cannot resolve &#39;$&#123;a.sql&#125;&#39; given input columns: [$from]&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case e: Expression if e.checkInputDataTypes().isFailure &#x3D;&gt;</span><br><span class=\"line\">            e.checkInputDataTypes() match &#123;</span><br><span class=\"line\">              case TypeCheckResult.TypeCheckFailure(message) &#x3D;&gt;</span><br><span class=\"line\">                e.failAnalysis(</span><br><span class=\"line\">                  s&quot;cannot resolve &#39;$&#123;e.sql&#125;&#39; due to data type mismatch: $message&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          case c: Cast if !c.resolved &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;invalid cast from $&#123;c.child.dataType.simpleString&#125; to $&#123;c.dataType.simpleString&#125;&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case g: Grouping &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;grouping() can only be used with GroupingSets&#x2F;Cube&#x2F;Rollup&quot;)</span><br><span class=\"line\">          case g: GroupingID &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;grouping_id() can only be used with GroupingSets&#x2F;Cube&#x2F;Rollup&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(AggregateExpression(_, _, true, _), _) &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(s&quot;Distinct window functions are not supported: $w&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(_: OffsetWindowFunction,</span><br><span class=\"line\">            WindowSpecDefinition(_, order, frame: SpecifiedWindowFrame))</span><br><span class=\"line\">             if order.isEmpty || !frame.isOffset &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;An offset window function can only be evaluated in an ordered &quot; +</span><br><span class=\"line\">              s&quot;row-based window frame with a single offset: $w&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case w @ WindowExpression(e, s) &#x3D;&gt;</span><br><span class=\"line\">            &#x2F;&#x2F; Only allow window functions with an aggregate expression or an offset window</span><br><span class=\"line\">            &#x2F;&#x2F; function.</span><br><span class=\"line\">            e match &#123;</span><br><span class=\"line\">              case _: AggregateExpression | _: OffsetWindowFunction | _: AggregateWindowFunction &#x3D;&gt;</span><br><span class=\"line\">                w</span><br><span class=\"line\">              case _ &#x3D;&gt;</span><br><span class=\"line\">                failAnalysis(s&quot;Expression &#39;$e&#39; not supported within a window function.&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">          case s: SubqueryExpression &#x3D;&gt;</span><br><span class=\"line\">            checkSubqueryExpression(operator, s)</span><br><span class=\"line\">            s</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        operator match &#123;</span><br><span class=\"line\">          case etw: EventTimeWatermark &#x3D;&gt;</span><br><span class=\"line\">            etw.eventTime.dataType match &#123;</span><br><span class=\"line\">              case s: StructType</span><br><span class=\"line\">                if s.find(_.name &#x3D;&#x3D; &quot;end&quot;).map(_.dataType) &#x3D;&#x3D; Some(TimestampType) &#x3D;&gt;</span><br><span class=\"line\">              case _: TimestampType &#x3D;&gt;</span><br><span class=\"line\">              case _ &#x3D;&gt;</span><br><span class=\"line\">                failAnalysis(</span><br><span class=\"line\">                  s&quot;Event time must be defined on a window or a timestamp, but &quot; +</span><br><span class=\"line\">                  s&quot;$&#123;etw.eventTime.name&#125; is of type $&#123;etw.eventTime.dataType.simpleString&#125;&quot;)</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case f: Filter if f.condition.dataType !&#x3D; BooleanType &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;filter expression &#39;$&#123;f.condition.sql&#125;&#39; &quot; +</span><br><span class=\"line\">                s&quot;of type $&#123;f.condition.dataType.simpleString&#125; is not a boolean.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case Filter(condition, _) if hasNullAwarePredicateWithinNot(condition) &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(&quot;Null-aware predicate sub-queries cannot be used in nested &quot; +</span><br><span class=\"line\">              s&quot;conditions: $condition&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">          case j @ Join(_, _, _, Some(condition)) if condition.dataType !&#x3D; BooleanType &#x3D;&gt;</span><br><span class=\"line\">            failAnalysis(</span><br><span class=\"line\">              s&quot;join condition &#39;$&#123;condition.sql&#125;&#39; &quot; +</span><br><span class=\"line\">                s&quot;of type $&#123;condition.dataType.simpleString&#125; is not a boolean.&quot;)</span><br><span class=\"line\">\t..........</span><br><span class=\"line\">\t&#x2F;&#x2F; 自定义的规则检查在内置规则检查之后才能开始</span><br><span class=\"line\">    extendedCheckRules.foreach(_(plan))</span><br><span class=\"line\">    plan.foreachUp &#123;</span><br><span class=\"line\">      case AnalysisBarrier(child) if !child.resolved &#x3D;&gt; checkAnalysis(child)</span><br><span class=\"line\">      case o if !o.resolved &#x3D;&gt; failAnalysis(s&quot;unresolved operator $&#123;o.simpleString&#125;&quot;)</span><br><span class=\"line\">      case _ &#x3D;&gt;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"TODO\"><a href=\"#TODO\" class=\"headerlink\" title=\"TODO\"></a>TODO</h2><p>UnResolvedLogicPlan 在绑定了 Catalog 之后可以转换为 RDD 模式来执行了，但是由于提交的 SQL 质量参差不齐，按照 ResolvedLogicPlan 按部就班的执行会导致代价/效率差距很大 且 要求用户对执行引擎的执行模式很了解并熟悉 SQL 优化手段才能写出效率高执行快的 SQL。所以为了尽可能忽略用户的代码质量，对SQL 优化的熟悉程度，SparkSQL 都需要以很高的效率执行，SparkSQL 在后续阶段需要对 UnResolvedLogicPlan 进行优化，即 Rule-Based Optimizer，也称为 RBO。    </p>\n<h2 id=\"附录：规则功能列表\"><a href=\"#附录：规则功能列表\" class=\"headerlink\" title=\"附录：规则功能列表\"></a>附录：规则功能列表</h2><p>规则库 Batch: 在 UnResolvedLogicPlan 上应用规则库 rules 的执行策略 strategy。 \n[Batch.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)</span><br></pre></td></tr></table></figure>\n<p>Strategy 表示规则批的最大迭代次数，有两个子类: Once 表明只需应用一次规则库即可；FixedPoint 表示最大可以应用规则库 maxIterations 次，如果应用中途无法再优化逻辑计划则跳出。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">abstract class Strategy &#123; def maxIterations: Int &#125;</span><br><span class=\"line\">case object Once extends Strategy &#123; val maxIterations &#x3D; 1 &#125;</span><br><span class=\"line\">case class FixedPoint(maxIterations: Int) extends Strategy</span><br></pre></td></tr></table></figure>\n<p>规则 Rule 的子类有很多,在 Analyze.scala 中使用的所有规则如下<br>[Analyzer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lazy val batches: Seq[Batch] &#x3D; Seq(</span><br><span class=\"line\">  Batch(&quot;Hints&quot;, fixedPoint,</span><br><span class=\"line\">    new ResolveHints.ResolveBroadcastHints(conf),</span><br><span class=\"line\">    ResolveHints.RemoveAllHints),</span><br><span class=\"line\">  Batch(&quot;Simple Sanity Check&quot;, Once,</span><br><span class=\"line\">    LookupFunctions),</span><br><span class=\"line\">  Batch(&quot;Substitution&quot;, fixedPoint,</span><br><span class=\"line\">    CTESubstitution,</span><br><span class=\"line\">    WindowsSubstitution,</span><br><span class=\"line\">    EliminateUnions,</span><br><span class=\"line\">    new SubstituteUnresolvedOrdinals(conf)),</span><br><span class=\"line\">  Batch(&quot;Resolution&quot;, fixedPoint,</span><br><span class=\"line\">    ResolveTableValuedFunctions ::</span><br><span class=\"line\">    ResolveRelations ::</span><br><span class=\"line\">    ResolveReferences ::</span><br><span class=\"line\">    ResolveCreateNamedStruct ::</span><br><span class=\"line\">    ResolveDeserializer ::</span><br><span class=\"line\">    ResolveNewInstance ::</span><br><span class=\"line\">    ResolveUpCast ::</span><br><span class=\"line\">    ResolveGroupingAnalytics ::</span><br><span class=\"line\">    ResolvePivot ::</span><br><span class=\"line\">    ResolveOrdinalInOrderByAndGroupBy ::</span><br><span class=\"line\">    ResolveAggAliasInGroupBy ::</span><br><span class=\"line\">    ResolveMissingReferences ::</span><br><span class=\"line\">    ExtractGenerator ::</span><br><span class=\"line\">    ResolveGenerate ::</span><br><span class=\"line\">    ResolveFunctions ::</span><br><span class=\"line\">    ResolveAliases ::</span><br><span class=\"line\">    ResolveSubquery ::</span><br><span class=\"line\">    ResolveSubqueryColumnAliases ::</span><br><span class=\"line\">    ResolveWindowOrder ::</span><br><span class=\"line\">    ResolveWindowFrame ::</span><br><span class=\"line\">    ResolveNaturalAndUsingJoin ::</span><br><span class=\"line\">    ExtractWindowExpressions ::</span><br><span class=\"line\">    GlobalAggregates ::</span><br><span class=\"line\">    ResolveAggregateFunctions ::</span><br><span class=\"line\">    TimeWindowing ::</span><br><span class=\"line\">    ResolveInlineTables(conf) ::</span><br><span class=\"line\">    ResolveTimeZone(conf) ::</span><br><span class=\"line\">    TypeCoercion.typeCoercionRules(conf) ++</span><br><span class=\"line\">    extendedResolutionRules : _*),</span><br><span class=\"line\">  Batch(&quot;Post-Hoc Resolution&quot;, Once, postHocResolutionRules: _*),</span><br><span class=\"line\">  Batch(&quot;View&quot;, Once,</span><br><span class=\"line\">    AliasViewChild(conf)),</span><br><span class=\"line\">  Batch(&quot;Nondeterministic&quot;, Once,</span><br><span class=\"line\">    PullOutNondeterministic),</span><br><span class=\"line\">  Batch(&quot;UDF&quot;, Once,</span><br><span class=\"line\">    HandleNullInputsForUDF),</span><br><span class=\"line\">  Batch(&quot;FixNullability&quot;, Once,</span><br><span class=\"line\">    FixNullability),</span><br><span class=\"line\">  Batch(&quot;Subquery&quot;, Once,</span><br><span class=\"line\">    UpdateOuterReferences),</span><br><span class=\"line\">  Batch(&quot;Cleanup&quot;, fixedPoint,</span><br><span class=\"line\">    CleanupAliases)</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<p>其中最重要的规则库是Substitution(替换)和Resolution(解析绑定)。这些规则库之间及规则库之内都是有序的，使用规则时也是串行执行的。打乱了顺序可能有转换不充分等未知的错误<br>下表主要参照 org.apache.spark.sql.catalyst.parser.SqlBase.g4 文件和源码，带问号的为不确定的。</p>\n<table>\n<thead>\n<tr>\n<th>规则批次</th>\n<th>规则</th>\n<th>功能</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hints</td>\n<td>ResolveBroadcastHints</td>\n<td>广播</td>\n<td>mapjoin</td>\n</tr>\n<tr>\n<td>Hints</td>\n<td>RemoveAllHints</td>\n<td>删除无效标识符</td>\n<td></td>\n</tr>\n<tr>\n<td>Simple Sanity Check</td>\n<td>LookupFunctions</td>\n<td>函数存在性检验</td>\n<td></td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>CTESubstitution</td>\n<td>with,合并计划</td>\n<td>with a as something select * from a</td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>WindowsSubstitution</td>\n<td>窗口函数</td>\n<td>row_number() over (partitioned by a sort by b desc)</td>\n</tr>\n<tr>\n<td>Substitution</td>\n<td>EliminateUnions</td>\n<td>union的计划只有一个时删除 union</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveTableValuedFunctions</td>\n<td>解析可作为表的函数</td>\n<td>range函数</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveRelations</td>\n<td>表绑定catalog中的逻辑计划</td>\n<td>对应createXXXView接口</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveReferences</td>\n<td>展开星号绑定列</td>\n<td>select a.* from</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveCreateNamedStruct</td>\n<td>解析结构体构造方法</td>\n<td>?,没见过hql中使用结构体</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveDeserializer</td>\n<td>解析反序列化类</td>\n<td>decoder</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveNewInstance</td>\n<td>解析创建实例</td>\n<td>encoder</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveUpCast</td>\n<td>解析类型转换</td>\n<td>cast,在丢失精度时抛异常</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveGroupingAnalytics</td>\n<td>解析rollup多维度分析</td>\n<td>group by a,b,c with rollup 等价于 group by a,b,c grouping sets((a,b,c),(a,b),(a),())</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolvePivot</td>\n<td>行转列</td>\n<td>pivot 接口</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveOrdinalInOrderByAndGroupBy</td>\n<td>解析order/sort/group by语句的下标数字</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAggAliasInGroupBy</td>\n<td>解析聚合时的表达式</td>\n<td>group by case when then x else y end</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveMissingReferences</td>\n<td>解析在排序时不存在的列,加上但隐藏该列</td>\n<td>select a from … order by b</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ExtractGenerator</td>\n<td>解析UDTF生成器</td>\n<td>select explode(xx) from ..</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveGenerate</td>\n<td>?</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveFunctions</td>\n<td>解析函数为表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAliases</td>\n<td>解析生成别名表达式</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveSubquery</td>\n<td>解析子查询</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveSubqueryColumnAliases</td>\n<td>解析子查询列别名</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveWindowOrder</td>\n<td>解析窗口函数中的排序</td>\n<td>over partitioned by .. order by..</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveWindowFrame</td>\n<td>解析检验窗口函数</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveNaturalAndUsingJoin</td>\n<td>通过输出列解析自然连接</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ExtractWindowExpressions</td>\n<td>提取窗口函数表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>GlobalAggregates</td>\n<td>解析全局聚合</td>\n<td>select max(a) from tbl</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveAggregateFunctions</td>\n<td>解析不在聚合中的聚合函数</td>\n<td>having/order by</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>TimeWindowing</td>\n<td>解析滑动时间窗口</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveInlineTables</td>\n<td>解析内联表为LocalRelation</td>\n<td>select * fom values(..),(..) as (columns)</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>ResolveTimeZone</td>\n<td>解析时区表达式</td>\n<td></td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>TypeCoercion.typeCoercionRules</td>\n<td>强制转换为兼容类型</td>\n<td>在比较和 union 时使用,不损失精度</td>\n</tr>\n<tr>\n<td>Resolution</td>\n<td>extendedResolutionRules</td>\n<td>拓展规则,空集合</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Post-Hoc Resolution</td>\n<td>postHocResolutionRules</td>\n<td>Resolution 规则后执行的规则.空集合</td>\n<td></td>\n</tr>\n<tr>\n<td>View</td>\n<td>AliasViewChild</td>\n<td>视图的分析规则</td>\n<td></td>\n</tr>\n<tr>\n<td>Nondeterministic</td>\n<td>PullOutNondeterministic</td>\n<td>提取非确定性表达式,放到child 中</td>\n<td>?</td>\n</tr>\n<tr>\n<td>UDF</td>\n<td>HandleNullInputsForUDF</td>\n<td>对UDF增加基本数据类型null处理(空输入则空输出)</td>\n<td></td>\n</tr>\n<tr>\n<td>FixNullability</td>\n<td>FixNullability</td>\n<td>通过 child 字段的 Nullablity 修复父逻辑计划字段的 Nullablity</td>\n<td></td>\n</tr>\n<tr>\n<td>Subquery</td>\n<td>UpdateOuterReferences</td>\n<td>聚合表达式下推</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Cleanup</td>\n<td>CleanupAliases</td>\n<td>删除不需要的别名</td>\n<td></td>\n</tr>\n</tbody></table>\n<h3 id=\"with\"><a href=\"#with\" class=\"headerlink\" title=\"with\"></a>with</h3><p>with a  as … select * from a 语法是 SQL 语句中的一个特殊 case，with 子句打乱了 SQL 解析的通用模式，无法从上至下从左至右进行解析，只能将整个语句解析为两个相对独立的逻辑算子树，然后通过别名将 with 语句的逻辑算子树加入到主体逻辑算子树的下面  </p>\n<p>[WindowsSubstitution.scala] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object WindowsSubstitution extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">  def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;使用多层 match case 来匹配拿到 window 函数,</span><br><span class=\"line\">    case WithWindowDefinition(windowDefinitions, child) &#x3D;&gt;</span><br><span class=\"line\">      child.transform &#123;</span><br><span class=\"line\">        case p &#x3D;&gt; p.transformExpressions &#123;</span><br><span class=\"line\">          case UnresolvedWindowExpression(c, WindowSpecReference(windowName)) &#x3D;&gt;</span><br><span class=\"line\">            val errorMessage &#x3D;</span><br><span class=\"line\">              s&quot;Window specification $windowName is not defined in the WINDOW clause.&quot;</span><br><span class=\"line\">            val windowSpecDefinition &#x3D;</span><br><span class=\"line\">              windowDefinitions.getOrElse(windowName, failAnalysis(errorMessage))</span><br><span class=\"line\">            WindowExpression(c, windowSpecDefinition)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"relation\"><a href=\"#relation\" class=\"headerlink\" title=\"relation\"></a>relation</h3><p>绑定关系<br>[ResolveRelations.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> object ResolveRelations extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">  &#x2F;&#x2F; 后序遍历尝试绑定每个匹配的节点到 ResolvedLogicPlan</span><br><span class=\"line\">   def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">     case i @ InsertIntoTable(u: UnresolvedRelation, parts, child, _, _) if child.resolved &#x3D;&gt;</span><br><span class=\"line\">      &#x2F;&#x2F; lookupTableFromCatalog(u) 从全局视图&gt;外接元数据库&gt;临时视图中绑定实体  </span><br><span class=\"line\">      &#x2F;&#x2F; EliminateSubqueryAliases 去掉别名将子查询查询计划剥离出来</span><br><span class=\"line\">       EliminateSubqueryAliases(lookupTableFromCatalog(u)) match &#123;</span><br><span class=\"line\">         case v: View &#x3D;&gt;</span><br><span class=\"line\">           u.failAnalysis(s&quot;Inserting into a view is not allowed. View: $&#123;v.desc.identifier&#125;.&quot;)</span><br><span class=\"line\">         case other &#x3D;&gt; i.copy(table &#x3D; other)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     case u: UnresolvedRelation &#x3D;&gt; resolveRelation(u)\t</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   先看第一个 case,如果是 insert into 句法的话,借助 catalog的&lt;tableName,LogicPlan&gt;缓存绑定表为已解析过的逻辑计划</span><br><span class=\"line\">   private def lookupTableFromCatalog(</span><br><span class=\"line\">       u: UnresolvedRelation,</span><br><span class=\"line\">       defaultDatabase: Option[String] &#x3D; None): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">     val tableIdentWithDb &#x3D; u.tableIdentifier.copy(</span><br><span class=\"line\">       database &#x3D; u.tableIdentifier.database.orElse(defaultDatabase))</span><br><span class=\"line\">     try &#123;</span><br><span class=\"line\">       catalog.lookupRelation(tableIdentWithDb)</span><br><span class=\"line\">     &#125; catch &#123;</span><br><span class=\"line\">\t....        </span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\">[SessionCatalog.scala] </span><br><span class=\"line\"> def lookupRelation(name: TableIdentifier): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">   synchronized &#123;</span><br><span class=\"line\">     val db &#x3D; formatDatabaseName(name.database.getOrElse(currentDb))</span><br><span class=\"line\">     val table &#x3D; formatTableName(name.table)</span><br><span class=\"line\">     &#x2F;&#x2F;从全局临时视图,外接元数据库,临时视图中查找该表的逻辑计划</span><br><span class=\"line\">     if (db &#x3D;&#x3D; globalTempViewManager.database) &#123;</span><br><span class=\"line\">       globalTempViewManager.get(table).map &#123; viewDef &#x3D;&gt;</span><br><span class=\"line\">         SubqueryAlias(table, viewDef)</span><br><span class=\"line\">       &#125;.getOrElse(throw new NoSuchTableException(db, table))</span><br><span class=\"line\">     &#125; else if (name.database.isDefined || !tempViews.contains(table)) &#123;</span><br><span class=\"line\">       val metadata &#x3D; externalCatalog.getTable(db, table)</span><br><span class=\"line\">       if (metadata.tableType &#x3D;&#x3D; CatalogTableType.VIEW) &#123;</span><br><span class=\"line\">         val viewText &#x3D; metadata.viewText.getOrElse(sys.error(&quot;Invalid view without text.&quot;))</span><br><span class=\"line\">         val child &#x3D; View(</span><br><span class=\"line\">           desc &#x3D; metadata,</span><br><span class=\"line\">           output &#x3D; metadata.schema.toAttributes,</span><br><span class=\"line\">           child &#x3D; parser.parsePlan(viewText))</span><br><span class=\"line\">         SubqueryAlias(table, child)</span><br><span class=\"line\">       &#125; else &#123;</span><br><span class=\"line\">         SubqueryAlias(table, UnresolvedCatalogRelation(metadata))</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125; else &#123;</span><br><span class=\"line\">       SubqueryAlias(table, tempViews(table))</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> 再看ResolveRelations.apply方法的第二个 case u:UnresolvedRelation &#x3D;&gt; resolveRelation(u),所以首先会进入下面方法的第一个 case, 即如果不是直接在文件上运行(from json.$path)的话,先从 Catalog 中绑定逻辑计划</span><br><span class=\"line\"> [ResolveRelation.scala]</span><br><span class=\"line\">   def resolveRelation(plan: LogicalPlan): LogicalPlan &#x3D; plan match &#123;</span><br><span class=\"line\">     case u: UnresolvedRelation if !isRunningDirectlyOnFiles(u.tableIdentifier) &#x3D;&gt;</span><br><span class=\"line\">       val defaultDatabase &#x3D; AnalysisContext.get.defaultDatabase</span><br><span class=\"line\">       val foundRelation &#x3D; lookupTableFromCatalog(u, defaultDatabase)</span><br><span class=\"line\">       resolveRelation(foundRelation)</span><br><span class=\"line\">     case view @ View(desc, _, child) if !child.resolved &#x3D;&gt;</span><br><span class=\"line\">       &#x2F;&#x2F; Resolve all the UnresolvedRelations and Views in the child.</span><br><span class=\"line\">       val newChild &#x3D; AnalysisContext.withAnalysisContext(desc.viewDefaultDatabase) &#123;</span><br><span class=\"line\">         if (AnalysisContext.get.nestedViewDepth &gt; conf.maxNestedViewDepth) &#123;</span><br><span class=\"line\">           view.failAnalysis(s&quot;The depth of view $&#123;view.desc.identifier&#125; exceeds the maximum &quot; +</span><br><span class=\"line\">             s&quot;view resolution depth ($&#123;conf.maxNestedViewDepth&#125;). Analysis is aborted to &quot; +</span><br><span class=\"line\">             s&quot;avoid errors. Increase the value of $&#123;SQLConf.MAX_NESTED_VIEW_DEPTH.key&#125; to work &quot; +</span><br><span class=\"line\">             &quot;around this.&quot;)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">         executeSameContext(child)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       view.copy(child &#x3D; newChild)</span><br><span class=\"line\">     case p @ SubqueryAlias(_, view: View) &#x3D;&gt;</span><br><span class=\"line\">       val newChild &#x3D; resolveRelation(view)</span><br><span class=\"line\">       p.copy(child &#x3D; newChild)</span><br><span class=\"line\">     case _ &#x3D;&gt; plan</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"LookupFunctions\"><a href=\"#LookupFunctions\" class=\"headerlink\" title=\"LookupFunctions\"></a>LookupFunctions</h3><p>函数存在性校验<br>[LookupFuncations.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object LookupFunctions extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">    override def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformAllExpressions &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 递归检测使用的函数在 catalog 中是否存在</span><br><span class=\"line\">      case f: UnresolvedFunction if !catalog.functionExists(f.name) &#x3D;&gt;</span><br><span class=\"line\">        withPosition(f) &#123;</span><br><span class=\"line\">          throw new NoSuchFunctionException(f.name.database.getOrElse(&quot;default&quot;), f.name.funcName)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[SessionCatalog.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def functionExists(name: FunctionIdentifier): Boolean &#x3D; &#123;</span><br><span class=\"line\">    val db &#x3D; formatDatabaseName(name.database.getOrElse(getCurrentDatabase))</span><br><span class=\"line\">    requireDbExists(db)</span><br><span class=\"line\">    &#x2F;&#x2F; 使用 FuncationRegistry 和外接元数据库检测函数是否存在</span><br><span class=\"line\">    functionRegistry.functionExists(name) ||</span><br><span class=\"line\">      externalCatalog.functionExists(db, name.funcName)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"ResolveFunction\"><a href=\"#ResolveFunction\" class=\"headerlink\" title=\"ResolveFunction\"></a>ResolveFunction</h3><p>将函数替换为具体的表达式<br>[ResolveFunction.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">object ResolveFunctions extends Rule[LogicalPlan] &#123;</span><br><span class=\"line\">    def apply(plan: LogicalPlan): LogicalPlan &#x3D; plan.transformUp &#123;</span><br><span class=\"line\">      case q: LogicalPlan &#x3D;&gt;</span><br><span class=\"line\">        q transformExpressions &#123;</span><br><span class=\"line\">          case u if !u.childrenResolved &#x3D;&gt; u &#x2F;&#x2F; Skip until children are resolved.</span><br><span class=\"line\">          case u: UnresolvedAttribute if resolver(u.name, VirtualColumn.hiveGroupingIdName) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              Alias(GroupingID(Nil), VirtualColumn.hiveGroupingIdName)()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case u @ UnresolvedGenerator(name, children) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              catalog.lookupFunction(name, children) match &#123;</span><br><span class=\"line\">                case generator: Generator &#x3D;&gt; generator</span><br><span class=\"line\">                case other &#x3D;&gt;</span><br><span class=\"line\">                  failAnalysis(s&quot;$name is expected to be a generator. However, &quot; +</span><br><span class=\"line\">                    s&quot;its class is $&#123;other.getClass.getCanonicalName&#125;, which is not a generator.&quot;)</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          case u @ UnresolvedFunction(funcId, children, isDistinct) &#x3D;&gt;</span><br><span class=\"line\">            withPosition(u) &#123;</span><br><span class=\"line\">              &#x2F;&#x2F; 使用 catalog 寻找函数</span><br><span class=\"line\">              catalog.lookupFunction(funcId, children) match &#123;</span><br><span class=\"line\">                case wf: AggregateWindowFunction &#x3D;&gt;</span><br><span class=\"line\">                  if (isDistinct) &#123;</span><br><span class=\"line\">                    failAnalysis(s&quot;$&#123;wf.prettyName&#125; does not support the modifier DISTINCT&quot;)</span><br><span class=\"line\">                  &#125; else &#123;</span><br><span class=\"line\">                    wf</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">                case agg: AggregateFunction &#x3D;&gt; AggregateExpression(agg, Complete, isDistinct)</span><br><span class=\"line\">                case other &#x3D;&gt;</span><br><span class=\"line\">                  if (isDistinct) &#123;</span><br><span class=\"line\">                    failAnalysis(s&quot;$&#123;other.prettyName&#125; does not support the modifier DISTINCT&quot;)</span><br><span class=\"line\">                  &#125; else &#123;</span><br><span class=\"line\">                    other</span><br><span class=\"line\">                  &#125;</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>使用 SessionCatalog 优先从 FunctionRegistry 中寻找，然后从外接元数据库寻找并加载资源注册到 FunctionRegistry，有些像双亲委派模型，保证 builtin 的函数不被 UDF覆盖。<br>[SessionCatalog.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def lookupFunction(</span><br><span class=\"line\">      name: FunctionIdentifier,</span><br><span class=\"line\">      children: Seq[Expression]): Expression &#x3D; synchronized &#123;</span><br><span class=\"line\">    if (name.database.isEmpty &amp;&amp; functionRegistry.functionExists(name)) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; This function has been already loaded into the function registry.</span><br><span class=\"line\">      return functionRegistry.lookupFunction(name, children)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; If the name itself is not qualified, add the current database to it.</span><br><span class=\"line\">    val database &#x3D; formatDatabaseName(name.database.getOrElse(getCurrentDatabase))</span><br><span class=\"line\">    val qualifiedName &#x3D; name.copy(database &#x3D; Some(database))</span><br><span class=\"line\"></span><br><span class=\"line\">    if (functionRegistry.functionExists(qualifiedName)) &#123;</span><br><span class=\"line\">      return functionRegistry.lookupFunction(qualifiedName, children)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    val catalogFunction &#x3D; try &#123;</span><br><span class=\"line\">      externalCatalog.getFunction(database, name.funcName)</span><br><span class=\"line\">    &#125; catch &#123;</span><br><span class=\"line\">      case _: AnalysisException &#x3D;&gt; failFunctionLookup(name)</span><br><span class=\"line\">      case _: NoSuchPermanentFunctionException &#x3D;&gt; failFunctionLookup(name)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    loadFunctionResources(catalogFunction.resources)</span><br><span class=\"line\">    registerFunction(catalogFunction.copy(identifier &#x3D; qualifiedName), overrideIfExists &#x3D; false)</span><br><span class=\"line\">    &#x2F;&#x2F; Now, we need to create the Expression.</span><br><span class=\"line\">    functionRegistry.lookupFunction(qualifiedName, children)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>"},{"layout":"post","title":"SparkSQL源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan","subtitle":"SparkSQL 源码解析","date":"2019-12-09T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 概述  \n\nParser 模块负责将 SQL 文本解析为 UnResolvedLogicPlan，Analyzer 模块负责将 UnResolvedLogicPlan 绑定解析为 ResolvedLogicPlan。Rule-Based Optimizer，简称 RBO 或 Optimizer 模块负责基于规则优化逻辑计划，主要思路是基于各类型规则对 ResolvedLogicPlan 进行优化达到减少每个逻辑计划树节点输入数据量或计算代价的目的，大名鼎鼎的列裁剪和谓词下推、常量折叠就是出自于 Optimizer 模块。      \n\n<!-- more -->\n\n## 源码解析  \n\n[QueryExecution.scala]\n\n```\n// 第一步: 使用 CacheManager 替换逻辑计划为已解析过的逻辑计划\nlazy val withCachedData: LogicalPlan = {\n    assertAnalyzed()\n    assertSupported()\n    sparkSession.sharedState.cacheManager.useCachedData(analyzed)\n  }\n // 第二步: 基于规则优化逻辑计划树\n  lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)\n```\n\n### CacheManager \n\n[CacheManager.scala]\n\n```\nprivate val cachedData = new java.util.LinkedList[CachedData]\n\ndef useCachedData(plan: LogicalPlan): LogicalPlan = {\n    // 先序遍历\n    val newPlan = plan transformDown {\n      case hint: ResolvedHint => hint\n      case currentFragment =>\n        // 检索 CacheManager\n        lookupCachedData(currentFragment)\n           // 使用当前计划的输出字段替换原有的\n          .map(_.cachedRepresentation.withOutput(currentFragment.output))\n          .getOrElse(currentFragment)\n    }\n    newPlan transformAllExpressions {\n      // 剥去子查询别名外壳\n      case s: SubqueryExpression => s.withNewPlan(useCachedData(s.plan))\n    }\n  }\n  \n def lookupCachedData(plan: LogicalPlan): Option[CachedData] = readLock {\n    // 遍历对比CacheManager 中是否存在和当前计划一致的  \n    cachedData.asScala.find(cd => plan.sameResult(cd.plan))\n} \n```\n通常判断两个查询计划是否相同是很复杂的问题，因此可能实际相同也可以返回 false，但是实际不同一定不能返回 true。这种行为不会影响正确性，仅仅相当于弃用了 CacheManager，多做了一些重复处理而已。这种思想有点像 BloomFilter 的误判率: 函数判断在集合但是实际上可能不在，仅仅是多个 hash 函数得到的位置在 bitmap 中恰好非 0；返回不在集合时一定不在。  \n[QueryPlan.scala]\n\n```\n// 将逻辑计划规范化之后再做比较\nfinal def sameResult(other: PlanType): Boolean = this.canonicalized == other.canonicalized\n// 规范化 逻辑计划涉及的所有字段的 id(从 0 递增)。\nprotected def doCanonicalize(): PlanType = {\n    val canonicalizedChildren = children.map(_.canonicalized)\n    var id = -1\n    mapExpressions {\n      case a: Alias =>\n        id += 1\n        val normalizedChild = QueryPlan.normalizeExprId(a.child, allAttributes)\n        Alias(normalizedChild, \"\")(ExprId(id), a.qualifier)\n\n      case ar: AttributeReference if allAttributes.indexOf(ar.exprId) == -1 =>\n        id += 1\n        ar.withExprId(ExprId(id)).canonicalized\n\n      case other => QueryPlan.normalizeExprId(other, allAttributes)\n    }.withNewChildren(canonicalizedChildren)\n  }\n```\n有必要提一下 InMemoryRelation 这个类，这是 CacheManager 缓存已被 RBO 优化过的内存数据结构，包括：逻辑计划的输出字段，是否压缩(spark.sql.inMemoryColumnarStorage.compressed，默认 true，SparkSQL 将根据元数据信息自动匹配编码器)，批处理大小(spark.sql.inMemoryColumnarStorage.batchSize，默认 10000，控制列式存储的批处理大小，较大的值能提高内存利用率和压缩率 但是读数据时可能会 OOM)，存储等级(默认 MEMORY_AND_DISK)，RBO 优化完成的物理计划 SparkPlan，表名，对应 RDD[CachedBatch]，物理数据字节数，额外有一个<b> statsOfPlanToCache 暂时不知用途</b>  \n[InMemoryRelation.scala]\n\n```\ncase class InMemoryRelation(\n    output: Seq[Attribute],\n    useCompression: Boolean,\n    batchSize: Int,\n    storageLevel: StorageLevel,\n    @transient child: SparkPlan,\n    tableName: Option[String])(\n    @transient var _cachedColumnBuffers: RDD[CachedBatch] = null,\n    val sizeInBytesStats: LongAccumulator = child.sqlContext.sparkContext.longAccumulator,\n    statsOfPlanToCache: Statistics)\n  extends logical.LeafNode with MultiInstanceRelation\n```\nCachedBatch 用以表明 batch 中数据行数，序列化的列数据 buffer，数据的元数据类型。  \n[CachedBatch.scala]\n\n```\ncase class CachedBatch(numRows: Int, buffers: Array[Array[Byte]], stats: InternalRow)\n```\n\n### RBO\nSessionState 初始化时指定的 Optimizer 为 SparkOptimizer，SparkOptimizer 继承自 Optimizer，Optimizer 与 Analyzer 均继承自 RuleExecutor，都使用RuleExecutor#execute 方法遍历逻辑计划树应用 Optimizer/Analyzer 各自指定的规则库和执行策略 优化每个逻辑计划。故不再引用 RuleExecutor#execute 方法。       \n[BaseSessionStateBuilder.scala]  \n\n```\nprotected def optimizer: Optimizer = {\n    new SparkOptimizer(catalog, experimentalMethods) {\n      override def extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] =\n        super.extendedOperatorOptimizationRules ++ customOperatorOptimizationRules\n    }\n  }\n```\n除了类似于前后拦截器功能的 preOptimizationBatches、postHocOptimizationBatches 和拓展规则之外，SparkOptimizer 主要引用了 Optimizer 定义的通用规则，还有 4 个自定义规则:  \n\n1.  OptimizeMetadataOnlyQuery，优化那些只遍历表分区键级别的元数据就可以完成的逻辑计划   　\n\t* 在分区键上做聚合。e.g. SELECT col FROM tbl GROUP by col;   \n\t* 在分区键上去重并使用聚合函数。e.g. SELECT col1,count(DISTINCT col2) FROM tbl GROUP BY col1;   \n\t* 在分区键上应用有去重功能的聚合函数。e.g. SELECT col1, Max(col2) FROM tbl GROUP BY col1;   \n2. ExtractPythonUDFFromAggregate，提取聚合操作中所有 PythonUDF。十分不推荐使用 pyspark   \n3. PruneFileSourcePartitions，物理文件分区下推。 读取 hadoop 目录时尽可能将过滤条件下推到分区键上，避免扫描所有文件。   \n4. PushDownOperatorsToDataSource  过滤操作下推到数据源，以提高性能。  \n\n[SparkOptimizer.scala]\n\n```\noverride def batches: Seq[Batch] = (preOptimizationBatches ++ super.batches :+\n    Batch(\"Optimize Metadata Only Query\", Once, OptimizeMetadataOnlyQuery(catalog)) :+\n    Batch(\"Extract Python UDF from Aggregate\", Once, ExtractPythonUDFFromAggregate) :+\n    Batch(\"Prune File Source Table Partitions\", Once, PruneFileSourcePartitions) :+\n    Batch(\"Push down operators to data source scan\", Once, PushDownOperatorsToDataSource)) ++\n    postHocOptimizationBatches :+\n    Batch(\"User Provided Optimizers\", fixedPoint, experimentalMethods.extraOptimizations: _*)\n```\n[Optimizer.scala]\n\n```\ndef batches: Seq[Batch] = {\n    val operatorOptimizationRuleSet =\n      Seq(\n        // Operator push down\n        PushProjectionThroughUnion,\n        ReorderJoin,\n        EliminateOuterJoin,\n        PushPredicateThroughJoin,\n        PushDownPredicate,\n        LimitPushDown,\n        ColumnPruning,\n        InferFiltersFromConstraints,\n        // Operator combine\n        CollapseRepartition,\n        CollapseProject,\n        CollapseWindow,\n        CombineFilters,\n        CombineLimits,\n        CombineUnions,\n        // Constant folding and strength reduction\n        NullPropagation,\n        ConstantPropagation,\n        FoldablePropagation,\n        OptimizeIn,\n        ConstantFolding,\n        ReorderAssociativeOperator,\n        LikeSimplification,\n        BooleanSimplification,\n        SimplifyConditionals,\n        RemoveDispensableExpressions,\n        SimplifyBinaryComparison,\n        PruneFilters,\n        EliminateSorts,\n        SimplifyCasts,\n        SimplifyCaseConversionExpressions,\n        RewriteCorrelatedScalarSubquery,\n        EliminateSerialization,\n        RemoveRedundantAliases,\n        RemoveRedundantProject,\n        SimplifyCreateStructOps,\n        SimplifyCreateArrayOps,\n        SimplifyCreateMapOps,\n        CombineConcats) ++\n        extendedOperatorOptimizationRules\n\n    val operatorOptimizationBatch: Seq[Batch] = {\n      val rulesWithoutInferFiltersFromConstraints =\n        operatorOptimizationRuleSet.filterNot(_ == InferFiltersFromConstraints)\n      Batch(\"Operator Optimization before Inferring Filters\", fixedPoint,\n        rulesWithoutInferFiltersFromConstraints: _*) ::\n      Batch(\"Infer Filters\", Once,\n        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,\n        rulesWithoutInferFiltersFromConstraints: _*) :: Nil\n    }\n\n    (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) ::\n    // 译: 从技术上将，”Finish Analysis“中的部分规则不是 optimizer rule 而是 analyzer rule，因为它们是保证正确性必须的。\n    // 但是 因为我们使用 analyzer 来规范化查询(视图定义)，我们在 analyzer 中不去除子查询或计算当前时间\n    Batch(\"Finish Analysis\", Once,\n      EliminateSubqueryAliases,\n      EliminateView,\n      ReplaceExpressions,\n      ComputeCurrentTime,\n      GetCurrentDatabase(sessionCatalog),\n      RewriteDistinctAggregates,\n      ReplaceDeduplicateWithAggregate) ::\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // Optimizer rules start here\n    //////////////////////////////////////////////////////////////////////////////////////////\n      // 译: 在应用主要的优化规则之前先调用 CombineUnions,因为可以减少迭代次数，而其他规则可以在两个相邻的 Union操作符之间添加/移动额外的操作符\n      // 在规则库\"Operator Optimizations\"中再次调用 CombineUnions,是因为其他操作符可能会导致两个单独的 Union 变得相邻。\n    Batch(\"Union\", Once,\n      CombineUnions) ::\n    Batch(\"Pullup Correlated Expressions\", Once,\n      PullupCorrelatedPredicates) ::\n    Batch(\"Subquery\", Once,\n      OptimizeSubqueries) ::\n    Batch(\"Replace Operators\", fixedPoint,\n      ReplaceIntersectWithSemiJoin,\n      ReplaceExceptWithFilter,\n      ReplaceExceptWithAntiJoin,\n      ReplaceDistinctWithAggregate) ::\n    Batch(\"Aggregate\", fixedPoint,\n      RemoveLiteralFromGroupExpressions,\n      RemoveRepetitionFromGroupExpressions) :: Nil ++\n    operatorOptimizationBatch) :+\n    Batch(\"Join Reorder\", Once,\n      CostBasedJoinReorder) :+\n    Batch(\"Decimal Optimizations\", fixedPoint,\n      DecimalAggregates) :+\n    Batch(\"Object Expressions Optimization\", fixedPoint,\n      EliminateMapObjects,\n      CombineTypedFilters) :+\n    Batch(\"LocalRelation\", fixedPoint,\n      ConvertToLocalRelation,\n      PropagateEmptyRelation) :+\n    // The following batch should be executed after batch \"Join Reorder\" and \"LocalRelation\".\n    Batch(\"Check Cartesian Products\", Once,\n      CheckCartesianProducts) :+\n    Batch(\"RewriteSubquery\", Once,\n      RewritePredicateSubquery,\n      ColumnPruning,\n      CollapseProject,\n      RemoveRedundantProject)\n  }\n```\n\n按规则(库)应用的顺序串行列出(下述规则均在 SparkOptimizer中规则之前)，<b>欢迎指出错误</b>       \n\n|规则库名 |规则类名  |功能 |备注|\n|:------------- |:---------------|:-------------|:-------------|\n|Eliminate Distinct|EliminateDistinct|去除 MAX/MIN 函数内的 DISTINCT|SELECT MAX(DISTINCT(age)) FROM a|\n|Finish Analysis|EliminateSubqueryAliases|去除子查询别名,子查询仅仅提供查询属性集的作用,analyzer 阶段结束后便可去除子查询别名|Finish Analysis 规则不在 analyzer 中删除的意义是保存原始的 analyzed logic plan|\n|Finish Analysis|EliminateView|去除视图操作符|视图的输出属性集必须和子查询输出属性集完全一致|\n|Finish Analysis|ReplaceExpressions|将RuntimeReplaceable 表达式替换为可执行表达式|一般用来兼容各类数据库，例如用 coalesce 替换 nvl 函数|\n|Finish Analysis|ComputeCurrentTime|记录当前时间,SQL 中任意位置和执行顺序的操作时间(date,timestamp)都会返回一致的结果||\n|Finish Analysis|GetCurrentDatabase|使用 SessionCatalog 的数据库作为当前数据库返回|CurrentDataBase 函数没必要在执行阶段一条条计算,在 RBO 阶段获取完作为常量替换了函数表达式|\n|Finish Analysis|RewriteDistinctAggregates|重写 count(distinct xx),展开数据通过一次聚合即可算出结果|SELECT count(distinct name) as name_cnt, count(distinct(age)) as age_cnt,sum(age) as age_sum FROM p GROUP BY year=> 将数据展开成[[year,null,null,type0,ageVal],[year,name,null,type1,null],[year,null,age,type2,ageVal]]],group by (year,name,age,type,age),sum(if(type1,ageVal,0));sum(if(type2,1,0));sum(if(type3,1,0))...|\n|Finish Analysis|ReplaceDeduplicateWithAggregate|替换 Deduplicate 操作为 Aggregate|没找到 Deduplicate 对应的语法,FIX ME|\n|Union|CombineUnions|将有父子关系的 union all 汇集叶子节点一层|SELECT * FROM(SELECT * FROM a UNION ALL SELECT * FROM b) UNION ALL SELECT * FROM c => SELECT * FROM (a,b,c)|\n|Pullup Correlated Expressions|PullupCorrelatedPredicates|?|?|\n|Subquery|OptimizeSubqueries|RBO 优化子查询|子查询的父阶段为 SubQueryExpression,当解析到 SubQueryExpression 时,使用 RBO 优化子查询逻辑计划|\n|Replace Operators|ReplaceIntersectWithSemiJoin|将 intersect 操作符转化为 left semi join 后再 distinct|left semi join 保留在右表中能关联到的左表选取列.SELECT name,age FROM a INTERSECT SELECT name,age FROM b => SELECT  DISTINCT name,age FROM a LEFT SEMI JOIN b ON a.name=b.name AND a.age=b.age|\n|Replace Operators|ReplaceExceptWithFilter|将两个相同查询计划的子句 except操作符转为 Filter 后 Distinct|SELECT a1,a2 FROM t WHERE a2=12 EXCEPT SELECT a1,a2 FROM t WHERE a1=5  =>SELECT DISTINCT a1,a2 FROM t WHERE a2=12 AND NOT(a1=5)。需要相同查询计划子句才能执行此转换|\n|Replace Operators|ReplaceExceptWithAntiJoin|不同查询计划的子句 except 操作符转为 left anti join 操作后 DISTINCT|SELECT a1,a2 FROM t1 EXCEPT SELECT b1,b2 FROM t2 => SELECT DISTINCT a1,a2 FROM t1 LEFT ANTI JOIN t2 ON t1.a1=t2.b1 AND t1.a2=t2.b2|\n|Replace Operators|ReplaceDistinctWithAggregate|将 distinct 操作符转为 group by|SELECT DISTINCT a,b FROM t => SELECT a,b FROM t GROUP BY a,b|\n|Aggregate|RemoveLiteralFromGroupExpressions|去除 group by 条件中的常量和可折叠常量表达式,不影响聚合结果但会减少 key 数据量| SELECT a,b FROM t GROUP BY a,b,1,1<0 =>SELECT a,b FROM t GROUP BY a,b|\n|Aggregate |RemoveRepetitionFromGroupExpressions|去除 group by 条件中的重复表达式,不影响聚合结果但会减少 key 数据量|SELECT a,b FROM t GROUP BY a,b,b =>|\n|Operator push down|PushProjectionThroughUnion|将查询列下推到各 union all 子句|SELECT a FROM (SELECT a,b FROM t1 UNION ALL SELECT c,d FROM t2) tbl_a => SELECT a FROM (SELECT a FROM t1 UNION ALL SELECT c FROM t2) tbl_a|\n|Operator push down|ReorderJoin|按照逻辑计划数据行数启发式寻找星型模型的事实表和维度表,事实表在左以避免大表加载到内存,并以此重定义连接顺序|取决于spark.sql.cbo.starSchemaDetection 和 spark.sql.cbo.enabled,默认不开启。SELECT * FROM a,b,c where a.key=b.key and a.val=c.val => SELECT * FROM a join c on a.val=c.val join b on a.key=b.key , 大表居左小表居右|\n|Operator push down|EliminateOuterJoin|尽可能转换 full join=>right/left join=>inner join|如果存在谓词可以削减 null-supplying 行(当输入数据为 null 时,谓词返回值为 null 或 false),则可以削弱 outer join. full join 可以转为 left/right/inner join,left/join join 可以转为 inner join. SELECT * FROM a left join b on a.key=b.key WHERE b.key IS NOT NULL =>SELECT * FROM a inner join b on a.key=b.key|\n|Operator push down|PushPredicateThroughJoin|join 谓词下推|包含两层含义: where 条件下推到 join 条件,join 条件下推到子查询|\n|Operator push down|PushDownPredicate|对于确定性的操作和谓词且谓词不能改变行的逻辑计划,尽可能下推谓词以减少输入数据量|对于支持类似 BloomFilter 的列式存储,有极大提升|\n|Operator push down|LimitPushDown|旨在减少输入数据量。1. union all 上级和下级 limit 数值不同则下级取 min 2.left join 上级和下级 limit 数值不同则下级左侧limit 取 min;right join 同理|SELECT * FROM (SELECT * FORM a limit 20 UNION ALL SELECT * FORM b limit 15) temp LIMIT 10 或者 SELECT * FROM (SELECT * FROM a LEFT JOIN b ON a.key=b.key) temp LIMIT 10|\n|Operator push down|ColumnPruning|列裁剪|去掉在查询/聚合/窗口函数/union 等用不到的列读取|\n|Operator push down|InferFiltersFromConstraints|在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件|在当前规则库会被过滤掉,在 Infer Filters 规则库执行|\n|Operator combine|CollapseRepartition|折叠有父子关系的再分区操作|当父节点和子节点均为再分区操作,但父节点无 shuffle(coalesce api)子节点有 shuffle 且父节点再分区数大于子节点再分区数则删除父节点,若再分区数不大于子节点则不改变结构。若 shuffle 关系不为(false,true)及其他情况则折叠子节点 RepartitionOperation |\n|Operator combine|CollapseProject|折叠有父子关系的父节点 Projection 操作|SELECT age FROM (SELECT name,age+1 as age FROM p) temp=>SELECT age+1 as age FROM p|\n|Operator combine|CollapseWindow|折叠分区字段一致、排序方式一致且表达式相互独立、有父子关系的子节点 Window 操作||\n|Operator combine|CombineFilters|折叠有父子关系的父节点 Filter 操作|SELECT name,age FROM (SELECT name,age FROM p where name is not null)temp where age>10 =>SELECT name,age FROM p WHERE name is not null AND age>10|\n|Operator combine|CombineLimits|折叠有父子关系的父节点 Limit 操作,limit 数值取 minimum|Limit 分为 GlobalLimit,LocalLimit 两类,GlobalLimit 需要shuffle,LocalLimit 不需要。GlobalLimit(Union(A,B))可以转化为 GlobalLimit(Union(LocalLimit(A),LocalLimit(B)))来降低 shuffle 数据量|\n|Operator combine|CombineUnions|重复规则,同 Union规则库的 CombineUnion 规则||\n|Constant folding and strength reduction|NullPropagation|null 值替换|将表达式中可评估的 null 值替换为等效常量,count(name)=>count(1)|\n|Constant folding and strength reduction|ConstantPropagation|Filter 操作中常量替换|SELECT * FROM p WHERE age=10 and age2=age+3 => SELECT * FROM p WHERE age=10 AND age2=13|\n|Constant folding and strength reduction|FoldablePropagation|可折叠表达式替换|尽可能将属性替换为原始可折叠表达式,其他优化规则将利用可折叠表达式进行优化。SELECT 1.0 as x,'abc' as y,now() as z order by x,y,z => SELECT 1.0 as x,'abc' as y,now() as z ORDER BY 1.0,'abc',now(); 这样其他规则可以去掉 order by 操作符|\n|Constant folding and strength reduction|OptimizeIn|IN 优化|尽可能优化 IN 谓词: 1.当 in 列表为空且指定列不可为空时直接返回 false 2.当 in 列表元素数量超过配置(默认 10),将列表转为 HashSet 以去重和使用哈希索引提高性能。In(value,seq[Literal])=>InSet(value,HashSet[Literal])|\n|Constant folding and strength reduction|ConstantFolding|常量折叠|将可静态计算的表达式替换为等效常量.SELECT 1+2 as a FROM p=>SELECT 3 FROM p|\n|Constant folding and strength reduction|ReorderAssociativeOperator|重排序所有整数类型运算符,将所有确定性整数折叠计算为一个结果|和 ConstantFolding 折叠不同，ConstantFolding 要在整个表达式都静态可计算(确定性)时才会应用。ReorderAssociativeOperator 这里尽管有不确定性部分，但会尽可能将确定性部分计算出来。SELECT (age+1)+2 as a FROM p =>SELECT age+3 as a FROM p|\n|Constant folding and strength reduction|LikeSimplification|正则匹配简化|尽可能将 rlike正则匹配简化为字符串startWith,endWith,equal,container 等操作|\n|Constant folding and strength reduction|BooleanSimplification|布尔表达式简化|尽可能简化 boolean 表达式/快速中断/删除不必要的 not 。false AND e=>false,true AND e=>e,a AND b=>Not(a).semanticEquals(b) => false,Not(Not(a))=>a|\n|Constant folding and strength reduction|SimplifyConditionals|条件表达式简化(if(condition,trueVal,falseVal),case when)|如果 if表达式 恒定true或 false 则修改逻辑计划为相应的 val;如果 case when 表达式中有恒 false 值则删除其分支，若所有分支恒 false 则取 else 语句值；若 case when 第一个表达式恒 true，则取其值；恒 true 分支会删除其后续所有分支|\n|Constant folding and strength reduction|RemoveDispensableExpressions|删除 UnaryPositive 节点(仅有标识子节点表达式作用)||\n|Constant folding and strength reduction|SimplifyBinaryComparison|简化比较|1.将<=>替换为 true;2.将=和<=和>= 在两侧均非空且逻辑计划结果一致时替换为 true;3. 将>和<在两侧均非空且逻辑计划结果一致时替换为 false。age<AGE  =>false|\n|Constant folding and strength reduction|PruneFilters|约束条件简化|1. 当约束条件恒 true 时,删除父节点;2.当约束条件恒 false 或 null 时,替换父节点的输入为空集;3.在父节点约束条件中去除子节点已有的约束条件|\n|Constant folding and strength reduction|EliminateSorts|删除无效的排序|删除排序中确定性的排序方式,甚至不排序.SELECT name,age FROM p ORDER BY 1 ASC, age DESC|\n|Constant folding and strength reduction|SimplifyCasts|强制类型转换简化|当强制转化的类型相同(仅限于 基础数据类型相同 或 Array 内嵌数据类型相同 或 Map 内嵌 key 和 value 类型均相同)时,去除 Cast (col to type) 操作|\n|Constant folding and strength reduction|SimplifyCaseConversionExpressions|简化大小写转换表达式|内部转换会被外部转换覆盖 Upper(Lower(x))=>Upper(x),Lower(Lower(x))=>Lower(x)|\n|Constant folding and strength reduction|RewriteCorrelatedScalarSubquery|？？||\n|Constant folding and strength reduction|EliminateSerialization|删除不必要的在 object 和 InternalRow 之间的(循环)序列化/反序列化操作|1.反序列化为父节点,序列化为子节点且操作字段相同;2.将数据反序列化并追加数据到末尾时AppendColumns可替换AppendColumnsWithObject操作符 直接操作序列化后的数据(类似于 UnsafeSortShuffle) 3. TypedFilter为父节点,序列化为子节点时 4.反序列化为父节点,TypedFilter 为子节点时|\n|Constant folding and strength reduction|RemoveRedundantAliases|删除无效别名|无效别名是指在子查询或者关联中不改变列名/列元数据的别名|\n|Constant folding and strength reduction|RemoveRedundantProject|删除无效查询|无效查询是指父查询和子查询目标字段相同|\n|Constant folding and strength reduction|SimplifyCreateStructOps|结构体创建简化|named_struct('name',name,'age',age).age=>age|\n|Constant folding and strength reduction|SimplifyCreateArrayOps|数组创建简化|1.当对创建的数组按下标取值时,减少创建数组的数据量.Array(elem0,elem1...)(1)=>elems(1);2.Array(named_stuct(name,\"nA\"),named_struct(age,12))[0].name=>named_struct(name,\"nA\").name|\n|Constant folding and strength reduction|SimplifyCreateMapOps|映射创建简化|map(key1->val2,key2->val2).key2 => case when key2 |\n|Constant folding and strength reduction|CombineConcats|合并 concat|将有父子关系的 concat 所有子节点扁平化 合并|\n|Infer Filters|InferFiltersFromConstraints|InferFiltersFromConstraints|在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件|消除无效的约束传递造成的计算代价|\n|Join Reorder|CostBasedJoinReorder|？|？|\n|Decimal Optimizations|DecimalAggregates|加速浮点数运算|float 和 double 运算中一般需要控制精度(precision) 和小数位(scale)。窗口函数内聚合和普通聚合的 sum/avg 场景下将浮点计算转为长整形计算并在结束时转回来|\n|Object Expressions Optimization|EliminateMapObjects|简化 MapObject 操作||\n|Object Expressions Optimization|CombineTypedFilters|简化 TypedFilter|去除具有父子关系的子节点 TypedFilter,合并两者的约束条件|\n|LocalRelation|ConvertToLocalRelation|简化为 LocalRelation|在 LocalRelation 上取 Limit 时,直接转化为 LocalRelation 在数据上取 Limit 减少输入数据数量|\n|LocalRelation|PropagateEmptyRelation|空 Relation 优化|对于上述优化规则(e.g. 列裁剪谓词下推)产生的或基础数据为空的 Relation 进行优化. 1,关联时按照左右空 Relation 和Join 类型分别讨论,左空 && 左外连接=>Empty 2.union all 所有子节点都空直接返回空数据集 3.一元节点的所有子节点都是空 Relation 则直接返回空数据集合, Select/Limit/Repartition 且 children 都为空 Relation=>空数据集合|\n|Check Cartesian Products|CheckCartesianProducts|笛卡尔积检测|检测逻辑计划树中是否有全外连接|\n|RewriteSubquery|RewritePredicateSubquery|重写谓词子查询|将 in/exists 转为 semi join,将 not in / not exists 转为 anti join|\n|RewriteSubquery|ColumnPruning|列裁剪|同 Aggregate规则库中的 ColumnPruning,去掉在查询/聚合/窗口函数/union 等用不到的列读取|\n|RewriteSubquery|CollapseProject|折叠有父子关系的父节点 Projection 操作|同 Aggregate 规则库的 RewriteSubquery|\n|RewriteSubquery |RemoveRedundantProject|删除无效查询|同 Aggregate 规则库的RemoveRedundantProject,无效查询是指父查询和子查询目标字段相同|\n\n\n\n\n","source":"_posts/2019-12-10-SparkSQL 源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan.md","raw":"---\nlayout:     post\ntitle:     SparkSQL源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan\nsubtitle:   SparkSQL 源码解析\ndate:       2019-12-10\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n    - Spark SQL\n    - 源码解析\n---\n\n\n## 概述  \n\nParser 模块负责将 SQL 文本解析为 UnResolvedLogicPlan，Analyzer 模块负责将 UnResolvedLogicPlan 绑定解析为 ResolvedLogicPlan。Rule-Based Optimizer，简称 RBO 或 Optimizer 模块负责基于规则优化逻辑计划，主要思路是基于各类型规则对 ResolvedLogicPlan 进行优化达到减少每个逻辑计划树节点输入数据量或计算代价的目的，大名鼎鼎的列裁剪和谓词下推、常量折叠就是出自于 Optimizer 模块。      \n\n<!-- more -->\n\n## 源码解析  \n\n[QueryExecution.scala]\n\n```\n// 第一步: 使用 CacheManager 替换逻辑计划为已解析过的逻辑计划\nlazy val withCachedData: LogicalPlan = {\n    assertAnalyzed()\n    assertSupported()\n    sparkSession.sharedState.cacheManager.useCachedData(analyzed)\n  }\n // 第二步: 基于规则优化逻辑计划树\n  lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)\n```\n\n### CacheManager \n\n[CacheManager.scala]\n\n```\nprivate val cachedData = new java.util.LinkedList[CachedData]\n\ndef useCachedData(plan: LogicalPlan): LogicalPlan = {\n    // 先序遍历\n    val newPlan = plan transformDown {\n      case hint: ResolvedHint => hint\n      case currentFragment =>\n        // 检索 CacheManager\n        lookupCachedData(currentFragment)\n           // 使用当前计划的输出字段替换原有的\n          .map(_.cachedRepresentation.withOutput(currentFragment.output))\n          .getOrElse(currentFragment)\n    }\n    newPlan transformAllExpressions {\n      // 剥去子查询别名外壳\n      case s: SubqueryExpression => s.withNewPlan(useCachedData(s.plan))\n    }\n  }\n  \n def lookupCachedData(plan: LogicalPlan): Option[CachedData] = readLock {\n    // 遍历对比CacheManager 中是否存在和当前计划一致的  \n    cachedData.asScala.find(cd => plan.sameResult(cd.plan))\n} \n```\n通常判断两个查询计划是否相同是很复杂的问题，因此可能实际相同也可以返回 false，但是实际不同一定不能返回 true。这种行为不会影响正确性，仅仅相当于弃用了 CacheManager，多做了一些重复处理而已。这种思想有点像 BloomFilter 的误判率: 函数判断在集合但是实际上可能不在，仅仅是多个 hash 函数得到的位置在 bitmap 中恰好非 0；返回不在集合时一定不在。  \n[QueryPlan.scala]\n\n```\n// 将逻辑计划规范化之后再做比较\nfinal def sameResult(other: PlanType): Boolean = this.canonicalized == other.canonicalized\n// 规范化 逻辑计划涉及的所有字段的 id(从 0 递增)。\nprotected def doCanonicalize(): PlanType = {\n    val canonicalizedChildren = children.map(_.canonicalized)\n    var id = -1\n    mapExpressions {\n      case a: Alias =>\n        id += 1\n        val normalizedChild = QueryPlan.normalizeExprId(a.child, allAttributes)\n        Alias(normalizedChild, \"\")(ExprId(id), a.qualifier)\n\n      case ar: AttributeReference if allAttributes.indexOf(ar.exprId) == -1 =>\n        id += 1\n        ar.withExprId(ExprId(id)).canonicalized\n\n      case other => QueryPlan.normalizeExprId(other, allAttributes)\n    }.withNewChildren(canonicalizedChildren)\n  }\n```\n有必要提一下 InMemoryRelation 这个类，这是 CacheManager 缓存已被 RBO 优化过的内存数据结构，包括：逻辑计划的输出字段，是否压缩(spark.sql.inMemoryColumnarStorage.compressed，默认 true，SparkSQL 将根据元数据信息自动匹配编码器)，批处理大小(spark.sql.inMemoryColumnarStorage.batchSize，默认 10000，控制列式存储的批处理大小，较大的值能提高内存利用率和压缩率 但是读数据时可能会 OOM)，存储等级(默认 MEMORY_AND_DISK)，RBO 优化完成的物理计划 SparkPlan，表名，对应 RDD[CachedBatch]，物理数据字节数，额外有一个<b> statsOfPlanToCache 暂时不知用途</b>  \n[InMemoryRelation.scala]\n\n```\ncase class InMemoryRelation(\n    output: Seq[Attribute],\n    useCompression: Boolean,\n    batchSize: Int,\n    storageLevel: StorageLevel,\n    @transient child: SparkPlan,\n    tableName: Option[String])(\n    @transient var _cachedColumnBuffers: RDD[CachedBatch] = null,\n    val sizeInBytesStats: LongAccumulator = child.sqlContext.sparkContext.longAccumulator,\n    statsOfPlanToCache: Statistics)\n  extends logical.LeafNode with MultiInstanceRelation\n```\nCachedBatch 用以表明 batch 中数据行数，序列化的列数据 buffer，数据的元数据类型。  \n[CachedBatch.scala]\n\n```\ncase class CachedBatch(numRows: Int, buffers: Array[Array[Byte]], stats: InternalRow)\n```\n\n### RBO\nSessionState 初始化时指定的 Optimizer 为 SparkOptimizer，SparkOptimizer 继承自 Optimizer，Optimizer 与 Analyzer 均继承自 RuleExecutor，都使用RuleExecutor#execute 方法遍历逻辑计划树应用 Optimizer/Analyzer 各自指定的规则库和执行策略 优化每个逻辑计划。故不再引用 RuleExecutor#execute 方法。       \n[BaseSessionStateBuilder.scala]  \n\n```\nprotected def optimizer: Optimizer = {\n    new SparkOptimizer(catalog, experimentalMethods) {\n      override def extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] =\n        super.extendedOperatorOptimizationRules ++ customOperatorOptimizationRules\n    }\n  }\n```\n除了类似于前后拦截器功能的 preOptimizationBatches、postHocOptimizationBatches 和拓展规则之外，SparkOptimizer 主要引用了 Optimizer 定义的通用规则，还有 4 个自定义规则:  \n\n1.  OptimizeMetadataOnlyQuery，优化那些只遍历表分区键级别的元数据就可以完成的逻辑计划   　\n\t* 在分区键上做聚合。e.g. SELECT col FROM tbl GROUP by col;   \n\t* 在分区键上去重并使用聚合函数。e.g. SELECT col1,count(DISTINCT col2) FROM tbl GROUP BY col1;   \n\t* 在分区键上应用有去重功能的聚合函数。e.g. SELECT col1, Max(col2) FROM tbl GROUP BY col1;   \n2. ExtractPythonUDFFromAggregate，提取聚合操作中所有 PythonUDF。十分不推荐使用 pyspark   \n3. PruneFileSourcePartitions，物理文件分区下推。 读取 hadoop 目录时尽可能将过滤条件下推到分区键上，避免扫描所有文件。   \n4. PushDownOperatorsToDataSource  过滤操作下推到数据源，以提高性能。  \n\n[SparkOptimizer.scala]\n\n```\noverride def batches: Seq[Batch] = (preOptimizationBatches ++ super.batches :+\n    Batch(\"Optimize Metadata Only Query\", Once, OptimizeMetadataOnlyQuery(catalog)) :+\n    Batch(\"Extract Python UDF from Aggregate\", Once, ExtractPythonUDFFromAggregate) :+\n    Batch(\"Prune File Source Table Partitions\", Once, PruneFileSourcePartitions) :+\n    Batch(\"Push down operators to data source scan\", Once, PushDownOperatorsToDataSource)) ++\n    postHocOptimizationBatches :+\n    Batch(\"User Provided Optimizers\", fixedPoint, experimentalMethods.extraOptimizations: _*)\n```\n[Optimizer.scala]\n\n```\ndef batches: Seq[Batch] = {\n    val operatorOptimizationRuleSet =\n      Seq(\n        // Operator push down\n        PushProjectionThroughUnion,\n        ReorderJoin,\n        EliminateOuterJoin,\n        PushPredicateThroughJoin,\n        PushDownPredicate,\n        LimitPushDown,\n        ColumnPruning,\n        InferFiltersFromConstraints,\n        // Operator combine\n        CollapseRepartition,\n        CollapseProject,\n        CollapseWindow,\n        CombineFilters,\n        CombineLimits,\n        CombineUnions,\n        // Constant folding and strength reduction\n        NullPropagation,\n        ConstantPropagation,\n        FoldablePropagation,\n        OptimizeIn,\n        ConstantFolding,\n        ReorderAssociativeOperator,\n        LikeSimplification,\n        BooleanSimplification,\n        SimplifyConditionals,\n        RemoveDispensableExpressions,\n        SimplifyBinaryComparison,\n        PruneFilters,\n        EliminateSorts,\n        SimplifyCasts,\n        SimplifyCaseConversionExpressions,\n        RewriteCorrelatedScalarSubquery,\n        EliminateSerialization,\n        RemoveRedundantAliases,\n        RemoveRedundantProject,\n        SimplifyCreateStructOps,\n        SimplifyCreateArrayOps,\n        SimplifyCreateMapOps,\n        CombineConcats) ++\n        extendedOperatorOptimizationRules\n\n    val operatorOptimizationBatch: Seq[Batch] = {\n      val rulesWithoutInferFiltersFromConstraints =\n        operatorOptimizationRuleSet.filterNot(_ == InferFiltersFromConstraints)\n      Batch(\"Operator Optimization before Inferring Filters\", fixedPoint,\n        rulesWithoutInferFiltersFromConstraints: _*) ::\n      Batch(\"Infer Filters\", Once,\n        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,\n        rulesWithoutInferFiltersFromConstraints: _*) :: Nil\n    }\n\n    (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) ::\n    // 译: 从技术上将，”Finish Analysis“中的部分规则不是 optimizer rule 而是 analyzer rule，因为它们是保证正确性必须的。\n    // 但是 因为我们使用 analyzer 来规范化查询(视图定义)，我们在 analyzer 中不去除子查询或计算当前时间\n    Batch(\"Finish Analysis\", Once,\n      EliminateSubqueryAliases,\n      EliminateView,\n      ReplaceExpressions,\n      ComputeCurrentTime,\n      GetCurrentDatabase(sessionCatalog),\n      RewriteDistinctAggregates,\n      ReplaceDeduplicateWithAggregate) ::\n    //////////////////////////////////////////////////////////////////////////////////////////\n    // Optimizer rules start here\n    //////////////////////////////////////////////////////////////////////////////////////////\n      // 译: 在应用主要的优化规则之前先调用 CombineUnions,因为可以减少迭代次数，而其他规则可以在两个相邻的 Union操作符之间添加/移动额外的操作符\n      // 在规则库\"Operator Optimizations\"中再次调用 CombineUnions,是因为其他操作符可能会导致两个单独的 Union 变得相邻。\n    Batch(\"Union\", Once,\n      CombineUnions) ::\n    Batch(\"Pullup Correlated Expressions\", Once,\n      PullupCorrelatedPredicates) ::\n    Batch(\"Subquery\", Once,\n      OptimizeSubqueries) ::\n    Batch(\"Replace Operators\", fixedPoint,\n      ReplaceIntersectWithSemiJoin,\n      ReplaceExceptWithFilter,\n      ReplaceExceptWithAntiJoin,\n      ReplaceDistinctWithAggregate) ::\n    Batch(\"Aggregate\", fixedPoint,\n      RemoveLiteralFromGroupExpressions,\n      RemoveRepetitionFromGroupExpressions) :: Nil ++\n    operatorOptimizationBatch) :+\n    Batch(\"Join Reorder\", Once,\n      CostBasedJoinReorder) :+\n    Batch(\"Decimal Optimizations\", fixedPoint,\n      DecimalAggregates) :+\n    Batch(\"Object Expressions Optimization\", fixedPoint,\n      EliminateMapObjects,\n      CombineTypedFilters) :+\n    Batch(\"LocalRelation\", fixedPoint,\n      ConvertToLocalRelation,\n      PropagateEmptyRelation) :+\n    // The following batch should be executed after batch \"Join Reorder\" and \"LocalRelation\".\n    Batch(\"Check Cartesian Products\", Once,\n      CheckCartesianProducts) :+\n    Batch(\"RewriteSubquery\", Once,\n      RewritePredicateSubquery,\n      ColumnPruning,\n      CollapseProject,\n      RemoveRedundantProject)\n  }\n```\n\n按规则(库)应用的顺序串行列出(下述规则均在 SparkOptimizer中规则之前)，<b>欢迎指出错误</b>       \n\n|规则库名 |规则类名  |功能 |备注|\n|:------------- |:---------------|:-------------|:-------------|\n|Eliminate Distinct|EliminateDistinct|去除 MAX/MIN 函数内的 DISTINCT|SELECT MAX(DISTINCT(age)) FROM a|\n|Finish Analysis|EliminateSubqueryAliases|去除子查询别名,子查询仅仅提供查询属性集的作用,analyzer 阶段结束后便可去除子查询别名|Finish Analysis 规则不在 analyzer 中删除的意义是保存原始的 analyzed logic plan|\n|Finish Analysis|EliminateView|去除视图操作符|视图的输出属性集必须和子查询输出属性集完全一致|\n|Finish Analysis|ReplaceExpressions|将RuntimeReplaceable 表达式替换为可执行表达式|一般用来兼容各类数据库，例如用 coalesce 替换 nvl 函数|\n|Finish Analysis|ComputeCurrentTime|记录当前时间,SQL 中任意位置和执行顺序的操作时间(date,timestamp)都会返回一致的结果||\n|Finish Analysis|GetCurrentDatabase|使用 SessionCatalog 的数据库作为当前数据库返回|CurrentDataBase 函数没必要在执行阶段一条条计算,在 RBO 阶段获取完作为常量替换了函数表达式|\n|Finish Analysis|RewriteDistinctAggregates|重写 count(distinct xx),展开数据通过一次聚合即可算出结果|SELECT count(distinct name) as name_cnt, count(distinct(age)) as age_cnt,sum(age) as age_sum FROM p GROUP BY year=> 将数据展开成[[year,null,null,type0,ageVal],[year,name,null,type1,null],[year,null,age,type2,ageVal]]],group by (year,name,age,type,age),sum(if(type1,ageVal,0));sum(if(type2,1,0));sum(if(type3,1,0))...|\n|Finish Analysis|ReplaceDeduplicateWithAggregate|替换 Deduplicate 操作为 Aggregate|没找到 Deduplicate 对应的语法,FIX ME|\n|Union|CombineUnions|将有父子关系的 union all 汇集叶子节点一层|SELECT * FROM(SELECT * FROM a UNION ALL SELECT * FROM b) UNION ALL SELECT * FROM c => SELECT * FROM (a,b,c)|\n|Pullup Correlated Expressions|PullupCorrelatedPredicates|?|?|\n|Subquery|OptimizeSubqueries|RBO 优化子查询|子查询的父阶段为 SubQueryExpression,当解析到 SubQueryExpression 时,使用 RBO 优化子查询逻辑计划|\n|Replace Operators|ReplaceIntersectWithSemiJoin|将 intersect 操作符转化为 left semi join 后再 distinct|left semi join 保留在右表中能关联到的左表选取列.SELECT name,age FROM a INTERSECT SELECT name,age FROM b => SELECT  DISTINCT name,age FROM a LEFT SEMI JOIN b ON a.name=b.name AND a.age=b.age|\n|Replace Operators|ReplaceExceptWithFilter|将两个相同查询计划的子句 except操作符转为 Filter 后 Distinct|SELECT a1,a2 FROM t WHERE a2=12 EXCEPT SELECT a1,a2 FROM t WHERE a1=5  =>SELECT DISTINCT a1,a2 FROM t WHERE a2=12 AND NOT(a1=5)。需要相同查询计划子句才能执行此转换|\n|Replace Operators|ReplaceExceptWithAntiJoin|不同查询计划的子句 except 操作符转为 left anti join 操作后 DISTINCT|SELECT a1,a2 FROM t1 EXCEPT SELECT b1,b2 FROM t2 => SELECT DISTINCT a1,a2 FROM t1 LEFT ANTI JOIN t2 ON t1.a1=t2.b1 AND t1.a2=t2.b2|\n|Replace Operators|ReplaceDistinctWithAggregate|将 distinct 操作符转为 group by|SELECT DISTINCT a,b FROM t => SELECT a,b FROM t GROUP BY a,b|\n|Aggregate|RemoveLiteralFromGroupExpressions|去除 group by 条件中的常量和可折叠常量表达式,不影响聚合结果但会减少 key 数据量| SELECT a,b FROM t GROUP BY a,b,1,1<0 =>SELECT a,b FROM t GROUP BY a,b|\n|Aggregate |RemoveRepetitionFromGroupExpressions|去除 group by 条件中的重复表达式,不影响聚合结果但会减少 key 数据量|SELECT a,b FROM t GROUP BY a,b,b =>|\n|Operator push down|PushProjectionThroughUnion|将查询列下推到各 union all 子句|SELECT a FROM (SELECT a,b FROM t1 UNION ALL SELECT c,d FROM t2) tbl_a => SELECT a FROM (SELECT a FROM t1 UNION ALL SELECT c FROM t2) tbl_a|\n|Operator push down|ReorderJoin|按照逻辑计划数据行数启发式寻找星型模型的事实表和维度表,事实表在左以避免大表加载到内存,并以此重定义连接顺序|取决于spark.sql.cbo.starSchemaDetection 和 spark.sql.cbo.enabled,默认不开启。SELECT * FROM a,b,c where a.key=b.key and a.val=c.val => SELECT * FROM a join c on a.val=c.val join b on a.key=b.key , 大表居左小表居右|\n|Operator push down|EliminateOuterJoin|尽可能转换 full join=>right/left join=>inner join|如果存在谓词可以削减 null-supplying 行(当输入数据为 null 时,谓词返回值为 null 或 false),则可以削弱 outer join. full join 可以转为 left/right/inner join,left/join join 可以转为 inner join. SELECT * FROM a left join b on a.key=b.key WHERE b.key IS NOT NULL =>SELECT * FROM a inner join b on a.key=b.key|\n|Operator push down|PushPredicateThroughJoin|join 谓词下推|包含两层含义: where 条件下推到 join 条件,join 条件下推到子查询|\n|Operator push down|PushDownPredicate|对于确定性的操作和谓词且谓词不能改变行的逻辑计划,尽可能下推谓词以减少输入数据量|对于支持类似 BloomFilter 的列式存储,有极大提升|\n|Operator push down|LimitPushDown|旨在减少输入数据量。1. union all 上级和下级 limit 数值不同则下级取 min 2.left join 上级和下级 limit 数值不同则下级左侧limit 取 min;right join 同理|SELECT * FROM (SELECT * FORM a limit 20 UNION ALL SELECT * FORM b limit 15) temp LIMIT 10 或者 SELECT * FROM (SELECT * FROM a LEFT JOIN b ON a.key=b.key) temp LIMIT 10|\n|Operator push down|ColumnPruning|列裁剪|去掉在查询/聚合/窗口函数/union 等用不到的列读取|\n|Operator push down|InferFiltersFromConstraints|在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件|在当前规则库会被过滤掉,在 Infer Filters 规则库执行|\n|Operator combine|CollapseRepartition|折叠有父子关系的再分区操作|当父节点和子节点均为再分区操作,但父节点无 shuffle(coalesce api)子节点有 shuffle 且父节点再分区数大于子节点再分区数则删除父节点,若再分区数不大于子节点则不改变结构。若 shuffle 关系不为(false,true)及其他情况则折叠子节点 RepartitionOperation |\n|Operator combine|CollapseProject|折叠有父子关系的父节点 Projection 操作|SELECT age FROM (SELECT name,age+1 as age FROM p) temp=>SELECT age+1 as age FROM p|\n|Operator combine|CollapseWindow|折叠分区字段一致、排序方式一致且表达式相互独立、有父子关系的子节点 Window 操作||\n|Operator combine|CombineFilters|折叠有父子关系的父节点 Filter 操作|SELECT name,age FROM (SELECT name,age FROM p where name is not null)temp where age>10 =>SELECT name,age FROM p WHERE name is not null AND age>10|\n|Operator combine|CombineLimits|折叠有父子关系的父节点 Limit 操作,limit 数值取 minimum|Limit 分为 GlobalLimit,LocalLimit 两类,GlobalLimit 需要shuffle,LocalLimit 不需要。GlobalLimit(Union(A,B))可以转化为 GlobalLimit(Union(LocalLimit(A),LocalLimit(B)))来降低 shuffle 数据量|\n|Operator combine|CombineUnions|重复规则,同 Union规则库的 CombineUnion 规则||\n|Constant folding and strength reduction|NullPropagation|null 值替换|将表达式中可评估的 null 值替换为等效常量,count(name)=>count(1)|\n|Constant folding and strength reduction|ConstantPropagation|Filter 操作中常量替换|SELECT * FROM p WHERE age=10 and age2=age+3 => SELECT * FROM p WHERE age=10 AND age2=13|\n|Constant folding and strength reduction|FoldablePropagation|可折叠表达式替换|尽可能将属性替换为原始可折叠表达式,其他优化规则将利用可折叠表达式进行优化。SELECT 1.0 as x,'abc' as y,now() as z order by x,y,z => SELECT 1.0 as x,'abc' as y,now() as z ORDER BY 1.0,'abc',now(); 这样其他规则可以去掉 order by 操作符|\n|Constant folding and strength reduction|OptimizeIn|IN 优化|尽可能优化 IN 谓词: 1.当 in 列表为空且指定列不可为空时直接返回 false 2.当 in 列表元素数量超过配置(默认 10),将列表转为 HashSet 以去重和使用哈希索引提高性能。In(value,seq[Literal])=>InSet(value,HashSet[Literal])|\n|Constant folding and strength reduction|ConstantFolding|常量折叠|将可静态计算的表达式替换为等效常量.SELECT 1+2 as a FROM p=>SELECT 3 FROM p|\n|Constant folding and strength reduction|ReorderAssociativeOperator|重排序所有整数类型运算符,将所有确定性整数折叠计算为一个结果|和 ConstantFolding 折叠不同，ConstantFolding 要在整个表达式都静态可计算(确定性)时才会应用。ReorderAssociativeOperator 这里尽管有不确定性部分，但会尽可能将确定性部分计算出来。SELECT (age+1)+2 as a FROM p =>SELECT age+3 as a FROM p|\n|Constant folding and strength reduction|LikeSimplification|正则匹配简化|尽可能将 rlike正则匹配简化为字符串startWith,endWith,equal,container 等操作|\n|Constant folding and strength reduction|BooleanSimplification|布尔表达式简化|尽可能简化 boolean 表达式/快速中断/删除不必要的 not 。false AND e=>false,true AND e=>e,a AND b=>Not(a).semanticEquals(b) => false,Not(Not(a))=>a|\n|Constant folding and strength reduction|SimplifyConditionals|条件表达式简化(if(condition,trueVal,falseVal),case when)|如果 if表达式 恒定true或 false 则修改逻辑计划为相应的 val;如果 case when 表达式中有恒 false 值则删除其分支，若所有分支恒 false 则取 else 语句值；若 case when 第一个表达式恒 true，则取其值；恒 true 分支会删除其后续所有分支|\n|Constant folding and strength reduction|RemoveDispensableExpressions|删除 UnaryPositive 节点(仅有标识子节点表达式作用)||\n|Constant folding and strength reduction|SimplifyBinaryComparison|简化比较|1.将<=>替换为 true;2.将=和<=和>= 在两侧均非空且逻辑计划结果一致时替换为 true;3. 将>和<在两侧均非空且逻辑计划结果一致时替换为 false。age<AGE  =>false|\n|Constant folding and strength reduction|PruneFilters|约束条件简化|1. 当约束条件恒 true 时,删除父节点;2.当约束条件恒 false 或 null 时,替换父节点的输入为空集;3.在父节点约束条件中去除子节点已有的约束条件|\n|Constant folding and strength reduction|EliminateSorts|删除无效的排序|删除排序中确定性的排序方式,甚至不排序.SELECT name,age FROM p ORDER BY 1 ASC, age DESC|\n|Constant folding and strength reduction|SimplifyCasts|强制类型转换简化|当强制转化的类型相同(仅限于 基础数据类型相同 或 Array 内嵌数据类型相同 或 Map 内嵌 key 和 value 类型均相同)时,去除 Cast (col to type) 操作|\n|Constant folding and strength reduction|SimplifyCaseConversionExpressions|简化大小写转换表达式|内部转换会被外部转换覆盖 Upper(Lower(x))=>Upper(x),Lower(Lower(x))=>Lower(x)|\n|Constant folding and strength reduction|RewriteCorrelatedScalarSubquery|？？||\n|Constant folding and strength reduction|EliminateSerialization|删除不必要的在 object 和 InternalRow 之间的(循环)序列化/反序列化操作|1.反序列化为父节点,序列化为子节点且操作字段相同;2.将数据反序列化并追加数据到末尾时AppendColumns可替换AppendColumnsWithObject操作符 直接操作序列化后的数据(类似于 UnsafeSortShuffle) 3. TypedFilter为父节点,序列化为子节点时 4.反序列化为父节点,TypedFilter 为子节点时|\n|Constant folding and strength reduction|RemoveRedundantAliases|删除无效别名|无效别名是指在子查询或者关联中不改变列名/列元数据的别名|\n|Constant folding and strength reduction|RemoveRedundantProject|删除无效查询|无效查询是指父查询和子查询目标字段相同|\n|Constant folding and strength reduction|SimplifyCreateStructOps|结构体创建简化|named_struct('name',name,'age',age).age=>age|\n|Constant folding and strength reduction|SimplifyCreateArrayOps|数组创建简化|1.当对创建的数组按下标取值时,减少创建数组的数据量.Array(elem0,elem1...)(1)=>elems(1);2.Array(named_stuct(name,\"nA\"),named_struct(age,12))[0].name=>named_struct(name,\"nA\").name|\n|Constant folding and strength reduction|SimplifyCreateMapOps|映射创建简化|map(key1->val2,key2->val2).key2 => case when key2 |\n|Constant folding and strength reduction|CombineConcats|合并 concat|将有父子关系的 concat 所有子节点扁平化 合并|\n|Infer Filters|InferFiltersFromConstraints|InferFiltersFromConstraints|在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件|消除无效的约束传递造成的计算代价|\n|Join Reorder|CostBasedJoinReorder|？|？|\n|Decimal Optimizations|DecimalAggregates|加速浮点数运算|float 和 double 运算中一般需要控制精度(precision) 和小数位(scale)。窗口函数内聚合和普通聚合的 sum/avg 场景下将浮点计算转为长整形计算并在结束时转回来|\n|Object Expressions Optimization|EliminateMapObjects|简化 MapObject 操作||\n|Object Expressions Optimization|CombineTypedFilters|简化 TypedFilter|去除具有父子关系的子节点 TypedFilter,合并两者的约束条件|\n|LocalRelation|ConvertToLocalRelation|简化为 LocalRelation|在 LocalRelation 上取 Limit 时,直接转化为 LocalRelation 在数据上取 Limit 减少输入数据数量|\n|LocalRelation|PropagateEmptyRelation|空 Relation 优化|对于上述优化规则(e.g. 列裁剪谓词下推)产生的或基础数据为空的 Relation 进行优化. 1,关联时按照左右空 Relation 和Join 类型分别讨论,左空 && 左外连接=>Empty 2.union all 所有子节点都空直接返回空数据集 3.一元节点的所有子节点都是空 Relation 则直接返回空数据集合, Select/Limit/Repartition 且 children 都为空 Relation=>空数据集合|\n|Check Cartesian Products|CheckCartesianProducts|笛卡尔积检测|检测逻辑计划树中是否有全外连接|\n|RewriteSubquery|RewritePredicateSubquery|重写谓词子查询|将 in/exists 转为 semi join,将 not in / not exists 转为 anti join|\n|RewriteSubquery|ColumnPruning|列裁剪|同 Aggregate规则库中的 ColumnPruning,去掉在查询/聚合/窗口函数/union 等用不到的列读取|\n|RewriteSubquery|CollapseProject|折叠有父子关系的父节点 Projection 操作|同 Aggregate 规则库的 RewriteSubquery|\n|RewriteSubquery |RemoveRedundantProject|删除无效查询|同 Aggregate 规则库的RemoveRedundantProject,无效查询是指父查询和子查询目标字段相同|\n\n\n\n\n","slug":"2019-12-10-SparkSQL 源码解析(3):从 ResolvedLogicPlan 到 OptimizedLogicPlan","published":1,"updated":"2020-03-16T10:01:28.111Z","comments":1,"photos":[],"link":"","_id":"cke2rms5e000fy7c7cijbczul","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Parser 模块负责将 SQL 文本解析为 UnResolvedLogicPlan，Analyzer 模块负责将 UnResolvedLogicPlan 绑定解析为 ResolvedLogicPlan。Rule-Based Optimizer，简称 RBO 或 Optimizer 模块负责基于规则优化逻辑计划，主要思路是基于各类型规则对 ResolvedLogicPlan 进行优化达到减少每个逻辑计划树节点输入数据量或计算代价的目的，大名鼎鼎的列裁剪和谓词下推、常量折叠就是出自于 Optimizer 模块。      </p>\n<a id=\"more\"></a>\n\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>[QueryExecution.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 第一步: 使用 CacheManager 替换逻辑计划为已解析过的逻辑计划</span><br><span class=\"line\">lazy val withCachedData: LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    assertAnalyzed()</span><br><span class=\"line\">    assertSupported()</span><br><span class=\"line\">    sparkSession.sharedState.cacheManager.useCachedData(analyzed)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> &#x2F;&#x2F; 第二步: 基于规则优化逻辑计划树</span><br><span class=\"line\">  lazy val optimizedPlan: LogicalPlan &#x3D; sparkSession.sessionState.optimizer.execute(withCachedData)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"CacheManager\"><a href=\"#CacheManager\" class=\"headerlink\" title=\"CacheManager\"></a>CacheManager</h3><p>[CacheManager.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private val cachedData &#x3D; new java.util.LinkedList[CachedData]</span><br><span class=\"line\"></span><br><span class=\"line\">def useCachedData(plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 先序遍历</span><br><span class=\"line\">    val newPlan &#x3D; plan transformDown &#123;</span><br><span class=\"line\">      case hint: ResolvedHint &#x3D;&gt; hint</span><br><span class=\"line\">      case currentFragment &#x3D;&gt;</span><br><span class=\"line\">        &#x2F;&#x2F; 检索 CacheManager</span><br><span class=\"line\">        lookupCachedData(currentFragment)</span><br><span class=\"line\">           &#x2F;&#x2F; 使用当前计划的输出字段替换原有的</span><br><span class=\"line\">          .map(_.cachedRepresentation.withOutput(currentFragment.output))</span><br><span class=\"line\">          .getOrElse(currentFragment)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    newPlan transformAllExpressions &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 剥去子查询别名外壳</span><br><span class=\"line\">      case s: SubqueryExpression &#x3D;&gt; s.withNewPlan(useCachedData(s.plan))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"> def lookupCachedData(plan: LogicalPlan): Option[CachedData] &#x3D; readLock &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 遍历对比CacheManager 中是否存在和当前计划一致的  </span><br><span class=\"line\">    cachedData.asScala.find(cd &#x3D;&gt; plan.sameResult(cd.plan))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通常判断两个查询计划是否相同是很复杂的问题，因此可能实际相同也可以返回 false，但是实际不同一定不能返回 true。这种行为不会影响正确性，仅仅相当于弃用了 CacheManager，多做了一些重复处理而已。这种思想有点像 BloomFilter 的误判率: 函数判断在集合但是实际上可能不在，仅仅是多个 hash 函数得到的位置在 bitmap 中恰好非 0；返回不在集合时一定不在。<br>[QueryPlan.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 将逻辑计划规范化之后再做比较</span><br><span class=\"line\">final def sameResult(other: PlanType): Boolean &#x3D; this.canonicalized &#x3D;&#x3D; other.canonicalized</span><br><span class=\"line\">&#x2F;&#x2F; 规范化 逻辑计划涉及的所有字段的 id(从 0 递增)。</span><br><span class=\"line\">protected def doCanonicalize(): PlanType &#x3D; &#123;</span><br><span class=\"line\">    val canonicalizedChildren &#x3D; children.map(_.canonicalized)</span><br><span class=\"line\">    var id &#x3D; -1</span><br><span class=\"line\">    mapExpressions &#123;</span><br><span class=\"line\">      case a: Alias &#x3D;&gt;</span><br><span class=\"line\">        id +&#x3D; 1</span><br><span class=\"line\">        val normalizedChild &#x3D; QueryPlan.normalizeExprId(a.child, allAttributes)</span><br><span class=\"line\">        Alias(normalizedChild, &quot;&quot;)(ExprId(id), a.qualifier)</span><br><span class=\"line\"></span><br><span class=\"line\">      case ar: AttributeReference if allAttributes.indexOf(ar.exprId) &#x3D;&#x3D; -1 &#x3D;&gt;</span><br><span class=\"line\">        id +&#x3D; 1</span><br><span class=\"line\">        ar.withExprId(ExprId(id)).canonicalized</span><br><span class=\"line\"></span><br><span class=\"line\">      case other &#x3D;&gt; QueryPlan.normalizeExprId(other, allAttributes)</span><br><span class=\"line\">    &#125;.withNewChildren(canonicalizedChildren)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>有必要提一下 InMemoryRelation 这个类，这是 CacheManager 缓存已被 RBO 优化过的内存数据结构，包括：逻辑计划的输出字段，是否压缩(spark.sql.inMemoryColumnarStorage.compressed，默认 true，SparkSQL 将根据元数据信息自动匹配编码器)，批处理大小(spark.sql.inMemoryColumnarStorage.batchSize，默认 10000，控制列式存储的批处理大小，较大的值能提高内存利用率和压缩率 但是读数据时可能会 OOM)，存储等级(默认 MEMORY_AND_DISK)，RBO 优化完成的物理计划 SparkPlan，表名，对应 RDD[CachedBatch]，物理数据字节数，额外有一个<b> statsOfPlanToCache 暂时不知用途</b><br>[InMemoryRelation.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class InMemoryRelation(</span><br><span class=\"line\">    output: Seq[Attribute],</span><br><span class=\"line\">    useCompression: Boolean,</span><br><span class=\"line\">    batchSize: Int,</span><br><span class=\"line\">    storageLevel: StorageLevel,</span><br><span class=\"line\">    @transient child: SparkPlan,</span><br><span class=\"line\">    tableName: Option[String])(</span><br><span class=\"line\">    @transient var _cachedColumnBuffers: RDD[CachedBatch] &#x3D; null,</span><br><span class=\"line\">    val sizeInBytesStats: LongAccumulator &#x3D; child.sqlContext.sparkContext.longAccumulator,</span><br><span class=\"line\">    statsOfPlanToCache: Statistics)</span><br><span class=\"line\">  extends logical.LeafNode with MultiInstanceRelation</span><br></pre></td></tr></table></figure>\n<p>CachedBatch 用以表明 batch 中数据行数，序列化的列数据 buffer，数据的元数据类型。<br>[CachedBatch.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class CachedBatch(numRows: Int, buffers: Array[Array[Byte]], stats: InternalRow)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"RBO\"><a href=\"#RBO\" class=\"headerlink\" title=\"RBO\"></a>RBO</h3><p>SessionState 初始化时指定的 Optimizer 为 SparkOptimizer，SparkOptimizer 继承自 Optimizer，Optimizer 与 Analyzer 均继承自 RuleExecutor，都使用RuleExecutor#execute 方法遍历逻辑计划树应用 Optimizer/Analyzer 各自指定的规则库和执行策略 优化每个逻辑计划。故不再引用 RuleExecutor#execute 方法。<br>[BaseSessionStateBuilder.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected def optimizer: Optimizer &#x3D; &#123;</span><br><span class=\"line\">    new SparkOptimizer(catalog, experimentalMethods) &#123;</span><br><span class=\"line\">      override def extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] &#x3D;</span><br><span class=\"line\">        super.extendedOperatorOptimizationRules ++ customOperatorOptimizationRules</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>除了类似于前后拦截器功能的 preOptimizationBatches、postHocOptimizationBatches 和拓展规则之外，SparkOptimizer 主要引用了 Optimizer 定义的通用规则，还有 4 个自定义规则:  </p>\n<ol>\n<li>OptimizeMetadataOnlyQuery，优化那些只遍历表分区键级别的元数据就可以完成的逻辑计划   　<ul>\n<li>在分区键上做聚合。e.g. SELECT col FROM tbl GROUP by col;   </li>\n<li>在分区键上去重并使用聚合函数。e.g. SELECT col1,count(DISTINCT col2) FROM tbl GROUP BY col1;   </li>\n<li>在分区键上应用有去重功能的聚合函数。e.g. SELECT col1, Max(col2) FROM tbl GROUP BY col1;   </li>\n</ul>\n</li>\n<li>ExtractPythonUDFFromAggregate，提取聚合操作中所有 PythonUDF。十分不推荐使用 pyspark   </li>\n<li>PruneFileSourcePartitions，物理文件分区下推。 读取 hadoop 目录时尽可能将过滤条件下推到分区键上，避免扫描所有文件。   </li>\n<li>PushDownOperatorsToDataSource  过滤操作下推到数据源，以提高性能。  </li>\n</ol>\n<p>[SparkOptimizer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def batches: Seq[Batch] &#x3D; (preOptimizationBatches ++ super.batches :+</span><br><span class=\"line\">    Batch(&quot;Optimize Metadata Only Query&quot;, Once, OptimizeMetadataOnlyQuery(catalog)) :+</span><br><span class=\"line\">    Batch(&quot;Extract Python UDF from Aggregate&quot;, Once, ExtractPythonUDFFromAggregate) :+</span><br><span class=\"line\">    Batch(&quot;Prune File Source Table Partitions&quot;, Once, PruneFileSourcePartitions) :+</span><br><span class=\"line\">    Batch(&quot;Push down operators to data source scan&quot;, Once, PushDownOperatorsToDataSource)) ++</span><br><span class=\"line\">    postHocOptimizationBatches :+</span><br><span class=\"line\">    Batch(&quot;User Provided Optimizers&quot;, fixedPoint, experimentalMethods.extraOptimizations: _*)</span><br></pre></td></tr></table></figure>\n<p>[Optimizer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def batches: Seq[Batch] &#x3D; &#123;</span><br><span class=\"line\">    val operatorOptimizationRuleSet &#x3D;</span><br><span class=\"line\">      Seq(</span><br><span class=\"line\">        &#x2F;&#x2F; Operator push down</span><br><span class=\"line\">        PushProjectionThroughUnion,</span><br><span class=\"line\">        ReorderJoin,</span><br><span class=\"line\">        EliminateOuterJoin,</span><br><span class=\"line\">        PushPredicateThroughJoin,</span><br><span class=\"line\">        PushDownPredicate,</span><br><span class=\"line\">        LimitPushDown,</span><br><span class=\"line\">        ColumnPruning,</span><br><span class=\"line\">        InferFiltersFromConstraints,</span><br><span class=\"line\">        &#x2F;&#x2F; Operator combine</span><br><span class=\"line\">        CollapseRepartition,</span><br><span class=\"line\">        CollapseProject,</span><br><span class=\"line\">        CollapseWindow,</span><br><span class=\"line\">        CombineFilters,</span><br><span class=\"line\">        CombineLimits,</span><br><span class=\"line\">        CombineUnions,</span><br><span class=\"line\">        &#x2F;&#x2F; Constant folding and strength reduction</span><br><span class=\"line\">        NullPropagation,</span><br><span class=\"line\">        ConstantPropagation,</span><br><span class=\"line\">        FoldablePropagation,</span><br><span class=\"line\">        OptimizeIn,</span><br><span class=\"line\">        ConstantFolding,</span><br><span class=\"line\">        ReorderAssociativeOperator,</span><br><span class=\"line\">        LikeSimplification,</span><br><span class=\"line\">        BooleanSimplification,</span><br><span class=\"line\">        SimplifyConditionals,</span><br><span class=\"line\">        RemoveDispensableExpressions,</span><br><span class=\"line\">        SimplifyBinaryComparison,</span><br><span class=\"line\">        PruneFilters,</span><br><span class=\"line\">        EliminateSorts,</span><br><span class=\"line\">        SimplifyCasts,</span><br><span class=\"line\">        SimplifyCaseConversionExpressions,</span><br><span class=\"line\">        RewriteCorrelatedScalarSubquery,</span><br><span class=\"line\">        EliminateSerialization,</span><br><span class=\"line\">        RemoveRedundantAliases,</span><br><span class=\"line\">        RemoveRedundantProject,</span><br><span class=\"line\">        SimplifyCreateStructOps,</span><br><span class=\"line\">        SimplifyCreateArrayOps,</span><br><span class=\"line\">        SimplifyCreateMapOps,</span><br><span class=\"line\">        CombineConcats) ++</span><br><span class=\"line\">        extendedOperatorOptimizationRules</span><br><span class=\"line\"></span><br><span class=\"line\">    val operatorOptimizationBatch: Seq[Batch] &#x3D; &#123;</span><br><span class=\"line\">      val rulesWithoutInferFiltersFromConstraints &#x3D;</span><br><span class=\"line\">        operatorOptimizationRuleSet.filterNot(_ &#x3D;&#x3D; InferFiltersFromConstraints)</span><br><span class=\"line\">      Batch(&quot;Operator Optimization before Inferring Filters&quot;, fixedPoint,</span><br><span class=\"line\">        rulesWithoutInferFiltersFromConstraints: _*) ::</span><br><span class=\"line\">      Batch(&quot;Infer Filters&quot;, Once,</span><br><span class=\"line\">        InferFiltersFromConstraints) ::</span><br><span class=\"line\">      Batch(&quot;Operator Optimization after Inferring Filters&quot;, fixedPoint,</span><br><span class=\"line\">        rulesWithoutInferFiltersFromConstraints: _*) :: Nil</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    (Batch(&quot;Eliminate Distinct&quot;, Once, EliminateDistinct) ::</span><br><span class=\"line\">    &#x2F;&#x2F; 译: 从技术上将，”Finish Analysis“中的部分规则不是 optimizer rule 而是 analyzer rule，因为它们是保证正确性必须的。</span><br><span class=\"line\">    &#x2F;&#x2F; 但是 因为我们使用 analyzer 来规范化查询(视图定义)，我们在 analyzer 中不去除子查询或计算当前时间</span><br><span class=\"line\">    Batch(&quot;Finish Analysis&quot;, Once,</span><br><span class=\"line\">      EliminateSubqueryAliases,</span><br><span class=\"line\">      EliminateView,</span><br><span class=\"line\">      ReplaceExpressions,</span><br><span class=\"line\">      ComputeCurrentTime,</span><br><span class=\"line\">      GetCurrentDatabase(sessionCatalog),</span><br><span class=\"line\">      RewriteDistinctAggregates,</span><br><span class=\"line\">      ReplaceDeduplicateWithAggregate) ::</span><br><span class=\"line\">    &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class=\"line\">    &#x2F;&#x2F; Optimizer rules start here</span><br><span class=\"line\">    &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class=\"line\">      &#x2F;&#x2F; 译: 在应用主要的优化规则之前先调用 CombineUnions,因为可以减少迭代次数，而其他规则可以在两个相邻的 Union操作符之间添加&#x2F;移动额外的操作符</span><br><span class=\"line\">      &#x2F;&#x2F; 在规则库&quot;Operator Optimizations&quot;中再次调用 CombineUnions,是因为其他操作符可能会导致两个单独的 Union 变得相邻。</span><br><span class=\"line\">    Batch(&quot;Union&quot;, Once,</span><br><span class=\"line\">      CombineUnions) ::</span><br><span class=\"line\">    Batch(&quot;Pullup Correlated Expressions&quot;, Once,</span><br><span class=\"line\">      PullupCorrelatedPredicates) ::</span><br><span class=\"line\">    Batch(&quot;Subquery&quot;, Once,</span><br><span class=\"line\">      OptimizeSubqueries) ::</span><br><span class=\"line\">    Batch(&quot;Replace Operators&quot;, fixedPoint,</span><br><span class=\"line\">      ReplaceIntersectWithSemiJoin,</span><br><span class=\"line\">      ReplaceExceptWithFilter,</span><br><span class=\"line\">      ReplaceExceptWithAntiJoin,</span><br><span class=\"line\">      ReplaceDistinctWithAggregate) ::</span><br><span class=\"line\">    Batch(&quot;Aggregate&quot;, fixedPoint,</span><br><span class=\"line\">      RemoveLiteralFromGroupExpressions,</span><br><span class=\"line\">      RemoveRepetitionFromGroupExpressions) :: Nil ++</span><br><span class=\"line\">    operatorOptimizationBatch) :+</span><br><span class=\"line\">    Batch(&quot;Join Reorder&quot;, Once,</span><br><span class=\"line\">      CostBasedJoinReorder) :+</span><br><span class=\"line\">    Batch(&quot;Decimal Optimizations&quot;, fixedPoint,</span><br><span class=\"line\">      DecimalAggregates) :+</span><br><span class=\"line\">    Batch(&quot;Object Expressions Optimization&quot;, fixedPoint,</span><br><span class=\"line\">      EliminateMapObjects,</span><br><span class=\"line\">      CombineTypedFilters) :+</span><br><span class=\"line\">    Batch(&quot;LocalRelation&quot;, fixedPoint,</span><br><span class=\"line\">      ConvertToLocalRelation,</span><br><span class=\"line\">      PropagateEmptyRelation) :+</span><br><span class=\"line\">    &#x2F;&#x2F; The following batch should be executed after batch &quot;Join Reorder&quot; and &quot;LocalRelation&quot;.</span><br><span class=\"line\">    Batch(&quot;Check Cartesian Products&quot;, Once,</span><br><span class=\"line\">      CheckCartesianProducts) :+</span><br><span class=\"line\">    Batch(&quot;RewriteSubquery&quot;, Once,</span><br><span class=\"line\">      RewritePredicateSubquery,</span><br><span class=\"line\">      ColumnPruning,</span><br><span class=\"line\">      CollapseProject,</span><br><span class=\"line\">      RemoveRedundantProject)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>按规则(库)应用的顺序串行列出(下述规则均在 SparkOptimizer中规则之前)，<b>欢迎指出错误</b>       </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">规则库名</th>\n<th align=\"left\">规则类名</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Eliminate Distinct</td>\n<td align=\"left\">EliminateDistinct</td>\n<td align=\"left\">去除 MAX/MIN 函数内的 DISTINCT</td>\n<td align=\"left\">SELECT MAX(DISTINCT(age)) FROM a</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">EliminateSubqueryAliases</td>\n<td align=\"left\">去除子查询别名,子查询仅仅提供查询属性集的作用,analyzer 阶段结束后便可去除子查询别名</td>\n<td align=\"left\">Finish Analysis 规则不在 analyzer 中删除的意义是保存原始的 analyzed logic plan</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">EliminateView</td>\n<td align=\"left\">去除视图操作符</td>\n<td align=\"left\">视图的输出属性集必须和子查询输出属性集完全一致</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ReplaceExpressions</td>\n<td align=\"left\">将RuntimeReplaceable 表达式替换为可执行表达式</td>\n<td align=\"left\">一般用来兼容各类数据库，例如用 coalesce 替换 nvl 函数</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ComputeCurrentTime</td>\n<td align=\"left\">记录当前时间,SQL 中任意位置和执行顺序的操作时间(date,timestamp)都会返回一致的结果</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">GetCurrentDatabase</td>\n<td align=\"left\">使用 SessionCatalog 的数据库作为当前数据库返回</td>\n<td align=\"left\">CurrentDataBase 函数没必要在执行阶段一条条计算,在 RBO 阶段获取完作为常量替换了函数表达式</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">RewriteDistinctAggregates</td>\n<td align=\"left\">重写 count(distinct xx),展开数据通过一次聚合即可算出结果</td>\n<td align=\"left\">SELECT count(distinct name) as name_cnt, count(distinct(age)) as age_cnt,sum(age) as age_sum FROM p GROUP BY year=&gt; 将数据展开成[[year,null,null,type0,ageVal],[year,name,null,type1,null],[year,null,age,type2,ageVal]]],group by (year,name,age,type,age),sum(if(type1,ageVal,0));sum(if(type2,1,0));sum(if(type3,1,0))…</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ReplaceDeduplicateWithAggregate</td>\n<td align=\"left\">替换 Deduplicate 操作为 Aggregate</td>\n<td align=\"left\">没找到 Deduplicate 对应的语法,FIX ME</td>\n</tr>\n<tr>\n<td align=\"left\">Union</td>\n<td align=\"left\">CombineUnions</td>\n<td align=\"left\">将有父子关系的 union all 汇集叶子节点一层</td>\n<td align=\"left\">SELECT * FROM(SELECT * FROM a UNION ALL SELECT * FROM b) UNION ALL SELECT * FROM c =&gt; SELECT * FROM (a,b,c)</td>\n</tr>\n<tr>\n<td align=\"left\">Pullup Correlated Expressions</td>\n<td align=\"left\">PullupCorrelatedPredicates</td>\n<td align=\"left\">?</td>\n<td align=\"left\">?</td>\n</tr>\n<tr>\n<td align=\"left\">Subquery</td>\n<td align=\"left\">OptimizeSubqueries</td>\n<td align=\"left\">RBO 优化子查询</td>\n<td align=\"left\">子查询的父阶段为 SubQueryExpression,当解析到 SubQueryExpression 时,使用 RBO 优化子查询逻辑计划</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceIntersectWithSemiJoin</td>\n<td align=\"left\">将 intersect 操作符转化为 left semi join 后再 distinct</td>\n<td align=\"left\">left semi join 保留在右表中能关联到的左表选取列.SELECT name,age FROM a INTERSECT SELECT name,age FROM b =&gt; SELECT  DISTINCT name,age FROM a LEFT SEMI JOIN b ON a.name=b.name AND a.age=b.age</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceExceptWithFilter</td>\n<td align=\"left\">将两个相同查询计划的子句 except操作符转为 Filter 后 Distinct</td>\n<td align=\"left\">SELECT a1,a2 FROM t WHERE a2=12 EXCEPT SELECT a1,a2 FROM t WHERE a1=5  =&gt;SELECT DISTINCT a1,a2 FROM t WHERE a2=12 AND NOT(a1=5)。需要相同查询计划子句才能执行此转换</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceExceptWithAntiJoin</td>\n<td align=\"left\">不同查询计划的子句 except 操作符转为 left anti join 操作后 DISTINCT</td>\n<td align=\"left\">SELECT a1,a2 FROM t1 EXCEPT SELECT b1,b2 FROM t2 =&gt; SELECT DISTINCT a1,a2 FROM t1 LEFT ANTI JOIN t2 ON t1.a1=t2.b1 AND t1.a2=t2.b2</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceDistinctWithAggregate</td>\n<td align=\"left\">将 distinct 操作符转为 group by</td>\n<td align=\"left\">SELECT DISTINCT a,b FROM t =&gt; SELECT a,b FROM t GROUP BY a,b</td>\n</tr>\n<tr>\n<td align=\"left\">Aggregate</td>\n<td align=\"left\">RemoveLiteralFromGroupExpressions</td>\n<td align=\"left\">去除 group by 条件中的常量和可折叠常量表达式,不影响聚合结果但会减少 key 数据量</td>\n<td align=\"left\">SELECT a,b FROM t GROUP BY a,b,1,1&lt;0 =&gt;SELECT a,b FROM t GROUP BY a,b</td>\n</tr>\n<tr>\n<td align=\"left\">Aggregate</td>\n<td align=\"left\">RemoveRepetitionFromGroupExpressions</td>\n<td align=\"left\">去除 group by 条件中的重复表达式,不影响聚合结果但会减少 key 数据量</td>\n<td align=\"left\">SELECT a,b FROM t GROUP BY a,b,b =&gt;</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushProjectionThroughUnion</td>\n<td align=\"left\">将查询列下推到各 union all 子句</td>\n<td align=\"left\">SELECT a FROM (SELECT a,b FROM t1 UNION ALL SELECT c,d FROM t2) tbl_a =&gt; SELECT a FROM (SELECT a FROM t1 UNION ALL SELECT c FROM t2) tbl_a</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">ReorderJoin</td>\n<td align=\"left\">按照逻辑计划数据行数启发式寻找星型模型的事实表和维度表,事实表在左以避免大表加载到内存,并以此重定义连接顺序</td>\n<td align=\"left\">取决于spark.sql.cbo.starSchemaDetection 和 spark.sql.cbo.enabled,默认不开启。SELECT * FROM a,b,c where a.key=b.key and a.val=c.val =&gt; SELECT * FROM a join c on a.val=c.val join b on a.key=b.key , 大表居左小表居右</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">EliminateOuterJoin</td>\n<td align=\"left\">尽可能转换 full join=&gt;right/left join=&gt;inner join</td>\n<td align=\"left\">如果存在谓词可以削减 null-supplying 行(当输入数据为 null 时,谓词返回值为 null 或 false),则可以削弱 outer join. full join 可以转为 left/right/inner join,left/join join 可以转为 inner join. SELECT * FROM a left join b on a.key=b.key WHERE b.key IS NOT NULL =&gt;SELECT * FROM a inner join b on a.key=b.key</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushPredicateThroughJoin</td>\n<td align=\"left\">join 谓词下推</td>\n<td align=\"left\">包含两层含义: where 条件下推到 join 条件,join 条件下推到子查询</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushDownPredicate</td>\n<td align=\"left\">对于确定性的操作和谓词且谓词不能改变行的逻辑计划,尽可能下推谓词以减少输入数据量</td>\n<td align=\"left\">对于支持类似 BloomFilter 的列式存储,有极大提升</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">LimitPushDown</td>\n<td align=\"left\">旨在减少输入数据量。1. union all 上级和下级 limit 数值不同则下级取 min 2.left join 上级和下级 limit 数值不同则下级左侧limit 取 min;right join 同理</td>\n<td align=\"left\">SELECT * FROM (SELECT * FORM a limit 20 UNION ALL SELECT * FORM b limit 15) temp LIMIT 10 或者 SELECT * FROM (SELECT * FROM a LEFT JOIN b ON a.key=b.key) temp LIMIT 10</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">ColumnPruning</td>\n<td align=\"left\">列裁剪</td>\n<td align=\"left\">去掉在查询/聚合/窗口函数/union 等用不到的列读取</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件</td>\n<td align=\"left\">在当前规则库会被过滤掉,在 Infer Filters 规则库执行</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseRepartition</td>\n<td align=\"left\">折叠有父子关系的再分区操作</td>\n<td align=\"left\">当父节点和子节点均为再分区操作,但父节点无 shuffle(coalesce api)子节点有 shuffle 且父节点再分区数大于子节点再分区数则删除父节点,若再分区数不大于子节点则不改变结构。若 shuffle 关系不为(false,true)及其他情况则折叠子节点 RepartitionOperation</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseProject</td>\n<td align=\"left\">折叠有父子关系的父节点 Projection 操作</td>\n<td align=\"left\">SELECT age FROM (SELECT name,age+1 as age FROM p) temp=&gt;SELECT age+1 as age FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseWindow</td>\n<td align=\"left\">折叠分区字段一致、排序方式一致且表达式相互独立、有父子关系的子节点 Window 操作</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineFilters</td>\n<td align=\"left\">折叠有父子关系的父节点 Filter 操作</td>\n<td align=\"left\">SELECT name,age FROM (SELECT name,age FROM p where name is not null)temp where age&gt;10 =&gt;SELECT name,age FROM p WHERE name is not null AND age&gt;10</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineLimits</td>\n<td align=\"left\">折叠有父子关系的父节点 Limit 操作,limit 数值取 minimum</td>\n<td align=\"left\">Limit 分为 GlobalLimit,LocalLimit 两类,GlobalLimit 需要shuffle,LocalLimit 不需要。GlobalLimit(Union(A,B))可以转化为 GlobalLimit(Union(LocalLimit(A),LocalLimit(B)))来降低 shuffle 数据量</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineUnions</td>\n<td align=\"left\">重复规则,同 Union规则库的 CombineUnion 规则</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">NullPropagation</td>\n<td align=\"left\">null 值替换</td>\n<td align=\"left\">将表达式中可评估的 null 值替换为等效常量,count(name)=&gt;count(1)</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ConstantPropagation</td>\n<td align=\"left\">Filter 操作中常量替换</td>\n<td align=\"left\">SELECT * FROM p WHERE age=10 and age2=age+3 =&gt; SELECT * FROM p WHERE age=10 AND age2=13</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">FoldablePropagation</td>\n<td align=\"left\">可折叠表达式替换</td>\n<td align=\"left\">尽可能将属性替换为原始可折叠表达式,其他优化规则将利用可折叠表达式进行优化。SELECT 1.0 as x,’abc’ as y,now() as z order by x,y,z =&gt; SELECT 1.0 as x,’abc’ as y,now() as z ORDER BY 1.0,’abc’,now(); 这样其他规则可以去掉 order by 操作符</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">OptimizeIn</td>\n<td align=\"left\">IN 优化</td>\n<td align=\"left\">尽可能优化 IN 谓词: 1.当 in 列表为空且指定列不可为空时直接返回 false 2.当 in 列表元素数量超过配置(默认 10),将列表转为 HashSet 以去重和使用哈希索引提高性能。In(value,seq[Literal])=&gt;InSet(value,HashSet[Literal])</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ConstantFolding</td>\n<td align=\"left\">常量折叠</td>\n<td align=\"left\">将可静态计算的表达式替换为等效常量.SELECT 1+2 as a FROM p=&gt;SELECT 3 FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ReorderAssociativeOperator</td>\n<td align=\"left\">重排序所有整数类型运算符,将所有确定性整数折叠计算为一个结果</td>\n<td align=\"left\">和 ConstantFolding 折叠不同，ConstantFolding 要在整个表达式都静态可计算(确定性)时才会应用。ReorderAssociativeOperator 这里尽管有不确定性部分，但会尽可能将确定性部分计算出来。SELECT (age+1)+2 as a FROM p =&gt;SELECT age+3 as a FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">LikeSimplification</td>\n<td align=\"left\">正则匹配简化</td>\n<td align=\"left\">尽可能将 rlike正则匹配简化为字符串startWith,endWith,equal,container 等操作</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">BooleanSimplification</td>\n<td align=\"left\">布尔表达式简化</td>\n<td align=\"left\">尽可能简化 boolean 表达式/快速中断/删除不必要的 not 。false AND e=&gt;false,true AND e=&gt;e,a AND b=&gt;Not(a).semanticEquals(b) =&gt; false,Not(Not(a))=&gt;a</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyConditionals</td>\n<td align=\"left\">条件表达式简化(if(condition,trueVal,falseVal),case when)</td>\n<td align=\"left\">如果 if表达式 恒定true或 false 则修改逻辑计划为相应的 val;如果 case when 表达式中有恒 false 值则删除其分支，若所有分支恒 false 则取 else 语句值；若 case when 第一个表达式恒 true，则取其值；恒 true 分支会删除其后续所有分支</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveDispensableExpressions</td>\n<td align=\"left\">删除 UnaryPositive 节点(仅有标识子节点表达式作用)</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyBinaryComparison</td>\n<td align=\"left\">简化比较</td>\n<td align=\"left\">1.将&lt;=&gt;替换为 true;2.将=和&lt;=和&gt;= 在两侧均非空且逻辑计划结果一致时替换为 true;3. 将&gt;和&lt;在两侧均非空且逻辑计划结果一致时替换为 false。age&lt;AGE  =&gt;false</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">PruneFilters</td>\n<td align=\"left\">约束条件简化</td>\n<td align=\"left\">1. 当约束条件恒 true 时,删除父节点;2.当约束条件恒 false 或 null 时,替换父节点的输入为空集;3.在父节点约束条件中去除子节点已有的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">EliminateSorts</td>\n<td align=\"left\">删除无效的排序</td>\n<td align=\"left\">删除排序中确定性的排序方式,甚至不排序.SELECT name,age FROM p ORDER BY 1 ASC, age DESC</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCasts</td>\n<td align=\"left\">强制类型转换简化</td>\n<td align=\"left\">当强制转化的类型相同(仅限于 基础数据类型相同 或 Array 内嵌数据类型相同 或 Map 内嵌 key 和 value 类型均相同)时,去除 Cast (col to type) 操作</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCaseConversionExpressions</td>\n<td align=\"left\">简化大小写转换表达式</td>\n<td align=\"left\">内部转换会被外部转换覆盖 Upper(Lower(x))=&gt;Upper(x),Lower(Lower(x))=&gt;Lower(x)</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RewriteCorrelatedScalarSubquery</td>\n<td align=\"left\">？？</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">EliminateSerialization</td>\n<td align=\"left\">删除不必要的在 object 和 InternalRow 之间的(循环)序列化/反序列化操作</td>\n<td align=\"left\">1.反序列化为父节点,序列化为子节点且操作字段相同;2.将数据反序列化并追加数据到末尾时AppendColumns可替换AppendColumnsWithObject操作符 直接操作序列化后的数据(类似于 UnsafeSortShuffle) 3. TypedFilter为父节点,序列化为子节点时 4.反序列化为父节点,TypedFilter 为子节点时</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveRedundantAliases</td>\n<td align=\"left\">删除无效别名</td>\n<td align=\"left\">无效别名是指在子查询或者关联中不改变列名/列元数据的别名</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveRedundantProject</td>\n<td align=\"left\">删除无效查询</td>\n<td align=\"left\">无效查询是指父查询和子查询目标字段相同</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateStructOps</td>\n<td align=\"left\">结构体创建简化</td>\n<td align=\"left\">named_struct(‘name’,name,’age’,age).age=&gt;age</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateArrayOps</td>\n<td align=\"left\">数组创建简化</td>\n<td align=\"left\">1.当对创建的数组按下标取值时,减少创建数组的数据量.Array(elem0,elem1…)(1)=&gt;elems(1);2.Array(named_stuct(name,”nA”),named_struct(age,12))[0].name=&gt;named_struct(name,”nA”).name</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateMapOps</td>\n<td align=\"left\">映射创建简化</td>\n<td align=\"left\">map(key1-&gt;val2,key2-&gt;val2).key2 =&gt; case when key2</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">CombineConcats</td>\n<td align=\"left\">合并 concat</td>\n<td align=\"left\">将有父子关系的 concat 所有子节点扁平化 合并</td>\n</tr>\n<tr>\n<td align=\"left\">Infer Filters</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">Join Reorder</td>\n<td align=\"left\">CostBasedJoinReorder</td>\n<td align=\"left\">？</td>\n<td align=\"left\">？</td>\n</tr>\n<tr>\n<td align=\"left\">Decimal Optimizations</td>\n<td align=\"left\">DecimalAggregates</td>\n<td align=\"left\">加速浮点数运算</td>\n<td align=\"left\">float 和 double 运算中一般需要控制精度(precision) 和小数位(scale)。窗口函数内聚合和普通聚合的 sum/avg 场景下将浮点计算转为长整形计算并在结束时转回来</td>\n</tr>\n<tr>\n<td align=\"left\">Object Expressions Optimization</td>\n<td align=\"left\">EliminateMapObjects</td>\n<td align=\"left\">简化 MapObject 操作</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Object Expressions Optimization</td>\n<td align=\"left\">CombineTypedFilters</td>\n<td align=\"left\">简化 TypedFilter</td>\n<td align=\"left\">去除具有父子关系的子节点 TypedFilter,合并两者的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">LocalRelation</td>\n<td align=\"left\">ConvertToLocalRelation</td>\n<td align=\"left\">简化为 LocalRelation</td>\n<td align=\"left\">在 LocalRelation 上取 Limit 时,直接转化为 LocalRelation 在数据上取 Limit 减少输入数据数量</td>\n</tr>\n<tr>\n<td align=\"left\">LocalRelation</td>\n<td align=\"left\">PropagateEmptyRelation</td>\n<td align=\"left\">空 Relation 优化</td>\n<td align=\"left\">对于上述优化规则(e.g. 列裁剪谓词下推)产生的或基础数据为空的 Relation 进行优化. 1,关联时按照左右空 Relation 和Join 类型分别讨论,左空 &amp;&amp; 左外连接=&gt;Empty 2.union all 所有子节点都空直接返回空数据集 3.一元节点的所有子节点都是空 Relation 则直接返回空数据集合, Select/Limit/Repartition 且 children 都为空 Relation=&gt;空数据集合</td>\n</tr>\n<tr>\n<td align=\"left\">Check Cartesian Products</td>\n<td align=\"left\">CheckCartesianProducts</td>\n<td align=\"left\">笛卡尔积检测</td>\n<td align=\"left\">检测逻辑计划树中是否有全外连接</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">RewritePredicateSubquery</td>\n<td align=\"left\">重写谓词子查询</td>\n<td align=\"left\">将 in/exists 转为 semi join,将 not in / not exists 转为 anti join</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">ColumnPruning</td>\n<td align=\"left\">列裁剪</td>\n<td align=\"left\">同 Aggregate规则库中的 ColumnPruning,去掉在查询/聚合/窗口函数/union 等用不到的列读取</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">CollapseProject</td>\n<td align=\"left\">折叠有父子关系的父节点 Projection 操作</td>\n<td align=\"left\">同 Aggregate 规则库的 RewriteSubquery</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">RemoveRedundantProject</td>\n<td align=\"left\">删除无效查询</td>\n<td align=\"left\">同 Aggregate 规则库的RemoveRedundantProject,无效查询是指父查询和子查询目标字段相同</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Parser 模块负责将 SQL 文本解析为 UnResolvedLogicPlan，Analyzer 模块负责将 UnResolvedLogicPlan 绑定解析为 ResolvedLogicPlan。Rule-Based Optimizer，简称 RBO 或 Optimizer 模块负责基于规则优化逻辑计划，主要思路是基于各类型规则对 ResolvedLogicPlan 进行优化达到减少每个逻辑计划树节点输入数据量或计算代价的目的，大名鼎鼎的列裁剪和谓词下推、常量折叠就是出自于 Optimizer 模块。      </p>","more":"<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>[QueryExecution.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 第一步: 使用 CacheManager 替换逻辑计划为已解析过的逻辑计划</span><br><span class=\"line\">lazy val withCachedData: LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    assertAnalyzed()</span><br><span class=\"line\">    assertSupported()</span><br><span class=\"line\">    sparkSession.sharedState.cacheManager.useCachedData(analyzed)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> &#x2F;&#x2F; 第二步: 基于规则优化逻辑计划树</span><br><span class=\"line\">  lazy val optimizedPlan: LogicalPlan &#x3D; sparkSession.sessionState.optimizer.execute(withCachedData)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"CacheManager\"><a href=\"#CacheManager\" class=\"headerlink\" title=\"CacheManager\"></a>CacheManager</h3><p>[CacheManager.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private val cachedData &#x3D; new java.util.LinkedList[CachedData]</span><br><span class=\"line\"></span><br><span class=\"line\">def useCachedData(plan: LogicalPlan): LogicalPlan &#x3D; &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 先序遍历</span><br><span class=\"line\">    val newPlan &#x3D; plan transformDown &#123;</span><br><span class=\"line\">      case hint: ResolvedHint &#x3D;&gt; hint</span><br><span class=\"line\">      case currentFragment &#x3D;&gt;</span><br><span class=\"line\">        &#x2F;&#x2F; 检索 CacheManager</span><br><span class=\"line\">        lookupCachedData(currentFragment)</span><br><span class=\"line\">           &#x2F;&#x2F; 使用当前计划的输出字段替换原有的</span><br><span class=\"line\">          .map(_.cachedRepresentation.withOutput(currentFragment.output))</span><br><span class=\"line\">          .getOrElse(currentFragment)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    newPlan transformAllExpressions &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 剥去子查询别名外壳</span><br><span class=\"line\">      case s: SubqueryExpression &#x3D;&gt; s.withNewPlan(useCachedData(s.plan))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"> def lookupCachedData(plan: LogicalPlan): Option[CachedData] &#x3D; readLock &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 遍历对比CacheManager 中是否存在和当前计划一致的  </span><br><span class=\"line\">    cachedData.asScala.find(cd &#x3D;&gt; plan.sameResult(cd.plan))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通常判断两个查询计划是否相同是很复杂的问题，因此可能实际相同也可以返回 false，但是实际不同一定不能返回 true。这种行为不会影响正确性，仅仅相当于弃用了 CacheManager，多做了一些重复处理而已。这种思想有点像 BloomFilter 的误判率: 函数判断在集合但是实际上可能不在，仅仅是多个 hash 函数得到的位置在 bitmap 中恰好非 0；返回不在集合时一定不在。<br>[QueryPlan.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; 将逻辑计划规范化之后再做比较</span><br><span class=\"line\">final def sameResult(other: PlanType): Boolean &#x3D; this.canonicalized &#x3D;&#x3D; other.canonicalized</span><br><span class=\"line\">&#x2F;&#x2F; 规范化 逻辑计划涉及的所有字段的 id(从 0 递增)。</span><br><span class=\"line\">protected def doCanonicalize(): PlanType &#x3D; &#123;</span><br><span class=\"line\">    val canonicalizedChildren &#x3D; children.map(_.canonicalized)</span><br><span class=\"line\">    var id &#x3D; -1</span><br><span class=\"line\">    mapExpressions &#123;</span><br><span class=\"line\">      case a: Alias &#x3D;&gt;</span><br><span class=\"line\">        id +&#x3D; 1</span><br><span class=\"line\">        val normalizedChild &#x3D; QueryPlan.normalizeExprId(a.child, allAttributes)</span><br><span class=\"line\">        Alias(normalizedChild, &quot;&quot;)(ExprId(id), a.qualifier)</span><br><span class=\"line\"></span><br><span class=\"line\">      case ar: AttributeReference if allAttributes.indexOf(ar.exprId) &#x3D;&#x3D; -1 &#x3D;&gt;</span><br><span class=\"line\">        id +&#x3D; 1</span><br><span class=\"line\">        ar.withExprId(ExprId(id)).canonicalized</span><br><span class=\"line\"></span><br><span class=\"line\">      case other &#x3D;&gt; QueryPlan.normalizeExprId(other, allAttributes)</span><br><span class=\"line\">    &#125;.withNewChildren(canonicalizedChildren)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>有必要提一下 InMemoryRelation 这个类，这是 CacheManager 缓存已被 RBO 优化过的内存数据结构，包括：逻辑计划的输出字段，是否压缩(spark.sql.inMemoryColumnarStorage.compressed，默认 true，SparkSQL 将根据元数据信息自动匹配编码器)，批处理大小(spark.sql.inMemoryColumnarStorage.batchSize，默认 10000，控制列式存储的批处理大小，较大的值能提高内存利用率和压缩率 但是读数据时可能会 OOM)，存储等级(默认 MEMORY_AND_DISK)，RBO 优化完成的物理计划 SparkPlan，表名，对应 RDD[CachedBatch]，物理数据字节数，额外有一个<b> statsOfPlanToCache 暂时不知用途</b><br>[InMemoryRelation.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class InMemoryRelation(</span><br><span class=\"line\">    output: Seq[Attribute],</span><br><span class=\"line\">    useCompression: Boolean,</span><br><span class=\"line\">    batchSize: Int,</span><br><span class=\"line\">    storageLevel: StorageLevel,</span><br><span class=\"line\">    @transient child: SparkPlan,</span><br><span class=\"line\">    tableName: Option[String])(</span><br><span class=\"line\">    @transient var _cachedColumnBuffers: RDD[CachedBatch] &#x3D; null,</span><br><span class=\"line\">    val sizeInBytesStats: LongAccumulator &#x3D; child.sqlContext.sparkContext.longAccumulator,</span><br><span class=\"line\">    statsOfPlanToCache: Statistics)</span><br><span class=\"line\">  extends logical.LeafNode with MultiInstanceRelation</span><br></pre></td></tr></table></figure>\n<p>CachedBatch 用以表明 batch 中数据行数，序列化的列数据 buffer，数据的元数据类型。<br>[CachedBatch.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">case class CachedBatch(numRows: Int, buffers: Array[Array[Byte]], stats: InternalRow)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"RBO\"><a href=\"#RBO\" class=\"headerlink\" title=\"RBO\"></a>RBO</h3><p>SessionState 初始化时指定的 Optimizer 为 SparkOptimizer，SparkOptimizer 继承自 Optimizer，Optimizer 与 Analyzer 均继承自 RuleExecutor，都使用RuleExecutor#execute 方法遍历逻辑计划树应用 Optimizer/Analyzer 各自指定的规则库和执行策略 优化每个逻辑计划。故不再引用 RuleExecutor#execute 方法。<br>[BaseSessionStateBuilder.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected def optimizer: Optimizer &#x3D; &#123;</span><br><span class=\"line\">    new SparkOptimizer(catalog, experimentalMethods) &#123;</span><br><span class=\"line\">      override def extendedOperatorOptimizationRules: Seq[Rule[LogicalPlan]] &#x3D;</span><br><span class=\"line\">        super.extendedOperatorOptimizationRules ++ customOperatorOptimizationRules</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>除了类似于前后拦截器功能的 preOptimizationBatches、postHocOptimizationBatches 和拓展规则之外，SparkOptimizer 主要引用了 Optimizer 定义的通用规则，还有 4 个自定义规则:  </p>\n<ol>\n<li>OptimizeMetadataOnlyQuery，优化那些只遍历表分区键级别的元数据就可以完成的逻辑计划   　<ul>\n<li>在分区键上做聚合。e.g. SELECT col FROM tbl GROUP by col;   </li>\n<li>在分区键上去重并使用聚合函数。e.g. SELECT col1,count(DISTINCT col2) FROM tbl GROUP BY col1;   </li>\n<li>在分区键上应用有去重功能的聚合函数。e.g. SELECT col1, Max(col2) FROM tbl GROUP BY col1;   </li>\n</ul>\n</li>\n<li>ExtractPythonUDFFromAggregate，提取聚合操作中所有 PythonUDF。十分不推荐使用 pyspark   </li>\n<li>PruneFileSourcePartitions，物理文件分区下推。 读取 hadoop 目录时尽可能将过滤条件下推到分区键上，避免扫描所有文件。   </li>\n<li>PushDownOperatorsToDataSource  过滤操作下推到数据源，以提高性能。  </li>\n</ol>\n<p>[SparkOptimizer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def batches: Seq[Batch] &#x3D; (preOptimizationBatches ++ super.batches :+</span><br><span class=\"line\">    Batch(&quot;Optimize Metadata Only Query&quot;, Once, OptimizeMetadataOnlyQuery(catalog)) :+</span><br><span class=\"line\">    Batch(&quot;Extract Python UDF from Aggregate&quot;, Once, ExtractPythonUDFFromAggregate) :+</span><br><span class=\"line\">    Batch(&quot;Prune File Source Table Partitions&quot;, Once, PruneFileSourcePartitions) :+</span><br><span class=\"line\">    Batch(&quot;Push down operators to data source scan&quot;, Once, PushDownOperatorsToDataSource)) ++</span><br><span class=\"line\">    postHocOptimizationBatches :+</span><br><span class=\"line\">    Batch(&quot;User Provided Optimizers&quot;, fixedPoint, experimentalMethods.extraOptimizations: _*)</span><br></pre></td></tr></table></figure>\n<p>[Optimizer.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def batches: Seq[Batch] &#x3D; &#123;</span><br><span class=\"line\">    val operatorOptimizationRuleSet &#x3D;</span><br><span class=\"line\">      Seq(</span><br><span class=\"line\">        &#x2F;&#x2F; Operator push down</span><br><span class=\"line\">        PushProjectionThroughUnion,</span><br><span class=\"line\">        ReorderJoin,</span><br><span class=\"line\">        EliminateOuterJoin,</span><br><span class=\"line\">        PushPredicateThroughJoin,</span><br><span class=\"line\">        PushDownPredicate,</span><br><span class=\"line\">        LimitPushDown,</span><br><span class=\"line\">        ColumnPruning,</span><br><span class=\"line\">        InferFiltersFromConstraints,</span><br><span class=\"line\">        &#x2F;&#x2F; Operator combine</span><br><span class=\"line\">        CollapseRepartition,</span><br><span class=\"line\">        CollapseProject,</span><br><span class=\"line\">        CollapseWindow,</span><br><span class=\"line\">        CombineFilters,</span><br><span class=\"line\">        CombineLimits,</span><br><span class=\"line\">        CombineUnions,</span><br><span class=\"line\">        &#x2F;&#x2F; Constant folding and strength reduction</span><br><span class=\"line\">        NullPropagation,</span><br><span class=\"line\">        ConstantPropagation,</span><br><span class=\"line\">        FoldablePropagation,</span><br><span class=\"line\">        OptimizeIn,</span><br><span class=\"line\">        ConstantFolding,</span><br><span class=\"line\">        ReorderAssociativeOperator,</span><br><span class=\"line\">        LikeSimplification,</span><br><span class=\"line\">        BooleanSimplification,</span><br><span class=\"line\">        SimplifyConditionals,</span><br><span class=\"line\">        RemoveDispensableExpressions,</span><br><span class=\"line\">        SimplifyBinaryComparison,</span><br><span class=\"line\">        PruneFilters,</span><br><span class=\"line\">        EliminateSorts,</span><br><span class=\"line\">        SimplifyCasts,</span><br><span class=\"line\">        SimplifyCaseConversionExpressions,</span><br><span class=\"line\">        RewriteCorrelatedScalarSubquery,</span><br><span class=\"line\">        EliminateSerialization,</span><br><span class=\"line\">        RemoveRedundantAliases,</span><br><span class=\"line\">        RemoveRedundantProject,</span><br><span class=\"line\">        SimplifyCreateStructOps,</span><br><span class=\"line\">        SimplifyCreateArrayOps,</span><br><span class=\"line\">        SimplifyCreateMapOps,</span><br><span class=\"line\">        CombineConcats) ++</span><br><span class=\"line\">        extendedOperatorOptimizationRules</span><br><span class=\"line\"></span><br><span class=\"line\">    val operatorOptimizationBatch: Seq[Batch] &#x3D; &#123;</span><br><span class=\"line\">      val rulesWithoutInferFiltersFromConstraints &#x3D;</span><br><span class=\"line\">        operatorOptimizationRuleSet.filterNot(_ &#x3D;&#x3D; InferFiltersFromConstraints)</span><br><span class=\"line\">      Batch(&quot;Operator Optimization before Inferring Filters&quot;, fixedPoint,</span><br><span class=\"line\">        rulesWithoutInferFiltersFromConstraints: _*) ::</span><br><span class=\"line\">      Batch(&quot;Infer Filters&quot;, Once,</span><br><span class=\"line\">        InferFiltersFromConstraints) ::</span><br><span class=\"line\">      Batch(&quot;Operator Optimization after Inferring Filters&quot;, fixedPoint,</span><br><span class=\"line\">        rulesWithoutInferFiltersFromConstraints: _*) :: Nil</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    (Batch(&quot;Eliminate Distinct&quot;, Once, EliminateDistinct) ::</span><br><span class=\"line\">    &#x2F;&#x2F; 译: 从技术上将，”Finish Analysis“中的部分规则不是 optimizer rule 而是 analyzer rule，因为它们是保证正确性必须的。</span><br><span class=\"line\">    &#x2F;&#x2F; 但是 因为我们使用 analyzer 来规范化查询(视图定义)，我们在 analyzer 中不去除子查询或计算当前时间</span><br><span class=\"line\">    Batch(&quot;Finish Analysis&quot;, Once,</span><br><span class=\"line\">      EliminateSubqueryAliases,</span><br><span class=\"line\">      EliminateView,</span><br><span class=\"line\">      ReplaceExpressions,</span><br><span class=\"line\">      ComputeCurrentTime,</span><br><span class=\"line\">      GetCurrentDatabase(sessionCatalog),</span><br><span class=\"line\">      RewriteDistinctAggregates,</span><br><span class=\"line\">      ReplaceDeduplicateWithAggregate) ::</span><br><span class=\"line\">    &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class=\"line\">    &#x2F;&#x2F; Optimizer rules start here</span><br><span class=\"line\">    &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class=\"line\">      &#x2F;&#x2F; 译: 在应用主要的优化规则之前先调用 CombineUnions,因为可以减少迭代次数，而其他规则可以在两个相邻的 Union操作符之间添加&#x2F;移动额外的操作符</span><br><span class=\"line\">      &#x2F;&#x2F; 在规则库&quot;Operator Optimizations&quot;中再次调用 CombineUnions,是因为其他操作符可能会导致两个单独的 Union 变得相邻。</span><br><span class=\"line\">    Batch(&quot;Union&quot;, Once,</span><br><span class=\"line\">      CombineUnions) ::</span><br><span class=\"line\">    Batch(&quot;Pullup Correlated Expressions&quot;, Once,</span><br><span class=\"line\">      PullupCorrelatedPredicates) ::</span><br><span class=\"line\">    Batch(&quot;Subquery&quot;, Once,</span><br><span class=\"line\">      OptimizeSubqueries) ::</span><br><span class=\"line\">    Batch(&quot;Replace Operators&quot;, fixedPoint,</span><br><span class=\"line\">      ReplaceIntersectWithSemiJoin,</span><br><span class=\"line\">      ReplaceExceptWithFilter,</span><br><span class=\"line\">      ReplaceExceptWithAntiJoin,</span><br><span class=\"line\">      ReplaceDistinctWithAggregate) ::</span><br><span class=\"line\">    Batch(&quot;Aggregate&quot;, fixedPoint,</span><br><span class=\"line\">      RemoveLiteralFromGroupExpressions,</span><br><span class=\"line\">      RemoveRepetitionFromGroupExpressions) :: Nil ++</span><br><span class=\"line\">    operatorOptimizationBatch) :+</span><br><span class=\"line\">    Batch(&quot;Join Reorder&quot;, Once,</span><br><span class=\"line\">      CostBasedJoinReorder) :+</span><br><span class=\"line\">    Batch(&quot;Decimal Optimizations&quot;, fixedPoint,</span><br><span class=\"line\">      DecimalAggregates) :+</span><br><span class=\"line\">    Batch(&quot;Object Expressions Optimization&quot;, fixedPoint,</span><br><span class=\"line\">      EliminateMapObjects,</span><br><span class=\"line\">      CombineTypedFilters) :+</span><br><span class=\"line\">    Batch(&quot;LocalRelation&quot;, fixedPoint,</span><br><span class=\"line\">      ConvertToLocalRelation,</span><br><span class=\"line\">      PropagateEmptyRelation) :+</span><br><span class=\"line\">    &#x2F;&#x2F; The following batch should be executed after batch &quot;Join Reorder&quot; and &quot;LocalRelation&quot;.</span><br><span class=\"line\">    Batch(&quot;Check Cartesian Products&quot;, Once,</span><br><span class=\"line\">      CheckCartesianProducts) :+</span><br><span class=\"line\">    Batch(&quot;RewriteSubquery&quot;, Once,</span><br><span class=\"line\">      RewritePredicateSubquery,</span><br><span class=\"line\">      ColumnPruning,</span><br><span class=\"line\">      CollapseProject,</span><br><span class=\"line\">      RemoveRedundantProject)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>按规则(库)应用的顺序串行列出(下述规则均在 SparkOptimizer中规则之前)，<b>欢迎指出错误</b>       </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">规则库名</th>\n<th align=\"left\">规则类名</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Eliminate Distinct</td>\n<td align=\"left\">EliminateDistinct</td>\n<td align=\"left\">去除 MAX/MIN 函数内的 DISTINCT</td>\n<td align=\"left\">SELECT MAX(DISTINCT(age)) FROM a</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">EliminateSubqueryAliases</td>\n<td align=\"left\">去除子查询别名,子查询仅仅提供查询属性集的作用,analyzer 阶段结束后便可去除子查询别名</td>\n<td align=\"left\">Finish Analysis 规则不在 analyzer 中删除的意义是保存原始的 analyzed logic plan</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">EliminateView</td>\n<td align=\"left\">去除视图操作符</td>\n<td align=\"left\">视图的输出属性集必须和子查询输出属性集完全一致</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ReplaceExpressions</td>\n<td align=\"left\">将RuntimeReplaceable 表达式替换为可执行表达式</td>\n<td align=\"left\">一般用来兼容各类数据库，例如用 coalesce 替换 nvl 函数</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ComputeCurrentTime</td>\n<td align=\"left\">记录当前时间,SQL 中任意位置和执行顺序的操作时间(date,timestamp)都会返回一致的结果</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">GetCurrentDatabase</td>\n<td align=\"left\">使用 SessionCatalog 的数据库作为当前数据库返回</td>\n<td align=\"left\">CurrentDataBase 函数没必要在执行阶段一条条计算,在 RBO 阶段获取完作为常量替换了函数表达式</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">RewriteDistinctAggregates</td>\n<td align=\"left\">重写 count(distinct xx),展开数据通过一次聚合即可算出结果</td>\n<td align=\"left\">SELECT count(distinct name) as name_cnt, count(distinct(age)) as age_cnt,sum(age) as age_sum FROM p GROUP BY year=&gt; 将数据展开成[[year,null,null,type0,ageVal],[year,name,null,type1,null],[year,null,age,type2,ageVal]]],group by (year,name,age,type,age),sum(if(type1,ageVal,0));sum(if(type2,1,0));sum(if(type3,1,0))…</td>\n</tr>\n<tr>\n<td align=\"left\">Finish Analysis</td>\n<td align=\"left\">ReplaceDeduplicateWithAggregate</td>\n<td align=\"left\">替换 Deduplicate 操作为 Aggregate</td>\n<td align=\"left\">没找到 Deduplicate 对应的语法,FIX ME</td>\n</tr>\n<tr>\n<td align=\"left\">Union</td>\n<td align=\"left\">CombineUnions</td>\n<td align=\"left\">将有父子关系的 union all 汇集叶子节点一层</td>\n<td align=\"left\">SELECT * FROM(SELECT * FROM a UNION ALL SELECT * FROM b) UNION ALL SELECT * FROM c =&gt; SELECT * FROM (a,b,c)</td>\n</tr>\n<tr>\n<td align=\"left\">Pullup Correlated Expressions</td>\n<td align=\"left\">PullupCorrelatedPredicates</td>\n<td align=\"left\">?</td>\n<td align=\"left\">?</td>\n</tr>\n<tr>\n<td align=\"left\">Subquery</td>\n<td align=\"left\">OptimizeSubqueries</td>\n<td align=\"left\">RBO 优化子查询</td>\n<td align=\"left\">子查询的父阶段为 SubQueryExpression,当解析到 SubQueryExpression 时,使用 RBO 优化子查询逻辑计划</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceIntersectWithSemiJoin</td>\n<td align=\"left\">将 intersect 操作符转化为 left semi join 后再 distinct</td>\n<td align=\"left\">left semi join 保留在右表中能关联到的左表选取列.SELECT name,age FROM a INTERSECT SELECT name,age FROM b =&gt; SELECT  DISTINCT name,age FROM a LEFT SEMI JOIN b ON a.name=b.name AND a.age=b.age</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceExceptWithFilter</td>\n<td align=\"left\">将两个相同查询计划的子句 except操作符转为 Filter 后 Distinct</td>\n<td align=\"left\">SELECT a1,a2 FROM t WHERE a2=12 EXCEPT SELECT a1,a2 FROM t WHERE a1=5  =&gt;SELECT DISTINCT a1,a2 FROM t WHERE a2=12 AND NOT(a1=5)。需要相同查询计划子句才能执行此转换</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceExceptWithAntiJoin</td>\n<td align=\"left\">不同查询计划的子句 except 操作符转为 left anti join 操作后 DISTINCT</td>\n<td align=\"left\">SELECT a1,a2 FROM t1 EXCEPT SELECT b1,b2 FROM t2 =&gt; SELECT DISTINCT a1,a2 FROM t1 LEFT ANTI JOIN t2 ON t1.a1=t2.b1 AND t1.a2=t2.b2</td>\n</tr>\n<tr>\n<td align=\"left\">Replace Operators</td>\n<td align=\"left\">ReplaceDistinctWithAggregate</td>\n<td align=\"left\">将 distinct 操作符转为 group by</td>\n<td align=\"left\">SELECT DISTINCT a,b FROM t =&gt; SELECT a,b FROM t GROUP BY a,b</td>\n</tr>\n<tr>\n<td align=\"left\">Aggregate</td>\n<td align=\"left\">RemoveLiteralFromGroupExpressions</td>\n<td align=\"left\">去除 group by 条件中的常量和可折叠常量表达式,不影响聚合结果但会减少 key 数据量</td>\n<td align=\"left\">SELECT a,b FROM t GROUP BY a,b,1,1&lt;0 =&gt;SELECT a,b FROM t GROUP BY a,b</td>\n</tr>\n<tr>\n<td align=\"left\">Aggregate</td>\n<td align=\"left\">RemoveRepetitionFromGroupExpressions</td>\n<td align=\"left\">去除 group by 条件中的重复表达式,不影响聚合结果但会减少 key 数据量</td>\n<td align=\"left\">SELECT a,b FROM t GROUP BY a,b,b =&gt;</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushProjectionThroughUnion</td>\n<td align=\"left\">将查询列下推到各 union all 子句</td>\n<td align=\"left\">SELECT a FROM (SELECT a,b FROM t1 UNION ALL SELECT c,d FROM t2) tbl_a =&gt; SELECT a FROM (SELECT a FROM t1 UNION ALL SELECT c FROM t2) tbl_a</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">ReorderJoin</td>\n<td align=\"left\">按照逻辑计划数据行数启发式寻找星型模型的事实表和维度表,事实表在左以避免大表加载到内存,并以此重定义连接顺序</td>\n<td align=\"left\">取决于spark.sql.cbo.starSchemaDetection 和 spark.sql.cbo.enabled,默认不开启。SELECT * FROM a,b,c where a.key=b.key and a.val=c.val =&gt; SELECT * FROM a join c on a.val=c.val join b on a.key=b.key , 大表居左小表居右</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">EliminateOuterJoin</td>\n<td align=\"left\">尽可能转换 full join=&gt;right/left join=&gt;inner join</td>\n<td align=\"left\">如果存在谓词可以削减 null-supplying 行(当输入数据为 null 时,谓词返回值为 null 或 false),则可以削弱 outer join. full join 可以转为 left/right/inner join,left/join join 可以转为 inner join. SELECT * FROM a left join b on a.key=b.key WHERE b.key IS NOT NULL =&gt;SELECT * FROM a inner join b on a.key=b.key</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushPredicateThroughJoin</td>\n<td align=\"left\">join 谓词下推</td>\n<td align=\"left\">包含两层含义: where 条件下推到 join 条件,join 条件下推到子查询</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">PushDownPredicate</td>\n<td align=\"left\">对于确定性的操作和谓词且谓词不能改变行的逻辑计划,尽可能下推谓词以减少输入数据量</td>\n<td align=\"left\">对于支持类似 BloomFilter 的列式存储,有极大提升</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">LimitPushDown</td>\n<td align=\"left\">旨在减少输入数据量。1. union all 上级和下级 limit 数值不同则下级取 min 2.left join 上级和下级 limit 数值不同则下级左侧limit 取 min;right join 同理</td>\n<td align=\"left\">SELECT * FROM (SELECT * FORM a limit 20 UNION ALL SELECT * FORM b limit 15) temp LIMIT 10 或者 SELECT * FROM (SELECT * FROM a LEFT JOIN b ON a.key=b.key) temp LIMIT 10</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">ColumnPruning</td>\n<td align=\"left\">列裁剪</td>\n<td align=\"left\">去掉在查询/聚合/窗口函数/union 等用不到的列读取</td>\n</tr>\n<tr>\n<td align=\"left\">Operator push down</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件</td>\n<td align=\"left\">在当前规则库会被过滤掉,在 Infer Filters 规则库执行</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseRepartition</td>\n<td align=\"left\">折叠有父子关系的再分区操作</td>\n<td align=\"left\">当父节点和子节点均为再分区操作,但父节点无 shuffle(coalesce api)子节点有 shuffle 且父节点再分区数大于子节点再分区数则删除父节点,若再分区数不大于子节点则不改变结构。若 shuffle 关系不为(false,true)及其他情况则折叠子节点 RepartitionOperation</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseProject</td>\n<td align=\"left\">折叠有父子关系的父节点 Projection 操作</td>\n<td align=\"left\">SELECT age FROM (SELECT name,age+1 as age FROM p) temp=&gt;SELECT age+1 as age FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CollapseWindow</td>\n<td align=\"left\">折叠分区字段一致、排序方式一致且表达式相互独立、有父子关系的子节点 Window 操作</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineFilters</td>\n<td align=\"left\">折叠有父子关系的父节点 Filter 操作</td>\n<td align=\"left\">SELECT name,age FROM (SELECT name,age FROM p where name is not null)temp where age&gt;10 =&gt;SELECT name,age FROM p WHERE name is not null AND age&gt;10</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineLimits</td>\n<td align=\"left\">折叠有父子关系的父节点 Limit 操作,limit 数值取 minimum</td>\n<td align=\"left\">Limit 分为 GlobalLimit,LocalLimit 两类,GlobalLimit 需要shuffle,LocalLimit 不需要。GlobalLimit(Union(A,B))可以转化为 GlobalLimit(Union(LocalLimit(A),LocalLimit(B)))来降低 shuffle 数据量</td>\n</tr>\n<tr>\n<td align=\"left\">Operator combine</td>\n<td align=\"left\">CombineUnions</td>\n<td align=\"left\">重复规则,同 Union规则库的 CombineUnion 规则</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">NullPropagation</td>\n<td align=\"left\">null 值替换</td>\n<td align=\"left\">将表达式中可评估的 null 值替换为等效常量,count(name)=&gt;count(1)</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ConstantPropagation</td>\n<td align=\"left\">Filter 操作中常量替换</td>\n<td align=\"left\">SELECT * FROM p WHERE age=10 and age2=age+3 =&gt; SELECT * FROM p WHERE age=10 AND age2=13</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">FoldablePropagation</td>\n<td align=\"left\">可折叠表达式替换</td>\n<td align=\"left\">尽可能将属性替换为原始可折叠表达式,其他优化规则将利用可折叠表达式进行优化。SELECT 1.0 as x,’abc’ as y,now() as z order by x,y,z =&gt; SELECT 1.0 as x,’abc’ as y,now() as z ORDER BY 1.0,’abc’,now(); 这样其他规则可以去掉 order by 操作符</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">OptimizeIn</td>\n<td align=\"left\">IN 优化</td>\n<td align=\"left\">尽可能优化 IN 谓词: 1.当 in 列表为空且指定列不可为空时直接返回 false 2.当 in 列表元素数量超过配置(默认 10),将列表转为 HashSet 以去重和使用哈希索引提高性能。In(value,seq[Literal])=&gt;InSet(value,HashSet[Literal])</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ConstantFolding</td>\n<td align=\"left\">常量折叠</td>\n<td align=\"left\">将可静态计算的表达式替换为等效常量.SELECT 1+2 as a FROM p=&gt;SELECT 3 FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">ReorderAssociativeOperator</td>\n<td align=\"left\">重排序所有整数类型运算符,将所有确定性整数折叠计算为一个结果</td>\n<td align=\"left\">和 ConstantFolding 折叠不同，ConstantFolding 要在整个表达式都静态可计算(确定性)时才会应用。ReorderAssociativeOperator 这里尽管有不确定性部分，但会尽可能将确定性部分计算出来。SELECT (age+1)+2 as a FROM p =&gt;SELECT age+3 as a FROM p</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">LikeSimplification</td>\n<td align=\"left\">正则匹配简化</td>\n<td align=\"left\">尽可能将 rlike正则匹配简化为字符串startWith,endWith,equal,container 等操作</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">BooleanSimplification</td>\n<td align=\"left\">布尔表达式简化</td>\n<td align=\"left\">尽可能简化 boolean 表达式/快速中断/删除不必要的 not 。false AND e=&gt;false,true AND e=&gt;e,a AND b=&gt;Not(a).semanticEquals(b) =&gt; false,Not(Not(a))=&gt;a</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyConditionals</td>\n<td align=\"left\">条件表达式简化(if(condition,trueVal,falseVal),case when)</td>\n<td align=\"left\">如果 if表达式 恒定true或 false 则修改逻辑计划为相应的 val;如果 case when 表达式中有恒 false 值则删除其分支，若所有分支恒 false 则取 else 语句值；若 case when 第一个表达式恒 true，则取其值；恒 true 分支会删除其后续所有分支</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveDispensableExpressions</td>\n<td align=\"left\">删除 UnaryPositive 节点(仅有标识子节点表达式作用)</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyBinaryComparison</td>\n<td align=\"left\">简化比较</td>\n<td align=\"left\">1.将&lt;=&gt;替换为 true;2.将=和&lt;=和&gt;= 在两侧均非空且逻辑计划结果一致时替换为 true;3. 将&gt;和&lt;在两侧均非空且逻辑计划结果一致时替换为 false。age&lt;AGE  =&gt;false</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">PruneFilters</td>\n<td align=\"left\">约束条件简化</td>\n<td align=\"left\">1. 当约束条件恒 true 时,删除父节点;2.当约束条件恒 false 或 null 时,替换父节点的输入为空集;3.在父节点约束条件中去除子节点已有的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">EliminateSorts</td>\n<td align=\"left\">删除无效的排序</td>\n<td align=\"left\">删除排序中确定性的排序方式,甚至不排序.SELECT name,age FROM p ORDER BY 1 ASC, age DESC</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCasts</td>\n<td align=\"left\">强制类型转换简化</td>\n<td align=\"left\">当强制转化的类型相同(仅限于 基础数据类型相同 或 Array 内嵌数据类型相同 或 Map 内嵌 key 和 value 类型均相同)时,去除 Cast (col to type) 操作</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCaseConversionExpressions</td>\n<td align=\"left\">简化大小写转换表达式</td>\n<td align=\"left\">内部转换会被外部转换覆盖 Upper(Lower(x))=&gt;Upper(x),Lower(Lower(x))=&gt;Lower(x)</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RewriteCorrelatedScalarSubquery</td>\n<td align=\"left\">？？</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">EliminateSerialization</td>\n<td align=\"left\">删除不必要的在 object 和 InternalRow 之间的(循环)序列化/反序列化操作</td>\n<td align=\"left\">1.反序列化为父节点,序列化为子节点且操作字段相同;2.将数据反序列化并追加数据到末尾时AppendColumns可替换AppendColumnsWithObject操作符 直接操作序列化后的数据(类似于 UnsafeSortShuffle) 3. TypedFilter为父节点,序列化为子节点时 4.反序列化为父节点,TypedFilter 为子节点时</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveRedundantAliases</td>\n<td align=\"left\">删除无效别名</td>\n<td align=\"left\">无效别名是指在子查询或者关联中不改变列名/列元数据的别名</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">RemoveRedundantProject</td>\n<td align=\"left\">删除无效查询</td>\n<td align=\"left\">无效查询是指父查询和子查询目标字段相同</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateStructOps</td>\n<td align=\"left\">结构体创建简化</td>\n<td align=\"left\">named_struct(‘name’,name,’age’,age).age=&gt;age</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateArrayOps</td>\n<td align=\"left\">数组创建简化</td>\n<td align=\"left\">1.当对创建的数组按下标取值时,减少创建数组的数据量.Array(elem0,elem1…)(1)=&gt;elems(1);2.Array(named_stuct(name,”nA”),named_struct(age,12))[0].name=&gt;named_struct(name,”nA”).name</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">SimplifyCreateMapOps</td>\n<td align=\"left\">映射创建简化</td>\n<td align=\"left\">map(key1-&gt;val2,key2-&gt;val2).key2 =&gt; case when key2</td>\n</tr>\n<tr>\n<td align=\"left\">Constant folding and strength reduction</td>\n<td align=\"left\">CombineConcats</td>\n<td align=\"left\">合并 concat</td>\n<td align=\"left\">将有父子关系的 concat 所有子节点扁平化 合并</td>\n</tr>\n<tr>\n<td align=\"left\">Infer Filters</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">InferFiltersFromConstraints</td>\n<td align=\"left\">在子查询后或者关联后的约束条件中删掉子查询内或者关联子节点内已有的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">Join Reorder</td>\n<td align=\"left\">CostBasedJoinReorder</td>\n<td align=\"left\">？</td>\n<td align=\"left\">？</td>\n</tr>\n<tr>\n<td align=\"left\">Decimal Optimizations</td>\n<td align=\"left\">DecimalAggregates</td>\n<td align=\"left\">加速浮点数运算</td>\n<td align=\"left\">float 和 double 运算中一般需要控制精度(precision) 和小数位(scale)。窗口函数内聚合和普通聚合的 sum/avg 场景下将浮点计算转为长整形计算并在结束时转回来</td>\n</tr>\n<tr>\n<td align=\"left\">Object Expressions Optimization</td>\n<td align=\"left\">EliminateMapObjects</td>\n<td align=\"left\">简化 MapObject 操作</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">Object Expressions Optimization</td>\n<td align=\"left\">CombineTypedFilters</td>\n<td align=\"left\">简化 TypedFilter</td>\n<td align=\"left\">去除具有父子关系的子节点 TypedFilter,合并两者的约束条件</td>\n</tr>\n<tr>\n<td align=\"left\">LocalRelation</td>\n<td align=\"left\">ConvertToLocalRelation</td>\n<td align=\"left\">简化为 LocalRelation</td>\n<td align=\"left\">在 LocalRelation 上取 Limit 时,直接转化为 LocalRelation 在数据上取 Limit 减少输入数据数量</td>\n</tr>\n<tr>\n<td align=\"left\">LocalRelation</td>\n<td align=\"left\">PropagateEmptyRelation</td>\n<td align=\"left\">空 Relation 优化</td>\n<td align=\"left\">对于上述优化规则(e.g. 列裁剪谓词下推)产生的或基础数据为空的 Relation 进行优化. 1,关联时按照左右空 Relation 和Join 类型分别讨论,左空 &amp;&amp; 左外连接=&gt;Empty 2.union all 所有子节点都空直接返回空数据集 3.一元节点的所有子节点都是空 Relation 则直接返回空数据集合, Select/Limit/Repartition 且 children 都为空 Relation=&gt;空数据集合</td>\n</tr>\n<tr>\n<td align=\"left\">Check Cartesian Products</td>\n<td align=\"left\">CheckCartesianProducts</td>\n<td align=\"left\">笛卡尔积检测</td>\n<td align=\"left\">检测逻辑计划树中是否有全外连接</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">RewritePredicateSubquery</td>\n<td align=\"left\">重写谓词子查询</td>\n<td align=\"left\">将 in/exists 转为 semi join,将 not in / not exists 转为 anti join</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">ColumnPruning</td>\n<td align=\"left\">列裁剪</td>\n<td align=\"left\">同 Aggregate规则库中的 ColumnPruning,去掉在查询/聚合/窗口函数/union 等用不到的列读取</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">CollapseProject</td>\n<td align=\"left\">折叠有父子关系的父节点 Projection 操作</td>\n<td align=\"left\">同 Aggregate 规则库的 RewriteSubquery</td>\n</tr>\n<tr>\n<td align=\"left\">RewriteSubquery</td>\n<td align=\"left\">RemoveRedundantProject</td>\n<td align=\"left\">删除无效查询</td>\n<td align=\"left\">同 Aggregate 规则库的RemoveRedundantProject,无效查询是指父查询和子查询目标字段相同</td>\n</tr>\n</tbody></table>"},{"layout":"post","title":"SparkSQL 文件读取及划分分区源码解析","subtitle":"SparkSQL 源码解析","date":"2019-12-12T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n不考虑 SQL 文本在 SparkSQL 中的全生命周期，仅关注 SQL 物理计划读取文件及分区划分的部分逻辑    \n\n```\nlinux> spark-shell --master yarn --queue high\nscala> val sql=\"SELECT count(1) FROM utest.u_tbl_ifs_cost_aggregations\"\nscala> val df = spark.sql(sql)\nscala> df.queryExecution.sparkPlan\nres0: org.apache.spark.sql.execution.SparkPlan =\nHashAggregate(keys=[], functions=[count(1)], output=[count(1)#47L])\n+- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#96L])\n   +- FileScan orc utest.u_tbl_ifs_cost_aggregations[] Batched: true, Format: ORC, Location: InMemoryFileIndex[hdfs://bj2/user/hive/warehouse/utest.db/u_tbl_ifs_cost_aggregations], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n```\n\n即 SparkSQL 中读取表部分实际是委托给 FileSourceScanExec 来处理  \n\n<!-- more -->\n\n## 源码解析  \n\n第一步: 读取逻辑分区(可能包含多个物理文件)做为数据迭代器    \n第二步: 按照是否分桶分别处理(分桶情况下，分区数和桶数量相关)    \n[FileSourceScanExec.scala]  \n\n```\nprivate lazy val inputRDD: RDD[InternalRow] = {\n\t// 第一步\n       // filters 为可下推到物理文件的过滤器(e.g. 分区)\n    val readFile: (PartitionedFile) => Iterator[InternalRow] =\n      relation.fileFormat.buildReaderWithPartitionValues(\n        sparkSession = relation.sparkSession,\n        dataSchema = relation.dataSchema,\n        partitionSchema = relation.partitionSchema,\n        requiredSchema = requiredSchema,\n        filters = pushedDownFilters,\n        options = relation.options,\n        hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))\n        // 第二步\n    relation.bucketSpec match {\n      case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled =>\n        createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)\n      case _ =>\n        createNonBucketedReadRDD(readFile, selectedPartitions, relation)\n    }\n  }\n```\n按照未分桶的情况来看  \n\n```\nprivate def createNonBucketedReadRDD(\n      readFile: (PartitionedFile) => Iterator[InternalRow],\n      selectedPartitions: Seq[PartitionDirectory],\n      fsRelation: HadoopFsRelation): RDD[InternalRow] = {\n      // 单(逻辑)分区最大数据量，默认 128 MB\n    val defaultMaxSplitBytes =\n      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes\n      // 打开文件的代价,默认 4MB。一般理解为单(逻辑)分区最小数据量，物理小文件较多时合并多个物理文件为一个逻辑分区\n      // 避免过多的小任务\n    val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes\n    // 默认并行度,取的是 CoarseGrainedSchedulerBackend#defaultParallelism\n    val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism\n    // 文件夹下所有文件累计大小\n    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum\n    // 每个 core 分均处理数据量  \n    val bytesPerCore = totalBytes / defaultParallelism\n    val files = fsRelation.inputFiles.mkString(\",\")\n    logInfo(s\"${fsRelation.toString}, sizeInBytes: ${fsRelation.location.sizeInBytes}, files: ${files}\")\n    val log = s\"defaultParallelism: ${defaultParallelism}, totalBytes: ${totalBytes}, bytesPerCore: ${bytesPerCore}\"\n    logInfo(log)\n    // 期望分区大小。 公式含义: 在单分区处理数据量不超过 128M 且不低于 4MB的条件下,所有数据负载均衡给各并发处理\n    val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))\n    logInfo(s\"Planning scan with bin packing, max size: $maxSplitBytes bytes, \" +\n      s\"open cost is considered as scanning $openCostInBytes bytes.\")\n\t\n\t// selectedPartitions 是分区下推之后的所有分区目录,非分区表或无需分区下推的情况下,为表在 hdfs 目录\n    val splitFiles = selectedPartitions.flatMap { partition =>\n      partition.files.flatMap { file =>\n        val blockLocations = getBlockLocations(file)\n        // 若文件可分割,则按照\"期望分区大小\"来划分作为 逻辑上的文件\n        // 若文件不可分割,每个物理文件作为一个 逻辑上的文件\n        if (fsRelation.fileFormat.isSplitable(\n            fsRelation.sparkSession, fsRelation.options, file.getPath)) {\n          (0L until file.getLen by maxSplitBytes).map { offset =>\n            val remaining = file.getLen - offset\n            // 文件尾部不足则剩余作为一个逻辑上的文件\n            val size = if (remaining > maxSplitBytes) maxSplitBytes else remaining\n            val hosts = getBlockHosts(blockLocations, offset, size)\n            PartitionedFile(\n              partition.values, file.getPath.toUri.toString, offset, size, hosts)\n          }\n        } else {\n          val hosts = getBlockHosts(blockLocations, 0, file.getLen)\n          Seq(PartitionedFile(\n            partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))\n        }\n      }\n    }.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)\n \t\n    // splitFiles 对大文件切分有效,但是如果小文件较多的话,则会产生与文件数相同的逻辑文件\n    // 需要将逻辑文件合并为逻辑分区,每个逻辑分区对于一个 spark task  \n    \n    val partitions = new ArrayBuffer[FilePartition]\n    val currentFiles = new ArrayBuffer[PartitionedFile]\n    var currentSize = 0L\n   \n    // 截断并新建一个逻辑分区\n   def closePartition(): Unit = {\n      if (currentFiles.nonEmpty) {\n        val newPartition =\n          FilePartition(\n            partitions.size,\n            currentFiles.toArray.toSeq) \n        partitions += newPartition\n      }\n      currentFiles.clear()\n      currentSize = 0\n    }\n    \n    splitFiles.foreach { file =>\n      // 如果已有数据量加上当前逻辑文件的长度超过了 期望分区大小,则不添加当前文件 截断为一个新的逻辑分区\n      if (currentSize + file.length > maxSplitBytes) {\n        closePartition()\n      }\n      \n      currentSize += file.length + openCostInBytes\n      currentFiles += file\n    }\n    // 余量作为一个新的逻辑分区\n    closePartition()\n    \n    // 创建 RDD,分区为所有的逻辑分区,readFile为文件读取逻辑 \n    new FileScanRDD(fsRelation.sparkSession, readFile, partitions)\n  }\n```\n\n[CoarseGrainedSchedulerBackend.scala]\n\n```\noverride def defaultParallelism(): Int = {\n    conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2))\n  }\n```\n\n>\n综上:  \nval maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))    \n即分区大小的逻辑取决于三个值:    \ndefaultMaxSplitBytes: 单分区最大数据量,默认128MB  \nopenCostInBytes: 单分区最小数据量,默认 4MB    \nbytesPerCore: 平均每 core 处理数据量, totalBytes/defaultParallelism    \ndefaultParallelism: 取值为配置项 spark.default.parallelism (默认未配置),然后是 totalCoreCount 与 2 的较大者    \ntotalCoreCount: 随 executor 的注册/注销动态变化(变化量为 spark.executor.cores),开启动态资源管理的情况下难以预估。  \n即在单分区数据量不低于 4MB,不高于 128MB 的情况下,所有数据均衡给所有并发数处理   \n\n## 修改分区数\ndefaultMaxSplitBytes 决定了上限，openCostInBytes决定了下限  \n\n1. 减小分区数  \n提高下限 openCostInBytes 值，并注意上限 defaultMaxSplitBytes即可  \n2. 增加分区数  \n配置合理的 spark.default.parallelism 值, 并注意下限 openCostInBytes 值即可  \n\n注: 高压缩率 ORC 文件 4MB 解压后有 4G 左右的大小，可能需要降低下限    \n\n \n","source":"_posts/2019-12-13-SparkSQL 文件读取分区源码解析.md","raw":"---\nlayout:     post\ntitle:     SparkSQL 文件读取及划分分区源码解析\nsubtitle:   SparkSQL 源码解析\ndate:       2019-12-13\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n    - Spark SQL\n    - 源码解析\n---\n\n\n不考虑 SQL 文本在 SparkSQL 中的全生命周期，仅关注 SQL 物理计划读取文件及分区划分的部分逻辑    \n\n```\nlinux> spark-shell --master yarn --queue high\nscala> val sql=\"SELECT count(1) FROM utest.u_tbl_ifs_cost_aggregations\"\nscala> val df = spark.sql(sql)\nscala> df.queryExecution.sparkPlan\nres0: org.apache.spark.sql.execution.SparkPlan =\nHashAggregate(keys=[], functions=[count(1)], output=[count(1)#47L])\n+- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#96L])\n   +- FileScan orc utest.u_tbl_ifs_cost_aggregations[] Batched: true, Format: ORC, Location: InMemoryFileIndex[hdfs://bj2/user/hive/warehouse/utest.db/u_tbl_ifs_cost_aggregations], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n```\n\n即 SparkSQL 中读取表部分实际是委托给 FileSourceScanExec 来处理  \n\n<!-- more -->\n\n## 源码解析  \n\n第一步: 读取逻辑分区(可能包含多个物理文件)做为数据迭代器    \n第二步: 按照是否分桶分别处理(分桶情况下，分区数和桶数量相关)    \n[FileSourceScanExec.scala]  \n\n```\nprivate lazy val inputRDD: RDD[InternalRow] = {\n\t// 第一步\n       // filters 为可下推到物理文件的过滤器(e.g. 分区)\n    val readFile: (PartitionedFile) => Iterator[InternalRow] =\n      relation.fileFormat.buildReaderWithPartitionValues(\n        sparkSession = relation.sparkSession,\n        dataSchema = relation.dataSchema,\n        partitionSchema = relation.partitionSchema,\n        requiredSchema = requiredSchema,\n        filters = pushedDownFilters,\n        options = relation.options,\n        hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))\n        // 第二步\n    relation.bucketSpec match {\n      case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled =>\n        createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)\n      case _ =>\n        createNonBucketedReadRDD(readFile, selectedPartitions, relation)\n    }\n  }\n```\n按照未分桶的情况来看  \n\n```\nprivate def createNonBucketedReadRDD(\n      readFile: (PartitionedFile) => Iterator[InternalRow],\n      selectedPartitions: Seq[PartitionDirectory],\n      fsRelation: HadoopFsRelation): RDD[InternalRow] = {\n      // 单(逻辑)分区最大数据量，默认 128 MB\n    val defaultMaxSplitBytes =\n      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes\n      // 打开文件的代价,默认 4MB。一般理解为单(逻辑)分区最小数据量，物理小文件较多时合并多个物理文件为一个逻辑分区\n      // 避免过多的小任务\n    val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes\n    // 默认并行度,取的是 CoarseGrainedSchedulerBackend#defaultParallelism\n    val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism\n    // 文件夹下所有文件累计大小\n    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum\n    // 每个 core 分均处理数据量  \n    val bytesPerCore = totalBytes / defaultParallelism\n    val files = fsRelation.inputFiles.mkString(\",\")\n    logInfo(s\"${fsRelation.toString}, sizeInBytes: ${fsRelation.location.sizeInBytes}, files: ${files}\")\n    val log = s\"defaultParallelism: ${defaultParallelism}, totalBytes: ${totalBytes}, bytesPerCore: ${bytesPerCore}\"\n    logInfo(log)\n    // 期望分区大小。 公式含义: 在单分区处理数据量不超过 128M 且不低于 4MB的条件下,所有数据负载均衡给各并发处理\n    val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))\n    logInfo(s\"Planning scan with bin packing, max size: $maxSplitBytes bytes, \" +\n      s\"open cost is considered as scanning $openCostInBytes bytes.\")\n\t\n\t// selectedPartitions 是分区下推之后的所有分区目录,非分区表或无需分区下推的情况下,为表在 hdfs 目录\n    val splitFiles = selectedPartitions.flatMap { partition =>\n      partition.files.flatMap { file =>\n        val blockLocations = getBlockLocations(file)\n        // 若文件可分割,则按照\"期望分区大小\"来划分作为 逻辑上的文件\n        // 若文件不可分割,每个物理文件作为一个 逻辑上的文件\n        if (fsRelation.fileFormat.isSplitable(\n            fsRelation.sparkSession, fsRelation.options, file.getPath)) {\n          (0L until file.getLen by maxSplitBytes).map { offset =>\n            val remaining = file.getLen - offset\n            // 文件尾部不足则剩余作为一个逻辑上的文件\n            val size = if (remaining > maxSplitBytes) maxSplitBytes else remaining\n            val hosts = getBlockHosts(blockLocations, offset, size)\n            PartitionedFile(\n              partition.values, file.getPath.toUri.toString, offset, size, hosts)\n          }\n        } else {\n          val hosts = getBlockHosts(blockLocations, 0, file.getLen)\n          Seq(PartitionedFile(\n            partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))\n        }\n      }\n    }.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)\n \t\n    // splitFiles 对大文件切分有效,但是如果小文件较多的话,则会产生与文件数相同的逻辑文件\n    // 需要将逻辑文件合并为逻辑分区,每个逻辑分区对于一个 spark task  \n    \n    val partitions = new ArrayBuffer[FilePartition]\n    val currentFiles = new ArrayBuffer[PartitionedFile]\n    var currentSize = 0L\n   \n    // 截断并新建一个逻辑分区\n   def closePartition(): Unit = {\n      if (currentFiles.nonEmpty) {\n        val newPartition =\n          FilePartition(\n            partitions.size,\n            currentFiles.toArray.toSeq) \n        partitions += newPartition\n      }\n      currentFiles.clear()\n      currentSize = 0\n    }\n    \n    splitFiles.foreach { file =>\n      // 如果已有数据量加上当前逻辑文件的长度超过了 期望分区大小,则不添加当前文件 截断为一个新的逻辑分区\n      if (currentSize + file.length > maxSplitBytes) {\n        closePartition()\n      }\n      \n      currentSize += file.length + openCostInBytes\n      currentFiles += file\n    }\n    // 余量作为一个新的逻辑分区\n    closePartition()\n    \n    // 创建 RDD,分区为所有的逻辑分区,readFile为文件读取逻辑 \n    new FileScanRDD(fsRelation.sparkSession, readFile, partitions)\n  }\n```\n\n[CoarseGrainedSchedulerBackend.scala]\n\n```\noverride def defaultParallelism(): Int = {\n    conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2))\n  }\n```\n\n>\n综上:  \nval maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))    \n即分区大小的逻辑取决于三个值:    \ndefaultMaxSplitBytes: 单分区最大数据量,默认128MB  \nopenCostInBytes: 单分区最小数据量,默认 4MB    \nbytesPerCore: 平均每 core 处理数据量, totalBytes/defaultParallelism    \ndefaultParallelism: 取值为配置项 spark.default.parallelism (默认未配置),然后是 totalCoreCount 与 2 的较大者    \ntotalCoreCount: 随 executor 的注册/注销动态变化(变化量为 spark.executor.cores),开启动态资源管理的情况下难以预估。  \n即在单分区数据量不低于 4MB,不高于 128MB 的情况下,所有数据均衡给所有并发数处理   \n\n## 修改分区数\ndefaultMaxSplitBytes 决定了上限，openCostInBytes决定了下限  \n\n1. 减小分区数  \n提高下限 openCostInBytes 值，并注意上限 defaultMaxSplitBytes即可  \n2. 增加分区数  \n配置合理的 spark.default.parallelism 值, 并注意下限 openCostInBytes 值即可  \n\n注: 高压缩率 ORC 文件 4MB 解压后有 4G 左右的大小，可能需要降低下限    \n\n \n","slug":"2019-12-13-SparkSQL 文件读取分区源码解析","published":1,"updated":"2020-03-16T10:01:28.111Z","comments":1,"photos":[],"link":"","_id":"cke2rms5i000iy7c79k542l2z","content":"<p>不考虑 SQL 文本在 SparkSQL 中的全生命周期，仅关注 SQL 物理计划读取文件及分区划分的部分逻辑    </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">linux&gt; spark-shell --master yarn --queue high</span><br><span class=\"line\">scala&gt; val sql&#x3D;&quot;SELECT count(1) FROM utest.u_tbl_ifs_cost_aggregations&quot;</span><br><span class=\"line\">scala&gt; val df &#x3D; spark.sql(sql)</span><br><span class=\"line\">scala&gt; df.queryExecution.sparkPlan</span><br><span class=\"line\">res0: org.apache.spark.sql.execution.SparkPlan &#x3D;</span><br><span class=\"line\">HashAggregate(keys&#x3D;[], functions&#x3D;[count(1)], output&#x3D;[count(1)#47L])</span><br><span class=\"line\">+- HashAggregate(keys&#x3D;[], functions&#x3D;[partial_count(1)], output&#x3D;[count#96L])</span><br><span class=\"line\">   +- FileScan orc utest.u_tbl_ifs_cost_aggregations[] Batched: true, Format: ORC, Location: InMemoryFileIndex[hdfs:&#x2F;&#x2F;bj2&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;utest.db&#x2F;u_tbl_ifs_cost_aggregations], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>即 SparkSQL 中读取表部分实际是委托给 FileSourceScanExec 来处理  </p>\n<a id=\"more\"></a>\n\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>第一步: 读取逻辑分区(可能包含多个物理文件)做为数据迭代器<br>第二步: 按照是否分桶分别处理(分桶情况下，分区数和桶数量相关)<br>[FileSourceScanExec.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private lazy val inputRDD: RDD[InternalRow] &#x3D; &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; 第一步</span><br><span class=\"line\">       &#x2F;&#x2F; filters 为可下推到物理文件的过滤器(e.g. 分区)</span><br><span class=\"line\">    val readFile: (PartitionedFile) &#x3D;&gt; Iterator[InternalRow] &#x3D;</span><br><span class=\"line\">      relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class=\"line\">        sparkSession &#x3D; relation.sparkSession,</span><br><span class=\"line\">        dataSchema &#x3D; relation.dataSchema,</span><br><span class=\"line\">        partitionSchema &#x3D; relation.partitionSchema,</span><br><span class=\"line\">        requiredSchema &#x3D; requiredSchema,</span><br><span class=\"line\">        filters &#x3D; pushedDownFilters,</span><br><span class=\"line\">        options &#x3D; relation.options,</span><br><span class=\"line\">        hadoopConf &#x3D; relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class=\"line\">        &#x2F;&#x2F; 第二步</span><br><span class=\"line\">    relation.bucketSpec match &#123;</span><br><span class=\"line\">      case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled &#x3D;&gt;</span><br><span class=\"line\">        createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class=\"line\">      case _ &#x3D;&gt;</span><br><span class=\"line\">        createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>按照未分桶的情况来看  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def createNonBucketedReadRDD(</span><br><span class=\"line\">      readFile: (PartitionedFile) &#x3D;&gt; Iterator[InternalRow],</span><br><span class=\"line\">      selectedPartitions: Seq[PartitionDirectory],</span><br><span class=\"line\">      fsRelation: HadoopFsRelation): RDD[InternalRow] &#x3D; &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 单(逻辑)分区最大数据量，默认 128 MB</span><br><span class=\"line\">    val defaultMaxSplitBytes &#x3D;</span><br><span class=\"line\">      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br><span class=\"line\">      &#x2F;&#x2F; 打开文件的代价,默认 4MB。一般理解为单(逻辑)分区最小数据量，物理小文件较多时合并多个物理文件为一个逻辑分区</span><br><span class=\"line\">      &#x2F;&#x2F; 避免过多的小任务</span><br><span class=\"line\">    val openCostInBytes &#x3D; fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br><span class=\"line\">    &#x2F;&#x2F; 默认并行度,取的是 CoarseGrainedSchedulerBackend#defaultParallelism</span><br><span class=\"line\">    val defaultParallelism &#x3D; fsRelation.sparkSession.sparkContext.defaultParallelism</span><br><span class=\"line\">    &#x2F;&#x2F; 文件夹下所有文件累计大小</span><br><span class=\"line\">    val totalBytes &#x3D; selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br><span class=\"line\">    &#x2F;&#x2F; 每个 core 分均处理数据量  </span><br><span class=\"line\">    val bytesPerCore &#x3D; totalBytes &#x2F; defaultParallelism</span><br><span class=\"line\">    val files &#x3D; fsRelation.inputFiles.mkString(&quot;,&quot;)</span><br><span class=\"line\">    logInfo(s&quot;$&#123;fsRelation.toString&#125;, sizeInBytes: $&#123;fsRelation.location.sizeInBytes&#125;, files: $&#123;files&#125;&quot;)</span><br><span class=\"line\">    val log &#x3D; s&quot;defaultParallelism: $&#123;defaultParallelism&#125;, totalBytes: $&#123;totalBytes&#125;, bytesPerCore: $&#123;bytesPerCore&#125;&quot;</span><br><span class=\"line\">    logInfo(log)</span><br><span class=\"line\">    &#x2F;&#x2F; 期望分区大小。 公式含义: 在单分区处理数据量不超过 128M 且不低于 4MB的条件下,所有数据负载均衡给各并发处理</span><br><span class=\"line\">    val maxSplitBytes &#x3D; Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br><span class=\"line\">    logInfo(s&quot;Planning scan with bin packing, max size: $maxSplitBytes bytes, &quot; +</span><br><span class=\"line\">      s&quot;open cost is considered as scanning $openCostInBytes bytes.&quot;)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&#x2F;&#x2F; selectedPartitions 是分区下推之后的所有分区目录,非分区表或无需分区下推的情况下,为表在 hdfs 目录</span><br><span class=\"line\">    val splitFiles &#x3D; selectedPartitions.flatMap &#123; partition &#x3D;&gt;</span><br><span class=\"line\">      partition.files.flatMap &#123; file &#x3D;&gt;</span><br><span class=\"line\">        val blockLocations &#x3D; getBlockLocations(file)</span><br><span class=\"line\">        &#x2F;&#x2F; 若文件可分割,则按照&quot;期望分区大小&quot;来划分作为 逻辑上的文件</span><br><span class=\"line\">        &#x2F;&#x2F; 若文件不可分割,每个物理文件作为一个 逻辑上的文件</span><br><span class=\"line\">        if (fsRelation.fileFormat.isSplitable(</span><br><span class=\"line\">            fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class=\"line\">          (0L until file.getLen by maxSplitBytes).map &#123; offset &#x3D;&gt;</span><br><span class=\"line\">            val remaining &#x3D; file.getLen - offset</span><br><span class=\"line\">            &#x2F;&#x2F; 文件尾部不足则剩余作为一个逻辑上的文件</span><br><span class=\"line\">            val size &#x3D; if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining</span><br><span class=\"line\">            val hosts &#x3D; getBlockHosts(blockLocations, offset, size)</span><br><span class=\"line\">            PartitionedFile(</span><br><span class=\"line\">              partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          val hosts &#x3D; getBlockHosts(blockLocations, 0, file.getLen)</span><br><span class=\"line\">          Seq(PartitionedFile(</span><br><span class=\"line\">            partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)</span><br><span class=\"line\"> \t</span><br><span class=\"line\">    &#x2F;&#x2F; splitFiles 对大文件切分有效,但是如果小文件较多的话,则会产生与文件数相同的逻辑文件</span><br><span class=\"line\">    &#x2F;&#x2F; 需要将逻辑文件合并为逻辑分区,每个逻辑分区对于一个 spark task  </span><br><span class=\"line\">    </span><br><span class=\"line\">    val partitions &#x3D; new ArrayBuffer[FilePartition]</span><br><span class=\"line\">    val currentFiles &#x3D; new ArrayBuffer[PartitionedFile]</span><br><span class=\"line\">    var currentSize &#x3D; 0L</span><br><span class=\"line\">   </span><br><span class=\"line\">    &#x2F;&#x2F; 截断并新建一个逻辑分区</span><br><span class=\"line\">   def closePartition(): Unit &#x3D; &#123;</span><br><span class=\"line\">      if (currentFiles.nonEmpty) &#123;</span><br><span class=\"line\">        val newPartition &#x3D;</span><br><span class=\"line\">          FilePartition(</span><br><span class=\"line\">            partitions.size,</span><br><span class=\"line\">            currentFiles.toArray.toSeq) </span><br><span class=\"line\">        partitions +&#x3D; newPartition</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      currentFiles.clear()</span><br><span class=\"line\">      currentSize &#x3D; 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    splitFiles.foreach &#123; file &#x3D;&gt;</span><br><span class=\"line\">      &#x2F;&#x2F; 如果已有数据量加上当前逻辑文件的长度超过了 期望分区大小,则不添加当前文件 截断为一个新的逻辑分区</span><br><span class=\"line\">      if (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class=\"line\">        closePartition()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      </span><br><span class=\"line\">      currentSize +&#x3D; file.length + openCostInBytes</span><br><span class=\"line\">      currentFiles +&#x3D; file</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 余量作为一个新的逻辑分区</span><br><span class=\"line\">    closePartition()</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 创建 RDD,分区为所有的逻辑分区,readFile为文件读取逻辑 </span><br><span class=\"line\">    new FileScanRDD(fsRelation.sparkSession, readFile, partitions)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[CoarseGrainedSchedulerBackend.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def defaultParallelism(): Int &#x3D; &#123;</span><br><span class=\"line\">    conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n</blockquote>\n<p>综上:<br>val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))<br>即分区大小的逻辑取决于三个值:<br>defaultMaxSplitBytes: 单分区最大数据量,默认128MB<br>openCostInBytes: 单分区最小数据量,默认 4MB<br>bytesPerCore: 平均每 core 处理数据量, totalBytes/defaultParallelism<br>defaultParallelism: 取值为配置项 spark.default.parallelism (默认未配置),然后是 totalCoreCount 与 2 的较大者<br>totalCoreCount: 随 executor 的注册/注销动态变化(变化量为 spark.executor.cores),开启动态资源管理的情况下难以预估。<br>即在单分区数据量不低于 4MB,不高于 128MB 的情况下,所有数据均衡给所有并发数处理   </p>\n<h2 id=\"修改分区数\"><a href=\"#修改分区数\" class=\"headerlink\" title=\"修改分区数\"></a>修改分区数</h2><p>defaultMaxSplitBytes 决定了上限，openCostInBytes决定了下限  </p>\n<ol>\n<li>减小分区数<br>提高下限 openCostInBytes 值，并注意上限 defaultMaxSplitBytes即可  </li>\n<li>增加分区数<br>配置合理的 spark.default.parallelism 值, 并注意下限 openCostInBytes 值即可  </li>\n</ol>\n<p>注: 高压缩率 ORC 文件 4MB 解压后有 4G 左右的大小，可能需要降低下限    </p>\n","site":{"data":{}},"excerpt":"<p>不考虑 SQL 文本在 SparkSQL 中的全生命周期，仅关注 SQL 物理计划读取文件及分区划分的部分逻辑    </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">linux&gt; spark-shell --master yarn --queue high</span><br><span class=\"line\">scala&gt; val sql&#x3D;&quot;SELECT count(1) FROM utest.u_tbl_ifs_cost_aggregations&quot;</span><br><span class=\"line\">scala&gt; val df &#x3D; spark.sql(sql)</span><br><span class=\"line\">scala&gt; df.queryExecution.sparkPlan</span><br><span class=\"line\">res0: org.apache.spark.sql.execution.SparkPlan &#x3D;</span><br><span class=\"line\">HashAggregate(keys&#x3D;[], functions&#x3D;[count(1)], output&#x3D;[count(1)#47L])</span><br><span class=\"line\">+- HashAggregate(keys&#x3D;[], functions&#x3D;[partial_count(1)], output&#x3D;[count#96L])</span><br><span class=\"line\">   +- FileScan orc utest.u_tbl_ifs_cost_aggregations[] Batched: true, Format: ORC, Location: InMemoryFileIndex[hdfs:&#x2F;&#x2F;bj2&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;utest.db&#x2F;u_tbl_ifs_cost_aggregations], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>即 SparkSQL 中读取表部分实际是委托给 FileSourceScanExec 来处理  </p>","more":"<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>第一步: 读取逻辑分区(可能包含多个物理文件)做为数据迭代器<br>第二步: 按照是否分桶分别处理(分桶情况下，分区数和桶数量相关)<br>[FileSourceScanExec.scala]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private lazy val inputRDD: RDD[InternalRow] &#x3D; &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; 第一步</span><br><span class=\"line\">       &#x2F;&#x2F; filters 为可下推到物理文件的过滤器(e.g. 分区)</span><br><span class=\"line\">    val readFile: (PartitionedFile) &#x3D;&gt; Iterator[InternalRow] &#x3D;</span><br><span class=\"line\">      relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class=\"line\">        sparkSession &#x3D; relation.sparkSession,</span><br><span class=\"line\">        dataSchema &#x3D; relation.dataSchema,</span><br><span class=\"line\">        partitionSchema &#x3D; relation.partitionSchema,</span><br><span class=\"line\">        requiredSchema &#x3D; requiredSchema,</span><br><span class=\"line\">        filters &#x3D; pushedDownFilters,</span><br><span class=\"line\">        options &#x3D; relation.options,</span><br><span class=\"line\">        hadoopConf &#x3D; relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class=\"line\">        &#x2F;&#x2F; 第二步</span><br><span class=\"line\">    relation.bucketSpec match &#123;</span><br><span class=\"line\">      case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled &#x3D;&gt;</span><br><span class=\"line\">        createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class=\"line\">      case _ &#x3D;&gt;</span><br><span class=\"line\">        createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>按照未分桶的情况来看  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private def createNonBucketedReadRDD(</span><br><span class=\"line\">      readFile: (PartitionedFile) &#x3D;&gt; Iterator[InternalRow],</span><br><span class=\"line\">      selectedPartitions: Seq[PartitionDirectory],</span><br><span class=\"line\">      fsRelation: HadoopFsRelation): RDD[InternalRow] &#x3D; &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 单(逻辑)分区最大数据量，默认 128 MB</span><br><span class=\"line\">    val defaultMaxSplitBytes &#x3D;</span><br><span class=\"line\">      fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes</span><br><span class=\"line\">      &#x2F;&#x2F; 打开文件的代价,默认 4MB。一般理解为单(逻辑)分区最小数据量，物理小文件较多时合并多个物理文件为一个逻辑分区</span><br><span class=\"line\">      &#x2F;&#x2F; 避免过多的小任务</span><br><span class=\"line\">    val openCostInBytes &#x3D; fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes</span><br><span class=\"line\">    &#x2F;&#x2F; 默认并行度,取的是 CoarseGrainedSchedulerBackend#defaultParallelism</span><br><span class=\"line\">    val defaultParallelism &#x3D; fsRelation.sparkSession.sparkContext.defaultParallelism</span><br><span class=\"line\">    &#x2F;&#x2F; 文件夹下所有文件累计大小</span><br><span class=\"line\">    val totalBytes &#x3D; selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum</span><br><span class=\"line\">    &#x2F;&#x2F; 每个 core 分均处理数据量  </span><br><span class=\"line\">    val bytesPerCore &#x3D; totalBytes &#x2F; defaultParallelism</span><br><span class=\"line\">    val files &#x3D; fsRelation.inputFiles.mkString(&quot;,&quot;)</span><br><span class=\"line\">    logInfo(s&quot;$&#123;fsRelation.toString&#125;, sizeInBytes: $&#123;fsRelation.location.sizeInBytes&#125;, files: $&#123;files&#125;&quot;)</span><br><span class=\"line\">    val log &#x3D; s&quot;defaultParallelism: $&#123;defaultParallelism&#125;, totalBytes: $&#123;totalBytes&#125;, bytesPerCore: $&#123;bytesPerCore&#125;&quot;</span><br><span class=\"line\">    logInfo(log)</span><br><span class=\"line\">    &#x2F;&#x2F; 期望分区大小。 公式含义: 在单分区处理数据量不超过 128M 且不低于 4MB的条件下,所有数据负载均衡给各并发处理</span><br><span class=\"line\">    val maxSplitBytes &#x3D; Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br><span class=\"line\">    logInfo(s&quot;Planning scan with bin packing, max size: $maxSplitBytes bytes, &quot; +</span><br><span class=\"line\">      s&quot;open cost is considered as scanning $openCostInBytes bytes.&quot;)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t&#x2F;&#x2F; selectedPartitions 是分区下推之后的所有分区目录,非分区表或无需分区下推的情况下,为表在 hdfs 目录</span><br><span class=\"line\">    val splitFiles &#x3D; selectedPartitions.flatMap &#123; partition &#x3D;&gt;</span><br><span class=\"line\">      partition.files.flatMap &#123; file &#x3D;&gt;</span><br><span class=\"line\">        val blockLocations &#x3D; getBlockLocations(file)</span><br><span class=\"line\">        &#x2F;&#x2F; 若文件可分割,则按照&quot;期望分区大小&quot;来划分作为 逻辑上的文件</span><br><span class=\"line\">        &#x2F;&#x2F; 若文件不可分割,每个物理文件作为一个 逻辑上的文件</span><br><span class=\"line\">        if (fsRelation.fileFormat.isSplitable(</span><br><span class=\"line\">            fsRelation.sparkSession, fsRelation.options, file.getPath)) &#123;</span><br><span class=\"line\">          (0L until file.getLen by maxSplitBytes).map &#123; offset &#x3D;&gt;</span><br><span class=\"line\">            val remaining &#x3D; file.getLen - offset</span><br><span class=\"line\">            &#x2F;&#x2F; 文件尾部不足则剩余作为一个逻辑上的文件</span><br><span class=\"line\">            val size &#x3D; if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining</span><br><span class=\"line\">            val hosts &#x3D; getBlockHosts(blockLocations, offset, size)</span><br><span class=\"line\">            PartitionedFile(</span><br><span class=\"line\">              partition.values, file.getPath.toUri.toString, offset, size, hosts)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          val hosts &#x3D; getBlockHosts(blockLocations, 0, file.getLen)</span><br><span class=\"line\">          Seq(PartitionedFile(</span><br><span class=\"line\">            partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse)</span><br><span class=\"line\"> \t</span><br><span class=\"line\">    &#x2F;&#x2F; splitFiles 对大文件切分有效,但是如果小文件较多的话,则会产生与文件数相同的逻辑文件</span><br><span class=\"line\">    &#x2F;&#x2F; 需要将逻辑文件合并为逻辑分区,每个逻辑分区对于一个 spark task  </span><br><span class=\"line\">    </span><br><span class=\"line\">    val partitions &#x3D; new ArrayBuffer[FilePartition]</span><br><span class=\"line\">    val currentFiles &#x3D; new ArrayBuffer[PartitionedFile]</span><br><span class=\"line\">    var currentSize &#x3D; 0L</span><br><span class=\"line\">   </span><br><span class=\"line\">    &#x2F;&#x2F; 截断并新建一个逻辑分区</span><br><span class=\"line\">   def closePartition(): Unit &#x3D; &#123;</span><br><span class=\"line\">      if (currentFiles.nonEmpty) &#123;</span><br><span class=\"line\">        val newPartition &#x3D;</span><br><span class=\"line\">          FilePartition(</span><br><span class=\"line\">            partitions.size,</span><br><span class=\"line\">            currentFiles.toArray.toSeq) </span><br><span class=\"line\">        partitions +&#x3D; newPartition</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      currentFiles.clear()</span><br><span class=\"line\">      currentSize &#x3D; 0</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    splitFiles.foreach &#123; file &#x3D;&gt;</span><br><span class=\"line\">      &#x2F;&#x2F; 如果已有数据量加上当前逻辑文件的长度超过了 期望分区大小,则不添加当前文件 截断为一个新的逻辑分区</span><br><span class=\"line\">      if (currentSize + file.length &gt; maxSplitBytes) &#123;</span><br><span class=\"line\">        closePartition()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      </span><br><span class=\"line\">      currentSize +&#x3D; file.length + openCostInBytes</span><br><span class=\"line\">      currentFiles +&#x3D; file</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 余量作为一个新的逻辑分区</span><br><span class=\"line\">    closePartition()</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 创建 RDD,分区为所有的逻辑分区,readFile为文件读取逻辑 </span><br><span class=\"line\">    new FileScanRDD(fsRelation.sparkSession, readFile, partitions)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[CoarseGrainedSchedulerBackend.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">override def defaultParallelism(): Int &#x3D; &#123;</span><br><span class=\"line\">    conf.getInt(&quot;spark.default.parallelism&quot;, math.max(totalCoreCount.get(), 2))</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n</blockquote>\n<p>综上:<br>val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))<br>即分区大小的逻辑取决于三个值:<br>defaultMaxSplitBytes: 单分区最大数据量,默认128MB<br>openCostInBytes: 单分区最小数据量,默认 4MB<br>bytesPerCore: 平均每 core 处理数据量, totalBytes/defaultParallelism<br>defaultParallelism: 取值为配置项 spark.default.parallelism (默认未配置),然后是 totalCoreCount 与 2 的较大者<br>totalCoreCount: 随 executor 的注册/注销动态变化(变化量为 spark.executor.cores),开启动态资源管理的情况下难以预估。<br>即在单分区数据量不低于 4MB,不高于 128MB 的情况下,所有数据均衡给所有并发数处理   </p>\n<h2 id=\"修改分区数\"><a href=\"#修改分区数\" class=\"headerlink\" title=\"修改分区数\"></a>修改分区数</h2><p>defaultMaxSplitBytes 决定了上限，openCostInBytes决定了下限  </p>\n<ol>\n<li>减小分区数<br>提高下限 openCostInBytes 值，并注意上限 defaultMaxSplitBytes即可  </li>\n<li>增加分区数<br>配置合理的 spark.default.parallelism 值, 并注意下限 openCostInBytes 值即可  </li>\n</ol>\n<p>注: 高压缩率 ORC 文件 4MB 解压后有 4G 左右的大小，可能需要降低下限    </p>"},{"layout":"post","title":"Apache Spark 3.0 Preview 新特性和改动","subtitle":"Spark 3.0 预览版","date":"2019-12-26T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 概述  \n\nApache Spark 3.0 Preview(预览版)发布于 2019-11-23。就 API 和功能而言预览版不是稳定版本，本文将列出预览版的重要新特性和改动。  \n\n## 新特性   \n\n### Adaptive Execution Of SparkSQL(适应性执行,AE)  \n\n适应性执行使用运行时的统计数据进行动态优化。通过使物理计划分阶段(不同于DAG的stage)并在运行期可修改，主要的优化点有:  \n\n1. BroadcastHashJoin(BHJ): 基础表的大小/行数来自于表 Statistics，中间操作产生的数据集大小/行数靠估计，基础表统计数据可能是错误的，中间数据集统计数据可能失真(e.g. 数据倾斜)。导致运行前制定的物理计划不适合运行时，例如运行前估计的中间数据集大小为 1GB 而选择 SortMergeJoin 但运行时发现中间数据集大小为 10MB 可以修改 join 策略为 BHJ。  \n2. 分区数确定问题: 当分区输入数据量很小时(数据空洞,数据倾斜)，可以使一个task 处理多个连续分区的数据量(总量不超过指定值, e.g. 64MB)。      \n3. 数据倾斜问题: 当分区输入数据量大于所有分区输入量中位数的指定倍数时，认为该分区数据倾斜，对此分区使用多个线程并行处理。    \n\n<!-- more -->\n注:  \n1. [SPARK-9850](https://issues.apache.org/jira/browse/SPARK-9850)  \n2. [Intel Spark AE](https://github.com/Intel-bigdata/spark-adaptive.git)  \n\n### Dynamic Partition Pruning(动态分区裁剪,DPP)  \n\nSpark3.0引入了动态分区裁剪特性，这是 SQL 分析工作的一项重大性能改进，可以与 BI 工具集成得更好。动态分区裁剪的原理是将应用在维度表上的过滤器集合直接应用到事实表上，因此可以跳过扫描非必需的分区，减少事实表扫描的数据量。在逻辑计划和物理计划上均可以实施 DPP 策略，DPP 加速了大部分 TCPDS 查询并且在无需非规范化表的情况下很好的兼容星型模型。  \n\n注:   \n1.  非规范化是一种在规范化数据库之前应用以提升性能的策略。原理是通过数据冗余减少数据关联 牺牲数据的写性能以提升读性能。  \n2.  [Dynamic Partition Prunning](https://databricks.com/session_eu19/dynamic-partition-pruning-in-apache-spark)    \n\n### Enhanced Support for Deep Learning\n\n加强了对深度学习的支持。在 Spark 3.0之前，Spark MLlib 并不专注于深度学习 没有提供图像/NLP 相关的深度学习算法。yahoo 的 TensorFlowOnSpark 等工程提供了在 Spark 上深度学习的可能，但是有很大的问题: 对 Spark 弹性计算兼容不足，单分区训练失败将在所有分区上再训练。Spark 3.0解决了这个问题，并兼容了Nvidia、AMD、Intel 等多类型 GPU    \n\n### DataSourceV2 \nDataSourceV1 对数据源的处理有一些劣势: 过于依赖 SQLContext/DataFrame,导致底层 API 依赖高层 API；不支持列式存储的批处理接口；不支持 SQLOnStreaming 等  \nSpark 3.0对 DataSourceV2的优化:  \n\n1. 改进谓词下推机制，可通过减少数据加载来加速查询  \n2. 执行可插拔的 Catalog 插件\n3. 支持列式存储批处理接口以提高性能  \n4. 支持读二进制文件 Binary File DataSource,但不支持写二进制 DataFrame 到外部数据源      \n\n注:  \n[SPARK-15689](https://issues.apache.org/jira/browse/SPARK-15689)  \n\n### Spark Delta Lake\n\nDelta Lake 是一个引入了 ACID事务的开源存储层，保证了数据湖的可靠性、性能和生命周期管理。Delta Lake 可以插件式得实现或升级现有程序来事务性删除/修改大数据中的小数据。对流处理(e.g. 流式数据仓库)、数仓中拉链表等场景提供了新的方案。    \n\n注:  \n[Delta Lake](https://docs.databricks.com/delta/quick-start.html)\n\n### Graph Feature(存疑)\n\nSpark 3.0加入了SparkGraph 模块用于进行图处理，可以使用 Neo4J开发的 Cypher 语言操作图模型及图算法。Neo4J 图数据库是单节点的，不适用于大数据量，SparkGraph 如何解决这个问题的?(未知)。    \n\n### Yarn  \n\nSpark 3.0 可以在 Yarn 集群上自动发现GPU，并可以在 GPU 节点上调度任务(task)。   \n\n### Kubernetes  \n\nSpark 2 时代对Kubernetes 的支持相对不太成熟，在生产环境中相比 Yarn 来说更难使用且性能较差。Spark3.0 引入了新的 Shuffle Service On Kubernetes 且支持 GPU 的 Pod 级隔离。  \n\n## 改动  \n\n### 语言支持  \n\n预览版及其后版本支持 Python3、Scala 2.12和 JDK11。Python2 将被弃用。  \n\n### hadoop支持\n\n支持 Hadoop 3，预览版默认为 Hadoop2.7.4。  \n\n### deprecated  \n删除了不高于 Spark2.2.0 的 deprecated API、参数和功能等。\n\n## issues附录  \n[issues](https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-printable/temp/SearchRequest.html?jqlQuery=statusCategory+%3D+done+AND+project+%3D+12315420+AND+fixVersion+%3D+12339177+ORDER+BY+priority+DESC%2C+key+ASC&tempMax=1000)\n","source":"_posts/2019-12-27-Apache Spark 3.0 Preview 新特性和改动.md","raw":"---\nlayout:     post\ntitle:     Apache Spark 3.0 Preview 新特性和改动 \nsubtitle:   Spark 3.0 预览版\ndate:       2019-12-27\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 3.0 \n---\n\n\n## 概述  \n\nApache Spark 3.0 Preview(预览版)发布于 2019-11-23。就 API 和功能而言预览版不是稳定版本，本文将列出预览版的重要新特性和改动。  \n\n## 新特性   \n\n### Adaptive Execution Of SparkSQL(适应性执行,AE)  \n\n适应性执行使用运行时的统计数据进行动态优化。通过使物理计划分阶段(不同于DAG的stage)并在运行期可修改，主要的优化点有:  \n\n1. BroadcastHashJoin(BHJ): 基础表的大小/行数来自于表 Statistics，中间操作产生的数据集大小/行数靠估计，基础表统计数据可能是错误的，中间数据集统计数据可能失真(e.g. 数据倾斜)。导致运行前制定的物理计划不适合运行时，例如运行前估计的中间数据集大小为 1GB 而选择 SortMergeJoin 但运行时发现中间数据集大小为 10MB 可以修改 join 策略为 BHJ。  \n2. 分区数确定问题: 当分区输入数据量很小时(数据空洞,数据倾斜)，可以使一个task 处理多个连续分区的数据量(总量不超过指定值, e.g. 64MB)。      \n3. 数据倾斜问题: 当分区输入数据量大于所有分区输入量中位数的指定倍数时，认为该分区数据倾斜，对此分区使用多个线程并行处理。    \n\n<!-- more -->\n注:  \n1. [SPARK-9850](https://issues.apache.org/jira/browse/SPARK-9850)  \n2. [Intel Spark AE](https://github.com/Intel-bigdata/spark-adaptive.git)  \n\n### Dynamic Partition Pruning(动态分区裁剪,DPP)  \n\nSpark3.0引入了动态分区裁剪特性，这是 SQL 分析工作的一项重大性能改进，可以与 BI 工具集成得更好。动态分区裁剪的原理是将应用在维度表上的过滤器集合直接应用到事实表上，因此可以跳过扫描非必需的分区，减少事实表扫描的数据量。在逻辑计划和物理计划上均可以实施 DPP 策略，DPP 加速了大部分 TCPDS 查询并且在无需非规范化表的情况下很好的兼容星型模型。  \n\n注:   \n1.  非规范化是一种在规范化数据库之前应用以提升性能的策略。原理是通过数据冗余减少数据关联 牺牲数据的写性能以提升读性能。  \n2.  [Dynamic Partition Prunning](https://databricks.com/session_eu19/dynamic-partition-pruning-in-apache-spark)    \n\n### Enhanced Support for Deep Learning\n\n加强了对深度学习的支持。在 Spark 3.0之前，Spark MLlib 并不专注于深度学习 没有提供图像/NLP 相关的深度学习算法。yahoo 的 TensorFlowOnSpark 等工程提供了在 Spark 上深度学习的可能，但是有很大的问题: 对 Spark 弹性计算兼容不足，单分区训练失败将在所有分区上再训练。Spark 3.0解决了这个问题，并兼容了Nvidia、AMD、Intel 等多类型 GPU    \n\n### DataSourceV2 \nDataSourceV1 对数据源的处理有一些劣势: 过于依赖 SQLContext/DataFrame,导致底层 API 依赖高层 API；不支持列式存储的批处理接口；不支持 SQLOnStreaming 等  \nSpark 3.0对 DataSourceV2的优化:  \n\n1. 改进谓词下推机制，可通过减少数据加载来加速查询  \n2. 执行可插拔的 Catalog 插件\n3. 支持列式存储批处理接口以提高性能  \n4. 支持读二进制文件 Binary File DataSource,但不支持写二进制 DataFrame 到外部数据源      \n\n注:  \n[SPARK-15689](https://issues.apache.org/jira/browse/SPARK-15689)  \n\n### Spark Delta Lake\n\nDelta Lake 是一个引入了 ACID事务的开源存储层，保证了数据湖的可靠性、性能和生命周期管理。Delta Lake 可以插件式得实现或升级现有程序来事务性删除/修改大数据中的小数据。对流处理(e.g. 流式数据仓库)、数仓中拉链表等场景提供了新的方案。    \n\n注:  \n[Delta Lake](https://docs.databricks.com/delta/quick-start.html)\n\n### Graph Feature(存疑)\n\nSpark 3.0加入了SparkGraph 模块用于进行图处理，可以使用 Neo4J开发的 Cypher 语言操作图模型及图算法。Neo4J 图数据库是单节点的，不适用于大数据量，SparkGraph 如何解决这个问题的?(未知)。    \n\n### Yarn  \n\nSpark 3.0 可以在 Yarn 集群上自动发现GPU，并可以在 GPU 节点上调度任务(task)。   \n\n### Kubernetes  \n\nSpark 2 时代对Kubernetes 的支持相对不太成熟，在生产环境中相比 Yarn 来说更难使用且性能较差。Spark3.0 引入了新的 Shuffle Service On Kubernetes 且支持 GPU 的 Pod 级隔离。  \n\n## 改动  \n\n### 语言支持  \n\n预览版及其后版本支持 Python3、Scala 2.12和 JDK11。Python2 将被弃用。  \n\n### hadoop支持\n\n支持 Hadoop 3，预览版默认为 Hadoop2.7.4。  \n\n### deprecated  \n删除了不高于 Spark2.2.0 的 deprecated API、参数和功能等。\n\n## issues附录  \n[issues](https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-printable/temp/SearchRequest.html?jqlQuery=statusCategory+%3D+done+AND+project+%3D+12315420+AND+fixVersion+%3D+12339177+ORDER+BY+priority+DESC%2C+key+ASC&tempMax=1000)\n","slug":"2019-12-27-Apache Spark 3.0 Preview 新特性和改动","published":1,"updated":"2020-03-16T10:01:28.111Z","comments":1,"photos":[],"link":"","_id":"cke2rms5k000jy7c7gba1ac0u","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Apache Spark 3.0 Preview(预览版)发布于 2019-11-23。就 API 和功能而言预览版不是稳定版本，本文将列出预览版的重要新特性和改动。  </p>\n<h2 id=\"新特性\"><a href=\"#新特性\" class=\"headerlink\" title=\"新特性\"></a>新特性</h2><h3 id=\"Adaptive-Execution-Of-SparkSQL-适应性执行-AE\"><a href=\"#Adaptive-Execution-Of-SparkSQL-适应性执行-AE\" class=\"headerlink\" title=\"Adaptive Execution Of SparkSQL(适应性执行,AE)\"></a>Adaptive Execution Of SparkSQL(适应性执行,AE)</h3><p>适应性执行使用运行时的统计数据进行动态优化。通过使物理计划分阶段(不同于DAG的stage)并在运行期可修改，主要的优化点有:  </p>\n<ol>\n<li>BroadcastHashJoin(BHJ): 基础表的大小/行数来自于表 Statistics，中间操作产生的数据集大小/行数靠估计，基础表统计数据可能是错误的，中间数据集统计数据可能失真(e.g. 数据倾斜)。导致运行前制定的物理计划不适合运行时，例如运行前估计的中间数据集大小为 1GB 而选择 SortMergeJoin 但运行时发现中间数据集大小为 10MB 可以修改 join 策略为 BHJ。  </li>\n<li>分区数确定问题: 当分区输入数据量很小时(数据空洞,数据倾斜)，可以使一个task 处理多个连续分区的数据量(总量不超过指定值, e.g. 64MB)。      </li>\n<li>数据倾斜问题: 当分区输入数据量大于所有分区输入量中位数的指定倍数时，认为该分区数据倾斜，对此分区使用多个线程并行处理。    </li>\n</ol>\n<a id=\"more\"></a>\n<p>注:  </p>\n<ol>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9850\" target=\"_blank\" rel=\"noopener\">SPARK-9850</a>  </li>\n<li><a href=\"https://github.com/Intel-bigdata/spark-adaptive.git\" target=\"_blank\" rel=\"noopener\">Intel Spark AE</a>  </li>\n</ol>\n<h3 id=\"Dynamic-Partition-Pruning-动态分区裁剪-DPP\"><a href=\"#Dynamic-Partition-Pruning-动态分区裁剪-DPP\" class=\"headerlink\" title=\"Dynamic Partition Pruning(动态分区裁剪,DPP)\"></a>Dynamic Partition Pruning(动态分区裁剪,DPP)</h3><p>Spark3.0引入了动态分区裁剪特性，这是 SQL 分析工作的一项重大性能改进，可以与 BI 工具集成得更好。动态分区裁剪的原理是将应用在维度表上的过滤器集合直接应用到事实表上，因此可以跳过扫描非必需的分区，减少事实表扫描的数据量。在逻辑计划和物理计划上均可以实施 DPP 策略，DPP 加速了大部分 TCPDS 查询并且在无需非规范化表的情况下很好的兼容星型模型。  </p>\n<p>注:   </p>\n<ol>\n<li>非规范化是一种在规范化数据库之前应用以提升性能的策略。原理是通过数据冗余减少数据关联 牺牲数据的写性能以提升读性能。  </li>\n<li><a href=\"https://databricks.com/session_eu19/dynamic-partition-pruning-in-apache-spark\" target=\"_blank\" rel=\"noopener\">Dynamic Partition Prunning</a>    </li>\n</ol>\n<h3 id=\"Enhanced-Support-for-Deep-Learning\"><a href=\"#Enhanced-Support-for-Deep-Learning\" class=\"headerlink\" title=\"Enhanced Support for Deep Learning\"></a>Enhanced Support for Deep Learning</h3><p>加强了对深度学习的支持。在 Spark 3.0之前，Spark MLlib 并不专注于深度学习 没有提供图像/NLP 相关的深度学习算法。yahoo 的 TensorFlowOnSpark 等工程提供了在 Spark 上深度学习的可能，但是有很大的问题: 对 Spark 弹性计算兼容不足，单分区训练失败将在所有分区上再训练。Spark 3.0解决了这个问题，并兼容了Nvidia、AMD、Intel 等多类型 GPU    </p>\n<h3 id=\"DataSourceV2\"><a href=\"#DataSourceV2\" class=\"headerlink\" title=\"DataSourceV2\"></a>DataSourceV2</h3><p>DataSourceV1 对数据源的处理有一些劣势: 过于依赖 SQLContext/DataFrame,导致底层 API 依赖高层 API；不支持列式存储的批处理接口；不支持 SQLOnStreaming 等<br>Spark 3.0对 DataSourceV2的优化:  </p>\n<ol>\n<li>改进谓词下推机制，可通过减少数据加载来加速查询  </li>\n<li>执行可插拔的 Catalog 插件</li>\n<li>支持列式存储批处理接口以提高性能  </li>\n<li>支持读二进制文件 Binary File DataSource,但不支持写二进制 DataFrame 到外部数据源      </li>\n</ol>\n<p>注:<br><a href=\"https://issues.apache.org/jira/browse/SPARK-15689\" target=\"_blank\" rel=\"noopener\">SPARK-15689</a>  </p>\n<h3 id=\"Spark-Delta-Lake\"><a href=\"#Spark-Delta-Lake\" class=\"headerlink\" title=\"Spark Delta Lake\"></a>Spark Delta Lake</h3><p>Delta Lake 是一个引入了 ACID事务的开源存储层，保证了数据湖的可靠性、性能和生命周期管理。Delta Lake 可以插件式得实现或升级现有程序来事务性删除/修改大数据中的小数据。对流处理(e.g. 流式数据仓库)、数仓中拉链表等场景提供了新的方案。    </p>\n<p>注:<br><a href=\"https://docs.databricks.com/delta/quick-start.html\" target=\"_blank\" rel=\"noopener\">Delta Lake</a></p>\n<h3 id=\"Graph-Feature-存疑\"><a href=\"#Graph-Feature-存疑\" class=\"headerlink\" title=\"Graph Feature(存疑)\"></a>Graph Feature(存疑)</h3><p>Spark 3.0加入了SparkGraph 模块用于进行图处理，可以使用 Neo4J开发的 Cypher 语言操作图模型及图算法。Neo4J 图数据库是单节点的，不适用于大数据量，SparkGraph 如何解决这个问题的?(未知)。    </p>\n<h3 id=\"Yarn\"><a href=\"#Yarn\" class=\"headerlink\" title=\"Yarn\"></a>Yarn</h3><p>Spark 3.0 可以在 Yarn 集群上自动发现GPU，并可以在 GPU 节点上调度任务(task)。   </p>\n<h3 id=\"Kubernetes\"><a href=\"#Kubernetes\" class=\"headerlink\" title=\"Kubernetes\"></a>Kubernetes</h3><p>Spark 2 时代对Kubernetes 的支持相对不太成熟，在生产环境中相比 Yarn 来说更难使用且性能较差。Spark3.0 引入了新的 Shuffle Service On Kubernetes 且支持 GPU 的 Pod 级隔离。  </p>\n<h2 id=\"改动\"><a href=\"#改动\" class=\"headerlink\" title=\"改动\"></a>改动</h2><h3 id=\"语言支持\"><a href=\"#语言支持\" class=\"headerlink\" title=\"语言支持\"></a>语言支持</h3><p>预览版及其后版本支持 Python3、Scala 2.12和 JDK11。Python2 将被弃用。  </p>\n<h3 id=\"hadoop支持\"><a href=\"#hadoop支持\" class=\"headerlink\" title=\"hadoop支持\"></a>hadoop支持</h3><p>支持 Hadoop 3，预览版默认为 Hadoop2.7.4。  </p>\n<h3 id=\"deprecated\"><a href=\"#deprecated\" class=\"headerlink\" title=\"deprecated\"></a>deprecated</h3><p>删除了不高于 Spark2.2.0 的 deprecated API、参数和功能等。</p>\n<h2 id=\"issues附录\"><a href=\"#issues附录\" class=\"headerlink\" title=\"issues附录\"></a>issues附录</h2><p><a href=\"https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-printable/temp/SearchRequest.html?jqlQuery=statusCategory+%3D+done+AND+project+%3D+12315420+AND+fixVersion+%3D+12339177+ORDER+BY+priority+DESC%2C+key+ASC&tempMax=1000\" target=\"_blank\" rel=\"noopener\">issues</a></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Apache Spark 3.0 Preview(预览版)发布于 2019-11-23。就 API 和功能而言预览版不是稳定版本，本文将列出预览版的重要新特性和改动。  </p>\n<h2 id=\"新特性\"><a href=\"#新特性\" class=\"headerlink\" title=\"新特性\"></a>新特性</h2><h3 id=\"Adaptive-Execution-Of-SparkSQL-适应性执行-AE\"><a href=\"#Adaptive-Execution-Of-SparkSQL-适应性执行-AE\" class=\"headerlink\" title=\"Adaptive Execution Of SparkSQL(适应性执行,AE)\"></a>Adaptive Execution Of SparkSQL(适应性执行,AE)</h3><p>适应性执行使用运行时的统计数据进行动态优化。通过使物理计划分阶段(不同于DAG的stage)并在运行期可修改，主要的优化点有:  </p>\n<ol>\n<li>BroadcastHashJoin(BHJ): 基础表的大小/行数来自于表 Statistics，中间操作产生的数据集大小/行数靠估计，基础表统计数据可能是错误的，中间数据集统计数据可能失真(e.g. 数据倾斜)。导致运行前制定的物理计划不适合运行时，例如运行前估计的中间数据集大小为 1GB 而选择 SortMergeJoin 但运行时发现中间数据集大小为 10MB 可以修改 join 策略为 BHJ。  </li>\n<li>分区数确定问题: 当分区输入数据量很小时(数据空洞,数据倾斜)，可以使一个task 处理多个连续分区的数据量(总量不超过指定值, e.g. 64MB)。      </li>\n<li>数据倾斜问题: 当分区输入数据量大于所有分区输入量中位数的指定倍数时，认为该分区数据倾斜，对此分区使用多个线程并行处理。    </li>\n</ol>","more":"<p>注:  </p>\n<ol>\n<li><a href=\"https://issues.apache.org/jira/browse/SPARK-9850\" target=\"_blank\" rel=\"noopener\">SPARK-9850</a>  </li>\n<li><a href=\"https://github.com/Intel-bigdata/spark-adaptive.git\" target=\"_blank\" rel=\"noopener\">Intel Spark AE</a>  </li>\n</ol>\n<h3 id=\"Dynamic-Partition-Pruning-动态分区裁剪-DPP\"><a href=\"#Dynamic-Partition-Pruning-动态分区裁剪-DPP\" class=\"headerlink\" title=\"Dynamic Partition Pruning(动态分区裁剪,DPP)\"></a>Dynamic Partition Pruning(动态分区裁剪,DPP)</h3><p>Spark3.0引入了动态分区裁剪特性，这是 SQL 分析工作的一项重大性能改进，可以与 BI 工具集成得更好。动态分区裁剪的原理是将应用在维度表上的过滤器集合直接应用到事实表上，因此可以跳过扫描非必需的分区，减少事实表扫描的数据量。在逻辑计划和物理计划上均可以实施 DPP 策略，DPP 加速了大部分 TCPDS 查询并且在无需非规范化表的情况下很好的兼容星型模型。  </p>\n<p>注:   </p>\n<ol>\n<li>非规范化是一种在规范化数据库之前应用以提升性能的策略。原理是通过数据冗余减少数据关联 牺牲数据的写性能以提升读性能。  </li>\n<li><a href=\"https://databricks.com/session_eu19/dynamic-partition-pruning-in-apache-spark\" target=\"_blank\" rel=\"noopener\">Dynamic Partition Prunning</a>    </li>\n</ol>\n<h3 id=\"Enhanced-Support-for-Deep-Learning\"><a href=\"#Enhanced-Support-for-Deep-Learning\" class=\"headerlink\" title=\"Enhanced Support for Deep Learning\"></a>Enhanced Support for Deep Learning</h3><p>加强了对深度学习的支持。在 Spark 3.0之前，Spark MLlib 并不专注于深度学习 没有提供图像/NLP 相关的深度学习算法。yahoo 的 TensorFlowOnSpark 等工程提供了在 Spark 上深度学习的可能，但是有很大的问题: 对 Spark 弹性计算兼容不足，单分区训练失败将在所有分区上再训练。Spark 3.0解决了这个问题，并兼容了Nvidia、AMD、Intel 等多类型 GPU    </p>\n<h3 id=\"DataSourceV2\"><a href=\"#DataSourceV2\" class=\"headerlink\" title=\"DataSourceV2\"></a>DataSourceV2</h3><p>DataSourceV1 对数据源的处理有一些劣势: 过于依赖 SQLContext/DataFrame,导致底层 API 依赖高层 API；不支持列式存储的批处理接口；不支持 SQLOnStreaming 等<br>Spark 3.0对 DataSourceV2的优化:  </p>\n<ol>\n<li>改进谓词下推机制，可通过减少数据加载来加速查询  </li>\n<li>执行可插拔的 Catalog 插件</li>\n<li>支持列式存储批处理接口以提高性能  </li>\n<li>支持读二进制文件 Binary File DataSource,但不支持写二进制 DataFrame 到外部数据源      </li>\n</ol>\n<p>注:<br><a href=\"https://issues.apache.org/jira/browse/SPARK-15689\" target=\"_blank\" rel=\"noopener\">SPARK-15689</a>  </p>\n<h3 id=\"Spark-Delta-Lake\"><a href=\"#Spark-Delta-Lake\" class=\"headerlink\" title=\"Spark Delta Lake\"></a>Spark Delta Lake</h3><p>Delta Lake 是一个引入了 ACID事务的开源存储层，保证了数据湖的可靠性、性能和生命周期管理。Delta Lake 可以插件式得实现或升级现有程序来事务性删除/修改大数据中的小数据。对流处理(e.g. 流式数据仓库)、数仓中拉链表等场景提供了新的方案。    </p>\n<p>注:<br><a href=\"https://docs.databricks.com/delta/quick-start.html\" target=\"_blank\" rel=\"noopener\">Delta Lake</a></p>\n<h3 id=\"Graph-Feature-存疑\"><a href=\"#Graph-Feature-存疑\" class=\"headerlink\" title=\"Graph Feature(存疑)\"></a>Graph Feature(存疑)</h3><p>Spark 3.0加入了SparkGraph 模块用于进行图处理，可以使用 Neo4J开发的 Cypher 语言操作图模型及图算法。Neo4J 图数据库是单节点的，不适用于大数据量，SparkGraph 如何解决这个问题的?(未知)。    </p>\n<h3 id=\"Yarn\"><a href=\"#Yarn\" class=\"headerlink\" title=\"Yarn\"></a>Yarn</h3><p>Spark 3.0 可以在 Yarn 集群上自动发现GPU，并可以在 GPU 节点上调度任务(task)。   </p>\n<h3 id=\"Kubernetes\"><a href=\"#Kubernetes\" class=\"headerlink\" title=\"Kubernetes\"></a>Kubernetes</h3><p>Spark 2 时代对Kubernetes 的支持相对不太成熟，在生产环境中相比 Yarn 来说更难使用且性能较差。Spark3.0 引入了新的 Shuffle Service On Kubernetes 且支持 GPU 的 Pod 级隔离。  </p>\n<h2 id=\"改动\"><a href=\"#改动\" class=\"headerlink\" title=\"改动\"></a>改动</h2><h3 id=\"语言支持\"><a href=\"#语言支持\" class=\"headerlink\" title=\"语言支持\"></a>语言支持</h3><p>预览版及其后版本支持 Python3、Scala 2.12和 JDK11。Python2 将被弃用。  </p>\n<h3 id=\"hadoop支持\"><a href=\"#hadoop支持\" class=\"headerlink\" title=\"hadoop支持\"></a>hadoop支持</h3><p>支持 Hadoop 3，预览版默认为 Hadoop2.7.4。  </p>\n<h3 id=\"deprecated\"><a href=\"#deprecated\" class=\"headerlink\" title=\"deprecated\"></a>deprecated</h3><p>删除了不高于 Spark2.2.0 的 deprecated API、参数和功能等。</p>\n<h2 id=\"issues附录\"><a href=\"#issues附录\" class=\"headerlink\" title=\"issues附录\"></a>issues附录</h2><p><a href=\"https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-printable/temp/SearchRequest.html?jqlQuery=statusCategory+%3D+done+AND+project+%3D+12315420+AND+fixVersion+%3D+12339177+ORDER+BY+priority+DESC%2C+key+ASC&tempMax=1000\" target=\"_blank\" rel=\"noopener\">issues</a></p>"},{"layout":"post","title":"Apache Spark TPC-DS 性能测试数据","subtitle":"TPCDS","date":"2020-02-13T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 关于 TPC-DS  \nTPC(Transaction Process Performance Council, 事务处理性能委员会)是制定商务应用基准程序(Benchmark)的标准规范、性能和价格度量的非营利性组织。TPC-DS 是一个决策支持标准测试，它对决策支持系统的普适性建模，包括查询和数据修改维护。作为一个通用的决策支持标准测试，其提供了具有代表性的性能评估，测试结果包括单用户模式下的查询相应时间、多用户模式下的查询吞吐量、在受控复杂多用户决策支持的工作负载下给定硬件 操作系统 数据处理系统软件配置的数据维护性能。TPC-DS 决策支持标准测试目的是为行业用户提供客观的性能数据。TPC-DS 决策支持一些新兴技术，例如 Hadoop、Spark等大数据解决方案。\n\n<!--more -->\n\n## TPC-DS 数据模型\nTPC-DS 使用零售业务建模，schema 包括客户、订单、销售、退货和产品数据等业务信息。标准测试模拟了任何成熟的决策支持系统都必须具备的两个重要组成部分:  \n\n1. 用户查询: 数据分析、挖掘，可将运营事实转换为商业智能  \n2. 数据维护: 可将管理分析过程与其所依赖的可操作外部数据源同步  \n\nTPC-DS 使用星型、雪花模型建模，有多个维度和事实表。每个维度表都有主键，事实表使用外键与维度表主键关联。\n维度表可以分为以下类型:  \n\n1. 静态：表内容在数据库加载期间仅加载一次，且数据不随着时间变化而变化。  \n2. 历史记录：为单个业务实体创建多行来保存维度数据更改的历史记录，每行包括创建/修改时间的列。\"项目\"是历史记录维度信息的一个范例。  \n3. 非历史记录：不保留对维度数据所做更改的历史记录。随着维度数据的更新，之前保留的信息将被覆盖。所有事实表都与维度信息的最新值关联。\"客户信息\"是非历史记录维度信息的一个范例。  \n\nTPC-DS 有 7 个事实表和 17 个维度表，平均每个表列数在 20 左右，且数据分布是真实而不均匀的，存在数据倾斜，与真实场景非常接近。因此TPC-DS成为客观衡量不同 Hadoop 版本以及 SQL on Hadoop 技术的最佳测试数据集。      \n事实表包含 商店、仓库和互联网三个销售渠道中每个渠道的建模产品销售和退货事实表；库存事实表。   \n维度表用于和每个销售渠道相关信息关联。  \n\n注： ER 图设计及事实表和维度表表结构详见官方文档(specification.pdf)    \n \n## TPC-DS 查询语句  \n \n由标准建模的查询语句(q1~q99)具有以下特征：  \n\n1. 解决复杂的业务问题 \n2. 使用各种访问模式、查询短语、运算符和约束  \n3. 使用会在各种查询过程之间变化的查询参数  \n\n## 下载 TP-CDS kit\nTPCDS  kit 是生成测试数据的工具  \n\n方法一: 官网下载 http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp     \n\t填写自己的邮箱后，官方会将下载链接发送到邮箱内，下载即可，文件包大小为 5MB 左右。   \n方法二: github 下载 https://github.com/gregrahn/tpcds-kit/tree/master  \n\t\n解压后目录数如下，  \n\n```\ntree -d v2.11.0rc2/  \n\nv2.11.0rc2/\n├── answer_sets\n├── query_templates\n├── query_variants\n├── specification\n├── tests\n└── tools\n```\n其中 specification为 tpcds 说明书，包括工具说明、数据模型、ER 图、度量信息等；query\\_templates 为查询语句模板；query\\_variants 为查询语句模板的参数；answer\\_sets 为查询结果集；tools 为数据集生成及查询语句生成工具集。Apache Spark sql/core module 内置了org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark 测试入口和 tpcds q1~q99 查询语句。因此只需将测试数据准备好即可。  \n\n## 生成 TPC-DS\n\nTPC-DS 可根据模拟场景产生以 GB 为最小粒度的数据集，常见的有 1TB、3TB、10TB、30TB、100TB等。  \n\n### 编译 tpcds kit  \n需要在每个节点上编译tpcds kit  \n\n```\nsudo yum install gcc make flex bison byacc git  \ncd v2.11.0rc2/tools  \nmake clean && make OS=LINUX\n```\n编译完成后，在 tools 目录下会多出两个可执行文件: dsdgen,dsqgen。dsdgen 用于生成测试数据，dsqgen 用于生成在不同 SQL 标准下的查询语句。  \n\n### 生成数据  \n\ndatabricks 为 Spark SQL 制作的数据生成工具 spark-sql-perf，似乎不如其宣传的那么好用。  \nspak-sql-perf github 地址:  https://github.com/databricks/spark-sql-perf    \n\n其原理是: 依次为每个表 并行调用 tpcds kit的 dsdgen 命令生成数据(本地磁盘、text格式、'|'分割、无压缩) 作为表数据集，即生成的所有数据为 DataFrame、每个并行调用 dsdgen 产生的数据为 DataFrame 的分区。 然后将此数据集转换(读取再存储)为 hdfs上的 parquet+snappy 文件，并创建为 Hive 的外部表。核心代码如下    \n\n调用 dsdgen 命令生成分区数据  \n[DSDGEN.scala]\n\n```\nclass DSDGEN(dsdgenDir: String) extends DataGenerator {\n  val dsdgen = s\"$dsdgenDir/dsdgen\"\n  def generate(sparkContext: SparkContext, name: String, partitions: Int, scaleFactor: String) = {\n    val generatedData = {\n      sparkContext.parallelize(1 to partitions, partitions).flatMap { i =>\n        val localToolsDir = if (new java.io.File(dsdgen).exists) {\n          dsdgenDir\n        } else if (new java.io.File(s\"/$dsdgen\").exists) {\n          s\"/$dsdgenDir\"\n        } else {\n          sys.error(s\"Could not find dsdgen at $dsdgen or /$dsdgen. Run install\")\n        }\n        val parallel = if (partitions > 1) s\"-parallel $partitions -child $i\" else \"\"\n        val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -table $name -filter Y -scale $scaleFactor -RNGSEED 100 $parallel\")\n        println(commands)\n        BlockingLineStream(commands)\n      }\n    }\n    generatedData.setName(s\"$name, sf=$scaleFactor, strings\")\n    generatedData\n  }\n}\n```\n\nspark-sql-perf 拼接的并行 dsdgen 命令类似于:  cd tpcds/v2.11.0rc2/tools && ./dsdgen -table catalog\\_sales -filter Y -scale 3000 -RNGSEED 100 -parallel 2000 -child 8，scala=3000意为生成 3000G数据集，-table= catalog_sales 当前生成表，-filter=Y 意为输出数据到 stdout，-RNGSEED=100 意为 RNG seed 值，-parallel=2000 意为并行度 2000 生成数据，-child 为当前并行的标识 用于命名本地数据文件。  \n\n命令存在的问题(tpcds-kit 分支 master、v2.10、v2.5、v2.3均存在):  \n\n1. tpcds kit 的识别的 option 为 _filter,但是 spark-sql-perf 拼接的命令为 -filter    \n2. tpcds 数据模型之间有依赖关系，不能独立生成，表数据之间不是完全独立的，e.g. 销售表和退货表。 \n为 table = catalog\\_returns 生成数据时 dsdgen 会抛出错误 ERROR: Table catalog\\_returns is a child; it is populated during the build of its parent (e.g., catalog\\_sales builds catalog\\_returns)   \n  \n尝试切换了 master、v2.10、 v2.5、 v2.3等分支 均无法解决上述问题。  \n针对上述，我采用的方案是 依旧以 spark-sql-perf 为主体(表结构、dsdgen 生成数据、数据转换 parquet 到 hdfs、创建 hive 外部表等)，修改点如下： \n\n1. 修改数据并行生成命令   \n         val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel\")\n2.  增加 并行数据生成后，把生成的数据上传到 hdfs 上的逻辑    \n3.  修改表 DataFrame(RDD)生成逻辑，原有的逻辑为 dsdgen 命令包装为BlockingLineStream，修改为从 hdfs 上读取 $tableName*.dat 的逻辑\n4.  text 文本转 parquet + snappy 逻辑不变、创建 hive 外部表关联到 parquet 文件的逻辑不变。  \n\n3TB 测试数据集文件上传到 hdfs 后，使用 spark 来转换数据为 parquet 并注册外部表程序如下  \n3TB 测试数据集使用 Spark 程序转换格式时，spark.executor.memory+spark.executor.memoryOverhead 不低于 10GB。  \n\n```\nspark-shell --master yarn-client --name \"transformation\" --queue high --executor-memory 8G --driver-memory 4G --num-executors 200 --conf \"spark.executor.memoryOverhead=8G\" --jars ~/tpcds/spark-sql-perf_2.11-0.5.1-SNAPSHOT.jar\n\nimport scala.sys.process._\nimport java.util.concurrent.LinkedBlockingQueue\nimport scala.collection.immutable.Stream\nimport com.databricks.spark.sql.perf.BlockingLineStream\nimport com.databricks.spark.sql.perf.BlockingLineStream.Spawn \nimport com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed\nimport com.databricks.spark.sql.perf.tpcds.TPCDSTables\nimport org.apache.spark.sql._\n\nval partitions = 200\nval scaleFactor = \"3000\" \nval localToolsDir = \"/home/jiulong.zhu/tpcds/v2.11.0rc2/tools\"\nval rootDir: String = \"hdfs://pub1/user/jiulong.zhu/tpcds3t\" \nval databaseName = \"tpcds3t\"\nval sqlContext = new SQLContext(sc)\nval format = \"parquet\"\nsc.parallelize(1 to partitions,partitions).flatMap { i =>\n        val parallel = if (partitions > 1) s\"-parallel $partitions -child $i\" else \"\"\n        val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -dir data/ -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel -force Y\")\n        println(commands)\n        val streamed = com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed[String](true)\n        val process = commands.run(BasicIO(false, streamed.process, None))\n        Spawn(streamed.done(process.exitValue()))\n        val hdfsCommand =  Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && .hadoop fs -put data/*.dat temp/\")\n          val putProcess = hdfsCommand.run(BasicIO(false, streamed.process, None))\n        Spawn(streamed.done(putProcess.exitValue()))          \n        Seq(streamed.stream())\n      }.collect()\n\n// Run:\nval tables = new TPCDSTables(sqlContext,\n    dsdgenDir = localToolsDir, // location of dsdgen\n    scaleFactor = scaleFactor,\n    useDoubleForDecimal = false, // true to replace DecimalType with DoubleType\n    useStringForDate = false) // true to replace DateType with StringType\n\ntables.genData(\n    location = rootDir,\n    format = format,\n    overwrite = true, // overwrite the data that is already there\n    partitionTables = true, // create the partitioned fact tables\n    clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files.\n    filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value\n    tableFilter = \"\", // \"\" means generate all tables\n    numPartitions = 2000) // how many dsdgen partitions to run - number of input tasks.\n\n// Create the specified database\nsql(s\"create database if not exists $databaseName\")\n// Create metastore tables in a specified database for your data.\n// Once tables are created, the current database will be switched to the specified database.\ntables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true)\n```\n\ntpcds 3TB 数据集在 text无压缩格式下为 2.7TB，转为 parquet snappy 格式后约为 900GB，orc snappy 格式约为 800GB。\n\n## 测试 SparkSQL  \n\nnohup sh TPCDSBench.sh &  \nTPCDSBench.sh  \n\n```\nexport SPARK_HOME=/home/jiulong.zhu/spark-2.3.0-bin-hadoop2.7\nMASTER=yarn-client\nAPP_NAME=AE_ENABLED\nDRIVER_MEMORY=8G\nEXECUTOR_MEMORY=10G\nNUM_EXECUTORS=100\nEXECUTOR_CORES=5\nEX_JARS=/home/jiulong.zhu/tpcds/spark-sql_2.11-2.3.1-SNAPSHOT-tests.jar\nAE_ENABLE=false\n\nspark-submit --master $MASTER --name ${APP_NAME}_${AE_ENABLE} \\\n--conf \"spark.executor.instances=$NUM_EXECUTORS\" \\\n--conf \"spark.executor.cores=$EXECUTOR_CORES\" \\\n--conf \"spark.sql.crossJoin.enabled=true\" \\\n--conf \"spark.sql.autoBroadcastJoinThreshold=21971520\" \\\n--conf \"spark.default.parallelism=1000\" \\\n--conf \"spark.sql.shuffle.partitions=1000\" \\\n--conf \"spark.sql.parquet.compression.codec=snappy\" \\\n--conf \"spark.sql.adaptive.enabled=${AE_ENABLE}\" \\\n--conf \"spark.dynamicAllocation.enabled=false\" \\\n--conf \"spark.executor.memoryOverhead=10G\" \\\n--conf \"spark.sql.adaptive.minNumPostShufflePartitions=1000\" \\\n--queue high \\\n--driver-memory $DRIVER_MEMORY --executor-memory $EXECUTOR_MEMORY \\\n--class org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark $EX_JARS \\\n--data-location hdfs://pub1/user/jiulong.zhu/tpcds3t\n```\n\n## 附录及参考\n\n1. 从大数据平台二次开发、性能优化方面，准备一套合适的性能测试数据、通用查询语句、工具和测试流程对于性能提升量化和成果量化有非比寻常的意义。    \n2. 可以试试 IBM 的 spark-tpc-ds-performance-test(https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套 TPCDS 测试数据集。   \n3. 参考:    \n　　https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds   \n　　https://github.com/databricks/spark-sql-perf   \n\n\n","source":"_posts/2020-02-14-Apache Spark TPCDS性能测试数据集.md","raw":"---\nlayout:     post\ntitle:     Apache Spark TPC-DS 性能测试数据\nsubtitle:   TPCDS\ndate:       2020-02-14\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n    - Adaptive Execution \n---\n\n\n## 关于 TPC-DS  \nTPC(Transaction Process Performance Council, 事务处理性能委员会)是制定商务应用基准程序(Benchmark)的标准规范、性能和价格度量的非营利性组织。TPC-DS 是一个决策支持标准测试，它对决策支持系统的普适性建模，包括查询和数据修改维护。作为一个通用的决策支持标准测试，其提供了具有代表性的性能评估，测试结果包括单用户模式下的查询相应时间、多用户模式下的查询吞吐量、在受控复杂多用户决策支持的工作负载下给定硬件 操作系统 数据处理系统软件配置的数据维护性能。TPC-DS 决策支持标准测试目的是为行业用户提供客观的性能数据。TPC-DS 决策支持一些新兴技术，例如 Hadoop、Spark等大数据解决方案。\n\n<!--more -->\n\n## TPC-DS 数据模型\nTPC-DS 使用零售业务建模，schema 包括客户、订单、销售、退货和产品数据等业务信息。标准测试模拟了任何成熟的决策支持系统都必须具备的两个重要组成部分:  \n\n1. 用户查询: 数据分析、挖掘，可将运营事实转换为商业智能  \n2. 数据维护: 可将管理分析过程与其所依赖的可操作外部数据源同步  \n\nTPC-DS 使用星型、雪花模型建模，有多个维度和事实表。每个维度表都有主键，事实表使用外键与维度表主键关联。\n维度表可以分为以下类型:  \n\n1. 静态：表内容在数据库加载期间仅加载一次，且数据不随着时间变化而变化。  \n2. 历史记录：为单个业务实体创建多行来保存维度数据更改的历史记录，每行包括创建/修改时间的列。\"项目\"是历史记录维度信息的一个范例。  \n3. 非历史记录：不保留对维度数据所做更改的历史记录。随着维度数据的更新，之前保留的信息将被覆盖。所有事实表都与维度信息的最新值关联。\"客户信息\"是非历史记录维度信息的一个范例。  \n\nTPC-DS 有 7 个事实表和 17 个维度表，平均每个表列数在 20 左右，且数据分布是真实而不均匀的，存在数据倾斜，与真实场景非常接近。因此TPC-DS成为客观衡量不同 Hadoop 版本以及 SQL on Hadoop 技术的最佳测试数据集。      \n事实表包含 商店、仓库和互联网三个销售渠道中每个渠道的建模产品销售和退货事实表；库存事实表。   \n维度表用于和每个销售渠道相关信息关联。  \n\n注： ER 图设计及事实表和维度表表结构详见官方文档(specification.pdf)    \n \n## TPC-DS 查询语句  \n \n由标准建模的查询语句(q1~q99)具有以下特征：  \n\n1. 解决复杂的业务问题 \n2. 使用各种访问模式、查询短语、运算符和约束  \n3. 使用会在各种查询过程之间变化的查询参数  \n\n## 下载 TP-CDS kit\nTPCDS  kit 是生成测试数据的工具  \n\n方法一: 官网下载 http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp     \n\t填写自己的邮箱后，官方会将下载链接发送到邮箱内，下载即可，文件包大小为 5MB 左右。   \n方法二: github 下载 https://github.com/gregrahn/tpcds-kit/tree/master  \n\t\n解压后目录数如下，  \n\n```\ntree -d v2.11.0rc2/  \n\nv2.11.0rc2/\n├── answer_sets\n├── query_templates\n├── query_variants\n├── specification\n├── tests\n└── tools\n```\n其中 specification为 tpcds 说明书，包括工具说明、数据模型、ER 图、度量信息等；query\\_templates 为查询语句模板；query\\_variants 为查询语句模板的参数；answer\\_sets 为查询结果集；tools 为数据集生成及查询语句生成工具集。Apache Spark sql/core module 内置了org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark 测试入口和 tpcds q1~q99 查询语句。因此只需将测试数据准备好即可。  \n\n## 生成 TPC-DS\n\nTPC-DS 可根据模拟场景产生以 GB 为最小粒度的数据集，常见的有 1TB、3TB、10TB、30TB、100TB等。  \n\n### 编译 tpcds kit  \n需要在每个节点上编译tpcds kit  \n\n```\nsudo yum install gcc make flex bison byacc git  \ncd v2.11.0rc2/tools  \nmake clean && make OS=LINUX\n```\n编译完成后，在 tools 目录下会多出两个可执行文件: dsdgen,dsqgen。dsdgen 用于生成测试数据，dsqgen 用于生成在不同 SQL 标准下的查询语句。  \n\n### 生成数据  \n\ndatabricks 为 Spark SQL 制作的数据生成工具 spark-sql-perf，似乎不如其宣传的那么好用。  \nspak-sql-perf github 地址:  https://github.com/databricks/spark-sql-perf    \n\n其原理是: 依次为每个表 并行调用 tpcds kit的 dsdgen 命令生成数据(本地磁盘、text格式、'|'分割、无压缩) 作为表数据集，即生成的所有数据为 DataFrame、每个并行调用 dsdgen 产生的数据为 DataFrame 的分区。 然后将此数据集转换(读取再存储)为 hdfs上的 parquet+snappy 文件，并创建为 Hive 的外部表。核心代码如下    \n\n调用 dsdgen 命令生成分区数据  \n[DSDGEN.scala]\n\n```\nclass DSDGEN(dsdgenDir: String) extends DataGenerator {\n  val dsdgen = s\"$dsdgenDir/dsdgen\"\n  def generate(sparkContext: SparkContext, name: String, partitions: Int, scaleFactor: String) = {\n    val generatedData = {\n      sparkContext.parallelize(1 to partitions, partitions).flatMap { i =>\n        val localToolsDir = if (new java.io.File(dsdgen).exists) {\n          dsdgenDir\n        } else if (new java.io.File(s\"/$dsdgen\").exists) {\n          s\"/$dsdgenDir\"\n        } else {\n          sys.error(s\"Could not find dsdgen at $dsdgen or /$dsdgen. Run install\")\n        }\n        val parallel = if (partitions > 1) s\"-parallel $partitions -child $i\" else \"\"\n        val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -table $name -filter Y -scale $scaleFactor -RNGSEED 100 $parallel\")\n        println(commands)\n        BlockingLineStream(commands)\n      }\n    }\n    generatedData.setName(s\"$name, sf=$scaleFactor, strings\")\n    generatedData\n  }\n}\n```\n\nspark-sql-perf 拼接的并行 dsdgen 命令类似于:  cd tpcds/v2.11.0rc2/tools && ./dsdgen -table catalog\\_sales -filter Y -scale 3000 -RNGSEED 100 -parallel 2000 -child 8，scala=3000意为生成 3000G数据集，-table= catalog_sales 当前生成表，-filter=Y 意为输出数据到 stdout，-RNGSEED=100 意为 RNG seed 值，-parallel=2000 意为并行度 2000 生成数据，-child 为当前并行的标识 用于命名本地数据文件。  \n\n命令存在的问题(tpcds-kit 分支 master、v2.10、v2.5、v2.3均存在):  \n\n1. tpcds kit 的识别的 option 为 _filter,但是 spark-sql-perf 拼接的命令为 -filter    \n2. tpcds 数据模型之间有依赖关系，不能独立生成，表数据之间不是完全独立的，e.g. 销售表和退货表。 \n为 table = catalog\\_returns 生成数据时 dsdgen 会抛出错误 ERROR: Table catalog\\_returns is a child; it is populated during the build of its parent (e.g., catalog\\_sales builds catalog\\_returns)   \n  \n尝试切换了 master、v2.10、 v2.5、 v2.3等分支 均无法解决上述问题。  \n针对上述，我采用的方案是 依旧以 spark-sql-perf 为主体(表结构、dsdgen 生成数据、数据转换 parquet 到 hdfs、创建 hive 外部表等)，修改点如下： \n\n1. 修改数据并行生成命令   \n         val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel\")\n2.  增加 并行数据生成后，把生成的数据上传到 hdfs 上的逻辑    \n3.  修改表 DataFrame(RDD)生成逻辑，原有的逻辑为 dsdgen 命令包装为BlockingLineStream，修改为从 hdfs 上读取 $tableName*.dat 的逻辑\n4.  text 文本转 parquet + snappy 逻辑不变、创建 hive 外部表关联到 parquet 文件的逻辑不变。  \n\n3TB 测试数据集文件上传到 hdfs 后，使用 spark 来转换数据为 parquet 并注册外部表程序如下  \n3TB 测试数据集使用 Spark 程序转换格式时，spark.executor.memory+spark.executor.memoryOverhead 不低于 10GB。  \n\n```\nspark-shell --master yarn-client --name \"transformation\" --queue high --executor-memory 8G --driver-memory 4G --num-executors 200 --conf \"spark.executor.memoryOverhead=8G\" --jars ~/tpcds/spark-sql-perf_2.11-0.5.1-SNAPSHOT.jar\n\nimport scala.sys.process._\nimport java.util.concurrent.LinkedBlockingQueue\nimport scala.collection.immutable.Stream\nimport com.databricks.spark.sql.perf.BlockingLineStream\nimport com.databricks.spark.sql.perf.BlockingLineStream.Spawn \nimport com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed\nimport com.databricks.spark.sql.perf.tpcds.TPCDSTables\nimport org.apache.spark.sql._\n\nval partitions = 200\nval scaleFactor = \"3000\" \nval localToolsDir = \"/home/jiulong.zhu/tpcds/v2.11.0rc2/tools\"\nval rootDir: String = \"hdfs://pub1/user/jiulong.zhu/tpcds3t\" \nval databaseName = \"tpcds3t\"\nval sqlContext = new SQLContext(sc)\nval format = \"parquet\"\nsc.parallelize(1 to partitions,partitions).flatMap { i =>\n        val parallel = if (partitions > 1) s\"-parallel $partitions -child $i\" else \"\"\n        val commands = Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && ./dsdgen -dir data/ -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel -force Y\")\n        println(commands)\n        val streamed = com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed[String](true)\n        val process = commands.run(BasicIO(false, streamed.process, None))\n        Spawn(streamed.done(process.exitValue()))\n        val hdfsCommand =  Seq(\n          \"bash\", \"-c\",\n          s\"cd $localToolsDir && .hadoop fs -put data/*.dat temp/\")\n          val putProcess = hdfsCommand.run(BasicIO(false, streamed.process, None))\n        Spawn(streamed.done(putProcess.exitValue()))          \n        Seq(streamed.stream())\n      }.collect()\n\n// Run:\nval tables = new TPCDSTables(sqlContext,\n    dsdgenDir = localToolsDir, // location of dsdgen\n    scaleFactor = scaleFactor,\n    useDoubleForDecimal = false, // true to replace DecimalType with DoubleType\n    useStringForDate = false) // true to replace DateType with StringType\n\ntables.genData(\n    location = rootDir,\n    format = format,\n    overwrite = true, // overwrite the data that is already there\n    partitionTables = true, // create the partitioned fact tables\n    clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files.\n    filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value\n    tableFilter = \"\", // \"\" means generate all tables\n    numPartitions = 2000) // how many dsdgen partitions to run - number of input tasks.\n\n// Create the specified database\nsql(s\"create database if not exists $databaseName\")\n// Create metastore tables in a specified database for your data.\n// Once tables are created, the current database will be switched to the specified database.\ntables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true)\n```\n\ntpcds 3TB 数据集在 text无压缩格式下为 2.7TB，转为 parquet snappy 格式后约为 900GB，orc snappy 格式约为 800GB。\n\n## 测试 SparkSQL  \n\nnohup sh TPCDSBench.sh &  \nTPCDSBench.sh  \n\n```\nexport SPARK_HOME=/home/jiulong.zhu/spark-2.3.0-bin-hadoop2.7\nMASTER=yarn-client\nAPP_NAME=AE_ENABLED\nDRIVER_MEMORY=8G\nEXECUTOR_MEMORY=10G\nNUM_EXECUTORS=100\nEXECUTOR_CORES=5\nEX_JARS=/home/jiulong.zhu/tpcds/spark-sql_2.11-2.3.1-SNAPSHOT-tests.jar\nAE_ENABLE=false\n\nspark-submit --master $MASTER --name ${APP_NAME}_${AE_ENABLE} \\\n--conf \"spark.executor.instances=$NUM_EXECUTORS\" \\\n--conf \"spark.executor.cores=$EXECUTOR_CORES\" \\\n--conf \"spark.sql.crossJoin.enabled=true\" \\\n--conf \"spark.sql.autoBroadcastJoinThreshold=21971520\" \\\n--conf \"spark.default.parallelism=1000\" \\\n--conf \"spark.sql.shuffle.partitions=1000\" \\\n--conf \"spark.sql.parquet.compression.codec=snappy\" \\\n--conf \"spark.sql.adaptive.enabled=${AE_ENABLE}\" \\\n--conf \"spark.dynamicAllocation.enabled=false\" \\\n--conf \"spark.executor.memoryOverhead=10G\" \\\n--conf \"spark.sql.adaptive.minNumPostShufflePartitions=1000\" \\\n--queue high \\\n--driver-memory $DRIVER_MEMORY --executor-memory $EXECUTOR_MEMORY \\\n--class org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark $EX_JARS \\\n--data-location hdfs://pub1/user/jiulong.zhu/tpcds3t\n```\n\n## 附录及参考\n\n1. 从大数据平台二次开发、性能优化方面，准备一套合适的性能测试数据、通用查询语句、工具和测试流程对于性能提升量化和成果量化有非比寻常的意义。    \n2. 可以试试 IBM 的 spark-tpc-ds-performance-test(https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套 TPCDS 测试数据集。   \n3. 参考:    \n　　https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds   \n　　https://github.com/databricks/spark-sql-perf   \n\n\n","slug":"2020-02-14-Apache Spark TPCDS性能测试数据集","published":1,"updated":"2020-06-16T06:43:50.842Z","comments":1,"photos":[],"link":"","_id":"cke2rms5m000ly7c737aiargo","content":"<h2 id=\"关于-TPC-DS\"><a href=\"#关于-TPC-DS\" class=\"headerlink\" title=\"关于 TPC-DS\"></a>关于 TPC-DS</h2><p>TPC(Transaction Process Performance Council, 事务处理性能委员会)是制定商务应用基准程序(Benchmark)的标准规范、性能和价格度量的非营利性组织。TPC-DS 是一个决策支持标准测试，它对决策支持系统的普适性建模，包括查询和数据修改维护。作为一个通用的决策支持标准测试，其提供了具有代表性的性能评估，测试结果包括单用户模式下的查询相应时间、多用户模式下的查询吞吐量、在受控复杂多用户决策支持的工作负载下给定硬件 操作系统 数据处理系统软件配置的数据维护性能。TPC-DS 决策支持标准测试目的是为行业用户提供客观的性能数据。TPC-DS 决策支持一些新兴技术，例如 Hadoop、Spark等大数据解决方案。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"TPC-DS-数据模型\"><a href=\"#TPC-DS-数据模型\" class=\"headerlink\" title=\"TPC-DS 数据模型\"></a>TPC-DS 数据模型</h2><p>TPC-DS 使用零售业务建模，schema 包括客户、订单、销售、退货和产品数据等业务信息。标准测试模拟了任何成熟的决策支持系统都必须具备的两个重要组成部分:  </p>\n<ol>\n<li>用户查询: 数据分析、挖掘，可将运营事实转换为商业智能  </li>\n<li>数据维护: 可将管理分析过程与其所依赖的可操作外部数据源同步  </li>\n</ol>\n<p>TPC-DS 使用星型、雪花模型建模，有多个维度和事实表。每个维度表都有主键，事实表使用外键与维度表主键关联。\n维度表可以分为以下类型:  </p>\n<ol>\n<li>静态：表内容在数据库加载期间仅加载一次，且数据不随着时间变化而变化。  </li>\n<li>历史记录：为单个业务实体创建多行来保存维度数据更改的历史记录，每行包括创建/修改时间的列。”项目”是历史记录维度信息的一个范例。  </li>\n<li>非历史记录：不保留对维度数据所做更改的历史记录。随着维度数据的更新，之前保留的信息将被覆盖。所有事实表都与维度信息的最新值关联。”客户信息”是非历史记录维度信息的一个范例。  </li>\n</ol>\n<p>TPC-DS 有 7 个事实表和 17 个维度表，平均每个表列数在 20 左右，且数据分布是真实而不均匀的，存在数据倾斜，与真实场景非常接近。因此TPC-DS成为客观衡量不同 Hadoop 版本以及 SQL on Hadoop 技术的最佳测试数据集。<br>事实表包含 商店、仓库和互联网三个销售渠道中每个渠道的建模产品销售和退货事实表；库存事实表。<br>维度表用于和每个销售渠道相关信息关联。  </p>\n<p>注： ER 图设计及事实表和维度表表结构详见官方文档(specification.pdf)    </p>\n<h2 id=\"TPC-DS-查询语句\"><a href=\"#TPC-DS-查询语句\" class=\"headerlink\" title=\"TPC-DS 查询语句\"></a>TPC-DS 查询语句</h2><p>由标准建模的查询语句(q1~q99)具有以下特征：  </p>\n<ol>\n<li>解决复杂的业务问题 </li>\n<li>使用各种访问模式、查询短语、运算符和约束  </li>\n<li>使用会在各种查询过程之间变化的查询参数  </li>\n</ol>\n<h2 id=\"下载-TP-CDS-kit\"><a href=\"#下载-TP-CDS-kit\" class=\"headerlink\" title=\"下载 TP-CDS kit\"></a>下载 TP-CDS kit</h2><p>TPCDS  kit 是生成测试数据的工具  </p>\n<p>方法一: 官网下载 <a href=\"http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp\" target=\"_blank\" rel=\"noopener\">http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp</a><br>    填写自己的邮箱后，官方会将下载链接发送到邮箱内，下载即可，文件包大小为 5MB 左右。<br>方法二: github 下载 <a href=\"https://github.com/gregrahn/tpcds-kit/tree/master\" target=\"_blank\" rel=\"noopener\">https://github.com/gregrahn/tpcds-kit/tree/master</a>  </p>\n<p>解压后目录数如下，  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree -d v2.11.0rc2&#x2F;  </span><br><span class=\"line\"></span><br><span class=\"line\">v2.11.0rc2&#x2F;</span><br><span class=\"line\">├── answer_sets</span><br><span class=\"line\">├── query_templates</span><br><span class=\"line\">├── query_variants</span><br><span class=\"line\">├── specification</span><br><span class=\"line\">├── tests</span><br><span class=\"line\">└── tools</span><br></pre></td></tr></table></figure>\n<p>其中 specification为 tpcds 说明书，包括工具说明、数据模型、ER 图、度量信息等；query_templates 为查询语句模板；query_variants 为查询语句模板的参数；answer_sets 为查询结果集；tools 为数据集生成及查询语句生成工具集。Apache Spark sql/core module 内置了org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark 测试入口和 tpcds q1~q99 查询语句。因此只需将测试数据准备好即可。  </p>\n<h2 id=\"生成-TPC-DS\"><a href=\"#生成-TPC-DS\" class=\"headerlink\" title=\"生成 TPC-DS\"></a>生成 TPC-DS</h2><p>TPC-DS 可根据模拟场景产生以 GB 为最小粒度的数据集，常见的有 1TB、3TB、10TB、30TB、100TB等。  </p>\n<h3 id=\"编译-tpcds-kit\"><a href=\"#编译-tpcds-kit\" class=\"headerlink\" title=\"编译 tpcds kit\"></a>编译 tpcds kit</h3><p>需要在每个节点上编译tpcds kit  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install gcc make flex bison byacc git  </span><br><span class=\"line\">cd v2.11.0rc2&#x2F;tools  </span><br><span class=\"line\">make clean &amp;&amp; make OS&#x3D;LINUX</span><br></pre></td></tr></table></figure>\n<p>编译完成后，在 tools 目录下会多出两个可执行文件: dsdgen,dsqgen。dsdgen 用于生成测试数据，dsqgen 用于生成在不同 SQL 标准下的查询语句。  </p>\n<h3 id=\"生成数据\"><a href=\"#生成数据\" class=\"headerlink\" title=\"生成数据\"></a>生成数据</h3><p>databricks 为 Spark SQL 制作的数据生成工具 spark-sql-perf，似乎不如其宣传的那么好用。<br>spak-sql-perf github 地址:  <a href=\"https://github.com/databricks/spark-sql-perf\" target=\"_blank\" rel=\"noopener\">https://github.com/databricks/spark-sql-perf</a>    </p>\n<p>其原理是: 依次为每个表 并行调用 tpcds kit的 dsdgen 命令生成数据(本地磁盘、text格式、’|’分割、无压缩) 作为表数据集，即生成的所有数据为 DataFrame、每个并行调用 dsdgen 产生的数据为 DataFrame 的分区。 然后将此数据集转换(读取再存储)为 hdfs上的 parquet+snappy 文件，并创建为 Hive 的外部表。核心代码如下    </p>\n<p>调用 dsdgen 命令生成分区数据<br>[DSDGEN.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DSDGEN(dsdgenDir: String) extends DataGenerator &#123;</span><br><span class=\"line\">  val dsdgen &#x3D; s&quot;$dsdgenDir&#x2F;dsdgen&quot;</span><br><span class=\"line\">  def generate(sparkContext: SparkContext, name: String, partitions: Int, scaleFactor: String) &#x3D; &#123;</span><br><span class=\"line\">    val generatedData &#x3D; &#123;</span><br><span class=\"line\">      sparkContext.parallelize(1 to partitions, partitions).flatMap &#123; i &#x3D;&gt;</span><br><span class=\"line\">        val localToolsDir &#x3D; if (new java.io.File(dsdgen).exists) &#123;</span><br><span class=\"line\">          dsdgenDir</span><br><span class=\"line\">        &#125; else if (new java.io.File(s&quot;&#x2F;$dsdgen&quot;).exists) &#123;</span><br><span class=\"line\">          s&quot;&#x2F;$dsdgenDir&quot;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          sys.error(s&quot;Could not find dsdgen at $dsdgen or &#x2F;$dsdgen. Run install&quot;)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        val parallel &#x3D; if (partitions &gt; 1) s&quot;-parallel $partitions -child $i&quot; else &quot;&quot;</span><br><span class=\"line\">        val commands &#x3D; Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .&#x2F;dsdgen -table $name -filter Y -scale $scaleFactor -RNGSEED 100 $parallel&quot;)</span><br><span class=\"line\">        println(commands)</span><br><span class=\"line\">        BlockingLineStream(commands)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    generatedData.setName(s&quot;$name, sf&#x3D;$scaleFactor, strings&quot;)</span><br><span class=\"line\">    generatedData</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>spark-sql-perf 拼接的并行 dsdgen 命令类似于:  cd tpcds/v2.11.0rc2/tools &amp;&amp; ./dsdgen -table catalog_sales -filter Y -scale 3000 -RNGSEED 100 -parallel 2000 -child 8，scala=3000意为生成 3000G数据集，-table= catalog_sales 当前生成表，-filter=Y 意为输出数据到 stdout，-RNGSEED=100 意为 RNG seed 值，-parallel=2000 意为并行度 2000 生成数据，-child 为当前并行的标识 用于命名本地数据文件。  </p>\n<p>命令存在的问题(tpcds-kit 分支 master、v2.10、v2.5、v2.3均存在):  </p>\n<ol>\n<li>tpcds kit 的识别的 option 为 _filter,但是 spark-sql-perf 拼接的命令为 -filter    </li>\n<li>tpcds 数据模型之间有依赖关系，不能独立生成，表数据之间不是完全独立的，e.g. 销售表和退货表。 \n为 table = catalog_returns 生成数据时 dsdgen 会抛出错误 ERROR: Table catalog_returns is a child; it is populated during the build of its parent (e.g., catalog_sales builds catalog_returns)   </li>\n</ol>\n<p>尝试切换了 master、v2.10、 v2.5、 v2.3等分支 均无法解决上述问题。<br>针对上述，我采用的方案是 依旧以 spark-sql-perf 为主体(表结构、dsdgen 生成数据、数据转换 parquet 到 hdfs、创建 hive 外部表等)，修改点如下： </p>\n<ol>\n<li>修改数据并行生成命令   <pre><code>val commands = Seq(\n &quot;bash&quot;, &quot;-c&quot;,\n s&quot;cd $localToolsDir &amp;&amp; ./dsdgen -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel&quot;)</code></pre></li>\n<li>增加 并行数据生成后，把生成的数据上传到 hdfs 上的逻辑    </li>\n<li>修改表 DataFrame(RDD)生成逻辑，原有的逻辑为 dsdgen 命令包装为BlockingLineStream，修改为从 hdfs 上读取 $tableName*.dat 的逻辑</li>\n<li>text 文本转 parquet + snappy 逻辑不变、创建 hive 外部表关联到 parquet 文件的逻辑不变。  </li>\n</ol>\n<p>3TB 测试数据集文件上传到 hdfs 后，使用 spark 来转换数据为 parquet 并注册外部表程序如下<br>3TB 测试数据集使用 Spark 程序转换格式时，spark.executor.memory+spark.executor.memoryOverhead 不低于 10GB。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-shell --master yarn-client --name &quot;transformation&quot; --queue high --executor-memory 8G --driver-memory 4G --num-executors 200 --conf &quot;spark.executor.memoryOverhead&#x3D;8G&quot; --jars ~&#x2F;tpcds&#x2F;spark-sql-perf_2.11-0.5.1-SNAPSHOT.jar</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.sys.process._</span><br><span class=\"line\">import java.util.concurrent.LinkedBlockingQueue</span><br><span class=\"line\">import scala.collection.immutable.Stream</span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream</span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream.Spawn </span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed</span><br><span class=\"line\">import com.databricks.spark.sql.perf.tpcds.TPCDSTables</span><br><span class=\"line\">import org.apache.spark.sql._</span><br><span class=\"line\"></span><br><span class=\"line\">val partitions &#x3D; 200</span><br><span class=\"line\">val scaleFactor &#x3D; &quot;3000&quot; </span><br><span class=\"line\">val localToolsDir &#x3D; &quot;&#x2F;home&#x2F;jiulong.zhu&#x2F;tpcds&#x2F;v2.11.0rc2&#x2F;tools&quot;</span><br><span class=\"line\">val rootDir: String &#x3D; &quot;hdfs:&#x2F;&#x2F;pub1&#x2F;user&#x2F;jiulong.zhu&#x2F;tpcds3t&quot; </span><br><span class=\"line\">val databaseName &#x3D; &quot;tpcds3t&quot;</span><br><span class=\"line\">val sqlContext &#x3D; new SQLContext(sc)</span><br><span class=\"line\">val format &#x3D; &quot;parquet&quot;</span><br><span class=\"line\">sc.parallelize(1 to partitions,partitions).flatMap &#123; i &#x3D;&gt;</span><br><span class=\"line\">        val parallel &#x3D; if (partitions &gt; 1) s&quot;-parallel $partitions -child $i&quot; else &quot;&quot;</span><br><span class=\"line\">        val commands &#x3D; Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .&#x2F;dsdgen -dir data&#x2F; -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel -force Y&quot;)</span><br><span class=\"line\">        println(commands)</span><br><span class=\"line\">        val streamed &#x3D; com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed[String](true)</span><br><span class=\"line\">        val process &#x3D; commands.run(BasicIO(false, streamed.process, None))</span><br><span class=\"line\">        Spawn(streamed.done(process.exitValue()))</span><br><span class=\"line\">        val hdfsCommand &#x3D;  Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .hadoop fs -put data&#x2F;*.dat temp&#x2F;&quot;)</span><br><span class=\"line\">          val putProcess &#x3D; hdfsCommand.run(BasicIO(false, streamed.process, None))</span><br><span class=\"line\">        Spawn(streamed.done(putProcess.exitValue()))          </span><br><span class=\"line\">        Seq(streamed.stream())</span><br><span class=\"line\">      &#125;.collect()</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;&#x2F; Run:</span><br><span class=\"line\">val tables &#x3D; new TPCDSTables(sqlContext,</span><br><span class=\"line\">    dsdgenDir &#x3D; localToolsDir, &#x2F;&#x2F; location of dsdgen</span><br><span class=\"line\">    scaleFactor &#x3D; scaleFactor,</span><br><span class=\"line\">    useDoubleForDecimal &#x3D; false, &#x2F;&#x2F; true to replace DecimalType with DoubleType</span><br><span class=\"line\">    useStringForDate &#x3D; false) &#x2F;&#x2F; true to replace DateType with StringType</span><br><span class=\"line\"></span><br><span class=\"line\">tables.genData(</span><br><span class=\"line\">    location &#x3D; rootDir,</span><br><span class=\"line\">    format &#x3D; format,</span><br><span class=\"line\">    overwrite &#x3D; true, &#x2F;&#x2F; overwrite the data that is already there</span><br><span class=\"line\">    partitionTables &#x3D; true, &#x2F;&#x2F; create the partitioned fact tables</span><br><span class=\"line\">    clusterByPartitionColumns &#x3D; true, &#x2F;&#x2F; shuffle to get partitions coalesced into single files.</span><br><span class=\"line\">    filterOutNullPartitionValues &#x3D; false, &#x2F;&#x2F; true to filter out the partition with NULL key value</span><br><span class=\"line\">    tableFilter &#x3D; &quot;&quot;, &#x2F;&#x2F; &quot;&quot; means generate all tables</span><br><span class=\"line\">    numPartitions &#x3D; 2000) &#x2F;&#x2F; how many dsdgen partitions to run - number of input tasks.</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;&#x2F; Create the specified database</span><br><span class=\"line\">sql(s&quot;create database if not exists $databaseName&quot;)</span><br><span class=\"line\">&#x2F;&#x2F; Create metastore tables in a specified database for your data.</span><br><span class=\"line\">&#x2F;&#x2F; Once tables are created, the current database will be switched to the specified database.</span><br><span class=\"line\">tables.createExternalTables(rootDir, &quot;parquet&quot;, databaseName, overwrite &#x3D; true, discoverPartitions &#x3D; true)</span><br></pre></td></tr></table></figure>\n\n<p>tpcds 3TB 数据集在 text无压缩格式下为 2.7TB，转为 parquet snappy 格式后约为 900GB，orc snappy 格式约为 800GB。</p>\n<h2 id=\"测试-SparkSQL\"><a href=\"#测试-SparkSQL\" class=\"headerlink\" title=\"测试 SparkSQL\"></a>测试 SparkSQL</h2><p>nohup sh TPCDSBench.sh &amp;<br>TPCDSBench.sh  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export SPARK_HOME&#x3D;&#x2F;home&#x2F;jiulong.zhu&#x2F;spark-2.3.0-bin-hadoop2.7</span><br><span class=\"line\">MASTER&#x3D;yarn-client</span><br><span class=\"line\">APP_NAME&#x3D;AE_ENABLED</span><br><span class=\"line\">DRIVER_MEMORY&#x3D;8G</span><br><span class=\"line\">EXECUTOR_MEMORY&#x3D;10G</span><br><span class=\"line\">NUM_EXECUTORS&#x3D;100</span><br><span class=\"line\">EXECUTOR_CORES&#x3D;5</span><br><span class=\"line\">EX_JARS&#x3D;&#x2F;home&#x2F;jiulong.zhu&#x2F;tpcds&#x2F;spark-sql_2.11-2.3.1-SNAPSHOT-tests.jar</span><br><span class=\"line\">AE_ENABLE&#x3D;false</span><br><span class=\"line\"></span><br><span class=\"line\">spark-submit --master $MASTER --name $&#123;APP_NAME&#125;_$&#123;AE_ENABLE&#125; \\</span><br><span class=\"line\">--conf &quot;spark.executor.instances&#x3D;$NUM_EXECUTORS&quot; \\</span><br><span class=\"line\">--conf &quot;spark.executor.cores&#x3D;$EXECUTOR_CORES&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.crossJoin.enabled&#x3D;true&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.autoBroadcastJoinThreshold&#x3D;21971520&quot; \\</span><br><span class=\"line\">--conf &quot;spark.default.parallelism&#x3D;1000&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.shuffle.partitions&#x3D;1000&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.parquet.compression.codec&#x3D;snappy&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.adaptive.enabled&#x3D;$&#123;AE_ENABLE&#125;&quot; \\</span><br><span class=\"line\">--conf &quot;spark.dynamicAllocation.enabled&#x3D;false&quot; \\</span><br><span class=\"line\">--conf &quot;spark.executor.memoryOverhead&#x3D;10G&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.adaptive.minNumPostShufflePartitions&#x3D;1000&quot; \\</span><br><span class=\"line\">--queue high \\</span><br><span class=\"line\">--driver-memory $DRIVER_MEMORY --executor-memory $EXECUTOR_MEMORY \\</span><br><span class=\"line\">--class org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark $EX_JARS \\</span><br><span class=\"line\">--data-location hdfs:&#x2F;&#x2F;pub1&#x2F;user&#x2F;jiulong.zhu&#x2F;tpcds3t</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"附录及参考\"><a href=\"#附录及参考\" class=\"headerlink\" title=\"附录及参考\"></a>附录及参考</h2><ol>\n<li>从大数据平台二次开发、性能优化方面，准备一套合适的性能测试数据、通用查询语句、工具和测试流程对于性能提升量化和成果量化有非比寻常的意义。    </li>\n<li>可以试试 IBM 的 spark-tpc-ds-performance-test(<a href=\"https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套\" target=\"_blank\" rel=\"noopener\">https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套</a> TPCDS 测试数据集。   </li>\n<li>参考:<br>　　<a href=\"https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds\" target=\"_blank\" rel=\"noopener\">https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds</a><br>　　<a href=\"https://github.com/databricks/spark-sql-perf\" target=\"_blank\" rel=\"noopener\">https://github.com/databricks/spark-sql-perf</a>   </li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"关于-TPC-DS\"><a href=\"#关于-TPC-DS\" class=\"headerlink\" title=\"关于 TPC-DS\"></a>关于 TPC-DS</h2><p>TPC(Transaction Process Performance Council, 事务处理性能委员会)是制定商务应用基准程序(Benchmark)的标准规范、性能和价格度量的非营利性组织。TPC-DS 是一个决策支持标准测试，它对决策支持系统的普适性建模，包括查询和数据修改维护。作为一个通用的决策支持标准测试，其提供了具有代表性的性能评估，测试结果包括单用户模式下的查询相应时间、多用户模式下的查询吞吐量、在受控复杂多用户决策支持的工作负载下给定硬件 操作系统 数据处理系统软件配置的数据维护性能。TPC-DS 决策支持标准测试目的是为行业用户提供客观的性能数据。TPC-DS 决策支持一些新兴技术，例如 Hadoop、Spark等大数据解决方案。</p>","more":"<h2 id=\"TPC-DS-数据模型\"><a href=\"#TPC-DS-数据模型\" class=\"headerlink\" title=\"TPC-DS 数据模型\"></a>TPC-DS 数据模型</h2><p>TPC-DS 使用零售业务建模，schema 包括客户、订单、销售、退货和产品数据等业务信息。标准测试模拟了任何成熟的决策支持系统都必须具备的两个重要组成部分:  </p>\n<ol>\n<li>用户查询: 数据分析、挖掘，可将运营事实转换为商业智能  </li>\n<li>数据维护: 可将管理分析过程与其所依赖的可操作外部数据源同步  </li>\n</ol>\n<p>TPC-DS 使用星型、雪花模型建模，有多个维度和事实表。每个维度表都有主键，事实表使用外键与维度表主键关联。\n维度表可以分为以下类型:  </p>\n<ol>\n<li>静态：表内容在数据库加载期间仅加载一次，且数据不随着时间变化而变化。  </li>\n<li>历史记录：为单个业务实体创建多行来保存维度数据更改的历史记录，每行包括创建/修改时间的列。”项目”是历史记录维度信息的一个范例。  </li>\n<li>非历史记录：不保留对维度数据所做更改的历史记录。随着维度数据的更新，之前保留的信息将被覆盖。所有事实表都与维度信息的最新值关联。”客户信息”是非历史记录维度信息的一个范例。  </li>\n</ol>\n<p>TPC-DS 有 7 个事实表和 17 个维度表，平均每个表列数在 20 左右，且数据分布是真实而不均匀的，存在数据倾斜，与真实场景非常接近。因此TPC-DS成为客观衡量不同 Hadoop 版本以及 SQL on Hadoop 技术的最佳测试数据集。<br>事实表包含 商店、仓库和互联网三个销售渠道中每个渠道的建模产品销售和退货事实表；库存事实表。<br>维度表用于和每个销售渠道相关信息关联。  </p>\n<p>注： ER 图设计及事实表和维度表表结构详见官方文档(specification.pdf)    </p>\n<h2 id=\"TPC-DS-查询语句\"><a href=\"#TPC-DS-查询语句\" class=\"headerlink\" title=\"TPC-DS 查询语句\"></a>TPC-DS 查询语句</h2><p>由标准建模的查询语句(q1~q99)具有以下特征：  </p>\n<ol>\n<li>解决复杂的业务问题 </li>\n<li>使用各种访问模式、查询短语、运算符和约束  </li>\n<li>使用会在各种查询过程之间变化的查询参数  </li>\n</ol>\n<h2 id=\"下载-TP-CDS-kit\"><a href=\"#下载-TP-CDS-kit\" class=\"headerlink\" title=\"下载 TP-CDS kit\"></a>下载 TP-CDS kit</h2><p>TPCDS  kit 是生成测试数据的工具  </p>\n<p>方法一: 官网下载 <a href=\"http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp\" target=\"_blank\" rel=\"noopener\">http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp</a><br>    填写自己的邮箱后，官方会将下载链接发送到邮箱内，下载即可，文件包大小为 5MB 左右。<br>方法二: github 下载 <a href=\"https://github.com/gregrahn/tpcds-kit/tree/master\" target=\"_blank\" rel=\"noopener\">https://github.com/gregrahn/tpcds-kit/tree/master</a>  </p>\n<p>解压后目录数如下，  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tree -d v2.11.0rc2&#x2F;  </span><br><span class=\"line\"></span><br><span class=\"line\">v2.11.0rc2&#x2F;</span><br><span class=\"line\">├── answer_sets</span><br><span class=\"line\">├── query_templates</span><br><span class=\"line\">├── query_variants</span><br><span class=\"line\">├── specification</span><br><span class=\"line\">├── tests</span><br><span class=\"line\">└── tools</span><br></pre></td></tr></table></figure>\n<p>其中 specification为 tpcds 说明书，包括工具说明、数据模型、ER 图、度量信息等；query_templates 为查询语句模板；query_variants 为查询语句模板的参数；answer_sets 为查询结果集；tools 为数据集生成及查询语句生成工具集。Apache Spark sql/core module 内置了org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark 测试入口和 tpcds q1~q99 查询语句。因此只需将测试数据准备好即可。  </p>\n<h2 id=\"生成-TPC-DS\"><a href=\"#生成-TPC-DS\" class=\"headerlink\" title=\"生成 TPC-DS\"></a>生成 TPC-DS</h2><p>TPC-DS 可根据模拟场景产生以 GB 为最小粒度的数据集，常见的有 1TB、3TB、10TB、30TB、100TB等。  </p>\n<h3 id=\"编译-tpcds-kit\"><a href=\"#编译-tpcds-kit\" class=\"headerlink\" title=\"编译 tpcds kit\"></a>编译 tpcds kit</h3><p>需要在每个节点上编译tpcds kit  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install gcc make flex bison byacc git  </span><br><span class=\"line\">cd v2.11.0rc2&#x2F;tools  </span><br><span class=\"line\">make clean &amp;&amp; make OS&#x3D;LINUX</span><br></pre></td></tr></table></figure>\n<p>编译完成后，在 tools 目录下会多出两个可执行文件: dsdgen,dsqgen。dsdgen 用于生成测试数据，dsqgen 用于生成在不同 SQL 标准下的查询语句。  </p>\n<h3 id=\"生成数据\"><a href=\"#生成数据\" class=\"headerlink\" title=\"生成数据\"></a>生成数据</h3><p>databricks 为 Spark SQL 制作的数据生成工具 spark-sql-perf，似乎不如其宣传的那么好用。<br>spak-sql-perf github 地址:  <a href=\"https://github.com/databricks/spark-sql-perf\" target=\"_blank\" rel=\"noopener\">https://github.com/databricks/spark-sql-perf</a>    </p>\n<p>其原理是: 依次为每个表 并行调用 tpcds kit的 dsdgen 命令生成数据(本地磁盘、text格式、’|’分割、无压缩) 作为表数据集，即生成的所有数据为 DataFrame、每个并行调用 dsdgen 产生的数据为 DataFrame 的分区。 然后将此数据集转换(读取再存储)为 hdfs上的 parquet+snappy 文件，并创建为 Hive 的外部表。核心代码如下    </p>\n<p>调用 dsdgen 命令生成分区数据<br>[DSDGEN.scala]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class DSDGEN(dsdgenDir: String) extends DataGenerator &#123;</span><br><span class=\"line\">  val dsdgen &#x3D; s&quot;$dsdgenDir&#x2F;dsdgen&quot;</span><br><span class=\"line\">  def generate(sparkContext: SparkContext, name: String, partitions: Int, scaleFactor: String) &#x3D; &#123;</span><br><span class=\"line\">    val generatedData &#x3D; &#123;</span><br><span class=\"line\">      sparkContext.parallelize(1 to partitions, partitions).flatMap &#123; i &#x3D;&gt;</span><br><span class=\"line\">        val localToolsDir &#x3D; if (new java.io.File(dsdgen).exists) &#123;</span><br><span class=\"line\">          dsdgenDir</span><br><span class=\"line\">        &#125; else if (new java.io.File(s&quot;&#x2F;$dsdgen&quot;).exists) &#123;</span><br><span class=\"line\">          s&quot;&#x2F;$dsdgenDir&quot;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          sys.error(s&quot;Could not find dsdgen at $dsdgen or &#x2F;$dsdgen. Run install&quot;)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        val parallel &#x3D; if (partitions &gt; 1) s&quot;-parallel $partitions -child $i&quot; else &quot;&quot;</span><br><span class=\"line\">        val commands &#x3D; Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .&#x2F;dsdgen -table $name -filter Y -scale $scaleFactor -RNGSEED 100 $parallel&quot;)</span><br><span class=\"line\">        println(commands)</span><br><span class=\"line\">        BlockingLineStream(commands)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    generatedData.setName(s&quot;$name, sf&#x3D;$scaleFactor, strings&quot;)</span><br><span class=\"line\">    generatedData</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>spark-sql-perf 拼接的并行 dsdgen 命令类似于:  cd tpcds/v2.11.0rc2/tools &amp;&amp; ./dsdgen -table catalog_sales -filter Y -scale 3000 -RNGSEED 100 -parallel 2000 -child 8，scala=3000意为生成 3000G数据集，-table= catalog_sales 当前生成表，-filter=Y 意为输出数据到 stdout，-RNGSEED=100 意为 RNG seed 值，-parallel=2000 意为并行度 2000 生成数据，-child 为当前并行的标识 用于命名本地数据文件。  </p>\n<p>命令存在的问题(tpcds-kit 分支 master、v2.10、v2.5、v2.3均存在):  </p>\n<ol>\n<li>tpcds kit 的识别的 option 为 _filter,但是 spark-sql-perf 拼接的命令为 -filter    </li>\n<li>tpcds 数据模型之间有依赖关系，不能独立生成，表数据之间不是完全独立的，e.g. 销售表和退货表。 \n为 table = catalog_returns 生成数据时 dsdgen 会抛出错误 ERROR: Table catalog_returns is a child; it is populated during the build of its parent (e.g., catalog_sales builds catalog_returns)   </li>\n</ol>\n<p>尝试切换了 master、v2.10、 v2.5、 v2.3等分支 均无法解决上述问题。<br>针对上述，我采用的方案是 依旧以 spark-sql-perf 为主体(表结构、dsdgen 生成数据、数据转换 parquet 到 hdfs、创建 hive 外部表等)，修改点如下： </p>\n<ol>\n<li>修改数据并行生成命令   <pre><code>val commands = Seq(\n &quot;bash&quot;, &quot;-c&quot;,\n s&quot;cd $localToolsDir &amp;&amp; ./dsdgen -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel&quot;)</code></pre></li>\n<li>增加 并行数据生成后，把生成的数据上传到 hdfs 上的逻辑    </li>\n<li>修改表 DataFrame(RDD)生成逻辑，原有的逻辑为 dsdgen 命令包装为BlockingLineStream，修改为从 hdfs 上读取 $tableName*.dat 的逻辑</li>\n<li>text 文本转 parquet + snappy 逻辑不变、创建 hive 外部表关联到 parquet 文件的逻辑不变。  </li>\n</ol>\n<p>3TB 测试数据集文件上传到 hdfs 后，使用 spark 来转换数据为 parquet 并注册外部表程序如下<br>3TB 测试数据集使用 Spark 程序转换格式时，spark.executor.memory+spark.executor.memoryOverhead 不低于 10GB。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark-shell --master yarn-client --name &quot;transformation&quot; --queue high --executor-memory 8G --driver-memory 4G --num-executors 200 --conf &quot;spark.executor.memoryOverhead&#x3D;8G&quot; --jars ~&#x2F;tpcds&#x2F;spark-sql-perf_2.11-0.5.1-SNAPSHOT.jar</span><br><span class=\"line\"></span><br><span class=\"line\">import scala.sys.process._</span><br><span class=\"line\">import java.util.concurrent.LinkedBlockingQueue</span><br><span class=\"line\">import scala.collection.immutable.Stream</span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream</span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream.Spawn </span><br><span class=\"line\">import com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed</span><br><span class=\"line\">import com.databricks.spark.sql.perf.tpcds.TPCDSTables</span><br><span class=\"line\">import org.apache.spark.sql._</span><br><span class=\"line\"></span><br><span class=\"line\">val partitions &#x3D; 200</span><br><span class=\"line\">val scaleFactor &#x3D; &quot;3000&quot; </span><br><span class=\"line\">val localToolsDir &#x3D; &quot;&#x2F;home&#x2F;jiulong.zhu&#x2F;tpcds&#x2F;v2.11.0rc2&#x2F;tools&quot;</span><br><span class=\"line\">val rootDir: String &#x3D; &quot;hdfs:&#x2F;&#x2F;pub1&#x2F;user&#x2F;jiulong.zhu&#x2F;tpcds3t&quot; </span><br><span class=\"line\">val databaseName &#x3D; &quot;tpcds3t&quot;</span><br><span class=\"line\">val sqlContext &#x3D; new SQLContext(sc)</span><br><span class=\"line\">val format &#x3D; &quot;parquet&quot;</span><br><span class=\"line\">sc.parallelize(1 to partitions,partitions).flatMap &#123; i &#x3D;&gt;</span><br><span class=\"line\">        val parallel &#x3D; if (partitions &gt; 1) s&quot;-parallel $partitions -child $i&quot; else &quot;&quot;</span><br><span class=\"line\">        val commands &#x3D; Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .&#x2F;dsdgen -dir data&#x2F; -_filter Y -scale $scaleFactor -RNGSEED 100 $parallel -force Y&quot;)</span><br><span class=\"line\">        println(commands)</span><br><span class=\"line\">        val streamed &#x3D; com.databricks.spark.sql.perf.BlockingLineStream.BlockingStreamed[String](true)</span><br><span class=\"line\">        val process &#x3D; commands.run(BasicIO(false, streamed.process, None))</span><br><span class=\"line\">        Spawn(streamed.done(process.exitValue()))</span><br><span class=\"line\">        val hdfsCommand &#x3D;  Seq(</span><br><span class=\"line\">          &quot;bash&quot;, &quot;-c&quot;,</span><br><span class=\"line\">          s&quot;cd $localToolsDir &amp;&amp; .hadoop fs -put data&#x2F;*.dat temp&#x2F;&quot;)</span><br><span class=\"line\">          val putProcess &#x3D; hdfsCommand.run(BasicIO(false, streamed.process, None))</span><br><span class=\"line\">        Spawn(streamed.done(putProcess.exitValue()))          </span><br><span class=\"line\">        Seq(streamed.stream())</span><br><span class=\"line\">      &#125;.collect()</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;&#x2F; Run:</span><br><span class=\"line\">val tables &#x3D; new TPCDSTables(sqlContext,</span><br><span class=\"line\">    dsdgenDir &#x3D; localToolsDir, &#x2F;&#x2F; location of dsdgen</span><br><span class=\"line\">    scaleFactor &#x3D; scaleFactor,</span><br><span class=\"line\">    useDoubleForDecimal &#x3D; false, &#x2F;&#x2F; true to replace DecimalType with DoubleType</span><br><span class=\"line\">    useStringForDate &#x3D; false) &#x2F;&#x2F; true to replace DateType with StringType</span><br><span class=\"line\"></span><br><span class=\"line\">tables.genData(</span><br><span class=\"line\">    location &#x3D; rootDir,</span><br><span class=\"line\">    format &#x3D; format,</span><br><span class=\"line\">    overwrite &#x3D; true, &#x2F;&#x2F; overwrite the data that is already there</span><br><span class=\"line\">    partitionTables &#x3D; true, &#x2F;&#x2F; create the partitioned fact tables</span><br><span class=\"line\">    clusterByPartitionColumns &#x3D; true, &#x2F;&#x2F; shuffle to get partitions coalesced into single files.</span><br><span class=\"line\">    filterOutNullPartitionValues &#x3D; false, &#x2F;&#x2F; true to filter out the partition with NULL key value</span><br><span class=\"line\">    tableFilter &#x3D; &quot;&quot;, &#x2F;&#x2F; &quot;&quot; means generate all tables</span><br><span class=\"line\">    numPartitions &#x3D; 2000) &#x2F;&#x2F; how many dsdgen partitions to run - number of input tasks.</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;&#x2F; Create the specified database</span><br><span class=\"line\">sql(s&quot;create database if not exists $databaseName&quot;)</span><br><span class=\"line\">&#x2F;&#x2F; Create metastore tables in a specified database for your data.</span><br><span class=\"line\">&#x2F;&#x2F; Once tables are created, the current database will be switched to the specified database.</span><br><span class=\"line\">tables.createExternalTables(rootDir, &quot;parquet&quot;, databaseName, overwrite &#x3D; true, discoverPartitions &#x3D; true)</span><br></pre></td></tr></table></figure>\n\n<p>tpcds 3TB 数据集在 text无压缩格式下为 2.7TB，转为 parquet snappy 格式后约为 900GB，orc snappy 格式约为 800GB。</p>\n<h2 id=\"测试-SparkSQL\"><a href=\"#测试-SparkSQL\" class=\"headerlink\" title=\"测试 SparkSQL\"></a>测试 SparkSQL</h2><p>nohup sh TPCDSBench.sh &amp;<br>TPCDSBench.sh  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export SPARK_HOME&#x3D;&#x2F;home&#x2F;jiulong.zhu&#x2F;spark-2.3.0-bin-hadoop2.7</span><br><span class=\"line\">MASTER&#x3D;yarn-client</span><br><span class=\"line\">APP_NAME&#x3D;AE_ENABLED</span><br><span class=\"line\">DRIVER_MEMORY&#x3D;8G</span><br><span class=\"line\">EXECUTOR_MEMORY&#x3D;10G</span><br><span class=\"line\">NUM_EXECUTORS&#x3D;100</span><br><span class=\"line\">EXECUTOR_CORES&#x3D;5</span><br><span class=\"line\">EX_JARS&#x3D;&#x2F;home&#x2F;jiulong.zhu&#x2F;tpcds&#x2F;spark-sql_2.11-2.3.1-SNAPSHOT-tests.jar</span><br><span class=\"line\">AE_ENABLE&#x3D;false</span><br><span class=\"line\"></span><br><span class=\"line\">spark-submit --master $MASTER --name $&#123;APP_NAME&#125;_$&#123;AE_ENABLE&#125; \\</span><br><span class=\"line\">--conf &quot;spark.executor.instances&#x3D;$NUM_EXECUTORS&quot; \\</span><br><span class=\"line\">--conf &quot;spark.executor.cores&#x3D;$EXECUTOR_CORES&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.crossJoin.enabled&#x3D;true&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.autoBroadcastJoinThreshold&#x3D;21971520&quot; \\</span><br><span class=\"line\">--conf &quot;spark.default.parallelism&#x3D;1000&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.shuffle.partitions&#x3D;1000&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.parquet.compression.codec&#x3D;snappy&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.adaptive.enabled&#x3D;$&#123;AE_ENABLE&#125;&quot; \\</span><br><span class=\"line\">--conf &quot;spark.dynamicAllocation.enabled&#x3D;false&quot; \\</span><br><span class=\"line\">--conf &quot;spark.executor.memoryOverhead&#x3D;10G&quot; \\</span><br><span class=\"line\">--conf &quot;spark.sql.adaptive.minNumPostShufflePartitions&#x3D;1000&quot; \\</span><br><span class=\"line\">--queue high \\</span><br><span class=\"line\">--driver-memory $DRIVER_MEMORY --executor-memory $EXECUTOR_MEMORY \\</span><br><span class=\"line\">--class org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark $EX_JARS \\</span><br><span class=\"line\">--data-location hdfs:&#x2F;&#x2F;pub1&#x2F;user&#x2F;jiulong.zhu&#x2F;tpcds3t</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"附录及参考\"><a href=\"#附录及参考\" class=\"headerlink\" title=\"附录及参考\"></a>附录及参考</h2><ol>\n<li>从大数据平台二次开发、性能优化方面，准备一套合适的性能测试数据、通用查询语句、工具和测试流程对于性能提升量化和成果量化有非比寻常的意义。    </li>\n<li>可以试试 IBM 的 spark-tpc-ds-performance-test(<a href=\"https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套\" target=\"_blank\" rel=\"noopener\">https://github.com/IBM/spark-tpc-ds-performance-test)，但是仍旧需要自备一套</a> TPCDS 测试数据集。   </li>\n<li>参考:<br>　　<a href=\"https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds\" target=\"_blank\" rel=\"noopener\">https://databricks.com/session/spark-sql-2-0-experiences-using-tpc-ds</a><br>　　<a href=\"https://github.com/databricks/spark-sql-perf\" target=\"_blank\" rel=\"noopener\">https://github.com/databricks/spark-sql-perf</a>   </li>\n</ol>"},{"layout":"post","title":"Spark Executor 内存动态扩展方案","subtitle":"内存动态扩展","date":"2020-01-09T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n## 概述 \n\nSpark Application 提交之后 Executor 内存被固定，业务环境上的长流程或长时处理任务经常由于 Executor 内存不足失败，导致人工干预、流程延迟且 Application 级恢复成本比较高。这种情况下，希望有套机制能够在 Executor 失败后再申请更大内存的 Executor 、对 Task 层面有更好的容错，在一定程度上减少 Task 由内存不足失败导致的 Application 失败恢复和人工干预成本。  \n\n<!-- more -->\n\n前置条件:   \n1.  Spark On Yarn  \n2. 开启动态资源分配(spark.dynamicAllocation.enabled)  \n\n## 调度流程 \n\n```mermaid\nsequenceDiagram\n\nRDD#action ->> SparkContext : 1.runJob(rdd,func,partitions,resultHandler)\nSparkContext ->> DAGScheduler : 2. submitJob(rdd,func,partitions,resultHandler)\nDAGScheduler ->> DAGScheduler : 3. JobSubmitted && createResultStage && getOrCreateParentStages && submitStages && submitMissingTasks\nDAGScheduler ->> TaskScheduler : 4.submitTasks(createTaskSet)\nTaskScheduler ->> SchedulerBackEnd : 5.reviveOffers\n\nloop period & event\n\tSchedulerBackEnd ->> TaskScheduler : 6.makeOffers && resourceOffers\n\tDriver ->> Executor : 7.LaunchTask\nend\nSchedulerBackEnd -->> - RDD#action : 8. statusUpdate && callBack\n```\n\n## 主要思路  \n\n1. Container(或 Executor) 由于内存超限被杀死的原因有两个: 物理内存(-104,137)/虚拟内存(-103,137)超限被 Yarn 资源监控线程杀死；堆内存 OOM 导致 JVM 进程被 kill，退出码是 143。退出码的不同决定了要增加的内存配置不同: -104,-103 和 137需要增加 Container 整体内存；143 需要增加堆内存。(YarnAllocator#allocateResource 拿到 Yarn 新分配的 container 和正常及异常退出的 container)   \n2. 内存拓展的 Executor 启动参数修改: Executor 内存提升次数(默认0)，堆内存，全内存。主要用于 Executor 向 Driver 注册时标识自身的特征，该特征可以使 TaskScheduler将 OOM Task 调度到该 Executor。(YarnAllocator#allocator 拿到 Yarn 新分配的 container,ExecutorRunnable,CoarseGrainedExecutorBackend,RegisterExecutor 数据结构修改)    \n3. 内存拓展的 Executor 注册/注销 ->> 额外维护的可运行 OOM Task 的 Executor 列表更新，ExecutorAdd、ExecutorLost、KillExecutor 等。(CoarseGrainedSchedulerBackEnd,ExecutorId和[提升次数,堆内存,全内存]的映射,提升次数和 ExecutorId 集合的映射)      \n4. 普通 Task, OOM Task 失败的处理 ->> OOM Task失败则必须调度到内存拓展过的 Executor，已经在内存拓展过的 Executor 失败的 OOM Task 后要调度到更大或当前最大内存的 Executor，非 OOM 失败的 task 走现有容错调度(TaskSchedulerImpl)  \n5. 大内存 ExecutorLost ->> 根据维护的ExecutorId 和[提升次数,堆内存,全内存]映射及 Container 退出码及内存拓展算法来决定下一次申请的 ContainerResource和 JVM 启动的 XMX    \n6. 因动态资源分配功能超时退出 或 Yarn 抢占导致的 ExecutorLost在 YarnAllocator 使用大根堆或 最大值来记录资源信息,在下一次申请时不通过内存提升算法。   \n7. OOM Task 容错: OOM Task 加入到一个特定的 pending 队列,以和普通错误 Task 做区分。TaskScheduler/ExecutorAdd/TaskCompletion 触发的 reviveOffer时,优先从特定 pending 队列调度 判断 ExecutorId 能否可能支撑该 Task(原所属失败的 Executor 提升内存次数必须小于当前 ExecutorId 提升内存次数)。若能则调度,若不能则不调度该任务,调度普通 pending 队列。  \n8. 动态资源分配使用 running tasks和 pending tasks 及 spark.task.cores 来推测需要的总 Executor 数量。在系统的某时刻，可能有 OOM Task 但是却无内存拓展过的 Executor,推测可能也不需要新的 Executor。即: 当有 OOM Task 却无拓展内存的 Executor 时,需要在 CoarseGrainedSchedulerBackend 通过 ExecutorAllocationClient 的 requestTotalExecutor/requestExecutor 接口来触发申请 Executor。数量为 Max{当前 Executor数量/5,1},取 Max 是为了增加本地化可能性，多申请的资源由动态资源分配来管理。    \n9. YarnAllocator 维护的状态需要有复原功能: OOM 时下一批次申请的 Executor 都是拓展过内存的，随后复原，以适配动态资源分配功能。YarnAllocator 取消堆积在 RM 的 ResourceRequest 时,优先取消没有拓展过内存的,不足则取消拓展过内存的 ResourceRequest。  \n\n  \n## Task 失败处理线  \n\n1. DriverEndPoint 首先接收到任务 StatusUpdate 事件转交由 CoarseGrainedSchedulerBackEnd 处理  \n2. CoarseGrainedSchedulerBackEnd 交由 TaskSchedulerImpl 处理 StatusUpdate 事件  \n3. TaskSchedulerImpl 解析 StatusUpdate 事件的任务状态,并判断其成功/失败，若失败则需要交由 TaskSetManager 维护状态，加入到特定的失败堆积队列  \n4. a.若此时全局无大内存 Executor，则需要绕过动态资源分配主动申请大内存 Executor，但此时全局无大内存 Executor 故暂不处理当前 OOM Task。 b.若有则阶梯式分配 OOM Task: 判断原有运行该 Task 的 Executor 提升次数，将失败任务分配到更高/最高提升次数的 Executor 上。若原有运行 Executor 内存已经最大，则依据加内存次数抛出异常终止 Application 或主动申请更大内存 Executor，暂不处理当前 OOM Task([3G,4G,5G] 3G上失败任务可指派给最高阶梯 5G,也可以指派给较高阶梯 4G)。  \n5. 对某个 Executor 分配任务(TaskCompletion/ExecutorAdd)时，对 Executor 进行判断，在内存满足的情况下，优先分配特定失败堆积队列的任务，再指派 TaskSetManager 中普通任务或 pending 任务。 \n\n## Executor 失败处理线    \n\n基于 AM 和 RM 资源申请模式，AM updateResourceRequest 更新最新 ResourceRequest 给 RM，RM 响应 AM 的 allocateResource 请求通知新分配的 container,正常异常退出的 container。由此 AM 在新分配的 container 上启动新的 Executor，判断退出的 container 的退出码确定退出原因   \n\n1. YarnAllocator 判断容器退出原因，若为 Yarn 抢占或 AM 主动杀死则记录到最大堆中，若为内存问题则使用内存拓展算法计算出下一次 updateResourceRequest 时 Container 配置、 Jvm 配置和内存拓展次数    \n2. 动态资源分配或主动申请容器时 判断是否需要提升内存，若是则通过 updateResourceRequest 来修改;若否则使用大跟堆中的配置  \n3. 动态资源分配当当前需要的 Executor 总量低于已有量时，取消 ResourceRequest 优先取消普通内存配置的 ResourceRequest，不足则取消内存拓展过的 Executor \n\n## 内存增加算法\n  \n1. 需要考虑 Yarn 规整(申请的 Container Resource 会被规整到 yarn.scheduler.minimum-allocation-mb 的整数倍)； \n2. 增加 Container 整体资源量时需要按比例分配(堆内:堆外默认为 10:1,可适当修改)；  \n3. 从低内存跃迁至高内存的次数不能太大,即提升快然后迅速收敛以降低中间的失败尝试次数，可选择初始高增量快速降低的方式(e.g. 对数或固定系数，第一次提升0.5,第二次0.3,第三次 0.1,超出最大允许拓展次数后如果OOM 则快速抛出异常终止任务人为干预)。  \n\n## 附录 \n\n\nDAGScheduler 处理的事件类型:    \n  \n* JobSubmitted   \n* JobCancelled  \n*  JobGroupCancelled  \n* AllJobsCancelled  \n* MapStageSubmitted   \n* ResubmitFailedStages  \n* StageCancelled  \n* ExecutorAdded  \n* ExecutorLost  \n* WorkerRemoved  \n* TaskSetFailed  \n* BeginEvent   \n* CompletionEvent \n* SpeculativeTaskSubmitted  \n* GettingResultEvent     \n      \nCoarseGrainedSchedulerBackEnd(Driver) 处理的事件类型:   \n\n* RegisterExecutor  \n* StopDriver  \n* StopExecutors  \n* RemoveWorker  \n* RetrieveSparkAppConfig    \n* StatusUpdate  \n* ReviveOffers  \n* KillTask  \n* KillExecutorsOnHost  \n* UpdateDelegationTokens  \n* RemoveExecutor  \n \n","source":"_posts/2020-01-10-Spark Executor 内存动态拓展方案.md","raw":"---\nlayout:     post\ntitle:     Spark Executor 内存动态扩展方案\nsubtitle:  内存动态扩展\ndate:       2020-01-10\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n    - 方案\n---\n\n## 概述 \n\nSpark Application 提交之后 Executor 内存被固定，业务环境上的长流程或长时处理任务经常由于 Executor 内存不足失败，导致人工干预、流程延迟且 Application 级恢复成本比较高。这种情况下，希望有套机制能够在 Executor 失败后再申请更大内存的 Executor 、对 Task 层面有更好的容错，在一定程度上减少 Task 由内存不足失败导致的 Application 失败恢复和人工干预成本。  \n\n<!-- more -->\n\n前置条件:   \n1.  Spark On Yarn  \n2. 开启动态资源分配(spark.dynamicAllocation.enabled)  \n\n## 调度流程 \n\n```mermaid\nsequenceDiagram\n\nRDD#action ->> SparkContext : 1.runJob(rdd,func,partitions,resultHandler)\nSparkContext ->> DAGScheduler : 2. submitJob(rdd,func,partitions,resultHandler)\nDAGScheduler ->> DAGScheduler : 3. JobSubmitted && createResultStage && getOrCreateParentStages && submitStages && submitMissingTasks\nDAGScheduler ->> TaskScheduler : 4.submitTasks(createTaskSet)\nTaskScheduler ->> SchedulerBackEnd : 5.reviveOffers\n\nloop period & event\n\tSchedulerBackEnd ->> TaskScheduler : 6.makeOffers && resourceOffers\n\tDriver ->> Executor : 7.LaunchTask\nend\nSchedulerBackEnd -->> - RDD#action : 8. statusUpdate && callBack\n```\n\n## 主要思路  \n\n1. Container(或 Executor) 由于内存超限被杀死的原因有两个: 物理内存(-104,137)/虚拟内存(-103,137)超限被 Yarn 资源监控线程杀死；堆内存 OOM 导致 JVM 进程被 kill，退出码是 143。退出码的不同决定了要增加的内存配置不同: -104,-103 和 137需要增加 Container 整体内存；143 需要增加堆内存。(YarnAllocator#allocateResource 拿到 Yarn 新分配的 container 和正常及异常退出的 container)   \n2. 内存拓展的 Executor 启动参数修改: Executor 内存提升次数(默认0)，堆内存，全内存。主要用于 Executor 向 Driver 注册时标识自身的特征，该特征可以使 TaskScheduler将 OOM Task 调度到该 Executor。(YarnAllocator#allocator 拿到 Yarn 新分配的 container,ExecutorRunnable,CoarseGrainedExecutorBackend,RegisterExecutor 数据结构修改)    \n3. 内存拓展的 Executor 注册/注销 ->> 额外维护的可运行 OOM Task 的 Executor 列表更新，ExecutorAdd、ExecutorLost、KillExecutor 等。(CoarseGrainedSchedulerBackEnd,ExecutorId和[提升次数,堆内存,全内存]的映射,提升次数和 ExecutorId 集合的映射)      \n4. 普通 Task, OOM Task 失败的处理 ->> OOM Task失败则必须调度到内存拓展过的 Executor，已经在内存拓展过的 Executor 失败的 OOM Task 后要调度到更大或当前最大内存的 Executor，非 OOM 失败的 task 走现有容错调度(TaskSchedulerImpl)  \n5. 大内存 ExecutorLost ->> 根据维护的ExecutorId 和[提升次数,堆内存,全内存]映射及 Container 退出码及内存拓展算法来决定下一次申请的 ContainerResource和 JVM 启动的 XMX    \n6. 因动态资源分配功能超时退出 或 Yarn 抢占导致的 ExecutorLost在 YarnAllocator 使用大根堆或 最大值来记录资源信息,在下一次申请时不通过内存提升算法。   \n7. OOM Task 容错: OOM Task 加入到一个特定的 pending 队列,以和普通错误 Task 做区分。TaskScheduler/ExecutorAdd/TaskCompletion 触发的 reviveOffer时,优先从特定 pending 队列调度 判断 ExecutorId 能否可能支撑该 Task(原所属失败的 Executor 提升内存次数必须小于当前 ExecutorId 提升内存次数)。若能则调度,若不能则不调度该任务,调度普通 pending 队列。  \n8. 动态资源分配使用 running tasks和 pending tasks 及 spark.task.cores 来推测需要的总 Executor 数量。在系统的某时刻，可能有 OOM Task 但是却无内存拓展过的 Executor,推测可能也不需要新的 Executor。即: 当有 OOM Task 却无拓展内存的 Executor 时,需要在 CoarseGrainedSchedulerBackend 通过 ExecutorAllocationClient 的 requestTotalExecutor/requestExecutor 接口来触发申请 Executor。数量为 Max{当前 Executor数量/5,1},取 Max 是为了增加本地化可能性，多申请的资源由动态资源分配来管理。    \n9. YarnAllocator 维护的状态需要有复原功能: OOM 时下一批次申请的 Executor 都是拓展过内存的，随后复原，以适配动态资源分配功能。YarnAllocator 取消堆积在 RM 的 ResourceRequest 时,优先取消没有拓展过内存的,不足则取消拓展过内存的 ResourceRequest。  \n\n  \n## Task 失败处理线  \n\n1. DriverEndPoint 首先接收到任务 StatusUpdate 事件转交由 CoarseGrainedSchedulerBackEnd 处理  \n2. CoarseGrainedSchedulerBackEnd 交由 TaskSchedulerImpl 处理 StatusUpdate 事件  \n3. TaskSchedulerImpl 解析 StatusUpdate 事件的任务状态,并判断其成功/失败，若失败则需要交由 TaskSetManager 维护状态，加入到特定的失败堆积队列  \n4. a.若此时全局无大内存 Executor，则需要绕过动态资源分配主动申请大内存 Executor，但此时全局无大内存 Executor 故暂不处理当前 OOM Task。 b.若有则阶梯式分配 OOM Task: 判断原有运行该 Task 的 Executor 提升次数，将失败任务分配到更高/最高提升次数的 Executor 上。若原有运行 Executor 内存已经最大，则依据加内存次数抛出异常终止 Application 或主动申请更大内存 Executor，暂不处理当前 OOM Task([3G,4G,5G] 3G上失败任务可指派给最高阶梯 5G,也可以指派给较高阶梯 4G)。  \n5. 对某个 Executor 分配任务(TaskCompletion/ExecutorAdd)时，对 Executor 进行判断，在内存满足的情况下，优先分配特定失败堆积队列的任务，再指派 TaskSetManager 中普通任务或 pending 任务。 \n\n## Executor 失败处理线    \n\n基于 AM 和 RM 资源申请模式，AM updateResourceRequest 更新最新 ResourceRequest 给 RM，RM 响应 AM 的 allocateResource 请求通知新分配的 container,正常异常退出的 container。由此 AM 在新分配的 container 上启动新的 Executor，判断退出的 container 的退出码确定退出原因   \n\n1. YarnAllocator 判断容器退出原因，若为 Yarn 抢占或 AM 主动杀死则记录到最大堆中，若为内存问题则使用内存拓展算法计算出下一次 updateResourceRequest 时 Container 配置、 Jvm 配置和内存拓展次数    \n2. 动态资源分配或主动申请容器时 判断是否需要提升内存，若是则通过 updateResourceRequest 来修改;若否则使用大跟堆中的配置  \n3. 动态资源分配当当前需要的 Executor 总量低于已有量时，取消 ResourceRequest 优先取消普通内存配置的 ResourceRequest，不足则取消内存拓展过的 Executor \n\n## 内存增加算法\n  \n1. 需要考虑 Yarn 规整(申请的 Container Resource 会被规整到 yarn.scheduler.minimum-allocation-mb 的整数倍)； \n2. 增加 Container 整体资源量时需要按比例分配(堆内:堆外默认为 10:1,可适当修改)；  \n3. 从低内存跃迁至高内存的次数不能太大,即提升快然后迅速收敛以降低中间的失败尝试次数，可选择初始高增量快速降低的方式(e.g. 对数或固定系数，第一次提升0.5,第二次0.3,第三次 0.1,超出最大允许拓展次数后如果OOM 则快速抛出异常终止任务人为干预)。  \n\n## 附录 \n\n\nDAGScheduler 处理的事件类型:    \n  \n* JobSubmitted   \n* JobCancelled  \n*  JobGroupCancelled  \n* AllJobsCancelled  \n* MapStageSubmitted   \n* ResubmitFailedStages  \n* StageCancelled  \n* ExecutorAdded  \n* ExecutorLost  \n* WorkerRemoved  \n* TaskSetFailed  \n* BeginEvent   \n* CompletionEvent \n* SpeculativeTaskSubmitted  \n* GettingResultEvent     \n      \nCoarseGrainedSchedulerBackEnd(Driver) 处理的事件类型:   \n\n* RegisterExecutor  \n* StopDriver  \n* StopExecutors  \n* RemoveWorker  \n* RetrieveSparkAppConfig    \n* StatusUpdate  \n* ReviveOffers  \n* KillTask  \n* KillExecutorsOnHost  \n* UpdateDelegationTokens  \n* RemoveExecutor  \n \n","slug":"2020-01-10-Spark Executor 内存动态拓展方案","published":1,"updated":"2020-07-03T11:20:25.462Z","comments":1,"photos":[],"link":"","_id":"cke2rms5o000ny7c77rdx2p6k","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Spark Application 提交之后 Executor 内存被固定，业务环境上的长流程或长时处理任务经常由于 Executor 内存不足失败，导致人工干预、流程延迟且 Application 级恢复成本比较高。这种情况下，希望有套机制能够在 Executor 失败后再申请更大内存的 Executor 、对 Task 层面有更好的容错，在一定程度上减少 Task 由内存不足失败导致的 Application 失败恢复和人工干预成本。  </p>\n<a id=\"more\"></a>\n\n<p>前置条件:   </p>\n<ol>\n<li>Spark On Yarn  </li>\n<li>开启动态资源分配(spark.dynamicAllocation.enabled)  </li>\n</ol>\n<h2 id=\"调度流程\"><a href=\"#调度流程\" class=\"headerlink\" title=\"调度流程\"></a>调度流程</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sequenceDiagram</span><br><span class=\"line\"></span><br><span class=\"line\">RDD#action -&gt;&gt; SparkContext : 1.runJob(rdd,func,partitions,resultHandler)</span><br><span class=\"line\">SparkContext -&gt;&gt; DAGScheduler : 2. submitJob(rdd,func,partitions,resultHandler)</span><br><span class=\"line\">DAGScheduler -&gt;&gt; DAGScheduler : 3. JobSubmitted &amp;&amp; createResultStage &amp;&amp; getOrCreateParentStages &amp;&amp; submitStages &amp;&amp; submitMissingTasks</span><br><span class=\"line\">DAGScheduler -&gt;&gt; TaskScheduler : 4.submitTasks(createTaskSet)</span><br><span class=\"line\">TaskScheduler -&gt;&gt; SchedulerBackEnd : 5.reviveOffers</span><br><span class=\"line\"></span><br><span class=\"line\">loop period &amp; event</span><br><span class=\"line\">\tSchedulerBackEnd -&gt;&gt; TaskScheduler : 6.makeOffers &amp;&amp; resourceOffers</span><br><span class=\"line\">\tDriver -&gt;&gt; Executor : 7.LaunchTask</span><br><span class=\"line\">end</span><br><span class=\"line\">SchedulerBackEnd --&gt;&gt; - RDD#action : 8. statusUpdate &amp;&amp; callBack</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"主要思路\"><a href=\"#主要思路\" class=\"headerlink\" title=\"主要思路\"></a>主要思路</h2><ol>\n<li>Container(或 Executor) 由于内存超限被杀死的原因有两个: 物理内存(-104,137)/虚拟内存(-103,137)超限被 Yarn 资源监控线程杀死；堆内存 OOM 导致 JVM 进程被 kill，退出码是 143。退出码的不同决定了要增加的内存配置不同: -104,-103 和 137需要增加 Container 整体内存；143 需要增加堆内存。(YarnAllocator#allocateResource 拿到 Yarn 新分配的 container 和正常及异常退出的 container)   </li>\n<li>内存拓展的 Executor 启动参数修改: Executor 内存提升次数(默认0)，堆内存，全内存。主要用于 Executor 向 Driver 注册时标识自身的特征，该特征可以使 TaskScheduler将 OOM Task 调度到该 Executor。(YarnAllocator#allocator 拿到 Yarn 新分配的 container,ExecutorRunnable,CoarseGrainedExecutorBackend,RegisterExecutor 数据结构修改)    </li>\n<li>内存拓展的 Executor 注册/注销 -&gt;&gt; 额外维护的可运行 OOM Task 的 Executor 列表更新，ExecutorAdd、ExecutorLost、KillExecutor 等。(CoarseGrainedSchedulerBackEnd,ExecutorId和[提升次数,堆内存,全内存]的映射,提升次数和 ExecutorId 集合的映射)      </li>\n<li>普通 Task, OOM Task 失败的处理 -&gt;&gt; OOM Task失败则必须调度到内存拓展过的 Executor，已经在内存拓展过的 Executor 失败的 OOM Task 后要调度到更大或当前最大内存的 Executor，非 OOM 失败的 task 走现有容错调度(TaskSchedulerImpl)  </li>\n<li>大内存 ExecutorLost -&gt;&gt; 根据维护的ExecutorId 和[提升次数,堆内存,全内存]映射及 Container 退出码及内存拓展算法来决定下一次申请的 ContainerResource和 JVM 启动的 XMX    </li>\n<li>因动态资源分配功能超时退出 或 Yarn 抢占导致的 ExecutorLost在 YarnAllocator 使用大根堆或 最大值来记录资源信息,在下一次申请时不通过内存提升算法。   </li>\n<li>OOM Task 容错: OOM Task 加入到一个特定的 pending 队列,以和普通错误 Task 做区分。TaskScheduler/ExecutorAdd/TaskCompletion 触发的 reviveOffer时,优先从特定 pending 队列调度 判断 ExecutorId 能否可能支撑该 Task(原所属失败的 Executor 提升内存次数必须小于当前 ExecutorId 提升内存次数)。若能则调度,若不能则不调度该任务,调度普通 pending 队列。  </li>\n<li>动态资源分配使用 running tasks和 pending tasks 及 spark.task.cores 来推测需要的总 Executor 数量。在系统的某时刻，可能有 OOM Task 但是却无内存拓展过的 Executor,推测可能也不需要新的 Executor。即: 当有 OOM Task 却无拓展内存的 Executor 时,需要在 CoarseGrainedSchedulerBackend 通过 ExecutorAllocationClient 的 requestTotalExecutor/requestExecutor 接口来触发申请 Executor。数量为 Max{当前 Executor数量/5,1},取 Max 是为了增加本地化可能性，多申请的资源由动态资源分配来管理。    </li>\n<li>YarnAllocator 维护的状态需要有复原功能: OOM 时下一批次申请的 Executor 都是拓展过内存的，随后复原，以适配动态资源分配功能。YarnAllocator 取消堆积在 RM 的 ResourceRequest 时,优先取消没有拓展过内存的,不足则取消拓展过内存的 ResourceRequest。  </li>\n</ol>\n<h2 id=\"Task-失败处理线\"><a href=\"#Task-失败处理线\" class=\"headerlink\" title=\"Task 失败处理线\"></a>Task 失败处理线</h2><ol>\n<li>DriverEndPoint 首先接收到任务 StatusUpdate 事件转交由 CoarseGrainedSchedulerBackEnd 处理  </li>\n<li>CoarseGrainedSchedulerBackEnd 交由 TaskSchedulerImpl 处理 StatusUpdate 事件  </li>\n<li>TaskSchedulerImpl 解析 StatusUpdate 事件的任务状态,并判断其成功/失败，若失败则需要交由 TaskSetManager 维护状态，加入到特定的失败堆积队列  </li>\n<li>a.若此时全局无大内存 Executor，则需要绕过动态资源分配主动申请大内存 Executor，但此时全局无大内存 Executor 故暂不处理当前 OOM Task。 b.若有则阶梯式分配 OOM Task: 判断原有运行该 Task 的 Executor 提升次数，将失败任务分配到更高/最高提升次数的 Executor 上。若原有运行 Executor 内存已经最大，则依据加内存次数抛出异常终止 Application 或主动申请更大内存 Executor，暂不处理当前 OOM Task([3G,4G,5G] 3G上失败任务可指派给最高阶梯 5G,也可以指派给较高阶梯 4G)。  </li>\n<li>对某个 Executor 分配任务(TaskCompletion/ExecutorAdd)时，对 Executor 进行判断，在内存满足的情况下，优先分配特定失败堆积队列的任务，再指派 TaskSetManager 中普通任务或 pending 任务。 </li>\n</ol>\n<h2 id=\"Executor-失败处理线\"><a href=\"#Executor-失败处理线\" class=\"headerlink\" title=\"Executor 失败处理线\"></a>Executor 失败处理线</h2><p>基于 AM 和 RM 资源申请模式，AM updateResourceRequest 更新最新 ResourceRequest 给 RM，RM 响应 AM 的 allocateResource 请求通知新分配的 container,正常异常退出的 container。由此 AM 在新分配的 container 上启动新的 Executor，判断退出的 container 的退出码确定退出原因   </p>\n<ol>\n<li>YarnAllocator 判断容器退出原因，若为 Yarn 抢占或 AM 主动杀死则记录到最大堆中，若为内存问题则使用内存拓展算法计算出下一次 updateResourceRequest 时 Container 配置、 Jvm 配置和内存拓展次数    </li>\n<li>动态资源分配或主动申请容器时 判断是否需要提升内存，若是则通过 updateResourceRequest 来修改;若否则使用大跟堆中的配置  </li>\n<li>动态资源分配当当前需要的 Executor 总量低于已有量时，取消 ResourceRequest 优先取消普通内存配置的 ResourceRequest，不足则取消内存拓展过的 Executor </li>\n</ol>\n<h2 id=\"内存增加算法\"><a href=\"#内存增加算法\" class=\"headerlink\" title=\"内存增加算法\"></a>内存增加算法</h2><ol>\n<li>需要考虑 Yarn 规整(申请的 Container Resource 会被规整到 yarn.scheduler.minimum-allocation-mb 的整数倍)； </li>\n<li>增加 Container 整体资源量时需要按比例分配(堆内:堆外默认为 10:1,可适当修改)；  </li>\n<li>从低内存跃迁至高内存的次数不能太大,即提升快然后迅速收敛以降低中间的失败尝试次数，可选择初始高增量快速降低的方式(e.g. 对数或固定系数，第一次提升0.5,第二次0.3,第三次 0.1,超出最大允许拓展次数后如果OOM 则快速抛出异常终止任务人为干预)。  </li>\n</ol>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>DAGScheduler 处理的事件类型:    </p>\n<ul>\n<li>JobSubmitted   </li>\n<li>JobCancelled  </li>\n<li>JobGroupCancelled  </li>\n<li>AllJobsCancelled  </li>\n<li>MapStageSubmitted   </li>\n<li>ResubmitFailedStages  </li>\n<li>StageCancelled  </li>\n<li>ExecutorAdded  </li>\n<li>ExecutorLost  </li>\n<li>WorkerRemoved  </li>\n<li>TaskSetFailed  </li>\n<li>BeginEvent   </li>\n<li>CompletionEvent </li>\n<li>SpeculativeTaskSubmitted  </li>\n<li>GettingResultEvent     </li>\n</ul>\n<p>CoarseGrainedSchedulerBackEnd(Driver) 处理的事件类型:   </p>\n<ul>\n<li>RegisterExecutor  </li>\n<li>StopDriver  </li>\n<li>StopExecutors  </li>\n<li>RemoveWorker  </li>\n<li>RetrieveSparkAppConfig    </li>\n<li>StatusUpdate  </li>\n<li>ReviveOffers  </li>\n<li>KillTask  </li>\n<li>KillExecutorsOnHost  </li>\n<li>UpdateDelegationTokens  </li>\n<li>RemoveExecutor  </li>\n</ul>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>Spark Application 提交之后 Executor 内存被固定，业务环境上的长流程或长时处理任务经常由于 Executor 内存不足失败，导致人工干预、流程延迟且 Application 级恢复成本比较高。这种情况下，希望有套机制能够在 Executor 失败后再申请更大内存的 Executor 、对 Task 层面有更好的容错，在一定程度上减少 Task 由内存不足失败导致的 Application 失败恢复和人工干预成本。  </p>","more":"<p>前置条件:   </p>\n<ol>\n<li>Spark On Yarn  </li>\n<li>开启动态资源分配(spark.dynamicAllocation.enabled)  </li>\n</ol>\n<h2 id=\"调度流程\"><a href=\"#调度流程\" class=\"headerlink\" title=\"调度流程\"></a>调度流程</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sequenceDiagram</span><br><span class=\"line\"></span><br><span class=\"line\">RDD#action -&gt;&gt; SparkContext : 1.runJob(rdd,func,partitions,resultHandler)</span><br><span class=\"line\">SparkContext -&gt;&gt; DAGScheduler : 2. submitJob(rdd,func,partitions,resultHandler)</span><br><span class=\"line\">DAGScheduler -&gt;&gt; DAGScheduler : 3. JobSubmitted &amp;&amp; createResultStage &amp;&amp; getOrCreateParentStages &amp;&amp; submitStages &amp;&amp; submitMissingTasks</span><br><span class=\"line\">DAGScheduler -&gt;&gt; TaskScheduler : 4.submitTasks(createTaskSet)</span><br><span class=\"line\">TaskScheduler -&gt;&gt; SchedulerBackEnd : 5.reviveOffers</span><br><span class=\"line\"></span><br><span class=\"line\">loop period &amp; event</span><br><span class=\"line\">\tSchedulerBackEnd -&gt;&gt; TaskScheduler : 6.makeOffers &amp;&amp; resourceOffers</span><br><span class=\"line\">\tDriver -&gt;&gt; Executor : 7.LaunchTask</span><br><span class=\"line\">end</span><br><span class=\"line\">SchedulerBackEnd --&gt;&gt; - RDD#action : 8. statusUpdate &amp;&amp; callBack</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"主要思路\"><a href=\"#主要思路\" class=\"headerlink\" title=\"主要思路\"></a>主要思路</h2><ol>\n<li>Container(或 Executor) 由于内存超限被杀死的原因有两个: 物理内存(-104,137)/虚拟内存(-103,137)超限被 Yarn 资源监控线程杀死；堆内存 OOM 导致 JVM 进程被 kill，退出码是 143。退出码的不同决定了要增加的内存配置不同: -104,-103 和 137需要增加 Container 整体内存；143 需要增加堆内存。(YarnAllocator#allocateResource 拿到 Yarn 新分配的 container 和正常及异常退出的 container)   </li>\n<li>内存拓展的 Executor 启动参数修改: Executor 内存提升次数(默认0)，堆内存，全内存。主要用于 Executor 向 Driver 注册时标识自身的特征，该特征可以使 TaskScheduler将 OOM Task 调度到该 Executor。(YarnAllocator#allocator 拿到 Yarn 新分配的 container,ExecutorRunnable,CoarseGrainedExecutorBackend,RegisterExecutor 数据结构修改)    </li>\n<li>内存拓展的 Executor 注册/注销 -&gt;&gt; 额外维护的可运行 OOM Task 的 Executor 列表更新，ExecutorAdd、ExecutorLost、KillExecutor 等。(CoarseGrainedSchedulerBackEnd,ExecutorId和[提升次数,堆内存,全内存]的映射,提升次数和 ExecutorId 集合的映射)      </li>\n<li>普通 Task, OOM Task 失败的处理 -&gt;&gt; OOM Task失败则必须调度到内存拓展过的 Executor，已经在内存拓展过的 Executor 失败的 OOM Task 后要调度到更大或当前最大内存的 Executor，非 OOM 失败的 task 走现有容错调度(TaskSchedulerImpl)  </li>\n<li>大内存 ExecutorLost -&gt;&gt; 根据维护的ExecutorId 和[提升次数,堆内存,全内存]映射及 Container 退出码及内存拓展算法来决定下一次申请的 ContainerResource和 JVM 启动的 XMX    </li>\n<li>因动态资源分配功能超时退出 或 Yarn 抢占导致的 ExecutorLost在 YarnAllocator 使用大根堆或 最大值来记录资源信息,在下一次申请时不通过内存提升算法。   </li>\n<li>OOM Task 容错: OOM Task 加入到一个特定的 pending 队列,以和普通错误 Task 做区分。TaskScheduler/ExecutorAdd/TaskCompletion 触发的 reviveOffer时,优先从特定 pending 队列调度 判断 ExecutorId 能否可能支撑该 Task(原所属失败的 Executor 提升内存次数必须小于当前 ExecutorId 提升内存次数)。若能则调度,若不能则不调度该任务,调度普通 pending 队列。  </li>\n<li>动态资源分配使用 running tasks和 pending tasks 及 spark.task.cores 来推测需要的总 Executor 数量。在系统的某时刻，可能有 OOM Task 但是却无内存拓展过的 Executor,推测可能也不需要新的 Executor。即: 当有 OOM Task 却无拓展内存的 Executor 时,需要在 CoarseGrainedSchedulerBackend 通过 ExecutorAllocationClient 的 requestTotalExecutor/requestExecutor 接口来触发申请 Executor。数量为 Max{当前 Executor数量/5,1},取 Max 是为了增加本地化可能性，多申请的资源由动态资源分配来管理。    </li>\n<li>YarnAllocator 维护的状态需要有复原功能: OOM 时下一批次申请的 Executor 都是拓展过内存的，随后复原，以适配动态资源分配功能。YarnAllocator 取消堆积在 RM 的 ResourceRequest 时,优先取消没有拓展过内存的,不足则取消拓展过内存的 ResourceRequest。  </li>\n</ol>\n<h2 id=\"Task-失败处理线\"><a href=\"#Task-失败处理线\" class=\"headerlink\" title=\"Task 失败处理线\"></a>Task 失败处理线</h2><ol>\n<li>DriverEndPoint 首先接收到任务 StatusUpdate 事件转交由 CoarseGrainedSchedulerBackEnd 处理  </li>\n<li>CoarseGrainedSchedulerBackEnd 交由 TaskSchedulerImpl 处理 StatusUpdate 事件  </li>\n<li>TaskSchedulerImpl 解析 StatusUpdate 事件的任务状态,并判断其成功/失败，若失败则需要交由 TaskSetManager 维护状态，加入到特定的失败堆积队列  </li>\n<li>a.若此时全局无大内存 Executor，则需要绕过动态资源分配主动申请大内存 Executor，但此时全局无大内存 Executor 故暂不处理当前 OOM Task。 b.若有则阶梯式分配 OOM Task: 判断原有运行该 Task 的 Executor 提升次数，将失败任务分配到更高/最高提升次数的 Executor 上。若原有运行 Executor 内存已经最大，则依据加内存次数抛出异常终止 Application 或主动申请更大内存 Executor，暂不处理当前 OOM Task([3G,4G,5G] 3G上失败任务可指派给最高阶梯 5G,也可以指派给较高阶梯 4G)。  </li>\n<li>对某个 Executor 分配任务(TaskCompletion/ExecutorAdd)时，对 Executor 进行判断，在内存满足的情况下，优先分配特定失败堆积队列的任务，再指派 TaskSetManager 中普通任务或 pending 任务。 </li>\n</ol>\n<h2 id=\"Executor-失败处理线\"><a href=\"#Executor-失败处理线\" class=\"headerlink\" title=\"Executor 失败处理线\"></a>Executor 失败处理线</h2><p>基于 AM 和 RM 资源申请模式，AM updateResourceRequest 更新最新 ResourceRequest 给 RM，RM 响应 AM 的 allocateResource 请求通知新分配的 container,正常异常退出的 container。由此 AM 在新分配的 container 上启动新的 Executor，判断退出的 container 的退出码确定退出原因   </p>\n<ol>\n<li>YarnAllocator 判断容器退出原因，若为 Yarn 抢占或 AM 主动杀死则记录到最大堆中，若为内存问题则使用内存拓展算法计算出下一次 updateResourceRequest 时 Container 配置、 Jvm 配置和内存拓展次数    </li>\n<li>动态资源分配或主动申请容器时 判断是否需要提升内存，若是则通过 updateResourceRequest 来修改;若否则使用大跟堆中的配置  </li>\n<li>动态资源分配当当前需要的 Executor 总量低于已有量时，取消 ResourceRequest 优先取消普通内存配置的 ResourceRequest，不足则取消内存拓展过的 Executor </li>\n</ol>\n<h2 id=\"内存增加算法\"><a href=\"#内存增加算法\" class=\"headerlink\" title=\"内存增加算法\"></a>内存增加算法</h2><ol>\n<li>需要考虑 Yarn 规整(申请的 Container Resource 会被规整到 yarn.scheduler.minimum-allocation-mb 的整数倍)； </li>\n<li>增加 Container 整体资源量时需要按比例分配(堆内:堆外默认为 10:1,可适当修改)；  </li>\n<li>从低内存跃迁至高内存的次数不能太大,即提升快然后迅速收敛以降低中间的失败尝试次数，可选择初始高增量快速降低的方式(e.g. 对数或固定系数，第一次提升0.5,第二次0.3,第三次 0.1,超出最大允许拓展次数后如果OOM 则快速抛出异常终止任务人为干预)。  </li>\n</ol>\n<h2 id=\"附录\"><a href=\"#附录\" class=\"headerlink\" title=\"附录\"></a>附录</h2><p>DAGScheduler 处理的事件类型:    </p>\n<ul>\n<li>JobSubmitted   </li>\n<li>JobCancelled  </li>\n<li>JobGroupCancelled  </li>\n<li>AllJobsCancelled  </li>\n<li>MapStageSubmitted   </li>\n<li>ResubmitFailedStages  </li>\n<li>StageCancelled  </li>\n<li>ExecutorAdded  </li>\n<li>ExecutorLost  </li>\n<li>WorkerRemoved  </li>\n<li>TaskSetFailed  </li>\n<li>BeginEvent   </li>\n<li>CompletionEvent </li>\n<li>SpeculativeTaskSubmitted  </li>\n<li>GettingResultEvent     </li>\n</ul>\n<p>CoarseGrainedSchedulerBackEnd(Driver) 处理的事件类型:   </p>\n<ul>\n<li>RegisterExecutor  </li>\n<li>StopDriver  </li>\n<li>StopExecutors  </li>\n<li>RemoveWorker  </li>\n<li>RetrieveSparkAppConfig    </li>\n<li>StatusUpdate  </li>\n<li>ReviveOffers  </li>\n<li>KillTask  </li>\n<li>KillExecutorsOnHost  </li>\n<li>UpdateDelegationTokens  </li>\n<li>RemoveExecutor  </li>\n</ul>"},{"layout":"post","title":"Maven 常用命令","date":"2020-02-22T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"　　\n \n<!-- more --> \n\n## 基础选项 \n  \n  -v，--version     显示 maven 版本   \n  -X，-debug       显示 debug 日志    \n  -e，--error        显示 error 日志    \n  -q,  --quite         仅仅显示 error 日志信息    \n  -o，--offline      离线构建   \n  -N，--non-recursive   仅构建当前模块，不递归到子模块  \n  -pl，--projects     仅在指定模块构建，避免构建所有模块，多模块用逗号分隔。e.g. -pl core,sql/core      \n  -am，--also-make   构建指定模块时，且构建指定模块依赖的所有模块  \n  -amd，--also-make-dependents  构建指定模块时，且构建依赖该指定模块的所有模块  \n  -D，--define     定义构建属性  e.g.  -Dhadoop.version=2.7.3，-DskipTests  \n  -P，--activate-profiles    激活pom 文件中指定 id的 profile，多个则逗号分隔。 e.g. -Pyarn,hive-1.2 \n  -T, --threads    并行构建，支持数字或 NC(N*core)个线程。e.g. -T  2C 每个 core 2 个线程  \n  -h, --help  显示帮助信息  \n  \n```\n关于-am 和-amd  \n\n假设 core 模块依赖 common 模块，sql 模块依赖 core 模块。使用--am 构建 core 模块时，会额外构建 common 模块；使用--am 构建 sql 模块时，会额外构建 common 和 core 模块；使用--amd 构建 core 模块时，会额外构建 sql 模块\n```\n\n## JVM\n\nMaven 运行在 JVM 中，可以使用 JVM 的优化选项。JVM 配置较多，不再赘述。  \n\n-Xmx2G -Xms2G -XX:+UseG1GC       JVM 内存和垃圾回收器配置  \n-XX:+TieredCompilation -XX:TieredStopAtLevel=1     JVM只进行基础的即时编译操作   \n\n## 常用生命周期  \n\nclean：删除工程的 target 目录，但是不会删除打包到本地仓库和远程仓库的 jar/war 文件    \ncompile：编译配置的源代码目录到 target 的 class 目录(具体情况可能不同, e.g. scala-2.11,scala-2.12)          \npackage：将 target 目录下编译过的代码文件打包成可发布的形式放到工程 target 目录下     \ninstall：将 package 打包的 jar/war 文件安装到本地仓库，可供本地其他工程使用    \ndeploy：将jar/war 包发布到远程仓库(e.g. 私服)  \ntest：使用单元测试框架运行测试       \n其他: validate、initialize、site... \n\n```\nMaven 生命周期的各个阶段是前后依赖的，进行构建时 只有该阶段及其之前的阶段会被调用。调用 mvn deploy 时，以前阶段依次执行(不包含 clean)    \n\nvalidate              验证工程是否正确,且文件信息完整可以完成工程构建     \ninitialize             初始化构建状态，如设置 properties、创建目录等  \ngenerate-sources     生成包含在编译阶段的任何源代码\nprocess-sources  \t     处理源代码  \ngenerate-resources      生成将包含在工程中的资源文件  \nprocess-resources      处理工程资源文件: 变量替换、复制到 target/classes 目录\ncompile                编译工程源代码(一般在 src/main目录)到 target/classes\nprocess-classes        处理编译生成的文件，如对 class 文件做字节码优化  \ngenerate-test-sources   生成包含在编译阶段的任何测试源代码  \nprocess-test-sources     处理测试源代码 \ngenerate-test-resources   生成将包含在工程的测试资源文件\nprocess-test-resources     处理工程测试资源文件: 变量替换、复制到 target/test-classes 目录\ntest-compile         编译工程测试源代码(一般在 src/test 目录)到 target/test-classes \nprocess-test-classes     处理测试编译生成的文件，如对 class 文件做字节码优化  \ntest                 使用单元测试框架运行测试   \nprepare-package    为打包做准备  \npackage              使用已编译好的 classes 和 test-classes，打包成可发布的形式放到工程 target 目录下\npre-integration-test   集成测试前的准备工作，如搭建集成测试环境\nintegration-test       集成测试  \npost-integration-test   集成测试后的工作，如清理集成测试环境  \nverify          检查工程可发布包有效且达到质量标准  \ninstall           将 target 目录下的可发布包安装到本地仓库，使其可被本地其他工程使用  \ndeploy      将本地仓库下的可发布包安装到远程仓库(e.g. mvn repository)，使其可被其他工程、开发人员下载使用  \n```\n\n## 常用命令  \n\nMAVEN_OPTS=\"-Xmx2G -Xms2G -XX:+UseG1GC XX:+TieredCompilation -XX:TieredStopAtLevel=1\"  \nmvn -T 2C clean install -pl module1,module2 -am -offline > install.log   \n  \nmvn install:install-file -Dfile=\\$file -DgroupId=\\$groupId --DartifactId=\\$artifactId -Dversion=\\$version -Dpackaging=jar   \n\nmvn dependency:tree  \n\n## 参考 \n\n生命周期  https://www.runoob.com/maven/maven-build-life-cycle.html  \nTieredCompilation   https://www.jianshu.com/p/318617435789\n\n","source":"_posts/2020-02-23-Maven 常用命令.md","raw":"---\nlayout:     post\ntitle:     Maven 常用命令\ndate:       2020-02-23\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - tools\n---\n　　\n \n<!-- more --> \n\n## 基础选项 \n  \n  -v，--version     显示 maven 版本   \n  -X，-debug       显示 debug 日志    \n  -e，--error        显示 error 日志    \n  -q,  --quite         仅仅显示 error 日志信息    \n  -o，--offline      离线构建   \n  -N，--non-recursive   仅构建当前模块，不递归到子模块  \n  -pl，--projects     仅在指定模块构建，避免构建所有模块，多模块用逗号分隔。e.g. -pl core,sql/core      \n  -am，--also-make   构建指定模块时，且构建指定模块依赖的所有模块  \n  -amd，--also-make-dependents  构建指定模块时，且构建依赖该指定模块的所有模块  \n  -D，--define     定义构建属性  e.g.  -Dhadoop.version=2.7.3，-DskipTests  \n  -P，--activate-profiles    激活pom 文件中指定 id的 profile，多个则逗号分隔。 e.g. -Pyarn,hive-1.2 \n  -T, --threads    并行构建，支持数字或 NC(N*core)个线程。e.g. -T  2C 每个 core 2 个线程  \n  -h, --help  显示帮助信息  \n  \n```\n关于-am 和-amd  \n\n假设 core 模块依赖 common 模块，sql 模块依赖 core 模块。使用--am 构建 core 模块时，会额外构建 common 模块；使用--am 构建 sql 模块时，会额外构建 common 和 core 模块；使用--amd 构建 core 模块时，会额外构建 sql 模块\n```\n\n## JVM\n\nMaven 运行在 JVM 中，可以使用 JVM 的优化选项。JVM 配置较多，不再赘述。  \n\n-Xmx2G -Xms2G -XX:+UseG1GC       JVM 内存和垃圾回收器配置  \n-XX:+TieredCompilation -XX:TieredStopAtLevel=1     JVM只进行基础的即时编译操作   \n\n## 常用生命周期  \n\nclean：删除工程的 target 目录，但是不会删除打包到本地仓库和远程仓库的 jar/war 文件    \ncompile：编译配置的源代码目录到 target 的 class 目录(具体情况可能不同, e.g. scala-2.11,scala-2.12)          \npackage：将 target 目录下编译过的代码文件打包成可发布的形式放到工程 target 目录下     \ninstall：将 package 打包的 jar/war 文件安装到本地仓库，可供本地其他工程使用    \ndeploy：将jar/war 包发布到远程仓库(e.g. 私服)  \ntest：使用单元测试框架运行测试       \n其他: validate、initialize、site... \n\n```\nMaven 生命周期的各个阶段是前后依赖的，进行构建时 只有该阶段及其之前的阶段会被调用。调用 mvn deploy 时，以前阶段依次执行(不包含 clean)    \n\nvalidate              验证工程是否正确,且文件信息完整可以完成工程构建     \ninitialize             初始化构建状态，如设置 properties、创建目录等  \ngenerate-sources     生成包含在编译阶段的任何源代码\nprocess-sources  \t     处理源代码  \ngenerate-resources      生成将包含在工程中的资源文件  \nprocess-resources      处理工程资源文件: 变量替换、复制到 target/classes 目录\ncompile                编译工程源代码(一般在 src/main目录)到 target/classes\nprocess-classes        处理编译生成的文件，如对 class 文件做字节码优化  \ngenerate-test-sources   生成包含在编译阶段的任何测试源代码  \nprocess-test-sources     处理测试源代码 \ngenerate-test-resources   生成将包含在工程的测试资源文件\nprocess-test-resources     处理工程测试资源文件: 变量替换、复制到 target/test-classes 目录\ntest-compile         编译工程测试源代码(一般在 src/test 目录)到 target/test-classes \nprocess-test-classes     处理测试编译生成的文件，如对 class 文件做字节码优化  \ntest                 使用单元测试框架运行测试   \nprepare-package    为打包做准备  \npackage              使用已编译好的 classes 和 test-classes，打包成可发布的形式放到工程 target 目录下\npre-integration-test   集成测试前的准备工作，如搭建集成测试环境\nintegration-test       集成测试  \npost-integration-test   集成测试后的工作，如清理集成测试环境  \nverify          检查工程可发布包有效且达到质量标准  \ninstall           将 target 目录下的可发布包安装到本地仓库，使其可被本地其他工程使用  \ndeploy      将本地仓库下的可发布包安装到远程仓库(e.g. mvn repository)，使其可被其他工程、开发人员下载使用  \n```\n\n## 常用命令  \n\nMAVEN_OPTS=\"-Xmx2G -Xms2G -XX:+UseG1GC XX:+TieredCompilation -XX:TieredStopAtLevel=1\"  \nmvn -T 2C clean install -pl module1,module2 -am -offline > install.log   \n  \nmvn install:install-file -Dfile=\\$file -DgroupId=\\$groupId --DartifactId=\\$artifactId -Dversion=\\$version -Dpackaging=jar   \n\nmvn dependency:tree  \n\n## 参考 \n\n生命周期  https://www.runoob.com/maven/maven-build-life-cycle.html  \nTieredCompilation   https://www.jianshu.com/p/318617435789\n\n","slug":"2020-02-23-Maven 常用命令","published":1,"updated":"2020-03-16T10:15:35.783Z","comments":1,"photos":[],"link":"","_id":"cke2rms5r000qy7c76k2t7rh8","content":"<p>　　</p>\n<a id=\"more\"></a> \n\n<h2 id=\"基础选项\"><a href=\"#基础选项\" class=\"headerlink\" title=\"基础选项\"></a>基础选项</h2><p>  -v，–version     显示 maven 版本<br>  -X，-debug       显示 debug 日志<br>  -e，–error        显示 error 日志<br>  -q,  –quite         仅仅显示 error 日志信息<br>  -o，–offline      离线构建<br>  -N，–non-recursive   仅构建当前模块，不递归到子模块<br>  -pl，–projects     仅在指定模块构建，避免构建所有模块，多模块用逗号分隔。e.g. -pl core,sql/core<br>  -am，–also-make   构建指定模块时，且构建指定模块依赖的所有模块<br>  -amd，–also-make-dependents  构建指定模块时，且构建依赖该指定模块的所有模块<br>  -D，–define     定义构建属性  e.g.  -Dhadoop.version=2.7.3，-DskipTests<br>  -P，–activate-profiles    激活pom 文件中指定 id的 profile，多个则逗号分隔。 e.g. -Pyarn,hive-1.2 \n  -T, –threads    并行构建，支持数字或 NC(N*core)个线程。e.g. -T  2C 每个 core 2 个线程<br>  -h, –help  显示帮助信息  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">关于-am 和-amd  </span><br><span class=\"line\"></span><br><span class=\"line\">假设 core 模块依赖 common 模块，sql 模块依赖 core 模块。使用--am 构建 core 模块时，会额外构建 common 模块；使用--am 构建 sql 模块时，会额外构建 common 和 core 模块；使用--amd 构建 core 模块时，会额外构建 sql 模块</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"JVM\"><a href=\"#JVM\" class=\"headerlink\" title=\"JVM\"></a>JVM</h2><p>Maven 运行在 JVM 中，可以使用 JVM 的优化选项。JVM 配置较多，不再赘述。  </p>\n<p>-Xmx2G -Xms2G -XX:+UseG1GC       JVM 内存和垃圾回收器配置<br>-XX:+TieredCompilation -XX:TieredStopAtLevel=1     JVM只进行基础的即时编译操作   </p>\n<h2 id=\"常用生命周期\"><a href=\"#常用生命周期\" class=\"headerlink\" title=\"常用生命周期\"></a>常用生命周期</h2><p>clean：删除工程的 target 目录，但是不会删除打包到本地仓库和远程仓库的 jar/war 文件<br>compile：编译配置的源代码目录到 target 的 class 目录(具体情况可能不同, e.g. scala-2.11,scala-2.12)<br>package：将 target 目录下编译过的代码文件打包成可发布的形式放到工程 target 目录下<br>install：将 package 打包的 jar/war 文件安装到本地仓库，可供本地其他工程使用<br>deploy：将jar/war 包发布到远程仓库(e.g. 私服)<br>test：使用单元测试框架运行测试<br>其他: validate、initialize、site… </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Maven 生命周期的各个阶段是前后依赖的，进行构建时 只有该阶段及其之前的阶段会被调用。调用 mvn deploy 时，以前阶段依次执行(不包含 clean)    </span><br><span class=\"line\"></span><br><span class=\"line\">validate              验证工程是否正确,且文件信息完整可以完成工程构建     </span><br><span class=\"line\">initialize             初始化构建状态，如设置 properties、创建目录等  </span><br><span class=\"line\">generate-sources     生成包含在编译阶段的任何源代码</span><br><span class=\"line\">process-sources  \t     处理源代码  </span><br><span class=\"line\">generate-resources      生成将包含在工程中的资源文件  </span><br><span class=\"line\">process-resources      处理工程资源文件: 变量替换、复制到 target&#x2F;classes 目录</span><br><span class=\"line\">compile                编译工程源代码(一般在 src&#x2F;main目录)到 target&#x2F;classes</span><br><span class=\"line\">process-classes        处理编译生成的文件，如对 class 文件做字节码优化  </span><br><span class=\"line\">generate-test-sources   生成包含在编译阶段的任何测试源代码  </span><br><span class=\"line\">process-test-sources     处理测试源代码 </span><br><span class=\"line\">generate-test-resources   生成将包含在工程的测试资源文件</span><br><span class=\"line\">process-test-resources     处理工程测试资源文件: 变量替换、复制到 target&#x2F;test-classes 目录</span><br><span class=\"line\">test-compile         编译工程测试源代码(一般在 src&#x2F;test 目录)到 target&#x2F;test-classes </span><br><span class=\"line\">process-test-classes     处理测试编译生成的文件，如对 class 文件做字节码优化  </span><br><span class=\"line\">test                 使用单元测试框架运行测试   </span><br><span class=\"line\">prepare-package    为打包做准备  </span><br><span class=\"line\">package              使用已编译好的 classes 和 test-classes，打包成可发布的形式放到工程 target 目录下</span><br><span class=\"line\">pre-integration-test   集成测试前的准备工作，如搭建集成测试环境</span><br><span class=\"line\">integration-test       集成测试  </span><br><span class=\"line\">post-integration-test   集成测试后的工作，如清理集成测试环境  </span><br><span class=\"line\">verify          检查工程可发布包有效且达到质量标准  </span><br><span class=\"line\">install           将 target 目录下的可发布包安装到本地仓库，使其可被本地其他工程使用  </span><br><span class=\"line\">deploy      将本地仓库下的可发布包安装到远程仓库(e.g. mvn repository)，使其可被其他工程、开发人员下载使用</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"常用命令\"><a href=\"#常用命令\" class=\"headerlink\" title=\"常用命令\"></a>常用命令</h2><p>MAVEN_OPTS=”-Xmx2G -Xms2G -XX:+UseG1GC XX:+TieredCompilation -XX:TieredStopAtLevel=1”<br>mvn -T 2C clean install -pl module1,module2 -am -offline &gt; install.log   </p>\n<p>mvn install:install-file -Dfile=$file -DgroupId=$groupId –DartifactId=$artifactId -Dversion=$version -Dpackaging=jar   </p>\n<p>mvn dependency:tree  </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>生命周期  <a href=\"https://www.runoob.com/maven/maven-build-life-cycle.html\" target=\"_blank\" rel=\"noopener\">https://www.runoob.com/maven/maven-build-life-cycle.html</a><br>TieredCompilation   <a href=\"https://www.jianshu.com/p/318617435789\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/318617435789</a></p>\n","site":{"data":{}},"excerpt":"<p>　　</p>","more":"<h2 id=\"基础选项\"><a href=\"#基础选项\" class=\"headerlink\" title=\"基础选项\"></a>基础选项</h2><p>  -v，–version     显示 maven 版本<br>  -X，-debug       显示 debug 日志<br>  -e，–error        显示 error 日志<br>  -q,  –quite         仅仅显示 error 日志信息<br>  -o，–offline      离线构建<br>  -N，–non-recursive   仅构建当前模块，不递归到子模块<br>  -pl，–projects     仅在指定模块构建，避免构建所有模块，多模块用逗号分隔。e.g. -pl core,sql/core<br>  -am，–also-make   构建指定模块时，且构建指定模块依赖的所有模块<br>  -amd，–also-make-dependents  构建指定模块时，且构建依赖该指定模块的所有模块<br>  -D，–define     定义构建属性  e.g.  -Dhadoop.version=2.7.3，-DskipTests<br>  -P，–activate-profiles    激活pom 文件中指定 id的 profile，多个则逗号分隔。 e.g. -Pyarn,hive-1.2 \n  -T, –threads    并行构建，支持数字或 NC(N*core)个线程。e.g. -T  2C 每个 core 2 个线程<br>  -h, –help  显示帮助信息  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">关于-am 和-amd  </span><br><span class=\"line\"></span><br><span class=\"line\">假设 core 模块依赖 common 模块，sql 模块依赖 core 模块。使用--am 构建 core 模块时，会额外构建 common 模块；使用--am 构建 sql 模块时，会额外构建 common 和 core 模块；使用--amd 构建 core 模块时，会额外构建 sql 模块</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"JVM\"><a href=\"#JVM\" class=\"headerlink\" title=\"JVM\"></a>JVM</h2><p>Maven 运行在 JVM 中，可以使用 JVM 的优化选项。JVM 配置较多，不再赘述。  </p>\n<p>-Xmx2G -Xms2G -XX:+UseG1GC       JVM 内存和垃圾回收器配置<br>-XX:+TieredCompilation -XX:TieredStopAtLevel=1     JVM只进行基础的即时编译操作   </p>\n<h2 id=\"常用生命周期\"><a href=\"#常用生命周期\" class=\"headerlink\" title=\"常用生命周期\"></a>常用生命周期</h2><p>clean：删除工程的 target 目录，但是不会删除打包到本地仓库和远程仓库的 jar/war 文件<br>compile：编译配置的源代码目录到 target 的 class 目录(具体情况可能不同, e.g. scala-2.11,scala-2.12)<br>package：将 target 目录下编译过的代码文件打包成可发布的形式放到工程 target 目录下<br>install：将 package 打包的 jar/war 文件安装到本地仓库，可供本地其他工程使用<br>deploy：将jar/war 包发布到远程仓库(e.g. 私服)<br>test：使用单元测试框架运行测试<br>其他: validate、initialize、site… </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Maven 生命周期的各个阶段是前后依赖的，进行构建时 只有该阶段及其之前的阶段会被调用。调用 mvn deploy 时，以前阶段依次执行(不包含 clean)    </span><br><span class=\"line\"></span><br><span class=\"line\">validate              验证工程是否正确,且文件信息完整可以完成工程构建     </span><br><span class=\"line\">initialize             初始化构建状态，如设置 properties、创建目录等  </span><br><span class=\"line\">generate-sources     生成包含在编译阶段的任何源代码</span><br><span class=\"line\">process-sources  \t     处理源代码  </span><br><span class=\"line\">generate-resources      生成将包含在工程中的资源文件  </span><br><span class=\"line\">process-resources      处理工程资源文件: 变量替换、复制到 target&#x2F;classes 目录</span><br><span class=\"line\">compile                编译工程源代码(一般在 src&#x2F;main目录)到 target&#x2F;classes</span><br><span class=\"line\">process-classes        处理编译生成的文件，如对 class 文件做字节码优化  </span><br><span class=\"line\">generate-test-sources   生成包含在编译阶段的任何测试源代码  </span><br><span class=\"line\">process-test-sources     处理测试源代码 </span><br><span class=\"line\">generate-test-resources   生成将包含在工程的测试资源文件</span><br><span class=\"line\">process-test-resources     处理工程测试资源文件: 变量替换、复制到 target&#x2F;test-classes 目录</span><br><span class=\"line\">test-compile         编译工程测试源代码(一般在 src&#x2F;test 目录)到 target&#x2F;test-classes </span><br><span class=\"line\">process-test-classes     处理测试编译生成的文件，如对 class 文件做字节码优化  </span><br><span class=\"line\">test                 使用单元测试框架运行测试   </span><br><span class=\"line\">prepare-package    为打包做准备  </span><br><span class=\"line\">package              使用已编译好的 classes 和 test-classes，打包成可发布的形式放到工程 target 目录下</span><br><span class=\"line\">pre-integration-test   集成测试前的准备工作，如搭建集成测试环境</span><br><span class=\"line\">integration-test       集成测试  </span><br><span class=\"line\">post-integration-test   集成测试后的工作，如清理集成测试环境  </span><br><span class=\"line\">verify          检查工程可发布包有效且达到质量标准  </span><br><span class=\"line\">install           将 target 目录下的可发布包安装到本地仓库，使其可被本地其他工程使用  </span><br><span class=\"line\">deploy      将本地仓库下的可发布包安装到远程仓库(e.g. mvn repository)，使其可被其他工程、开发人员下载使用</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"常用命令\"><a href=\"#常用命令\" class=\"headerlink\" title=\"常用命令\"></a>常用命令</h2><p>MAVEN_OPTS=”-Xmx2G -Xms2G -XX:+UseG1GC XX:+TieredCompilation -XX:TieredStopAtLevel=1”<br>mvn -T 2C clean install -pl module1,module2 -am -offline &gt; install.log   </p>\n<p>mvn install:install-file -Dfile=$file -DgroupId=$groupId –DartifactId=$artifactId -Dversion=$version -Dpackaging=jar   </p>\n<p>mvn dependency:tree  </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>生命周期  <a href=\"https://www.runoob.com/maven/maven-build-life-cycle.html\" target=\"_blank\" rel=\"noopener\">https://www.runoob.com/maven/maven-build-life-cycle.html</a><br>TieredCompilation   <a href=\"https://www.jianshu.com/p/318617435789\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/318617435789</a></p>"},{"layout":"post","title":".git 目录过大问题解决方案","date":"2020-03-17T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n.git 目录会记录所有文件的历史操作，当提交次数很多 特别是包含大文件时，很容易导致.git 目录很大，尤其是.git/objects/pack。造成的影响有流量超限、自动化部署慢等。   \n\n<!-- more -->\n\n>\nrepo 最好配置 gitignore 文件。   \ngit push 使用--force 或 -f 选项时，应通知 repo 的所有用户。   \nrepo 中使用正则表达式删除时一定要慎重。    \n\n### clone时目录大  \n使用--depth 指定 clone 深度，仅 clone 默认分支的最近一次 commit。如果需要 clone 其他分支，可以通过设置分支来拉取其他分支。  \n\n```\n$ git clone --depth=1 https://github.com/xxx/yyy.git\n$ git remote set-branches origin $remote_branch \n$ git fetch --depth=1 origin $remote_branch \n$ git checkout $remote_branch \n```\n\n### 不重要 repo 可重建  \n对于个人文件存储、传输用途等不重要的 repo，可以重建版本库。      \n\n```\n$ rm -rf  .git \n$ git init\n$ git remote add origin $repo_url \n$ git checkout -b $branch\n$ git add * \n$ git commit -m \"reBuild\" \n$ git push --set-upstream origin $branch --force\n```\n\n### rebase\n有选择性合并历史提交，人工成本较高。    \n\n```\n$ git logs \n$ git rebase -i $commitId \npick 93a7f37 upd\npick c70583b upd\npick 11cb78f upd\npick f11f716 upd\npick d1ee55c upd\n\n将待合并的 commit id 前的 pick 字符串改成 s\npick 93a7f37 upd\npick c70583b upd\npick 11cb78f upd\ns f11f716 upd\npick d1ee55c upd\n\n这样第四个commit 会合并到第三个提交上。  \n$ git push --force \n$ git gc -prune=now\t\t清理本地版本库\n```\n\n### 删除特定大文件的历史记录      \n\n##### 查询 .git 目录中最大的 10 个文件  \n\n```\ngit rev-list --objects --all | grep -f <(git verify-pack -v .git/objects/pack/*.idx| sort -k 3 -n | cut -f 1 -d \" \" | tail -10) \n```\n\n输出范例如下:  \n\n```\n...\n0afba5819d9df014734e70de6a3f237f4f56e33f spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz\n996132b957dc1604cb97cc0c6caaf4f17dfea90f jlz/spark-tpc-ds-performance-test-master/src/data/catalog\\_sales/catalog\\_sales\\_part1.dat \n5af74da26b3dac5ed53f0e7f31b4b95b31730315 jlz/spark-tpc-ds-performance-test-master/src/data/catalog\\_sales/catalog\\_sales\\_part2.dat\n...\n```\n\n##### 在版本库中删除特定文件\n\n```  \ngit filter-branch --index-filter 'git rm --cached --ignore-unmatch spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz catalog\\_sales\\_part*.dat' -- --all  \nrm -rf .git/refs/original   \nrm -rf .git/logs/  \ngit gc --aggressive --prune=now   \n```\n\n##### 提交到远程仓库 \n\n```\ngit push --force  \n```\n\n\n","source":"_posts/2020-03-18-.git目录过大解决方案.md","raw":"---\nlayout:     post\ntitle:      .git 目录过大问题解决方案\ndate:       2020-03-18\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - tools\n---\n\n.git 目录会记录所有文件的历史操作，当提交次数很多 特别是包含大文件时，很容易导致.git 目录很大，尤其是.git/objects/pack。造成的影响有流量超限、自动化部署慢等。   \n\n<!-- more -->\n\n>\nrepo 最好配置 gitignore 文件。   \ngit push 使用--force 或 -f 选项时，应通知 repo 的所有用户。   \nrepo 中使用正则表达式删除时一定要慎重。    \n\n### clone时目录大  \n使用--depth 指定 clone 深度，仅 clone 默认分支的最近一次 commit。如果需要 clone 其他分支，可以通过设置分支来拉取其他分支。  \n\n```\n$ git clone --depth=1 https://github.com/xxx/yyy.git\n$ git remote set-branches origin $remote_branch \n$ git fetch --depth=1 origin $remote_branch \n$ git checkout $remote_branch \n```\n\n### 不重要 repo 可重建  \n对于个人文件存储、传输用途等不重要的 repo，可以重建版本库。      \n\n```\n$ rm -rf  .git \n$ git init\n$ git remote add origin $repo_url \n$ git checkout -b $branch\n$ git add * \n$ git commit -m \"reBuild\" \n$ git push --set-upstream origin $branch --force\n```\n\n### rebase\n有选择性合并历史提交，人工成本较高。    \n\n```\n$ git logs \n$ git rebase -i $commitId \npick 93a7f37 upd\npick c70583b upd\npick 11cb78f upd\npick f11f716 upd\npick d1ee55c upd\n\n将待合并的 commit id 前的 pick 字符串改成 s\npick 93a7f37 upd\npick c70583b upd\npick 11cb78f upd\ns f11f716 upd\npick d1ee55c upd\n\n这样第四个commit 会合并到第三个提交上。  \n$ git push --force \n$ git gc -prune=now\t\t清理本地版本库\n```\n\n### 删除特定大文件的历史记录      \n\n##### 查询 .git 目录中最大的 10 个文件  \n\n```\ngit rev-list --objects --all | grep -f <(git verify-pack -v .git/objects/pack/*.idx| sort -k 3 -n | cut -f 1 -d \" \" | tail -10) \n```\n\n输出范例如下:  \n\n```\n...\n0afba5819d9df014734e70de6a3f237f4f56e33f spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz\n996132b957dc1604cb97cc0c6caaf4f17dfea90f jlz/spark-tpc-ds-performance-test-master/src/data/catalog\\_sales/catalog\\_sales\\_part1.dat \n5af74da26b3dac5ed53f0e7f31b4b95b31730315 jlz/spark-tpc-ds-performance-test-master/src/data/catalog\\_sales/catalog\\_sales\\_part2.dat\n...\n```\n\n##### 在版本库中删除特定文件\n\n```  \ngit filter-branch --index-filter 'git rm --cached --ignore-unmatch spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz catalog\\_sales\\_part*.dat' -- --all  \nrm -rf .git/refs/original   \nrm -rf .git/logs/  \ngit gc --aggressive --prune=now   \n```\n\n##### 提交到远程仓库 \n\n```\ngit push --force  \n```\n\n\n","slug":"2020-03-18-.git目录过大解决方案","published":1,"updated":"2020-05-06T11:29:59.473Z","comments":1,"photos":[],"link":"","_id":"cke2rms5t000ry7c7gio5acn0","content":"<p>.git 目录会记录所有文件的历史操作，当提交次数很多 特别是包含大文件时，很容易导致.git 目录很大，尤其是.git/objects/pack。造成的影响有流量超限、自动化部署慢等。   </p>\n<a id=\"more\"></a>\n\n<blockquote>\n</blockquote>\n<p>repo 最好配置 gitignore 文件。<br>git push 使用–force 或 -f 选项时，应通知 repo 的所有用户。<br>repo 中使用正则表达式删除时一定要慎重。    </p>\n<h3 id=\"clone时目录大\"><a href=\"#clone时目录大\" class=\"headerlink\" title=\"clone时目录大\"></a>clone时目录大</h3><p>使用–depth 指定 clone 深度，仅 clone 默认分支的最近一次 commit。如果需要 clone 其他分支，可以通过设置分支来拉取其他分支。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;xxx&#x2F;yyy.git</span><br><span class=\"line\">$ git remote set-branches origin $remote_branch </span><br><span class=\"line\">$ git fetch --depth&#x3D;1 origin $remote_branch </span><br><span class=\"line\">$ git checkout $remote_branch</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"不重要-repo-可重建\"><a href=\"#不重要-repo-可重建\" class=\"headerlink\" title=\"不重要 repo 可重建\"></a>不重要 repo 可重建</h3><p>对于个人文件存储、传输用途等不重要的 repo，可以重建版本库。      </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf  .git </span><br><span class=\"line\">$ git init</span><br><span class=\"line\">$ git remote add origin $repo_url </span><br><span class=\"line\">$ git checkout -b $branch</span><br><span class=\"line\">$ git add * </span><br><span class=\"line\">$ git commit -m &quot;reBuild&quot; </span><br><span class=\"line\">$ git push --set-upstream origin $branch --force</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"rebase\"><a href=\"#rebase\" class=\"headerlink\" title=\"rebase\"></a>rebase</h3><p>有选择性合并历史提交，人工成本较高。    </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git logs </span><br><span class=\"line\">$ git rebase -i $commitId </span><br><span class=\"line\">pick 93a7f37 upd</span><br><span class=\"line\">pick c70583b upd</span><br><span class=\"line\">pick 11cb78f upd</span><br><span class=\"line\">pick f11f716 upd</span><br><span class=\"line\">pick d1ee55c upd</span><br><span class=\"line\"></span><br><span class=\"line\">将待合并的 commit id 前的 pick 字符串改成 s</span><br><span class=\"line\">pick 93a7f37 upd</span><br><span class=\"line\">pick c70583b upd</span><br><span class=\"line\">pick 11cb78f upd</span><br><span class=\"line\">s f11f716 upd</span><br><span class=\"line\">pick d1ee55c upd</span><br><span class=\"line\"></span><br><span class=\"line\">这样第四个commit 会合并到第三个提交上。  </span><br><span class=\"line\">$ git push --force </span><br><span class=\"line\">$ git gc -prune&#x3D;now\t\t清理本地版本库</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"删除特定大文件的历史记录\"><a href=\"#删除特定大文件的历史记录\" class=\"headerlink\" title=\"删除特定大文件的历史记录\"></a>删除特定大文件的历史记录</h3><h5 id=\"查询-git-目录中最大的-10-个文件\"><a href=\"#查询-git-目录中最大的-10-个文件\" class=\"headerlink\" title=\"查询 .git 目录中最大的 10 个文件\"></a>查询 .git 目录中最大的 10 个文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rev-list --objects --all | grep -f &lt;(git verify-pack -v .git&#x2F;objects&#x2F;pack&#x2F;*.idx| sort -k 3 -n | cut -f 1 -d &quot; &quot; | tail -10)</span><br></pre></td></tr></table></figure>\n\n<p>输出范例如下:  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">0afba5819d9df014734e70de6a3f237f4f56e33f spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz</span><br><span class=\"line\">996132b957dc1604cb97cc0c6caaf4f17dfea90f jlz&#x2F;spark-tpc-ds-performance-test-master&#x2F;src&#x2F;data&#x2F;catalog\\_sales&#x2F;catalog\\_sales\\_part1.dat </span><br><span class=\"line\">5af74da26b3dac5ed53f0e7f31b4b95b31730315 jlz&#x2F;spark-tpc-ds-performance-test-master&#x2F;src&#x2F;data&#x2F;catalog\\_sales&#x2F;catalog\\_sales\\_part2.dat</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"在版本库中删除特定文件\"><a href=\"#在版本库中删除特定文件\" class=\"headerlink\" title=\"在版本库中删除特定文件\"></a>在版本库中删除特定文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git filter-branch --index-filter &#39;git rm --cached --ignore-unmatch spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz catalog\\_sales\\_part*.dat&#39; -- --all  </span><br><span class=\"line\">rm -rf .git&#x2F;refs&#x2F;original   </span><br><span class=\"line\">rm -rf .git&#x2F;logs&#x2F;  </span><br><span class=\"line\">git gc --aggressive --prune&#x3D;now</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"提交到远程仓库\"><a href=\"#提交到远程仓库\" class=\"headerlink\" title=\"提交到远程仓库\"></a>提交到远程仓库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push --force</span><br></pre></td></tr></table></figure>\n\n\n","site":{"data":{}},"excerpt":"<p>.git 目录会记录所有文件的历史操作，当提交次数很多 特别是包含大文件时，很容易导致.git 目录很大，尤其是.git/objects/pack。造成的影响有流量超限、自动化部署慢等。   </p>","more":"<blockquote>\n</blockquote>\n<p>repo 最好配置 gitignore 文件。<br>git push 使用–force 或 -f 选项时，应通知 repo 的所有用户。<br>repo 中使用正则表达式删除时一定要慎重。    </p>\n<h3 id=\"clone时目录大\"><a href=\"#clone时目录大\" class=\"headerlink\" title=\"clone时目录大\"></a>clone时目录大</h3><p>使用–depth 指定 clone 深度，仅 clone 默认分支的最近一次 commit。如果需要 clone 其他分支，可以通过设置分支来拉取其他分支。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;xxx&#x2F;yyy.git</span><br><span class=\"line\">$ git remote set-branches origin $remote_branch </span><br><span class=\"line\">$ git fetch --depth&#x3D;1 origin $remote_branch </span><br><span class=\"line\">$ git checkout $remote_branch</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"不重要-repo-可重建\"><a href=\"#不重要-repo-可重建\" class=\"headerlink\" title=\"不重要 repo 可重建\"></a>不重要 repo 可重建</h3><p>对于个人文件存储、传输用途等不重要的 repo，可以重建版本库。      </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ rm -rf  .git </span><br><span class=\"line\">$ git init</span><br><span class=\"line\">$ git remote add origin $repo_url </span><br><span class=\"line\">$ git checkout -b $branch</span><br><span class=\"line\">$ git add * </span><br><span class=\"line\">$ git commit -m &quot;reBuild&quot; </span><br><span class=\"line\">$ git push --set-upstream origin $branch --force</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"rebase\"><a href=\"#rebase\" class=\"headerlink\" title=\"rebase\"></a>rebase</h3><p>有选择性合并历史提交，人工成本较高。    </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git logs </span><br><span class=\"line\">$ git rebase -i $commitId </span><br><span class=\"line\">pick 93a7f37 upd</span><br><span class=\"line\">pick c70583b upd</span><br><span class=\"line\">pick 11cb78f upd</span><br><span class=\"line\">pick f11f716 upd</span><br><span class=\"line\">pick d1ee55c upd</span><br><span class=\"line\"></span><br><span class=\"line\">将待合并的 commit id 前的 pick 字符串改成 s</span><br><span class=\"line\">pick 93a7f37 upd</span><br><span class=\"line\">pick c70583b upd</span><br><span class=\"line\">pick 11cb78f upd</span><br><span class=\"line\">s f11f716 upd</span><br><span class=\"line\">pick d1ee55c upd</span><br><span class=\"line\"></span><br><span class=\"line\">这样第四个commit 会合并到第三个提交上。  </span><br><span class=\"line\">$ git push --force </span><br><span class=\"line\">$ git gc -prune&#x3D;now\t\t清理本地版本库</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"删除特定大文件的历史记录\"><a href=\"#删除特定大文件的历史记录\" class=\"headerlink\" title=\"删除特定大文件的历史记录\"></a>删除特定大文件的历史记录</h3><h5 id=\"查询-git-目录中最大的-10-个文件\"><a href=\"#查询-git-目录中最大的-10-个文件\" class=\"headerlink\" title=\"查询 .git 目录中最大的 10 个文件\"></a>查询 .git 目录中最大的 10 个文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rev-list --objects --all | grep -f &lt;(git verify-pack -v .git&#x2F;objects&#x2F;pack&#x2F;*.idx| sort -k 3 -n | cut -f 1 -d &quot; &quot; | tail -10)</span><br></pre></td></tr></table></figure>\n\n<p>输出范例如下:  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">0afba5819d9df014734e70de6a3f237f4f56e33f spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz</span><br><span class=\"line\">996132b957dc1604cb97cc0c6caaf4f17dfea90f jlz&#x2F;spark-tpc-ds-performance-test-master&#x2F;src&#x2F;data&#x2F;catalog\\_sales&#x2F;catalog\\_sales\\_part1.dat </span><br><span class=\"line\">5af74da26b3dac5ed53f0e7f31b4b95b31730315 jlz&#x2F;spark-tpc-ds-performance-test-master&#x2F;src&#x2F;data&#x2F;catalog\\_sales&#x2F;catalog\\_sales\\_part2.dat</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"在版本库中删除特定文件\"><a href=\"#在版本库中删除特定文件\" class=\"headerlink\" title=\"在版本库中删除特定文件\"></a>在版本库中删除特定文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git filter-branch --index-filter &#39;git rm --cached --ignore-unmatch spark-3.0.0-SNAPSHOT-bin-custom-spark.tgz catalog\\_sales\\_part*.dat&#39; -- --all  </span><br><span class=\"line\">rm -rf .git&#x2F;refs&#x2F;original   </span><br><span class=\"line\">rm -rf .git&#x2F;logs&#x2F;  </span><br><span class=\"line\">git gc --aggressive --prune&#x3D;now</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"提交到远程仓库\"><a href=\"#提交到远程仓库\" class=\"headerlink\" title=\"提交到远程仓库\"></a>提交到远程仓库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push --force</span><br></pre></td></tr></table></figure>"},{"layout":"post","title":"Spark Adaptive Execution TPCDS 3TB BenchMark","subtitle":"Spark Adaptive Execution","date":"2020-03-16T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\nApache Spark 2.3.0 合并了 Spark 3.0-Preview(预览版)的 Adaptive Execution 和 Dynamic Partition Prune 两个重大特性之后, 在 3TB TPCDS 上的性能测试报告。  \n\n[Spark Adaptive Execution TPCDS 3TB BenchMark](https://docs.google.com/spreadsheets/d/1g37JP4D92LjK1vs8fj5LaQ7ipAaQ6lJinS84R8xS9x4/edit#gid=0)\n","source":"_posts/2020-03-18-Spark-Adaptive-Execution-TPCDS-3TB-BenchMark-md.md","raw":"---\nlayout:     post\ntitle:      Spark Adaptive Execution TPCDS 3TB BenchMark\nsubtitle:   Spark Adaptive Execution\ndate:       2020-03-17\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Spark 2.3.0\n    - Adaptive Execution\n---\n\nApache Spark 2.3.0 合并了 Spark 3.0-Preview(预览版)的 Adaptive Execution 和 Dynamic Partition Prune 两个重大特性之后, 在 3TB TPCDS 上的性能测试报告。  \n\n[Spark Adaptive Execution TPCDS 3TB BenchMark](https://docs.google.com/spreadsheets/d/1g37JP4D92LjK1vs8fj5LaQ7ipAaQ6lJinS84R8xS9x4/edit#gid=0)\n","slug":"2020-03-18-Spark-Adaptive-Execution-TPCDS-3TB-BenchMark-md","published":1,"updated":"2020-03-17T07:32:19.915Z","comments":1,"photos":[],"link":"","_id":"cke2rms5v000ty7c77q6ggt99","content":"<p>Apache Spark 2.3.0 合并了 Spark 3.0-Preview(预览版)的 Adaptive Execution 和 Dynamic Partition Prune 两个重大特性之后, 在 3TB TPCDS 上的性能测试报告。  </p>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1g37JP4D92LjK1vs8fj5LaQ7ipAaQ6lJinS84R8xS9x4/edit#gid=0\" target=\"_blank\" rel=\"noopener\">Spark Adaptive Execution TPCDS 3TB BenchMark</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Apache Spark 2.3.0 合并了 Spark 3.0-Preview(预览版)的 Adaptive Execution 和 Dynamic Partition Prune 两个重大特性之后, 在 3TB TPCDS 上的性能测试报告。  </p>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1g37JP4D92LjK1vs8fj5LaQ7ipAaQ6lJinS84R8xS9x4/edit#gid=0\" target=\"_blank\" rel=\"noopener\">Spark Adaptive Execution TPCDS 3TB BenchMark</a></p>\n"},{"layout":"post","title":"Linux 服务器分析 JVM Dump 文件","date":"2020-04-07T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\nJVM Dump 文件传输到本地分析，通常需要等待较长时间，且本地环境配置不支持分析大内存 JVM 的 Dump 文件。因此合理的方式是使用 Eclipse 的 MAT(MemoryAnalyzeTool) 工具在服务器上分析 Dump 文件结束后将结果传输到本地查看。    \n\n<!-- more -->\n\n### 环境要求  \n\nJDK 1.8.0 以上  \nLinux 操作系统    \nMAT 工具(https://www.eclipse.org/mat/downloads.php)    \n\n### 配置 MAT \n\n```\nunzip MemoryAnalyzer-*-linux.gtk.*.zip \n\nvim MemoryAnalyzer.ini \n## 修改-Xmx，通常设置的比要分析的 JVM Xmx 略大即可  \n```\n\n### JVM Dump \n\n```\njmap -dump:format=b,file=PID.dump PID\n```\n\n>> \n1. 如果需要 Dump Docker 下的进程，需要进入到 Docker 内找到PID，dump 到 Docker 内然后复制 Dump 文件到宿主机 \n2. 如果遇到了 Error attaching to core file 错误,可使用 su -m USER -c \"jmap -dump:format=b,file=PID.dump PID\" 命令来解决    \n\n### 分析 Dump\n\n```\ncd mat  \nsh ParseHeapDump.sh PID.dump  org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components\n```  \n\n会产生 3 个 zip 文件: PID_Leak_Suspects.zip,PID_System_Overview.zip,PID_Top_Components.zip，即(疑似)内存泄露分析、系统内存使用概览、GC-ROOT 信息。 \n\n### 查看结果  \n\n将三个 zip 文件传输到本地，解压后在浏览器打开 index.html 即可。  \n\n\t\n### 参考 \n\nhttps://www.cnblogs.com/trust-freedom/p/6744948.html   \nhttp://moheqionglin.com/site/blogs/24/detail.html  \n","source":"_posts/2020-04-08-Linux 服务器分析 JVM Dump 文件.md","raw":"---\nlayout:     post\ntitle:      Linux 服务器分析 JVM Dump 文件\ndate:       2020-04-08\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - tools\n---\n\nJVM Dump 文件传输到本地分析，通常需要等待较长时间，且本地环境配置不支持分析大内存 JVM 的 Dump 文件。因此合理的方式是使用 Eclipse 的 MAT(MemoryAnalyzeTool) 工具在服务器上分析 Dump 文件结束后将结果传输到本地查看。    \n\n<!-- more -->\n\n### 环境要求  \n\nJDK 1.8.0 以上  \nLinux 操作系统    \nMAT 工具(https://www.eclipse.org/mat/downloads.php)    \n\n### 配置 MAT \n\n```\nunzip MemoryAnalyzer-*-linux.gtk.*.zip \n\nvim MemoryAnalyzer.ini \n## 修改-Xmx，通常设置的比要分析的 JVM Xmx 略大即可  \n```\n\n### JVM Dump \n\n```\njmap -dump:format=b,file=PID.dump PID\n```\n\n>> \n1. 如果需要 Dump Docker 下的进程，需要进入到 Docker 内找到PID，dump 到 Docker 内然后复制 Dump 文件到宿主机 \n2. 如果遇到了 Error attaching to core file 错误,可使用 su -m USER -c \"jmap -dump:format=b,file=PID.dump PID\" 命令来解决    \n\n### 分析 Dump\n\n```\ncd mat  \nsh ParseHeapDump.sh PID.dump  org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components\n```  \n\n会产生 3 个 zip 文件: PID_Leak_Suspects.zip,PID_System_Overview.zip,PID_Top_Components.zip，即(疑似)内存泄露分析、系统内存使用概览、GC-ROOT 信息。 \n\n### 查看结果  \n\n将三个 zip 文件传输到本地，解压后在浏览器打开 index.html 即可。  \n\n\t\n### 参考 \n\nhttps://www.cnblogs.com/trust-freedom/p/6744948.html   \nhttp://moheqionglin.com/site/blogs/24/detail.html  \n","slug":"2020-04-08-Linux 服务器分析 JVM Dump 文件","published":1,"updated":"2020-04-08T04:36:05.556Z","comments":1,"photos":[],"link":"","_id":"cke2rms5w000uy7c78u3e8kek","content":"<p>JVM Dump 文件传输到本地分析，通常需要等待较长时间，且本地环境配置不支持分析大内存 JVM 的 Dump 文件。因此合理的方式是使用 Eclipse 的 MAT(MemoryAnalyzeTool) 工具在服务器上分析 Dump 文件结束后将结果传输到本地查看。    </p>\n<a id=\"more\"></a>\n\n<h3 id=\"环境要求\"><a href=\"#环境要求\" class=\"headerlink\" title=\"环境要求\"></a>环境要求</h3><p>JDK 1.8.0 以上<br>Linux 操作系统<br>MAT 工具(<a href=\"https://www.eclipse.org/mat/downloads.php\" target=\"_blank\" rel=\"noopener\">https://www.eclipse.org/mat/downloads.php</a>)    </p>\n<h3 id=\"配置-MAT\"><a href=\"#配置-MAT\" class=\"headerlink\" title=\"配置 MAT\"></a>配置 MAT</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unzip MemoryAnalyzer-*-linux.gtk.*.zip </span><br><span class=\"line\"></span><br><span class=\"line\">vim MemoryAnalyzer.ini </span><br><span class=\"line\">## 修改-Xmx，通常设置的比要分析的 JVM Xmx 略大即可</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"JVM-Dump\"><a href=\"#JVM-Dump\" class=\"headerlink\" title=\"JVM Dump\"></a>JVM Dump</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jmap -dump:format&#x3D;b,file&#x3D;PID.dump PID</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<blockquote>\n</blockquote>\n</blockquote>\n<ol>\n<li>如果需要 Dump Docker 下的进程，需要进入到 Docker 内找到PID，dump 到 Docker 内然后复制 Dump 文件到宿主机 </li>\n<li>如果遇到了 Error attaching to core file 错误,可使用 su -m USER -c “jmap -dump:format=b,file=PID.dump PID” 命令来解决    </li>\n</ol>\n<h3 id=\"分析-Dump\"><a href=\"#分析-Dump\" class=\"headerlink\" title=\"分析 Dump\"></a>分析 Dump</h3><pre><code>cd mat  \nsh ParseHeapDump.sh PID.dump  org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components</code></pre><p>会产生 3 个 zip 文件: PID_Leak_Suspects.zip,PID_System_Overview.zip,PID_Top_Components.zip，即(疑似)内存泄露分析、系统内存使用概览、GC-ROOT 信息。 </p>\n<h3 id=\"查看结果\"><a href=\"#查看结果\" class=\"headerlink\" title=\"查看结果\"></a>查看结果</h3><p>将三个 zip 文件传输到本地，解压后在浏览器打开 index.html 即可。  </p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/trust-freedom/p/6744948.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/trust-freedom/p/6744948.html</a><br><a href=\"http://moheqionglin.com/site/blogs/24/detail.html\" target=\"_blank\" rel=\"noopener\">http://moheqionglin.com/site/blogs/24/detail.html</a>  </p>\n","site":{"data":{}},"excerpt":"<p>JVM Dump 文件传输到本地分析，通常需要等待较长时间，且本地环境配置不支持分析大内存 JVM 的 Dump 文件。因此合理的方式是使用 Eclipse 的 MAT(MemoryAnalyzeTool) 工具在服务器上分析 Dump 文件结束后将结果传输到本地查看。    </p>","more":"<h3 id=\"环境要求\"><a href=\"#环境要求\" class=\"headerlink\" title=\"环境要求\"></a>环境要求</h3><p>JDK 1.8.0 以上<br>Linux 操作系统<br>MAT 工具(<a href=\"https://www.eclipse.org/mat/downloads.php\" target=\"_blank\" rel=\"noopener\">https://www.eclipse.org/mat/downloads.php</a>)    </p>\n<h3 id=\"配置-MAT\"><a href=\"#配置-MAT\" class=\"headerlink\" title=\"配置 MAT\"></a>配置 MAT</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unzip MemoryAnalyzer-*-linux.gtk.*.zip </span><br><span class=\"line\"></span><br><span class=\"line\">vim MemoryAnalyzer.ini </span><br><span class=\"line\">## 修改-Xmx，通常设置的比要分析的 JVM Xmx 略大即可</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"JVM-Dump\"><a href=\"#JVM-Dump\" class=\"headerlink\" title=\"JVM Dump\"></a>JVM Dump</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jmap -dump:format&#x3D;b,file&#x3D;PID.dump PID</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<blockquote>\n</blockquote>\n</blockquote>\n<ol>\n<li>如果需要 Dump Docker 下的进程，需要进入到 Docker 内找到PID，dump 到 Docker 内然后复制 Dump 文件到宿主机 </li>\n<li>如果遇到了 Error attaching to core file 错误,可使用 su -m USER -c “jmap -dump:format=b,file=PID.dump PID” 命令来解决    </li>\n</ol>\n<h3 id=\"分析-Dump\"><a href=\"#分析-Dump\" class=\"headerlink\" title=\"分析 Dump\"></a>分析 Dump</h3><pre><code>cd mat  \nsh ParseHeapDump.sh PID.dump  org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components</code></pre><p>会产生 3 个 zip 文件: PID_Leak_Suspects.zip,PID_System_Overview.zip,PID_Top_Components.zip，即(疑似)内存泄露分析、系统内存使用概览、GC-ROOT 信息。 </p>\n<h3 id=\"查看结果\"><a href=\"#查看结果\" class=\"headerlink\" title=\"查看结果\"></a>查看结果</h3><p>将三个 zip 文件传输到本地，解压后在浏览器打开 index.html 即可。  </p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/trust-freedom/p/6744948.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/trust-freedom/p/6744948.html</a><br><a href=\"http://moheqionglin.com/site/blogs/24/detail.html\" target=\"_blank\" rel=\"noopener\">http://moheqionglin.com/site/blogs/24/detail.html</a>  </p>"},{"layout":"post","title":"Spark 自动合并小文件","date":"2020-07-02T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\nSpark SQL/DataFrame application 在高并发度或使用动态分区时产生大量小文件的平台化治理实践。  \n\n<!-- more -->   \n\n## 背景  \nspark application 至少会输出 spark.sql.shuffle.partitions(默认 200) 个文件，如果开启了动态分区则更多。    \n一般情况下数据开发可以通过 repartition/coalesce 或/**+ COALESCE/REPARTITION(numPartitions) */ 来控制输出文件数量，但数据通过 filter、flatmap、join、group by 等算子之后难以根据输入数据量级来估算输出数据量级，且实际输出文件大小还会受到数据本身、文件格式和压缩算法的影响。在某些场景下，例如医院科室和疾病类型重复率较高导致数据压缩率极高 输出文件并不大。  \n更多数情况下会设置高 spark.sql.shuffle.partitions 以增加并发度或避免可能出现的 reduce 端 OOM。因此从 application 内部通过设置分区数的方式来控制输出文件数量十分困难，至于 SQL application 则更难做到。      \n\n小文件过多的负面影响主要是增大了 HDFS namenode/datanode 的内存负载及检索速度，其次是降低了碎片文件寻址速度 增加读取成本。  \n\n面临的困境主要是，线上表绝大部分是由 SQL 产出的，手动合并 hdfs 目录的成本极高。因此希望能在 Spark 源代码上做一些二次开发能够自动合并 application 产出的小文件。   \n\n## 思路 \n主要考虑了以下问题:  \n\n1. 有办法获取最后一个 ShuffleMapStage 各个分区的输入数据量和条数，但是最后一个 ShuffleMapStage 后续可能有 filter、flatmap 等操作，因此无法在 application 未写出数据时，按照输入数据量和压缩方式预估出分区输出文件大小。因而只能在 application 写出数据时或写出后判断及合并输出文件。     \n2. 无法在数据写出后通过优化策略修改当前执行计划，相当于在已经执行完成的执行计划树上加节点，且数据写入完成后无法通过当前执行计划的 DataWritingCommand 再操作数据(Seq.empty[Row])。   \n3. 可选的方式有两种: 一、在当前执行计划的数据写出阶段(临时数据写出完成但是还未移动到最终目录)做合并操作 二、在当前执行计划的数据写出阶段结束后拉起一个合并操作。本质上差不多,但是第二种方式会造成额外的元数据修改操作，例如在 spark catalog 中 refresh table、uncache table 以及更新 statistics 数据。故选择第一种方式。             \n4. 在数据写出后，执行合并操作然后再写出数据属于递归操作，需要仔细设置终止条件。        \n5. 当文件数量较少时无需合并(e.g. 一个分区只有两个小文件)，为减少合并任务的启动成本可以允许存在少量小文件; 使用压缩率较高的算法时，无需将文件大小严格合并到 dfs.blocksize，会导致后续任务读取解压该文件时 OOM。暂取0.25 * dfs.blocksize  \n6. Spark 内置的写数据方式(物理计划)有五种: CreateHiveTableAsSelectCommand,InsertIntoHiveDirCommand,InsertIntoHiveTable,CreateDataSourceTableAsSelectCommand,InsertIntoHadoopFsRelation，但最终都是在 FileFormatWriter 定义 task、构建 Spark Job 来实现写入的。CreateHiveTableAsSelectCommand 内部复用了 InsertIntoHiveDirCommand,CreateDataSourceTableAsSelectCommand 复用了 InsertIntoHadooopFsRelation; InsertIntoHiveDirCommand 和 InsertIntoHiveTable 都继承自 SaveAsHiveTable 接口，接口内 saveAsHiveFile方法使用 FileFormatWriter 写出数据返回并返回受写出操作影响的分区名称。在 SaveAsHiveTable 接口内添加合并 hive 小文件逻辑影响的范围是 CreateHiveTableAsSelectCommand(对应语法 create table tbl_name as select statement)、InsertIntoHiveDirCommand(对应语法 insert overwrite directory path row formatted .. select statement)和 InsertIntoHiveTable(对应语法 insert into/overwrite table tbl_name partition(...) select statement)，正是 hive 数据源数据写入的所有操作方式。开源大佬设计模式用的真好。InsertIntoHadooopFsRelation 对应 DataFrame 编程中 df.write.format(source).saveAsTable(...) 或 df.write.orc(hdfs path) 两种写入方式。               \n\n## 设计  \n\n![Spark 自动合并小文件流程](/img/pictures/spark/auto_merge_small_file.png)  \n\n\n| 参数名  | 默认值  | 功能 |\n|:------------- |:---------------|:-------------|\n|spark.hive.automerge.enabled|true|是否开启自动合并小文件功能|\n|spark.hive.automerge.least.filenum|3|分区内文件数小于此值时,不合并小文件(考虑启动任务成本)|\n|spark.hive.automerge.block.percent|0.25|合并后文件的理想大小是 dfs.blocksize 与此参数的乘积(考虑压缩率)|     \n|spark.hive.automerge.shuffle.enabled|false|重分区时是否开启 shuffle。true 时为 repartition，false 时为coalesce|\n\n> 测试效果样例  \n1. 日志信息   \nINFO MergeHiveSmallFileUtil: merge table: user_profile.dwd_user_profile_di ,partition:second=visit/third=visit_7d finished, cost 164 ms.before {total length in bytes: 233 ,file number: 3} ,after {file number: 1}    \n2. Spark History EventLog   \n{\"Event\":\"org.apache.spark.sql.hive.execution.AutoMergeSmallFileEvent\",\"table\":\"user_profile.dwd_user_profile_di\",\"part\":\"second=visit/third=visit_7d\",\"totalLenInByte\":233,\"numBefore\":3,\"numAfter\":1,\"diff\":2,\"costInMillSec\":164}    \n\n\n## todo \n\n1. 临时文件以 Hive TableDesc 定义的 output format(e.g. org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat) 形式写出，但是无法用 TableDesc定义的 input format来读取。因为 Hive TableDesc 定义的 input/output format 不是一个可用的 Spark SQL DataSource。使用 sparkSession.read.option(\"inputFormat\",TableDesc.getInputFormatClassName).option('outputformat',TableDesc.getOutputFormatClassName)也无法解决此问题，因为 spark 默认的 DataSource 为 parquet。没有在源代码中找到更优雅的方式来映射 Hive output format 到 Spark SQL DataSource，求助于 spark dev list 也未收到回复，目前采用的是死板的枚举映射判断 Hive outputformat 的后缀(e.g. orcoutputformat->orc)。  \n2. 这套合并小文件逻辑稍作修改就可以扩展为 \"对指定表或分区进行小文件合并\"，扩展 Spark SQL 支持的语法糖，但是由于我对 antlr 语法十分陌生，未敢妄动。      \n3. 在线上灰度一定时间后，将通过 SparkListenerEvent 统计出的信息和 hdfs namenode 平均文件大小来评估具体效果。  \n\n\n  \n\n\n   \n","source":"_posts/2020-07-03-Spark 自动合并小文件.md","raw":"---\nlayout:     post\ntitle:     Spark 自动合并小文件\ndate:       2020-07-03\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    -  Spark 2.3.0\n    -  方案\n---\n\nSpark SQL/DataFrame application 在高并发度或使用动态分区时产生大量小文件的平台化治理实践。  \n\n<!-- more -->   \n\n## 背景  \nspark application 至少会输出 spark.sql.shuffle.partitions(默认 200) 个文件，如果开启了动态分区则更多。    \n一般情况下数据开发可以通过 repartition/coalesce 或/**+ COALESCE/REPARTITION(numPartitions) */ 来控制输出文件数量，但数据通过 filter、flatmap、join、group by 等算子之后难以根据输入数据量级来估算输出数据量级，且实际输出文件大小还会受到数据本身、文件格式和压缩算法的影响。在某些场景下，例如医院科室和疾病类型重复率较高导致数据压缩率极高 输出文件并不大。  \n更多数情况下会设置高 spark.sql.shuffle.partitions 以增加并发度或避免可能出现的 reduce 端 OOM。因此从 application 内部通过设置分区数的方式来控制输出文件数量十分困难，至于 SQL application 则更难做到。      \n\n小文件过多的负面影响主要是增大了 HDFS namenode/datanode 的内存负载及检索速度，其次是降低了碎片文件寻址速度 增加读取成本。  \n\n面临的困境主要是，线上表绝大部分是由 SQL 产出的，手动合并 hdfs 目录的成本极高。因此希望能在 Spark 源代码上做一些二次开发能够自动合并 application 产出的小文件。   \n\n## 思路 \n主要考虑了以下问题:  \n\n1. 有办法获取最后一个 ShuffleMapStage 各个分区的输入数据量和条数，但是最后一个 ShuffleMapStage 后续可能有 filter、flatmap 等操作，因此无法在 application 未写出数据时，按照输入数据量和压缩方式预估出分区输出文件大小。因而只能在 application 写出数据时或写出后判断及合并输出文件。     \n2. 无法在数据写出后通过优化策略修改当前执行计划，相当于在已经执行完成的执行计划树上加节点，且数据写入完成后无法通过当前执行计划的 DataWritingCommand 再操作数据(Seq.empty[Row])。   \n3. 可选的方式有两种: 一、在当前执行计划的数据写出阶段(临时数据写出完成但是还未移动到最终目录)做合并操作 二、在当前执行计划的数据写出阶段结束后拉起一个合并操作。本质上差不多,但是第二种方式会造成额外的元数据修改操作，例如在 spark catalog 中 refresh table、uncache table 以及更新 statistics 数据。故选择第一种方式。             \n4. 在数据写出后，执行合并操作然后再写出数据属于递归操作，需要仔细设置终止条件。        \n5. 当文件数量较少时无需合并(e.g. 一个分区只有两个小文件)，为减少合并任务的启动成本可以允许存在少量小文件; 使用压缩率较高的算法时，无需将文件大小严格合并到 dfs.blocksize，会导致后续任务读取解压该文件时 OOM。暂取0.25 * dfs.blocksize  \n6. Spark 内置的写数据方式(物理计划)有五种: CreateHiveTableAsSelectCommand,InsertIntoHiveDirCommand,InsertIntoHiveTable,CreateDataSourceTableAsSelectCommand,InsertIntoHadoopFsRelation，但最终都是在 FileFormatWriter 定义 task、构建 Spark Job 来实现写入的。CreateHiveTableAsSelectCommand 内部复用了 InsertIntoHiveDirCommand,CreateDataSourceTableAsSelectCommand 复用了 InsertIntoHadooopFsRelation; InsertIntoHiveDirCommand 和 InsertIntoHiveTable 都继承自 SaveAsHiveTable 接口，接口内 saveAsHiveFile方法使用 FileFormatWriter 写出数据返回并返回受写出操作影响的分区名称。在 SaveAsHiveTable 接口内添加合并 hive 小文件逻辑影响的范围是 CreateHiveTableAsSelectCommand(对应语法 create table tbl_name as select statement)、InsertIntoHiveDirCommand(对应语法 insert overwrite directory path row formatted .. select statement)和 InsertIntoHiveTable(对应语法 insert into/overwrite table tbl_name partition(...) select statement)，正是 hive 数据源数据写入的所有操作方式。开源大佬设计模式用的真好。InsertIntoHadooopFsRelation 对应 DataFrame 编程中 df.write.format(source).saveAsTable(...) 或 df.write.orc(hdfs path) 两种写入方式。               \n\n## 设计  \n\n![Spark 自动合并小文件流程](/img/pictures/spark/auto_merge_small_file.png)  \n\n\n| 参数名  | 默认值  | 功能 |\n|:------------- |:---------------|:-------------|\n|spark.hive.automerge.enabled|true|是否开启自动合并小文件功能|\n|spark.hive.automerge.least.filenum|3|分区内文件数小于此值时,不合并小文件(考虑启动任务成本)|\n|spark.hive.automerge.block.percent|0.25|合并后文件的理想大小是 dfs.blocksize 与此参数的乘积(考虑压缩率)|     \n|spark.hive.automerge.shuffle.enabled|false|重分区时是否开启 shuffle。true 时为 repartition，false 时为coalesce|\n\n> 测试效果样例  \n1. 日志信息   \nINFO MergeHiveSmallFileUtil: merge table: user_profile.dwd_user_profile_di ,partition:second=visit/third=visit_7d finished, cost 164 ms.before {total length in bytes: 233 ,file number: 3} ,after {file number: 1}    \n2. Spark History EventLog   \n{\"Event\":\"org.apache.spark.sql.hive.execution.AutoMergeSmallFileEvent\",\"table\":\"user_profile.dwd_user_profile_di\",\"part\":\"second=visit/third=visit_7d\",\"totalLenInByte\":233,\"numBefore\":3,\"numAfter\":1,\"diff\":2,\"costInMillSec\":164}    \n\n\n## todo \n\n1. 临时文件以 Hive TableDesc 定义的 output format(e.g. org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat) 形式写出，但是无法用 TableDesc定义的 input format来读取。因为 Hive TableDesc 定义的 input/output format 不是一个可用的 Spark SQL DataSource。使用 sparkSession.read.option(\"inputFormat\",TableDesc.getInputFormatClassName).option('outputformat',TableDesc.getOutputFormatClassName)也无法解决此问题，因为 spark 默认的 DataSource 为 parquet。没有在源代码中找到更优雅的方式来映射 Hive output format 到 Spark SQL DataSource，求助于 spark dev list 也未收到回复，目前采用的是死板的枚举映射判断 Hive outputformat 的后缀(e.g. orcoutputformat->orc)。  \n2. 这套合并小文件逻辑稍作修改就可以扩展为 \"对指定表或分区进行小文件合并\"，扩展 Spark SQL 支持的语法糖，但是由于我对 antlr 语法十分陌生，未敢妄动。      \n3. 在线上灰度一定时间后，将通过 SparkListenerEvent 统计出的信息和 hdfs namenode 平均文件大小来评估具体效果。  \n\n\n  \n\n\n   \n","slug":"2020-07-03-Spark 自动合并小文件","published":1,"updated":"2020-08-03T03:34:35.699Z","comments":1,"photos":[],"link":"","_id":"cke2rms61000wy7c7fgg246ju","content":"<p>Spark SQL/DataFrame application 在高并发度或使用动态分区时产生大量小文件的平台化治理实践。  </p>\n<a id=\"more\"></a>   \n\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>spark application 至少会输出 spark.sql.shuffle.partitions(默认 200) 个文件，如果开启了动态分区则更多。<br>一般情况下数据开发可以通过 repartition/coalesce 或/**+ COALESCE/REPARTITION(numPartitions) */ 来控制输出文件数量，但数据通过 filter、flatmap、join、group by 等算子之后难以根据输入数据量级来估算输出数据量级，且实际输出文件大小还会受到数据本身、文件格式和压缩算法的影响。在某些场景下，例如医院科室和疾病类型重复率较高导致数据压缩率极高 输出文件并不大。<br>更多数情况下会设置高 spark.sql.shuffle.partitions 以增加并发度或避免可能出现的 reduce 端 OOM。因此从 application 内部通过设置分区数的方式来控制输出文件数量十分困难，至于 SQL application 则更难做到。      </p>\n<p>小文件过多的负面影响主要是增大了 HDFS namenode/datanode 的内存负载及检索速度，其次是降低了碎片文件寻址速度 增加读取成本。  </p>\n<p>面临的困境主要是，线上表绝大部分是由 SQL 产出的，手动合并 hdfs 目录的成本极高。因此希望能在 Spark 源代码上做一些二次开发能够自动合并 application 产出的小文件。   </p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>主要考虑了以下问题:  </p>\n<ol>\n<li>有办法获取最后一个 ShuffleMapStage 各个分区的输入数据量和条数，但是最后一个 ShuffleMapStage 后续可能有 filter、flatmap 等操作，因此无法在 application 未写出数据时，按照输入数据量和压缩方式预估出分区输出文件大小。因而只能在 application 写出数据时或写出后判断及合并输出文件。     </li>\n<li>无法在数据写出后通过优化策略修改当前执行计划，相当于在已经执行完成的执行计划树上加节点，且数据写入完成后无法通过当前执行计划的 DataWritingCommand 再操作数据(Seq.empty[Row])。   </li>\n<li>可选的方式有两种: 一、在当前执行计划的数据写出阶段(临时数据写出完成但是还未移动到最终目录)做合并操作 二、在当前执行计划的数据写出阶段结束后拉起一个合并操作。本质上差不多,但是第二种方式会造成额外的元数据修改操作，例如在 spark catalog 中 refresh table、uncache table 以及更新 statistics 数据。故选择第一种方式。             </li>\n<li>在数据写出后，执行合并操作然后再写出数据属于递归操作，需要仔细设置终止条件。        </li>\n<li>当文件数量较少时无需合并(e.g. 一个分区只有两个小文件)，为减少合并任务的启动成本可以允许存在少量小文件; 使用压缩率较高的算法时，无需将文件大小严格合并到 dfs.blocksize，会导致后续任务读取解压该文件时 OOM。暂取0.25 * dfs.blocksize  </li>\n<li>Spark 内置的写数据方式(物理计划)有五种: CreateHiveTableAsSelectCommand,InsertIntoHiveDirCommand,InsertIntoHiveTable,CreateDataSourceTableAsSelectCommand,InsertIntoHadoopFsRelation，但最终都是在 FileFormatWriter 定义 task、构建 Spark Job 来实现写入的。CreateHiveTableAsSelectCommand 内部复用了 InsertIntoHiveDirCommand,CreateDataSourceTableAsSelectCommand 复用了 InsertIntoHadooopFsRelation; InsertIntoHiveDirCommand 和 InsertIntoHiveTable 都继承自 SaveAsHiveTable 接口，接口内 saveAsHiveFile方法使用 FileFormatWriter 写出数据返回并返回受写出操作影响的分区名称。在 SaveAsHiveTable 接口内添加合并 hive 小文件逻辑影响的范围是 CreateHiveTableAsSelectCommand(对应语法 create table tbl_name as select statement)、InsertIntoHiveDirCommand(对应语法 insert overwrite directory path row formatted .. select statement)和 InsertIntoHiveTable(对应语法 insert into/overwrite table tbl_name partition(…) select statement)，正是 hive 数据源数据写入的所有操作方式。开源大佬设计模式用的真好。InsertIntoHadooopFsRelation 对应 DataFrame 编程中 df.write.format(source).saveAsTable(…) 或 df.write.orc(hdfs path) 两种写入方式。               </li>\n</ol>\n<h2 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h2><p><img src=\"/img/pictures/spark/auto_merge_small_file.png\" alt=\"Spark 自动合并小文件流程\">  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数名</th>\n<th align=\"left\">默认值</th>\n<th align=\"left\">功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">spark.hive.automerge.enabled</td>\n<td align=\"left\">true</td>\n<td align=\"left\">是否开启自动合并小文件功能</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.least.filenum</td>\n<td align=\"left\">3</td>\n<td align=\"left\">分区内文件数小于此值时,不合并小文件(考虑启动任务成本)</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.block.percent</td>\n<td align=\"left\">0.25</td>\n<td align=\"left\">合并后文件的理想大小是 dfs.blocksize 与此参数的乘积(考虑压缩率)</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.shuffle.enabled</td>\n<td align=\"left\">false</td>\n<td align=\"left\">重分区时是否开启 shuffle。true 时为 repartition，false 时为coalesce</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>测试效果样例  </p>\n</blockquote>\n<ol>\n<li>日志信息<br>INFO MergeHiveSmallFileUtil: merge table: user_profile.dwd_user_profile_di ,partition:second=visit/third=visit_7d finished, cost 164 ms.before {total length in bytes: 233 ,file number: 3} ,after {file number: 1}    </li>\n<li>Spark History EventLog<br>{“Event”:”org.apache.spark.sql.hive.execution.AutoMergeSmallFileEvent”,”table”:”user_profile.dwd_user_profile_di”,”part”:”second=visit/third=visit_7d”,”totalLenInByte”:233,”numBefore”:3,”numAfter”:1,”diff”:2,”costInMillSec”:164}    </li>\n</ol>\n<h2 id=\"todo\"><a href=\"#todo\" class=\"headerlink\" title=\"todo\"></a>todo</h2><ol>\n<li>临时文件以 Hive TableDesc 定义的 output format(e.g. org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat) 形式写出，但是无法用 TableDesc定义的 input format来读取。因为 Hive TableDesc 定义的 input/output format 不是一个可用的 Spark SQL DataSource。使用 sparkSession.read.option(“inputFormat”,TableDesc.getInputFormatClassName).option(‘outputformat’,TableDesc.getOutputFormatClassName)也无法解决此问题，因为 spark 默认的 DataSource 为 parquet。没有在源代码中找到更优雅的方式来映射 Hive output format 到 Spark SQL DataSource，求助于 spark dev list 也未收到回复，目前采用的是死板的枚举映射判断 Hive outputformat 的后缀(e.g. orcoutputformat-&gt;orc)。  </li>\n<li>这套合并小文件逻辑稍作修改就可以扩展为 “对指定表或分区进行小文件合并”，扩展 Spark SQL 支持的语法糖，但是由于我对 antlr 语法十分陌生，未敢妄动。      </li>\n<li>在线上灰度一定时间后，将通过 SparkListenerEvent 统计出的信息和 hdfs namenode 平均文件大小来评估具体效果。  </li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>Spark SQL/DataFrame application 在高并发度或使用动态分区时产生大量小文件的平台化治理实践。  </p>","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>spark application 至少会输出 spark.sql.shuffle.partitions(默认 200) 个文件，如果开启了动态分区则更多。<br>一般情况下数据开发可以通过 repartition/coalesce 或/**+ COALESCE/REPARTITION(numPartitions) */ 来控制输出文件数量，但数据通过 filter、flatmap、join、group by 等算子之后难以根据输入数据量级来估算输出数据量级，且实际输出文件大小还会受到数据本身、文件格式和压缩算法的影响。在某些场景下，例如医院科室和疾病类型重复率较高导致数据压缩率极高 输出文件并不大。<br>更多数情况下会设置高 spark.sql.shuffle.partitions 以增加并发度或避免可能出现的 reduce 端 OOM。因此从 application 内部通过设置分区数的方式来控制输出文件数量十分困难，至于 SQL application 则更难做到。      </p>\n<p>小文件过多的负面影响主要是增大了 HDFS namenode/datanode 的内存负载及检索速度，其次是降低了碎片文件寻址速度 增加读取成本。  </p>\n<p>面临的困境主要是，线上表绝大部分是由 SQL 产出的，手动合并 hdfs 目录的成本极高。因此希望能在 Spark 源代码上做一些二次开发能够自动合并 application 产出的小文件。   </p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p>主要考虑了以下问题:  </p>\n<ol>\n<li>有办法获取最后一个 ShuffleMapStage 各个分区的输入数据量和条数，但是最后一个 ShuffleMapStage 后续可能有 filter、flatmap 等操作，因此无法在 application 未写出数据时，按照输入数据量和压缩方式预估出分区输出文件大小。因而只能在 application 写出数据时或写出后判断及合并输出文件。     </li>\n<li>无法在数据写出后通过优化策略修改当前执行计划，相当于在已经执行完成的执行计划树上加节点，且数据写入完成后无法通过当前执行计划的 DataWritingCommand 再操作数据(Seq.empty[Row])。   </li>\n<li>可选的方式有两种: 一、在当前执行计划的数据写出阶段(临时数据写出完成但是还未移动到最终目录)做合并操作 二、在当前执行计划的数据写出阶段结束后拉起一个合并操作。本质上差不多,但是第二种方式会造成额外的元数据修改操作，例如在 spark catalog 中 refresh table、uncache table 以及更新 statistics 数据。故选择第一种方式。             </li>\n<li>在数据写出后，执行合并操作然后再写出数据属于递归操作，需要仔细设置终止条件。        </li>\n<li>当文件数量较少时无需合并(e.g. 一个分区只有两个小文件)，为减少合并任务的启动成本可以允许存在少量小文件; 使用压缩率较高的算法时，无需将文件大小严格合并到 dfs.blocksize，会导致后续任务读取解压该文件时 OOM。暂取0.25 * dfs.blocksize  </li>\n<li>Spark 内置的写数据方式(物理计划)有五种: CreateHiveTableAsSelectCommand,InsertIntoHiveDirCommand,InsertIntoHiveTable,CreateDataSourceTableAsSelectCommand,InsertIntoHadoopFsRelation，但最终都是在 FileFormatWriter 定义 task、构建 Spark Job 来实现写入的。CreateHiveTableAsSelectCommand 内部复用了 InsertIntoHiveDirCommand,CreateDataSourceTableAsSelectCommand 复用了 InsertIntoHadooopFsRelation; InsertIntoHiveDirCommand 和 InsertIntoHiveTable 都继承自 SaveAsHiveTable 接口，接口内 saveAsHiveFile方法使用 FileFormatWriter 写出数据返回并返回受写出操作影响的分区名称。在 SaveAsHiveTable 接口内添加合并 hive 小文件逻辑影响的范围是 CreateHiveTableAsSelectCommand(对应语法 create table tbl_name as select statement)、InsertIntoHiveDirCommand(对应语法 insert overwrite directory path row formatted .. select statement)和 InsertIntoHiveTable(对应语法 insert into/overwrite table tbl_name partition(…) select statement)，正是 hive 数据源数据写入的所有操作方式。开源大佬设计模式用的真好。InsertIntoHadooopFsRelation 对应 DataFrame 编程中 df.write.format(source).saveAsTable(…) 或 df.write.orc(hdfs path) 两种写入方式。               </li>\n</ol>\n<h2 id=\"设计\"><a href=\"#设计\" class=\"headerlink\" title=\"设计\"></a>设计</h2><p><img src=\"/img/pictures/spark/auto_merge_small_file.png\" alt=\"Spark 自动合并小文件流程\">  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数名</th>\n<th align=\"left\">默认值</th>\n<th align=\"left\">功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">spark.hive.automerge.enabled</td>\n<td align=\"left\">true</td>\n<td align=\"left\">是否开启自动合并小文件功能</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.least.filenum</td>\n<td align=\"left\">3</td>\n<td align=\"left\">分区内文件数小于此值时,不合并小文件(考虑启动任务成本)</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.block.percent</td>\n<td align=\"left\">0.25</td>\n<td align=\"left\">合并后文件的理想大小是 dfs.blocksize 与此参数的乘积(考虑压缩率)</td>\n</tr>\n<tr>\n<td align=\"left\">spark.hive.automerge.shuffle.enabled</td>\n<td align=\"left\">false</td>\n<td align=\"left\">重分区时是否开启 shuffle。true 时为 repartition，false 时为coalesce</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>测试效果样例  </p>\n</blockquote>\n<ol>\n<li>日志信息<br>INFO MergeHiveSmallFileUtil: merge table: user_profile.dwd_user_profile_di ,partition:second=visit/third=visit_7d finished, cost 164 ms.before {total length in bytes: 233 ,file number: 3} ,after {file number: 1}    </li>\n<li>Spark History EventLog<br>{“Event”:”org.apache.spark.sql.hive.execution.AutoMergeSmallFileEvent”,”table”:”user_profile.dwd_user_profile_di”,”part”:”second=visit/third=visit_7d”,”totalLenInByte”:233,”numBefore”:3,”numAfter”:1,”diff”:2,”costInMillSec”:164}    </li>\n</ol>\n<h2 id=\"todo\"><a href=\"#todo\" class=\"headerlink\" title=\"todo\"></a>todo</h2><ol>\n<li>临时文件以 Hive TableDesc 定义的 output format(e.g. org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat) 形式写出，但是无法用 TableDesc定义的 input format来读取。因为 Hive TableDesc 定义的 input/output format 不是一个可用的 Spark SQL DataSource。使用 sparkSession.read.option(“inputFormat”,TableDesc.getInputFormatClassName).option(‘outputformat’,TableDesc.getOutputFormatClassName)也无法解决此问题，因为 spark 默认的 DataSource 为 parquet。没有在源代码中找到更优雅的方式来映射 Hive output format 到 Spark SQL DataSource，求助于 spark dev list 也未收到回复，目前采用的是死板的枚举映射判断 Hive outputformat 的后缀(e.g. orcoutputformat-&gt;orc)。  </li>\n<li>这套合并小文件逻辑稍作修改就可以扩展为 “对指定表或分区进行小文件合并”，扩展 Spark SQL 支持的语法糖，但是由于我对 antlr 语法十分陌生，未敢妄动。      </li>\n<li>在线上灰度一定时间后，将通过 SparkListenerEvent 统计出的信息和 hdfs namenode 平均文件大小来评估具体效果。  </li>\n</ol>"},{"layout":"post","title":"01 Flink Connector","date":"2020-08-14T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":" \n\n与 Spark 类似，Flink 作为分布式计算框架，从外接数据引擎获取数据，计算完成后，并把计算结果输出到外部数据引擎或者触发某些操作。Flink 使用 Connector 来描述外部数据引擎，使用 Source 来定义如何从 Connector 中获取数据，使用 Sink 来定义如何向 Collector 写入数据或者触发某些操作, e.g. 客户端 Feed 实时推荐、告警短信等。  \nFlink 预定义了一些 Source 和 Sink，分为五类:  \n\n1. 基于内存数据结构\n2. 基于文件\n3. 基于 Socket\n4. 自定义 Connector  \n5. 异步 IO\n\n<!-- more -->\n\n![Flink Connector](/img/pictures/flink/flink_connector.png)  \n\n## 基于内存数据结构 \n\n可以直接基于内存中的数据结构：不定长数组(T*)、序列(Seq[T])、迭代器(Iterator[T]) 使用 StreamExecutionEnvironment 相应的接口来构建 Source；输出数据可以直接通过 DataStream#print 或 DataStream#printToErr 来写出到标准输出 / 错误流。  \n\n## 基于文件\n\n从本地 / 远程文件和目录中读取文件内容，可以使用 StreamExecutionEnvironment#readFile 或 readFileStream 相关接口来构建 Source，并支持各类 FileInputFormat；输出数据可以通过 DataStream#writeAsText 或 writeAsCsv 来写入到文本文件或 CSV 文件。  \n\n从文件读取数据时，所有接口最终调用的接口定义如下，需要注意不同场景下的 exactly once 语义： FileProcessingMode 分两种，PROCESS_ONCE 表示只处理目录 / 文件中已有的内容，处理完即退出不再监控文件后续的变化；PROCESS_CONTINUOUSLY 表示周期性(interval 参数)地扫描目录 / 文件以处理新的内容，对处理过的文件做修改会导致该文件二次处理。   \n\n```\ndef readFile[T: TypeInformation](\n                                    inputFormat: FileInputFormat[T],\n                                    filePath: String,\n                                    watchType: FileProcessingMode,\n                                    interval: Long,\n                                    filter: FilePathFilter): DataStream[T]\n```\n\n向文件写入数据时，所有接口最终调用的接口定义如下，写入 TextFile / CsvFile 使用的分别是 TextOutputFormat / ScalaCsvOutputFormat。OutputFormat 可以设置 WriteMode：OVERWRITE 表示 删除目录 / 文件后再创建新的目录 / 文件；NO_OVERWRITE 表示在目录 / 文件已存在时不覆盖，最终会抛出 IOException。  \n\n```\n@deprecated\ndef writeUsingOutputFormat(format: OutputFormat[T]): DataStreamSink[T]\n```\n\n\n## 基于 Socket\n可以从 Socket(host,port)中接收文本数据；也可以将结果数据写入到 Socket     \n \n## 自定义 Connector \nStreamExecutionEnvironment#createInput(InputFormat) 和 DataStream#writeUsingOutputFormat(OutputFormat) 是创建 DataSource 和 Sink 的通用方法，例如使用 AvroInput(Output)Format 连接 Avro 文件、使用 PrintingOutputFormat 实现 DataStream#print、使用 HBaseOutputFormat 写入到 HBase 等。  \napache-flink/flink-connectors 模块内置包括 Kafka、Jdbc 和 ElasticSearch 在内的最常用 Connector，Apache Bahir 工程也提供了写入 Flume、Redis 等数据源的Connector。内置的 Connector 虽然在 Flink 的源代码中，但是由于版本问题(kafka-0.10 / kafka-0.11,es-5 / es-6 / es-7)没有打包到二进制发布包中，因此需要应用提供相关依赖；引用 Apache Bahir 中的 Connector 时也需要引入相关依赖。     \n\n![Flink Source Connectors](/img/pictures/flink/flink_src_connector.png)\n\n## 异步 IO\n使用 Connector 并非 Flink 接入 / 输出数据的唯一手段，另一种不太常见的模式是：在 Flink 函数中调用第三方服务(e.g. webservice、邮件服务器)作为 Source / Sink。Fink 为这种场景提供了异步 IO，以减少阻塞等待延时提高吞吐量。     \n\n\n## 参考 \n\n1. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/  \n2. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html   \n","source":"_posts/2020-08-15-01Flink Connector.md","raw":"---\nlayout:     post\ntitle:     01 Flink Connector\ndate:       2020-08-15\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    -  Flink 1.12\n---\n \n\n与 Spark 类似，Flink 作为分布式计算框架，从外接数据引擎获取数据，计算完成后，并把计算结果输出到外部数据引擎或者触发某些操作。Flink 使用 Connector 来描述外部数据引擎，使用 Source 来定义如何从 Connector 中获取数据，使用 Sink 来定义如何向 Collector 写入数据或者触发某些操作, e.g. 客户端 Feed 实时推荐、告警短信等。  \nFlink 预定义了一些 Source 和 Sink，分为五类:  \n\n1. 基于内存数据结构\n2. 基于文件\n3. 基于 Socket\n4. 自定义 Connector  \n5. 异步 IO\n\n<!-- more -->\n\n![Flink Connector](/img/pictures/flink/flink_connector.png)  \n\n## 基于内存数据结构 \n\n可以直接基于内存中的数据结构：不定长数组(T*)、序列(Seq[T])、迭代器(Iterator[T]) 使用 StreamExecutionEnvironment 相应的接口来构建 Source；输出数据可以直接通过 DataStream#print 或 DataStream#printToErr 来写出到标准输出 / 错误流。  \n\n## 基于文件\n\n从本地 / 远程文件和目录中读取文件内容，可以使用 StreamExecutionEnvironment#readFile 或 readFileStream 相关接口来构建 Source，并支持各类 FileInputFormat；输出数据可以通过 DataStream#writeAsText 或 writeAsCsv 来写入到文本文件或 CSV 文件。  \n\n从文件读取数据时，所有接口最终调用的接口定义如下，需要注意不同场景下的 exactly once 语义： FileProcessingMode 分两种，PROCESS_ONCE 表示只处理目录 / 文件中已有的内容，处理完即退出不再监控文件后续的变化；PROCESS_CONTINUOUSLY 表示周期性(interval 参数)地扫描目录 / 文件以处理新的内容，对处理过的文件做修改会导致该文件二次处理。   \n\n```\ndef readFile[T: TypeInformation](\n                                    inputFormat: FileInputFormat[T],\n                                    filePath: String,\n                                    watchType: FileProcessingMode,\n                                    interval: Long,\n                                    filter: FilePathFilter): DataStream[T]\n```\n\n向文件写入数据时，所有接口最终调用的接口定义如下，写入 TextFile / CsvFile 使用的分别是 TextOutputFormat / ScalaCsvOutputFormat。OutputFormat 可以设置 WriteMode：OVERWRITE 表示 删除目录 / 文件后再创建新的目录 / 文件；NO_OVERWRITE 表示在目录 / 文件已存在时不覆盖，最终会抛出 IOException。  \n\n```\n@deprecated\ndef writeUsingOutputFormat(format: OutputFormat[T]): DataStreamSink[T]\n```\n\n\n## 基于 Socket\n可以从 Socket(host,port)中接收文本数据；也可以将结果数据写入到 Socket     \n \n## 自定义 Connector \nStreamExecutionEnvironment#createInput(InputFormat) 和 DataStream#writeUsingOutputFormat(OutputFormat) 是创建 DataSource 和 Sink 的通用方法，例如使用 AvroInput(Output)Format 连接 Avro 文件、使用 PrintingOutputFormat 实现 DataStream#print、使用 HBaseOutputFormat 写入到 HBase 等。  \napache-flink/flink-connectors 模块内置包括 Kafka、Jdbc 和 ElasticSearch 在内的最常用 Connector，Apache Bahir 工程也提供了写入 Flume、Redis 等数据源的Connector。内置的 Connector 虽然在 Flink 的源代码中，但是由于版本问题(kafka-0.10 / kafka-0.11,es-5 / es-6 / es-7)没有打包到二进制发布包中，因此需要应用提供相关依赖；引用 Apache Bahir 中的 Connector 时也需要引入相关依赖。     \n\n![Flink Source Connectors](/img/pictures/flink/flink_src_connector.png)\n\n## 异步 IO\n使用 Connector 并非 Flink 接入 / 输出数据的唯一手段，另一种不太常见的模式是：在 Flink 函数中调用第三方服务(e.g. webservice、邮件服务器)作为 Source / Sink。Fink 为这种场景提供了异步 IO，以减少阻塞等待延时提高吞吐量。     \n\n\n## 参考 \n\n1. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/  \n2. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html   \n","slug":"2020-08-15-01Flink Connector","published":1,"updated":"2020-08-20T10:55:37.535Z","comments":1,"photos":[],"link":"","_id":"cke2rms64000yy7c74xqm08ga","content":"<p>与 Spark 类似，Flink 作为分布式计算框架，从外接数据引擎获取数据，计算完成后，并把计算结果输出到外部数据引擎或者触发某些操作。Flink 使用 Connector 来描述外部数据引擎，使用 Source 来定义如何从 Connector 中获取数据，使用 Sink 来定义如何向 Collector 写入数据或者触发某些操作, e.g. 客户端 Feed 实时推荐、告警短信等。<br>Flink 预定义了一些 Source 和 Sink，分为五类:  </p>\n<ol>\n<li>基于内存数据结构</li>\n<li>基于文件</li>\n<li>基于 Socket</li>\n<li>自定义 Connector  </li>\n<li>异步 IO</li>\n</ol>\n<a id=\"more\"></a>\n\n<p><img src=\"/img/pictures/flink/flink_connector.png\" alt=\"Flink Connector\">  </p>\n<h2 id=\"基于内存数据结构\"><a href=\"#基于内存数据结构\" class=\"headerlink\" title=\"基于内存数据结构\"></a>基于内存数据结构</h2><p>可以直接基于内存中的数据结构：不定长数组(T*)、序列(Seq[T])、迭代器(Iterator[T]) 使用 StreamExecutionEnvironment 相应的接口来构建 Source；输出数据可以直接通过 DataStream#print 或 DataStream#printToErr 来写出到标准输出 / 错误流。  </p>\n<h2 id=\"基于文件\"><a href=\"#基于文件\" class=\"headerlink\" title=\"基于文件\"></a>基于文件</h2><p>从本地 / 远程文件和目录中读取文件内容，可以使用 StreamExecutionEnvironment#readFile 或 readFileStream 相关接口来构建 Source，并支持各类 FileInputFormat；输出数据可以通过 DataStream#writeAsText 或 writeAsCsv 来写入到文本文件或 CSV 文件。  </p>\n<p>从文件读取数据时，所有接口最终调用的接口定义如下，需要注意不同场景下的 exactly once 语义： FileProcessingMode 分两种，PROCESS_ONCE 表示只处理目录 / 文件中已有的内容，处理完即退出不再监控文件后续的变化；PROCESS_CONTINUOUSLY 表示周期性(interval 参数)地扫描目录 / 文件以处理新的内容，对处理过的文件做修改会导致该文件二次处理。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def readFile[T: TypeInformation](</span><br><span class=\"line\">                                    inputFormat: FileInputFormat[T],</span><br><span class=\"line\">                                    filePath: String,</span><br><span class=\"line\">                                    watchType: FileProcessingMode,</span><br><span class=\"line\">                                    interval: Long,</span><br><span class=\"line\">                                    filter: FilePathFilter): DataStream[T]</span><br></pre></td></tr></table></figure>\n\n<p>向文件写入数据时，所有接口最终调用的接口定义如下，写入 TextFile / CsvFile 使用的分别是 TextOutputFormat / ScalaCsvOutputFormat。OutputFormat 可以设置 WriteMode：OVERWRITE 表示 删除目录 / 文件后再创建新的目录 / 文件；NO_OVERWRITE 表示在目录 / 文件已存在时不覆盖，最终会抛出 IOException。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@deprecated</span><br><span class=\"line\">def writeUsingOutputFormat(format: OutputFormat[T]): DataStreamSink[T]</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"基于-Socket\"><a href=\"#基于-Socket\" class=\"headerlink\" title=\"基于 Socket\"></a>基于 Socket</h2><p>可以从 Socket(host,port)中接收文本数据；也可以将结果数据写入到 Socket     </p>\n<h2 id=\"自定义-Connector\"><a href=\"#自定义-Connector\" class=\"headerlink\" title=\"自定义 Connector\"></a>自定义 Connector</h2><p>StreamExecutionEnvironment#createInput(InputFormat) 和 DataStream#writeUsingOutputFormat(OutputFormat) 是创建 DataSource 和 Sink 的通用方法，例如使用 AvroInput(Output)Format 连接 Avro 文件、使用 PrintingOutputFormat 实现 DataStream#print、使用 HBaseOutputFormat 写入到 HBase 等。<br>apache-flink/flink-connectors 模块内置包括 Kafka、Jdbc 和 ElasticSearch 在内的最常用 Connector，Apache Bahir 工程也提供了写入 Flume、Redis 等数据源的Connector。内置的 Connector 虽然在 Flink 的源代码中，但是由于版本问题(kafka-0.10 / kafka-0.11,es-5 / es-6 / es-7)没有打包到二进制发布包中，因此需要应用提供相关依赖；引用 Apache Bahir 中的 Connector 时也需要引入相关依赖。     </p>\n<p><img src=\"/img/pictures/flink/flink_src_connector.png\" alt=\"Flink Source Connectors\"></p>\n<h2 id=\"异步-IO\"><a href=\"#异步-IO\" class=\"headerlink\" title=\"异步 IO\"></a>异步 IO</h2><p>使用 Connector 并非 Flink 接入 / 输出数据的唯一手段，另一种不太常见的模式是：在 Flink 函数中调用第三方服务(e.g. webservice、邮件服务器)作为 Source / Sink。Fink 为这种场景提供了异步 IO，以减少阻塞等待延时提高吞吐量。     </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/</a>  </li>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html</a>   </li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>与 Spark 类似，Flink 作为分布式计算框架，从外接数据引擎获取数据，计算完成后，并把计算结果输出到外部数据引擎或者触发某些操作。Flink 使用 Connector 来描述外部数据引擎，使用 Source 来定义如何从 Connector 中获取数据，使用 Sink 来定义如何向 Collector 写入数据或者触发某些操作, e.g. 客户端 Feed 实时推荐、告警短信等。<br>Flink 预定义了一些 Source 和 Sink，分为五类:  </p>\n<ol>\n<li>基于内存数据结构</li>\n<li>基于文件</li>\n<li>基于 Socket</li>\n<li>自定义 Connector  </li>\n<li>异步 IO</li>\n</ol>","more":"<p><img src=\"/img/pictures/flink/flink_connector.png\" alt=\"Flink Connector\">  </p>\n<h2 id=\"基于内存数据结构\"><a href=\"#基于内存数据结构\" class=\"headerlink\" title=\"基于内存数据结构\"></a>基于内存数据结构</h2><p>可以直接基于内存中的数据结构：不定长数组(T*)、序列(Seq[T])、迭代器(Iterator[T]) 使用 StreamExecutionEnvironment 相应的接口来构建 Source；输出数据可以直接通过 DataStream#print 或 DataStream#printToErr 来写出到标准输出 / 错误流。  </p>\n<h2 id=\"基于文件\"><a href=\"#基于文件\" class=\"headerlink\" title=\"基于文件\"></a>基于文件</h2><p>从本地 / 远程文件和目录中读取文件内容，可以使用 StreamExecutionEnvironment#readFile 或 readFileStream 相关接口来构建 Source，并支持各类 FileInputFormat；输出数据可以通过 DataStream#writeAsText 或 writeAsCsv 来写入到文本文件或 CSV 文件。  </p>\n<p>从文件读取数据时，所有接口最终调用的接口定义如下，需要注意不同场景下的 exactly once 语义： FileProcessingMode 分两种，PROCESS_ONCE 表示只处理目录 / 文件中已有的内容，处理完即退出不再监控文件后续的变化；PROCESS_CONTINUOUSLY 表示周期性(interval 参数)地扫描目录 / 文件以处理新的内容，对处理过的文件做修改会导致该文件二次处理。   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def readFile[T: TypeInformation](</span><br><span class=\"line\">                                    inputFormat: FileInputFormat[T],</span><br><span class=\"line\">                                    filePath: String,</span><br><span class=\"line\">                                    watchType: FileProcessingMode,</span><br><span class=\"line\">                                    interval: Long,</span><br><span class=\"line\">                                    filter: FilePathFilter): DataStream[T]</span><br></pre></td></tr></table></figure>\n\n<p>向文件写入数据时，所有接口最终调用的接口定义如下，写入 TextFile / CsvFile 使用的分别是 TextOutputFormat / ScalaCsvOutputFormat。OutputFormat 可以设置 WriteMode：OVERWRITE 表示 删除目录 / 文件后再创建新的目录 / 文件；NO_OVERWRITE 表示在目录 / 文件已存在时不覆盖，最终会抛出 IOException。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@deprecated</span><br><span class=\"line\">def writeUsingOutputFormat(format: OutputFormat[T]): DataStreamSink[T]</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"基于-Socket\"><a href=\"#基于-Socket\" class=\"headerlink\" title=\"基于 Socket\"></a>基于 Socket</h2><p>可以从 Socket(host,port)中接收文本数据；也可以将结果数据写入到 Socket     </p>\n<h2 id=\"自定义-Connector\"><a href=\"#自定义-Connector\" class=\"headerlink\" title=\"自定义 Connector\"></a>自定义 Connector</h2><p>StreamExecutionEnvironment#createInput(InputFormat) 和 DataStream#writeUsingOutputFormat(OutputFormat) 是创建 DataSource 和 Sink 的通用方法，例如使用 AvroInput(Output)Format 连接 Avro 文件、使用 PrintingOutputFormat 实现 DataStream#print、使用 HBaseOutputFormat 写入到 HBase 等。<br>apache-flink/flink-connectors 模块内置包括 Kafka、Jdbc 和 ElasticSearch 在内的最常用 Connector，Apache Bahir 工程也提供了写入 Flume、Redis 等数据源的Connector。内置的 Connector 虽然在 Flink 的源代码中，但是由于版本问题(kafka-0.10 / kafka-0.11,es-5 / es-6 / es-7)没有打包到二进制发布包中，因此需要应用提供相关依赖；引用 Apache Bahir 中的 Connector 时也需要引入相关依赖。     </p>\n<p><img src=\"/img/pictures/flink/flink_src_connector.png\" alt=\"Flink Source Connectors\"></p>\n<h2 id=\"异步-IO\"><a href=\"#异步-IO\" class=\"headerlink\" title=\"异步 IO\"></a>异步 IO</h2><p>使用 Connector 并非 Flink 接入 / 输出数据的唯一手段，另一种不太常见的模式是：在 Flink 函数中调用第三方服务(e.g. webservice、邮件服务器)作为 Source / Sink。Fink 为这种场景提供了异步 IO，以减少阻塞等待延时提高吞吐量。     </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/</a>  </li>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/stream/operators/asyncio.html</a>   </li>\n</ol>"},{"layout":"post","title":"02 FlinkKafkaConnector","date":"2020-08-19T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n基于 Flink 1.12-SNAPSHOT 源代码和 Flink-1.11 官方文档   \n\nFlink 提供了 Flink Kafka Connector 读取 / 写入 Kafka，并可以保证\"exactly-once\"语义。\n\n<!-- more -->\n\n## Dependency\nKafka 不同版本之间的通信协议是不同的，因此 Flink 提供了多版本的 Flink-Kafka-Connector。虽然 Flink 源代码中包含多版本的 Flink-Kafka-Connector，但发布安装包中不包含，因此应用需要自带Flink-Kafka-Connector 依赖。\n\nartifactId 中 011 / 010指 kafka 版本，2.11 指 scala 版本。\n\n```\n<!-- for kafka-0.11 -->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-kafka-011_2.11</artifactId>\n    <version>1.11.0</version>\n</dependency>\n```\n\n```\n<!-- for kafka-0.10; FlinkKafkaConnector010 不能在写入 kafka 时保证 exactly-once 语义-->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-kafka-010_2.11</artifactId>\n    <version>1.11.0</version>\n</dependency>\n```\n\n## Kafka Consumer  \n\nFlinkKafkaConsumer 提供了读取 Kafka 一个或多个 Topic 数据的入口，FlinkKafkaConsumer011 对应 flink-connector-kafka-011、FlinkKafkaConsumer010 对应 010....  \n\n必须提供的信息：\n\n1. topic 或者 topic 列表\n2. 序列化 / 序列化 schema，以将 Kafka 二进制数据转换为内存中的对象\n3. consumer 信息：broker (逗号分隔)、consumer group id\n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nstream = env\n    .addSource(new FlinkKafkaConsumer[String](\"topic\", new SimpleStringSchema(), properties))\n```\n\n### 序列化 / 反序列化 schema\n\nFlink 提供了一下 序列化 / 反序列化 schema：\n\n1. TypeInformationSerializationSchema / TypeInformationKeyValueSerializationSchema，前者用于把记录序列化或反序列化，后者用于把记录的 key / value 分别序列化或反序列化。这两种模式应用于 Kafka 数据是被 Flink 写入且读取时，是通用的序列化反序列化 schema 的高性能替代方式    \n2. Json(De)serializationSchema / JSONKeyValue(De)serializationSchema，使用 Jackson 将数据以 ObjectNode 和 Json 之间互转。    \n3. Avro(De)serializationSchema，使用静态的 AVRO 模式定义来 序列化 / 反序列化 数据，依赖是 org.apache.flink:flink-avro；并提供了另外一种变式，可以在 Kafka 的 Registry Schema 中读取数据写入时的模式，依赖是 org.apache.flink:flink-avro-confluent-registry。 AVRO 中最好不包含嵌套结构。    \n4. 其他常用：SimpleStringSchema 等  \n当遇到任何原因导致的反序列化失败时，反序列化返回 null 值。这将触发 Flink 的容错，最终会陷入 反序列化失败->容错重试->反序列化失败 的循环。  \n\n### 定制开始消费位置\n\nFlink Kafka Connector 提供了四种方式 允许定制在 topic partition 开始消费的位置：   \n\n1. setStartFromGroupOffsets(默认方式)：从 consumer group 提交给 zookeeper 的 offset 处开始读取，如果 partition 当前 offset 区间不包含此 offset，那么 auto.offset.reset 将生效。   \n2. setStartFromEarliest / setStartFromLatest：从 partition 的最老 / 最新 数据处开始读取。此模式将忽略 consumer group 提交给 zookeeper 的 offset。   \n3. setStartFromTimestamp：每条记录发送到 broker 或 broker 确认记录时为数据打上的时间戳，使用此方式时将从 时间戳大于等于指定时间戳的记录处开始消费，如果 partition 中最新数据的时间戳低于此值，那么将从最新的数据处开始消费(存疑，等待还是消费)。此模式将忽略 consumer group 提交给 zookeeper 的 offset。    \n4. setStartFromSpecificOffsets：可以指定从每个 partition 的 offset 处开始消费。如果在 topic 的所有 partition 中存在未指定 offset 的 partition，那么为此 partition 将回退(fallback)到 setStartFromGroupOffsets 模式从 zookeeper offset 处开始读取。  \n\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\n \nval myConsumer = new FlinkKafkaConsumer[String](...)\nmyConsumer.setStartFromEarliest()      // start from the earliest record possible\nmyConsumer.setStartFromLatest()        // start from the latest record\nmyConsumer.setStartFromTimestamp(...)  // start from specified epoch timestamp (milliseconds)\nmyConsumer.setStartFromGroupOffsets()  // the default behaviour\n \nval specificStartOffsets = new java.util.HashMap[KafkaTopicPartition, java.lang.Long]()  // start from specific offset\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 0), 23L)\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 1), 31L)\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 2), 43L)\nmyConsumer.setStartFromSpecificOffsets(specificStartOffsets)\nval stream = env.addSource(myConsumer)\n\n```\n\n>\n>当作业从 Checkpoint 或 SavePoint 中恢复时，这些消费位置的定制方法不会影响开始消费位置。  \n>在恢复时，每个 topic partition offset 取值于存储在 SavePoint 或 CheckPoint 中的 offset。   \n\n\n### Consumer 容错  \n\n1. 当启用了 checkpoint 时，Flink Kafka Consumer 会从 topic 中消费记录并周期性地 checkpoint kafka offset 和其他操作的状态。当 Flink 应用失败时 会从最新的 checkpoint 中恢复并从  checkpoint 存储的 topic offset 处开始消费数据。  \n2. 当禁用了 checkpoint 时，Flink Kafka Consumer 会周期性地提交 offset 到 zookeeper。   \n\n### topic / partition 自动发现  \n\n在生产环境中，应用消费的 topic 数量或者 topic 的 partition 数量可能是随 需求 / 负载扩容 等变化的。Flink 提供了机制以满足 topic 和 partition 自动发现。  \n\npartition 自动发现：默认情况是分区自动发现是禁用的，设置 flink.partition-discovery.interval-millis 非负即可启用，所有自动发现的分区都会从分区最开始处消费。   \ntopic 自动发现：同分区自动发现，设置 flink.partition-discovery.interval-millis 非负且设置 topic 为正则表达式即可启用。  \nFlinkKafkaConsumer 内部会启动一个独立的线程定期去 Kafka 获取最新的 meta 信息，并调整作业。  \n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nproperties.set(\"flink.partition-discovery.interval-millis\",\"600000\")\n \nval myConsumer = new FlinkKafkaConsumer[String](\n  java.util.regex.Pattern.compile(\"test-topic-[0-9]\"),\n  new SimpleStringSchema,\n  properties)\n\n```\n\n可以持续自动发现名称满足 以\"test-topic\"开头以 [0-9] 任一数字结尾的 topic。如果仅设置正则表达式而不设置 flink.partition-discovery.interval-millis，则只在应用启动时发现一次，不能持续自动发现。如果仅设置 flink.partition-discovery.interval-millis 不设置正则表达式，则只对分区自动发现。     \n\n### Consumer 提交 Offset 回 Kafka 配置 \n\nFlink Kafka Consumer 允许配置 Consumer 提交 Offset 回 Kafka 的方式，但是并不是为了使用 zookeeper offset 保证容错，而是为了展示消费进度以便于监控和滞后调整。  \nConsumer 提交 Offset  回 Kafka 的方式取决于作业是否启用了 checkpoint。    \n\n1. 启用 checkpoint：当 checkpoint 完成时，Flink Kafka Consumer 将提交存储在 checkpoint 中的 offset，这可以确保 checkpoint offset 和 zookeeper offset 一致。自动提交间隔取决于 checkpoint 间隔，且延时较大。用户可以调用 setCommitOffsetsOnCheckpoints(false) 来禁止提交到 zookeeper，默认情况下为 true。启用 checkpoint 时将忽略 enable.auto.commit / auto.commit.interval.ms 配置。  \n2. 禁用 checkpoint：Flink Kafka Consumer 使用内部 Kafka Client 来周期性提交 offset。因此可以通过配置来 enable.auto.commit / auto.commit.interval.ms 来 启用 / 禁用 / 订制 自动提交 offset 到 zookeeper。 \n\n### 水印线(WaterMark)  \n\n一般称作水印，但是称作水印线会更贴切一些。  \n很多场景下，记录的时间戳嵌入在记录本身或者 ConsumerRecord 的元数据中，此外 consumer 可能需要根据记录的时间戳 兼容数据乱序和触发一些操作。Flink Kafka Consumer 允许指定一个水印线策略。  \n\n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nval myConsumer =\n    new FlinkKafkaConsumer(\"topic\", new SimpleStringSchema(), properties);\nmyConsumer.assignTimestampsAndWatermarks(\n    WatermarkStrategy.\n        .forBoundedOutOfOrderness(Duration.ofSeconds(20)))\nval stream = env.addSource(myConsumer)\n\n```\n\n\nWatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20)) 一般用于为无序数据创建水印策略时设置数据无序程度的上限。假设无序程度的上限为 B，那么当当前数据中时间戳为 T 时，Flink 认为不会再出现比 (T-B)更早的数据，即 如 B=20s，T=2020-08-10 10：00：20，那么认为不会再有早于 2020-08-10 10：00：00 的数据。 如果有早于 T-B 的数据 那么\n会为每一条 早于 T-B 的数据触发独立的行为，e.g. window / 数据更新。过多早于 T-B 的数据会导致性能问题和数据一致性问题。  \n因此无序程度上限 B 的设置十分重要，必须深度了解业务和数据情况才能设置。   \n\n![WaterMark 和 Window](/img/pictures/flink/flink_watermark.png)   \n\n\n## Kafka Producer\n\nFlink 提供了写入数据流到 Kafka 一个或多个 Topic 的功能，FlinkKafkaProducer011 对应 kafka-0.11，FlinkKafkaProducer010 对应 kafka-010...  \n必须提供的信息：  \n\n1. 一个写入数据时默认的 topic 名称  \n2. 序列化方式，将内存中的数据结构序列化成 Kafka 二进制数据  \n3. Kafka Client 配置，bootstrap.servers 是必须的(逗号分隔)  \n4. 一个容错语义，一般是 exactly-once   \n\n\n```\nval stream: DataStream[String] = ...\nProperties properties = new Properties\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nval myProducer = new FlinkKafkaProducer[String](\n        \"my-topic\",                  // default target topic\n        new SimpleStringSchema(),    // serialization schema\n        properties,                  // producer config\n        FlinkKafkaProducer.Semantic.EXACTLY_ONCE) // fault-tolerance\nstream.addSink(myProducer)\n\n```\n\n### Producer 容错\n\n当启用 checkpoint 时，FlinkKafkaProducer011 可以提供 exactly-once 语义，FlinkKafkaProducer010 可以提供 at-least-once 语义 不能提供 exactly-once 语义。    \n可以指定的语义：  \n\n1. Semantic.NONE: Flink 不保证任何事，记录可能会丢失也可能会重复   \n2. Semantic.AT\\_LEAST\\_ONCE(默认): Flink 保证记录不会丢失，但是可能会重复  \n3. Semantic.EXACTLY\\_ONCE: 使用 Kafka 事务来保证 exactly-once 语义。当使用事务向 Kafka 写数据时，需要对相关 topic 的 consumer 设置\"隔离级别(isolation.level)\"。隔离级别分两种 read\\_committed 或 read\\_uncommitted(默认)。   \n\n>  \n>注意事项:\n>      \n>1. Semantic.EXACTLY\\_ONCE mode relies on the ability to commit transactions that were started before taking a checkpoint, after recovering from the said checkpoint. If the time between Flink application crash and completed restart is larger than Kafka’s transaction timeout there will be data loss (Kafka will automatically abort transactions that exceeded timeout time).\n>exactly-once 语义依赖于 提交 “开始于 checkpoint 之前” 和 “从 checkpoint 恢复之后”的事务的能力。如果 Flink 应用从崩溃到完全恢复所用的时间大于 Kafka 事务超时时间，那么数据会丢失(Kafka 会自动放弃超时事务)。 \n>2. 由于“译不准\"，此处似乎也可以理解为 \"exactly-once 语义依赖于提交特殊事务的能力，特殊事务开始于 checkpoint 之前，终止于从 checkpoint 恢复之后\"。事务开启，状态尚未存储而应用挂掉，恢复后事务超时被 Broker 放弃导致了数据丢失，无法保证\"exactly-once\"语义。  \n\n\nFlinkKafkaProducer 要求\"exactly-once\"语义时，topic 的所有 consumer 都必须设置隔离级别，默认为 read\\_uncommitted。    \n\n>  \n>关于 read_committed 模式，在 KafkaConsumer 启用了 read\\_committed 模式时，任何 未完成 / 未终止 的事务将会阻塞对该事务之后的所有事务的读取。  例如:     \n>\n>1. producer 开启 transaction1 并写入了一些记录    \n>2. producer 开启了 transaction2 并写入了其他记录    \n>3. producer commit transaction2  \n>即使 transaction2 已经被 commit，其记录对于所有的 consumer 都不可见，直到 transaction1 被提交或终止。  这种模式有两个含义：  \n>1. 在 Flink application 正常运行期间，输出到 topic 记录的可见性会有一定延迟，这等于已完成 checkpoint 的平均时间。   \n>2. 当 Flink application 失败，将阻塞其写入的 topic 的所有 consumer，直到应用重启或者 Kafka transaction 超时。在有多个 producer / consumer 同时操作同一个 topic 时\"exacty-once\"造成的阻塞风险需要评估。      \n \n### 分区方案  \n\n使用 FlinkKafkaProducer 向 Kafka 写数据时，如果不指定分区器 Partitioner，默认使用 FlinkFixedPartitioner。该 Partitioner 分区的方式是将 Task 所在的实例 Id 按照 topic partition 总数取余将 sink 映射到单个的 topic partition 上，即：partitions[parallelInstanceId % partitions.length]。如果 topic partition 总数为 5，sink 并发度为 2，那么最终只有两个 topic partition 有数据；如果 sink 并发度为 6，那么 1 号 partition 有两个 sink 在写，会导致 Broker 负载均衡问题。  \n如果指定 Partitioner 为 null，会使用 kafka producer 默认的分区方式，sink 可能会轮询写所有 partition。topic partition 的数据比较均衡，但是会相对保持更多的网络连接。     \n因此使用 FlinkFixedPartitioner 并配置 sink 并发度为 topic partition 数量的整数倍较合理。    \n\n## Kerberos 认证  \nFlink 为连接到开启了 Kerberos 认证的 Kafka 集群 通过 FlinkKafkaConnector 提供了支持。只需要在 flink-conf.yaml 中简单配置即可启用 kerberos    \n\n1. 配置以下配置项  \n　　　- security.kerberos.login.use-ticket-cache：默认情况下为 true，Flink 尝试使用 kinit 管理的 ticket cache 中的 kerberos 证书(credentials)，但是当 Flink Job 被部署到 Yarn 和 Mesos 时，这个配置是不生效的，因为 Yarn 和 Mesos 不支持使用 ticket cache 中的证书认证      \n　　　- security.kerberos.login.keytab and security.kerberos.login.principal：使用 kerberos keytab 来认证     \n2. 在 security.kerberos.login.contexts 配置后追加 KafkaClient：此配置告诉 Flink 提供已配置的 kerberos 证书给 Kafka Login Context 以进行 Kafka 认证    \n\n\n一旦基于 Kerberos 认证的 Flink 安全机制启用后，可以在 FlinkKafkaProducer / FlinkKafkaConsumer 中添加两个配置以传给内部的 KafkaClient 来进行 Kafka 认证。    \n\n1. 设置 security.protocol 为 SASL\\_PLAINTEXT(默认 NONE): 此协议用于和 Broker 通信。使用 standalone 部署 Flink 时可以设置 security.protocol 为 SASL\\_SSL。\n2. 设置 sasl.kerberos.service.name 为 kafka(默认 kafka)：这个值应该和 Broker 配置的 sasl.kerberos.service.name 值一致。客户端和服务端的服务名称不一致将导致认证失败\n\n\n## 参考  \n\n1. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer  \n2. https://blog.csdn.net/lmalds/article/details/52704170  \n\n \n","source":"_posts/2020-08-20-02FlinkKafkaConnector.md","raw":"---\nlayout:     post\ntitle:     02 FlinkKafkaConnector\ndate:       2020-08-20\nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    -  Flink 1.12\n---\n\n基于 Flink 1.12-SNAPSHOT 源代码和 Flink-1.11 官方文档   \n\nFlink 提供了 Flink Kafka Connector 读取 / 写入 Kafka，并可以保证\"exactly-once\"语义。\n\n<!-- more -->\n\n## Dependency\nKafka 不同版本之间的通信协议是不同的，因此 Flink 提供了多版本的 Flink-Kafka-Connector。虽然 Flink 源代码中包含多版本的 Flink-Kafka-Connector，但发布安装包中不包含，因此应用需要自带Flink-Kafka-Connector 依赖。\n\nartifactId 中 011 / 010指 kafka 版本，2.11 指 scala 版本。\n\n```\n<!-- for kafka-0.11 -->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-kafka-011_2.11</artifactId>\n    <version>1.11.0</version>\n</dependency>\n```\n\n```\n<!-- for kafka-0.10; FlinkKafkaConnector010 不能在写入 kafka 时保证 exactly-once 语义-->\n<dependency>\n    <groupId>org.apache.flink</groupId>\n    <artifactId>flink-connector-kafka-010_2.11</artifactId>\n    <version>1.11.0</version>\n</dependency>\n```\n\n## Kafka Consumer  \n\nFlinkKafkaConsumer 提供了读取 Kafka 一个或多个 Topic 数据的入口，FlinkKafkaConsumer011 对应 flink-connector-kafka-011、FlinkKafkaConsumer010 对应 010....  \n\n必须提供的信息：\n\n1. topic 或者 topic 列表\n2. 序列化 / 序列化 schema，以将 Kafka 二进制数据转换为内存中的对象\n3. consumer 信息：broker (逗号分隔)、consumer group id\n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nstream = env\n    .addSource(new FlinkKafkaConsumer[String](\"topic\", new SimpleStringSchema(), properties))\n```\n\n### 序列化 / 反序列化 schema\n\nFlink 提供了一下 序列化 / 反序列化 schema：\n\n1. TypeInformationSerializationSchema / TypeInformationKeyValueSerializationSchema，前者用于把记录序列化或反序列化，后者用于把记录的 key / value 分别序列化或反序列化。这两种模式应用于 Kafka 数据是被 Flink 写入且读取时，是通用的序列化反序列化 schema 的高性能替代方式    \n2. Json(De)serializationSchema / JSONKeyValue(De)serializationSchema，使用 Jackson 将数据以 ObjectNode 和 Json 之间互转。    \n3. Avro(De)serializationSchema，使用静态的 AVRO 模式定义来 序列化 / 反序列化 数据，依赖是 org.apache.flink:flink-avro；并提供了另外一种变式，可以在 Kafka 的 Registry Schema 中读取数据写入时的模式，依赖是 org.apache.flink:flink-avro-confluent-registry。 AVRO 中最好不包含嵌套结构。    \n4. 其他常用：SimpleStringSchema 等  \n当遇到任何原因导致的反序列化失败时，反序列化返回 null 值。这将触发 Flink 的容错，最终会陷入 反序列化失败->容错重试->反序列化失败 的循环。  \n\n### 定制开始消费位置\n\nFlink Kafka Connector 提供了四种方式 允许定制在 topic partition 开始消费的位置：   \n\n1. setStartFromGroupOffsets(默认方式)：从 consumer group 提交给 zookeeper 的 offset 处开始读取，如果 partition 当前 offset 区间不包含此 offset，那么 auto.offset.reset 将生效。   \n2. setStartFromEarliest / setStartFromLatest：从 partition 的最老 / 最新 数据处开始读取。此模式将忽略 consumer group 提交给 zookeeper 的 offset。   \n3. setStartFromTimestamp：每条记录发送到 broker 或 broker 确认记录时为数据打上的时间戳，使用此方式时将从 时间戳大于等于指定时间戳的记录处开始消费，如果 partition 中最新数据的时间戳低于此值，那么将从最新的数据处开始消费(存疑，等待还是消费)。此模式将忽略 consumer group 提交给 zookeeper 的 offset。    \n4. setStartFromSpecificOffsets：可以指定从每个 partition 的 offset 处开始消费。如果在 topic 的所有 partition 中存在未指定 offset 的 partition，那么为此 partition 将回退(fallback)到 setStartFromGroupOffsets 模式从 zookeeper offset 处开始读取。  \n\n```\nval env = StreamExecutionEnvironment.getExecutionEnvironment()\n \nval myConsumer = new FlinkKafkaConsumer[String](...)\nmyConsumer.setStartFromEarliest()      // start from the earliest record possible\nmyConsumer.setStartFromLatest()        // start from the latest record\nmyConsumer.setStartFromTimestamp(...)  // start from specified epoch timestamp (milliseconds)\nmyConsumer.setStartFromGroupOffsets()  // the default behaviour\n \nval specificStartOffsets = new java.util.HashMap[KafkaTopicPartition, java.lang.Long]()  // start from specific offset\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 0), 23L)\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 1), 31L)\nspecificStartOffsets.put(new KafkaTopicPartition(\"myTopic\", 2), 43L)\nmyConsumer.setStartFromSpecificOffsets(specificStartOffsets)\nval stream = env.addSource(myConsumer)\n\n```\n\n>\n>当作业从 Checkpoint 或 SavePoint 中恢复时，这些消费位置的定制方法不会影响开始消费位置。  \n>在恢复时，每个 topic partition offset 取值于存储在 SavePoint 或 CheckPoint 中的 offset。   \n\n\n### Consumer 容错  \n\n1. 当启用了 checkpoint 时，Flink Kafka Consumer 会从 topic 中消费记录并周期性地 checkpoint kafka offset 和其他操作的状态。当 Flink 应用失败时 会从最新的 checkpoint 中恢复并从  checkpoint 存储的 topic offset 处开始消费数据。  \n2. 当禁用了 checkpoint 时，Flink Kafka Consumer 会周期性地提交 offset 到 zookeeper。   \n\n### topic / partition 自动发现  \n\n在生产环境中，应用消费的 topic 数量或者 topic 的 partition 数量可能是随 需求 / 负载扩容 等变化的。Flink 提供了机制以满足 topic 和 partition 自动发现。  \n\npartition 自动发现：默认情况是分区自动发现是禁用的，设置 flink.partition-discovery.interval-millis 非负即可启用，所有自动发现的分区都会从分区最开始处消费。   \ntopic 自动发现：同分区自动发现，设置 flink.partition-discovery.interval-millis 非负且设置 topic 为正则表达式即可启用。  \nFlinkKafkaConsumer 内部会启动一个独立的线程定期去 Kafka 获取最新的 meta 信息，并调整作业。  \n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nproperties.set(\"flink.partition-discovery.interval-millis\",\"600000\")\n \nval myConsumer = new FlinkKafkaConsumer[String](\n  java.util.regex.Pattern.compile(\"test-topic-[0-9]\"),\n  new SimpleStringSchema,\n  properties)\n\n```\n\n可以持续自动发现名称满足 以\"test-topic\"开头以 [0-9] 任一数字结尾的 topic。如果仅设置正则表达式而不设置 flink.partition-discovery.interval-millis，则只在应用启动时发现一次，不能持续自动发现。如果仅设置 flink.partition-discovery.interval-millis 不设置正则表达式，则只对分区自动发现。     \n\n### Consumer 提交 Offset 回 Kafka 配置 \n\nFlink Kafka Consumer 允许配置 Consumer 提交 Offset 回 Kafka 的方式，但是并不是为了使用 zookeeper offset 保证容错，而是为了展示消费进度以便于监控和滞后调整。  \nConsumer 提交 Offset  回 Kafka 的方式取决于作业是否启用了 checkpoint。    \n\n1. 启用 checkpoint：当 checkpoint 完成时，Flink Kafka Consumer 将提交存储在 checkpoint 中的 offset，这可以确保 checkpoint offset 和 zookeeper offset 一致。自动提交间隔取决于 checkpoint 间隔，且延时较大。用户可以调用 setCommitOffsetsOnCheckpoints(false) 来禁止提交到 zookeeper，默认情况下为 true。启用 checkpoint 时将忽略 enable.auto.commit / auto.commit.interval.ms 配置。  \n2. 禁用 checkpoint：Flink Kafka Consumer 使用内部 Kafka Client 来周期性提交 offset。因此可以通过配置来 enable.auto.commit / auto.commit.interval.ms 来 启用 / 禁用 / 订制 自动提交 offset 到 zookeeper。 \n\n### 水印线(WaterMark)  \n\n一般称作水印，但是称作水印线会更贴切一些。  \n很多场景下，记录的时间戳嵌入在记录本身或者 ConsumerRecord 的元数据中，此外 consumer 可能需要根据记录的时间戳 兼容数据乱序和触发一些操作。Flink Kafka Consumer 允许指定一个水印线策略。  \n\n\n```\nval properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"test\")\nval myConsumer =\n    new FlinkKafkaConsumer(\"topic\", new SimpleStringSchema(), properties);\nmyConsumer.assignTimestampsAndWatermarks(\n    WatermarkStrategy.\n        .forBoundedOutOfOrderness(Duration.ofSeconds(20)))\nval stream = env.addSource(myConsumer)\n\n```\n\n\nWatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20)) 一般用于为无序数据创建水印策略时设置数据无序程度的上限。假设无序程度的上限为 B，那么当当前数据中时间戳为 T 时，Flink 认为不会再出现比 (T-B)更早的数据，即 如 B=20s，T=2020-08-10 10：00：20，那么认为不会再有早于 2020-08-10 10：00：00 的数据。 如果有早于 T-B 的数据 那么\n会为每一条 早于 T-B 的数据触发独立的行为，e.g. window / 数据更新。过多早于 T-B 的数据会导致性能问题和数据一致性问题。  \n因此无序程度上限 B 的设置十分重要，必须深度了解业务和数据情况才能设置。   \n\n![WaterMark 和 Window](/img/pictures/flink/flink_watermark.png)   \n\n\n## Kafka Producer\n\nFlink 提供了写入数据流到 Kafka 一个或多个 Topic 的功能，FlinkKafkaProducer011 对应 kafka-0.11，FlinkKafkaProducer010 对应 kafka-010...  \n必须提供的信息：  \n\n1. 一个写入数据时默认的 topic 名称  \n2. 序列化方式，将内存中的数据结构序列化成 Kafka 二进制数据  \n3. Kafka Client 配置，bootstrap.servers 是必须的(逗号分隔)  \n4. 一个容错语义，一般是 exactly-once   \n\n\n```\nval stream: DataStream[String] = ...\nProperties properties = new Properties\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nval myProducer = new FlinkKafkaProducer[String](\n        \"my-topic\",                  // default target topic\n        new SimpleStringSchema(),    // serialization schema\n        properties,                  // producer config\n        FlinkKafkaProducer.Semantic.EXACTLY_ONCE) // fault-tolerance\nstream.addSink(myProducer)\n\n```\n\n### Producer 容错\n\n当启用 checkpoint 时，FlinkKafkaProducer011 可以提供 exactly-once 语义，FlinkKafkaProducer010 可以提供 at-least-once 语义 不能提供 exactly-once 语义。    \n可以指定的语义：  \n\n1. Semantic.NONE: Flink 不保证任何事，记录可能会丢失也可能会重复   \n2. Semantic.AT\\_LEAST\\_ONCE(默认): Flink 保证记录不会丢失，但是可能会重复  \n3. Semantic.EXACTLY\\_ONCE: 使用 Kafka 事务来保证 exactly-once 语义。当使用事务向 Kafka 写数据时，需要对相关 topic 的 consumer 设置\"隔离级别(isolation.level)\"。隔离级别分两种 read\\_committed 或 read\\_uncommitted(默认)。   \n\n>  \n>注意事项:\n>      \n>1. Semantic.EXACTLY\\_ONCE mode relies on the ability to commit transactions that were started before taking a checkpoint, after recovering from the said checkpoint. If the time between Flink application crash and completed restart is larger than Kafka’s transaction timeout there will be data loss (Kafka will automatically abort transactions that exceeded timeout time).\n>exactly-once 语义依赖于 提交 “开始于 checkpoint 之前” 和 “从 checkpoint 恢复之后”的事务的能力。如果 Flink 应用从崩溃到完全恢复所用的时间大于 Kafka 事务超时时间，那么数据会丢失(Kafka 会自动放弃超时事务)。 \n>2. 由于“译不准\"，此处似乎也可以理解为 \"exactly-once 语义依赖于提交特殊事务的能力，特殊事务开始于 checkpoint 之前，终止于从 checkpoint 恢复之后\"。事务开启，状态尚未存储而应用挂掉，恢复后事务超时被 Broker 放弃导致了数据丢失，无法保证\"exactly-once\"语义。  \n\n\nFlinkKafkaProducer 要求\"exactly-once\"语义时，topic 的所有 consumer 都必须设置隔离级别，默认为 read\\_uncommitted。    \n\n>  \n>关于 read_committed 模式，在 KafkaConsumer 启用了 read\\_committed 模式时，任何 未完成 / 未终止 的事务将会阻塞对该事务之后的所有事务的读取。  例如:     \n>\n>1. producer 开启 transaction1 并写入了一些记录    \n>2. producer 开启了 transaction2 并写入了其他记录    \n>3. producer commit transaction2  \n>即使 transaction2 已经被 commit，其记录对于所有的 consumer 都不可见，直到 transaction1 被提交或终止。  这种模式有两个含义：  \n>1. 在 Flink application 正常运行期间，输出到 topic 记录的可见性会有一定延迟，这等于已完成 checkpoint 的平均时间。   \n>2. 当 Flink application 失败，将阻塞其写入的 topic 的所有 consumer，直到应用重启或者 Kafka transaction 超时。在有多个 producer / consumer 同时操作同一个 topic 时\"exacty-once\"造成的阻塞风险需要评估。      \n \n### 分区方案  \n\n使用 FlinkKafkaProducer 向 Kafka 写数据时，如果不指定分区器 Partitioner，默认使用 FlinkFixedPartitioner。该 Partitioner 分区的方式是将 Task 所在的实例 Id 按照 topic partition 总数取余将 sink 映射到单个的 topic partition 上，即：partitions[parallelInstanceId % partitions.length]。如果 topic partition 总数为 5，sink 并发度为 2，那么最终只有两个 topic partition 有数据；如果 sink 并发度为 6，那么 1 号 partition 有两个 sink 在写，会导致 Broker 负载均衡问题。  \n如果指定 Partitioner 为 null，会使用 kafka producer 默认的分区方式，sink 可能会轮询写所有 partition。topic partition 的数据比较均衡，但是会相对保持更多的网络连接。     \n因此使用 FlinkFixedPartitioner 并配置 sink 并发度为 topic partition 数量的整数倍较合理。    \n\n## Kerberos 认证  \nFlink 为连接到开启了 Kerberos 认证的 Kafka 集群 通过 FlinkKafkaConnector 提供了支持。只需要在 flink-conf.yaml 中简单配置即可启用 kerberos    \n\n1. 配置以下配置项  \n　　　- security.kerberos.login.use-ticket-cache：默认情况下为 true，Flink 尝试使用 kinit 管理的 ticket cache 中的 kerberos 证书(credentials)，但是当 Flink Job 被部署到 Yarn 和 Mesos 时，这个配置是不生效的，因为 Yarn 和 Mesos 不支持使用 ticket cache 中的证书认证      \n　　　- security.kerberos.login.keytab and security.kerberos.login.principal：使用 kerberos keytab 来认证     \n2. 在 security.kerberos.login.contexts 配置后追加 KafkaClient：此配置告诉 Flink 提供已配置的 kerberos 证书给 Kafka Login Context 以进行 Kafka 认证    \n\n\n一旦基于 Kerberos 认证的 Flink 安全机制启用后，可以在 FlinkKafkaProducer / FlinkKafkaConsumer 中添加两个配置以传给内部的 KafkaClient 来进行 Kafka 认证。    \n\n1. 设置 security.protocol 为 SASL\\_PLAINTEXT(默认 NONE): 此协议用于和 Broker 通信。使用 standalone 部署 Flink 时可以设置 security.protocol 为 SASL\\_SSL。\n2. 设置 sasl.kerberos.service.name 为 kafka(默认 kafka)：这个值应该和 Broker 配置的 sasl.kerberos.service.name 值一致。客户端和服务端的服务名称不一致将导致认证失败\n\n\n## 参考  \n\n1. https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer  \n2. https://blog.csdn.net/lmalds/article/details/52704170  \n\n \n","slug":"2020-08-20-02FlinkKafkaConnector","published":1,"updated":"2020-08-20T12:12:53.370Z","comments":1,"photos":[],"link":"","_id":"cke2rms690011y7c7exja3fjr","content":"<p>基于 Flink 1.12-SNAPSHOT 源代码和 Flink-1.11 官方文档   </p>\n<p>Flink 提供了 Flink Kafka Connector 读取 / 写入 Kafka，并可以保证”exactly-once”语义。</p>\n<a id=\"more\"></a>\n\n<h2 id=\"Dependency\"><a href=\"#Dependency\" class=\"headerlink\" title=\"Dependency\"></a>Dependency</h2><p>Kafka 不同版本之间的通信协议是不同的，因此 Flink 提供了多版本的 Flink-Kafka-Connector。虽然 Flink 源代码中包含多版本的 Flink-Kafka-Connector，但发布安装包中不包含，因此应用需要自带Flink-Kafka-Connector 依赖。</p>\n<p>artifactId 中 011 / 010指 kafka 版本，2.11 指 scala 版本。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- for kafka-0.11 --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka-011_2.11&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- for kafka-0.10; FlinkKafkaConnector010 不能在写入 kafka 时保证 exactly-once 语义--&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka-010_2.11&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Kafka-Consumer\"><a href=\"#Kafka-Consumer\" class=\"headerlink\" title=\"Kafka Consumer\"></a>Kafka Consumer</h2><p>FlinkKafkaConsumer 提供了读取 Kafka 一个或多个 Topic 数据的入口，FlinkKafkaConsumer011 对应 flink-connector-kafka-011、FlinkKafkaConsumer010 对应 010….  </p>\n<p>必须提供的信息：</p>\n<ol>\n<li>topic 或者 topic 列表</li>\n<li>序列化 / 序列化 schema，以将 Kafka 二进制数据转换为内存中的对象</li>\n<li>consumer 信息：broker (逗号分隔)、consumer group id</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">stream &#x3D; env</span><br><span class=\"line\">    .addSource(new FlinkKafkaConsumer[String](&quot;topic&quot;, new SimpleStringSchema(), properties))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"序列化-反序列化-schema\"><a href=\"#序列化-反序列化-schema\" class=\"headerlink\" title=\"序列化 / 反序列化 schema\"></a>序列化 / 反序列化 schema</h3><p>Flink 提供了一下 序列化 / 反序列化 schema：</p>\n<ol>\n<li>TypeInformationSerializationSchema / TypeInformationKeyValueSerializationSchema，前者用于把记录序列化或反序列化，后者用于把记录的 key / value 分别序列化或反序列化。这两种模式应用于 Kafka 数据是被 Flink 写入且读取时，是通用的序列化反序列化 schema 的高性能替代方式    </li>\n<li>Json(De)serializationSchema / JSONKeyValue(De)serializationSchema，使用 Jackson 将数据以 ObjectNode 和 Json 之间互转。    </li>\n<li>Avro(De)serializationSchema，使用静态的 AVRO 模式定义来 序列化 / 反序列化 数据，依赖是 org.apache.flink:flink-avro；并提供了另外一种变式，可以在 Kafka 的 Registry Schema 中读取数据写入时的模式，依赖是 org.apache.flink:flink-avro-confluent-registry。 AVRO 中最好不包含嵌套结构。    </li>\n<li>其他常用：SimpleStringSchema 等<br>当遇到任何原因导致的反序列化失败时，反序列化返回 null 值。这将触发 Flink 的容错，最终会陷入 反序列化失败-&gt;容错重试-&gt;反序列化失败 的循环。  </li>\n</ol>\n<h3 id=\"定制开始消费位置\"><a href=\"#定制开始消费位置\" class=\"headerlink\" title=\"定制开始消费位置\"></a>定制开始消费位置</h3><p>Flink Kafka Connector 提供了四种方式 允许定制在 topic partition 开始消费的位置：   </p>\n<ol>\n<li>setStartFromGroupOffsets(默认方式)：从 consumer group 提交给 zookeeper 的 offset 处开始读取，如果 partition 当前 offset 区间不包含此 offset，那么 auto.offset.reset 将生效。   </li>\n<li>setStartFromEarliest / setStartFromLatest：从 partition 的最老 / 最新 数据处开始读取。此模式将忽略 consumer group 提交给 zookeeper 的 offset。   </li>\n<li>setStartFromTimestamp：每条记录发送到 broker 或 broker 确认记录时为数据打上的时间戳，使用此方式时将从 时间戳大于等于指定时间戳的记录处开始消费，如果 partition 中最新数据的时间戳低于此值，那么将从最新的数据处开始消费(存疑，等待还是消费)。此模式将忽略 consumer group 提交给 zookeeper 的 offset。    </li>\n<li>setStartFromSpecificOffsets：可以指定从每个 partition 的 offset 处开始消费。如果在 topic 的所有 partition 中存在未指定 offset 的 partition，那么为此 partition 将回退(fallback)到 setStartFromGroupOffsets 模式从 zookeeper offset 处开始读取。  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\"> </span><br><span class=\"line\">val myConsumer &#x3D; new FlinkKafkaConsumer[String](...)</span><br><span class=\"line\">myConsumer.setStartFromEarliest()      &#x2F;&#x2F; start from the earliest record possible</span><br><span class=\"line\">myConsumer.setStartFromLatest()        &#x2F;&#x2F; start from the latest record</span><br><span class=\"line\">myConsumer.setStartFromTimestamp(...)  &#x2F;&#x2F; start from specified epoch timestamp (milliseconds)</span><br><span class=\"line\">myConsumer.setStartFromGroupOffsets()  &#x2F;&#x2F; the default behaviour</span><br><span class=\"line\"> </span><br><span class=\"line\">val specificStartOffsets &#x3D; new java.util.HashMap[KafkaTopicPartition, java.lang.Long]()  &#x2F;&#x2F; start from specific offset</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 0), 23L)</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 1), 31L)</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 2), 43L)</span><br><span class=\"line\">myConsumer.setStartFromSpecificOffsets(specificStartOffsets)</span><br><span class=\"line\">val stream &#x3D; env.addSource(myConsumer)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>当作业从 Checkpoint 或 SavePoint 中恢复时，这些消费位置的定制方法不会影响开始消费位置。<br>在恢复时，每个 topic partition offset 取值于存储在 SavePoint 或 CheckPoint 中的 offset。   </p>\n</blockquote>\n<h3 id=\"Consumer-容错\"><a href=\"#Consumer-容错\" class=\"headerlink\" title=\"Consumer 容错\"></a>Consumer 容错</h3><ol>\n<li>当启用了 checkpoint 时，Flink Kafka Consumer 会从 topic 中消费记录并周期性地 checkpoint kafka offset 和其他操作的状态。当 Flink 应用失败时 会从最新的 checkpoint 中恢复并从  checkpoint 存储的 topic offset 处开始消费数据。  </li>\n<li>当禁用了 checkpoint 时，Flink Kafka Consumer 会周期性地提交 offset 到 zookeeper。   </li>\n</ol>\n<h3 id=\"topic-partition-自动发现\"><a href=\"#topic-partition-自动发现\" class=\"headerlink\" title=\"topic / partition 自动发现\"></a>topic / partition 自动发现</h3><p>在生产环境中，应用消费的 topic 数量或者 topic 的 partition 数量可能是随 需求 / 负载扩容 等变化的。Flink 提供了机制以满足 topic 和 partition 自动发现。  </p>\n<p>partition 自动发现：默认情况是分区自动发现是禁用的，设置 flink.partition-discovery.interval-millis 非负即可启用，所有自动发现的分区都会从分区最开始处消费。<br>topic 自动发现：同分区自动发现，设置 flink.partition-discovery.interval-millis 非负且设置 topic 为正则表达式即可启用。<br>FlinkKafkaConsumer 内部会启动一个独立的线程定期去 Kafka 获取最新的 meta 信息，并调整作业。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">properties.set(&quot;flink.partition-discovery.interval-millis&quot;,&quot;600000&quot;)</span><br><span class=\"line\"> </span><br><span class=\"line\">val myConsumer &#x3D; new FlinkKafkaConsumer[String](</span><br><span class=\"line\">  java.util.regex.Pattern.compile(&quot;test-topic-[0-9]&quot;),</span><br><span class=\"line\">  new SimpleStringSchema,</span><br><span class=\"line\">  properties)</span><br></pre></td></tr></table></figure>\n\n<p>可以持续自动发现名称满足 以”test-topic”开头以 [0-9] 任一数字结尾的 topic。如果仅设置正则表达式而不设置 flink.partition-discovery.interval-millis，则只在应用启动时发现一次，不能持续自动发现。如果仅设置 flink.partition-discovery.interval-millis 不设置正则表达式，则只对分区自动发现。     </p>\n<h3 id=\"Consumer-提交-Offset-回-Kafka-配置\"><a href=\"#Consumer-提交-Offset-回-Kafka-配置\" class=\"headerlink\" title=\"Consumer 提交 Offset 回 Kafka 配置\"></a>Consumer 提交 Offset 回 Kafka 配置</h3><p>Flink Kafka Consumer 允许配置 Consumer 提交 Offset 回 Kafka 的方式，但是并不是为了使用 zookeeper offset 保证容错，而是为了展示消费进度以便于监控和滞后调整。<br>Consumer 提交 Offset  回 Kafka 的方式取决于作业是否启用了 checkpoint。    </p>\n<ol>\n<li>启用 checkpoint：当 checkpoint 完成时，Flink Kafka Consumer 将提交存储在 checkpoint 中的 offset，这可以确保 checkpoint offset 和 zookeeper offset 一致。自动提交间隔取决于 checkpoint 间隔，且延时较大。用户可以调用 setCommitOffsetsOnCheckpoints(false) 来禁止提交到 zookeeper，默认情况下为 true。启用 checkpoint 时将忽略 enable.auto.commit / auto.commit.interval.ms 配置。  </li>\n<li>禁用 checkpoint：Flink Kafka Consumer 使用内部 Kafka Client 来周期性提交 offset。因此可以通过配置来 enable.auto.commit / auto.commit.interval.ms 来 启用 / 禁用 / 订制 自动提交 offset 到 zookeeper。 </li>\n</ol>\n<h3 id=\"水印线-WaterMark\"><a href=\"#水印线-WaterMark\" class=\"headerlink\" title=\"水印线(WaterMark)\"></a>水印线(WaterMark)</h3><p>一般称作水印，但是称作水印线会更贴切一些。<br>很多场景下，记录的时间戳嵌入在记录本身或者 ConsumerRecord 的元数据中，此外 consumer 可能需要根据记录的时间戳 兼容数据乱序和触发一些操作。Flink Kafka Consumer 允许指定一个水印线策略。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">val myConsumer &#x3D;</span><br><span class=\"line\">    new FlinkKafkaConsumer(&quot;topic&quot;, new SimpleStringSchema(), properties);</span><br><span class=\"line\">myConsumer.assignTimestampsAndWatermarks(</span><br><span class=\"line\">    WatermarkStrategy.</span><br><span class=\"line\">        .forBoundedOutOfOrderness(Duration.ofSeconds(20)))</span><br><span class=\"line\">val stream &#x3D; env.addSource(myConsumer)</span><br></pre></td></tr></table></figure>\n\n\n<p>WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20)) 一般用于为无序数据创建水印策略时设置数据无序程度的上限。假设无序程度的上限为 B，那么当当前数据中时间戳为 T 时，Flink 认为不会再出现比 (T-B)更早的数据，即 如 B=20s，T=2020-08-10 10：00：20，那么认为不会再有早于 2020-08-10 10：00：00 的数据。 如果有早于 T-B 的数据 那么\n会为每一条 早于 T-B 的数据触发独立的行为，e.g. window / 数据更新。过多早于 T-B 的数据会导致性能问题和数据一致性问题。<br>因此无序程度上限 B 的设置十分重要，必须深度了解业务和数据情况才能设置。   </p>\n<p><img src=\"/img/pictures/flink/flink_watermark.png\" alt=\"WaterMark 和 Window\">   </p>\n<h2 id=\"Kafka-Producer\"><a href=\"#Kafka-Producer\" class=\"headerlink\" title=\"Kafka Producer\"></a>Kafka Producer</h2><p>Flink 提供了写入数据流到 Kafka 一个或多个 Topic 的功能，FlinkKafkaProducer011 对应 kafka-0.11，FlinkKafkaProducer010 对应 kafka-010…<br>必须提供的信息：  </p>\n<ol>\n<li>一个写入数据时默认的 topic 名称  </li>\n<li>序列化方式，将内存中的数据结构序列化成 Kafka 二进制数据  </li>\n<li>Kafka Client 配置，bootstrap.servers 是必须的(逗号分隔)  </li>\n<li>一个容错语义，一般是 exactly-once   </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[String] &#x3D; ...</span><br><span class=\"line\">Properties properties &#x3D; new Properties</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">val myProducer &#x3D; new FlinkKafkaProducer[String](</span><br><span class=\"line\">        &quot;my-topic&quot;,                  &#x2F;&#x2F; default target topic</span><br><span class=\"line\">        new SimpleStringSchema(),    &#x2F;&#x2F; serialization schema</span><br><span class=\"line\">        properties,                  &#x2F;&#x2F; producer config</span><br><span class=\"line\">        FlinkKafkaProducer.Semantic.EXACTLY_ONCE) &#x2F;&#x2F; fault-tolerance</span><br><span class=\"line\">stream.addSink(myProducer)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Producer-容错\"><a href=\"#Producer-容错\" class=\"headerlink\" title=\"Producer 容错\"></a>Producer 容错</h3><p>当启用 checkpoint 时，FlinkKafkaProducer011 可以提供 exactly-once 语义，FlinkKafkaProducer010 可以提供 at-least-once 语义 不能提供 exactly-once 语义。<br>可以指定的语义：  </p>\n<ol>\n<li>Semantic.NONE: Flink 不保证任何事，记录可能会丢失也可能会重复   </li>\n<li>Semantic.AT_LEAST_ONCE(默认): Flink 保证记录不会丢失，但是可能会重复  </li>\n<li>Semantic.EXACTLY_ONCE: 使用 Kafka 事务来保证 exactly-once 语义。当使用事务向 Kafka 写数据时，需要对相关 topic 的 consumer 设置”隔离级别(isolation.level)”。隔离级别分两种 read_committed 或 read_uncommitted(默认)。   </li>\n</ol>\n<blockquote>\n<p>注意事项:</p>\n<ol>\n<li>Semantic.EXACTLY_ONCE mode relies on the ability to commit transactions that were started before taking a checkpoint, after recovering from the said checkpoint. If the time between Flink application crash and completed restart is larger than Kafka’s transaction timeout there will be data loss (Kafka will automatically abort transactions that exceeded timeout time).\nexactly-once 语义依赖于 提交 “开始于 checkpoint 之前” 和 “从 checkpoint 恢复之后”的事务的能力。如果 Flink 应用从崩溃到完全恢复所用的时间大于 Kafka 事务超时时间，那么数据会丢失(Kafka 会自动放弃超时事务)。 </li>\n<li>由于“译不准”，此处似乎也可以理解为 “exactly-once 语义依赖于提交特殊事务的能力，特殊事务开始于 checkpoint 之前，终止于从 checkpoint 恢复之后”。事务开启，状态尚未存储而应用挂掉，恢复后事务超时被 Broker 放弃导致了数据丢失，无法保证”exactly-once”语义。  </li>\n</ol>\n</blockquote>\n<p>FlinkKafkaProducer 要求”exactly-once”语义时，topic 的所有 consumer 都必须设置隔离级别，默认为 read_uncommitted。    </p>\n<blockquote>\n<p>关于 read_committed 模式，在 KafkaConsumer 启用了 read_committed 模式时，任何 未完成 / 未终止 的事务将会阻塞对该事务之后的所有事务的读取。  例如:     </p>\n<ol>\n<li>producer 开启 transaction1 并写入了一些记录    </li>\n<li>producer 开启了 transaction2 并写入了其他记录    </li>\n<li>producer commit transaction2<br>即使 transaction2 已经被 commit，其记录对于所有的 consumer 都不可见，直到 transaction1 被提交或终止。  这种模式有两个含义：  </li>\n<li>在 Flink application 正常运行期间，输出到 topic 记录的可见性会有一定延迟，这等于已完成 checkpoint 的平均时间。   </li>\n<li>当 Flink application 失败，将阻塞其写入的 topic 的所有 consumer，直到应用重启或者 Kafka transaction 超时。在有多个 producer / consumer 同时操作同一个 topic 时”exacty-once”造成的阻塞风险需要评估。      </li>\n</ol>\n</blockquote>\n<h3 id=\"分区方案\"><a href=\"#分区方案\" class=\"headerlink\" title=\"分区方案\"></a>分区方案</h3><p>使用 FlinkKafkaProducer 向 Kafka 写数据时，如果不指定分区器 Partitioner，默认使用 FlinkFixedPartitioner。该 Partitioner 分区的方式是将 Task 所在的实例 Id 按照 topic partition 总数取余将 sink 映射到单个的 topic partition 上，即：partitions[parallelInstanceId % partitions.length]。如果 topic partition 总数为 5，sink 并发度为 2，那么最终只有两个 topic partition 有数据；如果 sink 并发度为 6，那么 1 号 partition 有两个 sink 在写，会导致 Broker 负载均衡问题。<br>如果指定 Partitioner 为 null，会使用 kafka producer 默认的分区方式，sink 可能会轮询写所有 partition。topic partition 的数据比较均衡，但是会相对保持更多的网络连接。<br>因此使用 FlinkFixedPartitioner 并配置 sink 并发度为 topic partition 数量的整数倍较合理。    </p>\n<h2 id=\"Kerberos-认证\"><a href=\"#Kerberos-认证\" class=\"headerlink\" title=\"Kerberos 认证\"></a>Kerberos 认证</h2><p>Flink 为连接到开启了 Kerberos 认证的 Kafka 集群 通过 FlinkKafkaConnector 提供了支持。只需要在 flink-conf.yaml 中简单配置即可启用 kerberos    </p>\n<ol>\n<li>配置以下配置项<br>　　　- security.kerberos.login.use-ticket-cache：默认情况下为 true，Flink 尝试使用 kinit 管理的 ticket cache 中的 kerberos 证书(credentials)，但是当 Flink Job 被部署到 Yarn 和 Mesos 时，这个配置是不生效的，因为 Yarn 和 Mesos 不支持使用 ticket cache 中的证书认证<br>　　　- security.kerberos.login.keytab and security.kerberos.login.principal：使用 kerberos keytab 来认证     </li>\n<li>在 security.kerberos.login.contexts 配置后追加 KafkaClient：此配置告诉 Flink 提供已配置的 kerberos 证书给 Kafka Login Context 以进行 Kafka 认证    </li>\n</ol>\n<p>一旦基于 Kerberos 认证的 Flink 安全机制启用后，可以在 FlinkKafkaProducer / FlinkKafkaConsumer 中添加两个配置以传给内部的 KafkaClient 来进行 Kafka 认证。    </p>\n<ol>\n<li>设置 security.protocol 为 SASL_PLAINTEXT(默认 NONE): 此协议用于和 Broker 通信。使用 standalone 部署 Flink 时可以设置 security.protocol 为 SASL_SSL。</li>\n<li>设置 sasl.kerberos.service.name 为 kafka(默认 kafka)：这个值应该和 Broker 配置的 sasl.kerberos.service.name 值一致。客户端和服务端的服务名称不一致将导致认证失败</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer</a>  </li>\n<li><a href=\"https://blog.csdn.net/lmalds/article/details/52704170\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/lmalds/article/details/52704170</a>  </li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>基于 Flink 1.12-SNAPSHOT 源代码和 Flink-1.11 官方文档   </p>\n<p>Flink 提供了 Flink Kafka Connector 读取 / 写入 Kafka，并可以保证”exactly-once”语义。</p>","more":"<h2 id=\"Dependency\"><a href=\"#Dependency\" class=\"headerlink\" title=\"Dependency\"></a>Dependency</h2><p>Kafka 不同版本之间的通信协议是不同的，因此 Flink 提供了多版本的 Flink-Kafka-Connector。虽然 Flink 源代码中包含多版本的 Flink-Kafka-Connector，但发布安装包中不包含，因此应用需要自带Flink-Kafka-Connector 依赖。</p>\n<p>artifactId 中 011 / 010指 kafka 版本，2.11 指 scala 版本。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- for kafka-0.11 --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka-011_2.11&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- for kafka-0.10; FlinkKafkaConnector010 不能在写入 kafka 时保证 exactly-once 语义--&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;flink-connector-kafka-010_2.11&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Kafka-Consumer\"><a href=\"#Kafka-Consumer\" class=\"headerlink\" title=\"Kafka Consumer\"></a>Kafka Consumer</h2><p>FlinkKafkaConsumer 提供了读取 Kafka 一个或多个 Topic 数据的入口，FlinkKafkaConsumer011 对应 flink-connector-kafka-011、FlinkKafkaConsumer010 对应 010….  </p>\n<p>必须提供的信息：</p>\n<ol>\n<li>topic 或者 topic 列表</li>\n<li>序列化 / 序列化 schema，以将 Kafka 二进制数据转换为内存中的对象</li>\n<li>consumer 信息：broker (逗号分隔)、consumer group id</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">stream &#x3D; env</span><br><span class=\"line\">    .addSource(new FlinkKafkaConsumer[String](&quot;topic&quot;, new SimpleStringSchema(), properties))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"序列化-反序列化-schema\"><a href=\"#序列化-反序列化-schema\" class=\"headerlink\" title=\"序列化 / 反序列化 schema\"></a>序列化 / 反序列化 schema</h3><p>Flink 提供了一下 序列化 / 反序列化 schema：</p>\n<ol>\n<li>TypeInformationSerializationSchema / TypeInformationKeyValueSerializationSchema，前者用于把记录序列化或反序列化，后者用于把记录的 key / value 分别序列化或反序列化。这两种模式应用于 Kafka 数据是被 Flink 写入且读取时，是通用的序列化反序列化 schema 的高性能替代方式    </li>\n<li>Json(De)serializationSchema / JSONKeyValue(De)serializationSchema，使用 Jackson 将数据以 ObjectNode 和 Json 之间互转。    </li>\n<li>Avro(De)serializationSchema，使用静态的 AVRO 模式定义来 序列化 / 反序列化 数据，依赖是 org.apache.flink:flink-avro；并提供了另外一种变式，可以在 Kafka 的 Registry Schema 中读取数据写入时的模式，依赖是 org.apache.flink:flink-avro-confluent-registry。 AVRO 中最好不包含嵌套结构。    </li>\n<li>其他常用：SimpleStringSchema 等<br>当遇到任何原因导致的反序列化失败时，反序列化返回 null 值。这将触发 Flink 的容错，最终会陷入 反序列化失败-&gt;容错重试-&gt;反序列化失败 的循环。  </li>\n</ol>\n<h3 id=\"定制开始消费位置\"><a href=\"#定制开始消费位置\" class=\"headerlink\" title=\"定制开始消费位置\"></a>定制开始消费位置</h3><p>Flink Kafka Connector 提供了四种方式 允许定制在 topic partition 开始消费的位置：   </p>\n<ol>\n<li>setStartFromGroupOffsets(默认方式)：从 consumer group 提交给 zookeeper 的 offset 处开始读取，如果 partition 当前 offset 区间不包含此 offset，那么 auto.offset.reset 将生效。   </li>\n<li>setStartFromEarliest / setStartFromLatest：从 partition 的最老 / 最新 数据处开始读取。此模式将忽略 consumer group 提交给 zookeeper 的 offset。   </li>\n<li>setStartFromTimestamp：每条记录发送到 broker 或 broker 确认记录时为数据打上的时间戳，使用此方式时将从 时间戳大于等于指定时间戳的记录处开始消费，如果 partition 中最新数据的时间戳低于此值，那么将从最新的数据处开始消费(存疑，等待还是消费)。此模式将忽略 consumer group 提交给 zookeeper 的 offset。    </li>\n<li>setStartFromSpecificOffsets：可以指定从每个 partition 的 offset 处开始消费。如果在 topic 的所有 partition 中存在未指定 offset 的 partition，那么为此 partition 将回退(fallback)到 setStartFromGroupOffsets 模式从 zookeeper offset 处开始读取。  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class=\"line\"> </span><br><span class=\"line\">val myConsumer &#x3D; new FlinkKafkaConsumer[String](...)</span><br><span class=\"line\">myConsumer.setStartFromEarliest()      &#x2F;&#x2F; start from the earliest record possible</span><br><span class=\"line\">myConsumer.setStartFromLatest()        &#x2F;&#x2F; start from the latest record</span><br><span class=\"line\">myConsumer.setStartFromTimestamp(...)  &#x2F;&#x2F; start from specified epoch timestamp (milliseconds)</span><br><span class=\"line\">myConsumer.setStartFromGroupOffsets()  &#x2F;&#x2F; the default behaviour</span><br><span class=\"line\"> </span><br><span class=\"line\">val specificStartOffsets &#x3D; new java.util.HashMap[KafkaTopicPartition, java.lang.Long]()  &#x2F;&#x2F; start from specific offset</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 0), 23L)</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 1), 31L)</span><br><span class=\"line\">specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 2), 43L)</span><br><span class=\"line\">myConsumer.setStartFromSpecificOffsets(specificStartOffsets)</span><br><span class=\"line\">val stream &#x3D; env.addSource(myConsumer)</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>当作业从 Checkpoint 或 SavePoint 中恢复时，这些消费位置的定制方法不会影响开始消费位置。<br>在恢复时，每个 topic partition offset 取值于存储在 SavePoint 或 CheckPoint 中的 offset。   </p>\n</blockquote>\n<h3 id=\"Consumer-容错\"><a href=\"#Consumer-容错\" class=\"headerlink\" title=\"Consumer 容错\"></a>Consumer 容错</h3><ol>\n<li>当启用了 checkpoint 时，Flink Kafka Consumer 会从 topic 中消费记录并周期性地 checkpoint kafka offset 和其他操作的状态。当 Flink 应用失败时 会从最新的 checkpoint 中恢复并从  checkpoint 存储的 topic offset 处开始消费数据。  </li>\n<li>当禁用了 checkpoint 时，Flink Kafka Consumer 会周期性地提交 offset 到 zookeeper。   </li>\n</ol>\n<h3 id=\"topic-partition-自动发现\"><a href=\"#topic-partition-自动发现\" class=\"headerlink\" title=\"topic / partition 自动发现\"></a>topic / partition 自动发现</h3><p>在生产环境中，应用消费的 topic 数量或者 topic 的 partition 数量可能是随 需求 / 负载扩容 等变化的。Flink 提供了机制以满足 topic 和 partition 自动发现。  </p>\n<p>partition 自动发现：默认情况是分区自动发现是禁用的，设置 flink.partition-discovery.interval-millis 非负即可启用，所有自动发现的分区都会从分区最开始处消费。<br>topic 自动发现：同分区自动发现，设置 flink.partition-discovery.interval-millis 非负且设置 topic 为正则表达式即可启用。<br>FlinkKafkaConsumer 内部会启动一个独立的线程定期去 Kafka 获取最新的 meta 信息，并调整作业。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">properties.set(&quot;flink.partition-discovery.interval-millis&quot;,&quot;600000&quot;)</span><br><span class=\"line\"> </span><br><span class=\"line\">val myConsumer &#x3D; new FlinkKafkaConsumer[String](</span><br><span class=\"line\">  java.util.regex.Pattern.compile(&quot;test-topic-[0-9]&quot;),</span><br><span class=\"line\">  new SimpleStringSchema,</span><br><span class=\"line\">  properties)</span><br></pre></td></tr></table></figure>\n\n<p>可以持续自动发现名称满足 以”test-topic”开头以 [0-9] 任一数字结尾的 topic。如果仅设置正则表达式而不设置 flink.partition-discovery.interval-millis，则只在应用启动时发现一次，不能持续自动发现。如果仅设置 flink.partition-discovery.interval-millis 不设置正则表达式，则只对分区自动发现。     </p>\n<h3 id=\"Consumer-提交-Offset-回-Kafka-配置\"><a href=\"#Consumer-提交-Offset-回-Kafka-配置\" class=\"headerlink\" title=\"Consumer 提交 Offset 回 Kafka 配置\"></a>Consumer 提交 Offset 回 Kafka 配置</h3><p>Flink Kafka Consumer 允许配置 Consumer 提交 Offset 回 Kafka 的方式，但是并不是为了使用 zookeeper offset 保证容错，而是为了展示消费进度以便于监控和滞后调整。<br>Consumer 提交 Offset  回 Kafka 的方式取决于作业是否启用了 checkpoint。    </p>\n<ol>\n<li>启用 checkpoint：当 checkpoint 完成时，Flink Kafka Consumer 将提交存储在 checkpoint 中的 offset，这可以确保 checkpoint offset 和 zookeeper offset 一致。自动提交间隔取决于 checkpoint 间隔，且延时较大。用户可以调用 setCommitOffsetsOnCheckpoints(false) 来禁止提交到 zookeeper，默认情况下为 true。启用 checkpoint 时将忽略 enable.auto.commit / auto.commit.interval.ms 配置。  </li>\n<li>禁用 checkpoint：Flink Kafka Consumer 使用内部 Kafka Client 来周期性提交 offset。因此可以通过配置来 enable.auto.commit / auto.commit.interval.ms 来 启用 / 禁用 / 订制 自动提交 offset 到 zookeeper。 </li>\n</ol>\n<h3 id=\"水印线-WaterMark\"><a href=\"#水印线-WaterMark\" class=\"headerlink\" title=\"水印线(WaterMark)\"></a>水印线(WaterMark)</h3><p>一般称作水印，但是称作水印线会更贴切一些。<br>很多场景下，记录的时间戳嵌入在记录本身或者 ConsumerRecord 的元数据中，此外 consumer 可能需要根据记录的时间戳 兼容数据乱序和触发一些操作。Flink Kafka Consumer 允许指定一个水印线策略。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val properties &#x3D; new Properties()</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class=\"line\">val myConsumer &#x3D;</span><br><span class=\"line\">    new FlinkKafkaConsumer(&quot;topic&quot;, new SimpleStringSchema(), properties);</span><br><span class=\"line\">myConsumer.assignTimestampsAndWatermarks(</span><br><span class=\"line\">    WatermarkStrategy.</span><br><span class=\"line\">        .forBoundedOutOfOrderness(Duration.ofSeconds(20)))</span><br><span class=\"line\">val stream &#x3D; env.addSource(myConsumer)</span><br></pre></td></tr></table></figure>\n\n\n<p>WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(20)) 一般用于为无序数据创建水印策略时设置数据无序程度的上限。假设无序程度的上限为 B，那么当当前数据中时间戳为 T 时，Flink 认为不会再出现比 (T-B)更早的数据，即 如 B=20s，T=2020-08-10 10：00：20，那么认为不会再有早于 2020-08-10 10：00：00 的数据。 如果有早于 T-B 的数据 那么\n会为每一条 早于 T-B 的数据触发独立的行为，e.g. window / 数据更新。过多早于 T-B 的数据会导致性能问题和数据一致性问题。<br>因此无序程度上限 B 的设置十分重要，必须深度了解业务和数据情况才能设置。   </p>\n<p><img src=\"/img/pictures/flink/flink_watermark.png\" alt=\"WaterMark 和 Window\">   </p>\n<h2 id=\"Kafka-Producer\"><a href=\"#Kafka-Producer\" class=\"headerlink\" title=\"Kafka Producer\"></a>Kafka Producer</h2><p>Flink 提供了写入数据流到 Kafka 一个或多个 Topic 的功能，FlinkKafkaProducer011 对应 kafka-0.11，FlinkKafkaProducer010 对应 kafka-010…<br>必须提供的信息：  </p>\n<ol>\n<li>一个写入数据时默认的 topic 名称  </li>\n<li>序列化方式，将内存中的数据结构序列化成 Kafka 二进制数据  </li>\n<li>Kafka Client 配置，bootstrap.servers 是必须的(逗号分隔)  </li>\n<li>一个容错语义，一般是 exactly-once   </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val stream: DataStream[String] &#x3D; ...</span><br><span class=\"line\">Properties properties &#x3D; new Properties</span><br><span class=\"line\">properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class=\"line\">val myProducer &#x3D; new FlinkKafkaProducer[String](</span><br><span class=\"line\">        &quot;my-topic&quot;,                  &#x2F;&#x2F; default target topic</span><br><span class=\"line\">        new SimpleStringSchema(),    &#x2F;&#x2F; serialization schema</span><br><span class=\"line\">        properties,                  &#x2F;&#x2F; producer config</span><br><span class=\"line\">        FlinkKafkaProducer.Semantic.EXACTLY_ONCE) &#x2F;&#x2F; fault-tolerance</span><br><span class=\"line\">stream.addSink(myProducer)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Producer-容错\"><a href=\"#Producer-容错\" class=\"headerlink\" title=\"Producer 容错\"></a>Producer 容错</h3><p>当启用 checkpoint 时，FlinkKafkaProducer011 可以提供 exactly-once 语义，FlinkKafkaProducer010 可以提供 at-least-once 语义 不能提供 exactly-once 语义。<br>可以指定的语义：  </p>\n<ol>\n<li>Semantic.NONE: Flink 不保证任何事，记录可能会丢失也可能会重复   </li>\n<li>Semantic.AT_LEAST_ONCE(默认): Flink 保证记录不会丢失，但是可能会重复  </li>\n<li>Semantic.EXACTLY_ONCE: 使用 Kafka 事务来保证 exactly-once 语义。当使用事务向 Kafka 写数据时，需要对相关 topic 的 consumer 设置”隔离级别(isolation.level)”。隔离级别分两种 read_committed 或 read_uncommitted(默认)。   </li>\n</ol>\n<blockquote>\n<p>注意事项:</p>\n<ol>\n<li>Semantic.EXACTLY_ONCE mode relies on the ability to commit transactions that were started before taking a checkpoint, after recovering from the said checkpoint. If the time between Flink application crash and completed restart is larger than Kafka’s transaction timeout there will be data loss (Kafka will automatically abort transactions that exceeded timeout time).\nexactly-once 语义依赖于 提交 “开始于 checkpoint 之前” 和 “从 checkpoint 恢复之后”的事务的能力。如果 Flink 应用从崩溃到完全恢复所用的时间大于 Kafka 事务超时时间，那么数据会丢失(Kafka 会自动放弃超时事务)。 </li>\n<li>由于“译不准”，此处似乎也可以理解为 “exactly-once 语义依赖于提交特殊事务的能力，特殊事务开始于 checkpoint 之前，终止于从 checkpoint 恢复之后”。事务开启，状态尚未存储而应用挂掉，恢复后事务超时被 Broker 放弃导致了数据丢失，无法保证”exactly-once”语义。  </li>\n</ol>\n</blockquote>\n<p>FlinkKafkaProducer 要求”exactly-once”语义时，topic 的所有 consumer 都必须设置隔离级别，默认为 read_uncommitted。    </p>\n<blockquote>\n<p>关于 read_committed 模式，在 KafkaConsumer 启用了 read_committed 模式时，任何 未完成 / 未终止 的事务将会阻塞对该事务之后的所有事务的读取。  例如:     </p>\n<ol>\n<li>producer 开启 transaction1 并写入了一些记录    </li>\n<li>producer 开启了 transaction2 并写入了其他记录    </li>\n<li>producer commit transaction2<br>即使 transaction2 已经被 commit，其记录对于所有的 consumer 都不可见，直到 transaction1 被提交或终止。  这种模式有两个含义：  </li>\n<li>在 Flink application 正常运行期间，输出到 topic 记录的可见性会有一定延迟，这等于已完成 checkpoint 的平均时间。   </li>\n<li>当 Flink application 失败，将阻塞其写入的 topic 的所有 consumer，直到应用重启或者 Kafka transaction 超时。在有多个 producer / consumer 同时操作同一个 topic 时”exacty-once”造成的阻塞风险需要评估。      </li>\n</ol>\n</blockquote>\n<h3 id=\"分区方案\"><a href=\"#分区方案\" class=\"headerlink\" title=\"分区方案\"></a>分区方案</h3><p>使用 FlinkKafkaProducer 向 Kafka 写数据时，如果不指定分区器 Partitioner，默认使用 FlinkFixedPartitioner。该 Partitioner 分区的方式是将 Task 所在的实例 Id 按照 topic partition 总数取余将 sink 映射到单个的 topic partition 上，即：partitions[parallelInstanceId % partitions.length]。如果 topic partition 总数为 5，sink 并发度为 2，那么最终只有两个 topic partition 有数据；如果 sink 并发度为 6，那么 1 号 partition 有两个 sink 在写，会导致 Broker 负载均衡问题。<br>如果指定 Partitioner 为 null，会使用 kafka producer 默认的分区方式，sink 可能会轮询写所有 partition。topic partition 的数据比较均衡，但是会相对保持更多的网络连接。<br>因此使用 FlinkFixedPartitioner 并配置 sink 并发度为 topic partition 数量的整数倍较合理。    </p>\n<h2 id=\"Kerberos-认证\"><a href=\"#Kerberos-认证\" class=\"headerlink\" title=\"Kerberos 认证\"></a>Kerberos 认证</h2><p>Flink 为连接到开启了 Kerberos 认证的 Kafka 集群 通过 FlinkKafkaConnector 提供了支持。只需要在 flink-conf.yaml 中简单配置即可启用 kerberos    </p>\n<ol>\n<li>配置以下配置项<br>　　　- security.kerberos.login.use-ticket-cache：默认情况下为 true，Flink 尝试使用 kinit 管理的 ticket cache 中的 kerberos 证书(credentials)，但是当 Flink Job 被部署到 Yarn 和 Mesos 时，这个配置是不生效的，因为 Yarn 和 Mesos 不支持使用 ticket cache 中的证书认证<br>　　　- security.kerberos.login.keytab and security.kerberos.login.principal：使用 kerberos keytab 来认证     </li>\n<li>在 security.kerberos.login.contexts 配置后追加 KafkaClient：此配置告诉 Flink 提供已配置的 kerberos 证书给 Kafka Login Context 以进行 Kafka 认证    </li>\n</ol>\n<p>一旦基于 Kerberos 认证的 Flink 安全机制启用后，可以在 FlinkKafkaProducer / FlinkKafkaConsumer 中添加两个配置以传给内部的 KafkaClient 来进行 Kafka 认证。    </p>\n<ol>\n<li>设置 security.protocol 为 SASL_PLAINTEXT(默认 NONE): 此协议用于和 Broker 通信。使用 standalone 部署 Flink 时可以设置 security.protocol 为 SASL_SSL。</li>\n<li>设置 sasl.kerberos.service.name 为 kafka(默认 kafka)：这个值应该和 Broker 配置的 sasl.kerberos.service.name 值一致。客户端和服务端的服务名称不一致将导致认证失败</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer\" target=\"_blank\" rel=\"noopener\">https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/connectors/kafka.html#kafka-consumer</a>  </li>\n<li><a href=\"https://blog.csdn.net/lmalds/article/details/52704170\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/lmalds/article/details/52704170</a>  </li>\n</ol>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n<!-- more -->\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2020-03-16T10:01:28.112Z","updated":"2020-03-16T10:01:28.112Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke2rms6b0013y7c7hoyw2sfh","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<a id=\"more\"></a>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>","more":"<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>"},{"layout":"post","title":"Yarn movetoqueue 导致的UI指标错误的修复思路","subtitle":"Yarn 源码解析","date":"2019-09-14T16:00:00.000Z","author":"jiulongzhu","catalog":true,"_content":"\n## 问题背景 \n\n多个线上运行状态私有云的某些资源队列,在无任何 Application 提交及运行、资源队列完全空闲的情况下, \nUsed Capacity、Absolute Used Capacity、Used Resource、Num Containers指标非零异常  \n\n<!-- more -->\n\n>\n在Yarn ResourceManager管理界面中,有 scheduler 选项卡,展示了Yarn 当前使用的调度器及各资源队列的信息(YarnUI->scheduler->Application Queue),其中每项指标代表的含义是:  \nQueue State: 表示当前队列的状态,有 RUNNING/STOPPED 两种状态  \nUsed Capacity: 表示当前队列已使用的资源占当前队列总资源的百分比  \nConfigured Capacity: 表示当前队列的资源占父队列资源的百分比  \nConfigured Max Capacity: 表示当前队列资源最大能占父队列资源的百分比  \nAbsolute Used Capacity: 表示当前队列已使用的资源占 root 队列资源(整个集群)的百分比  \nAbsolute Configured Capacity: 表示当前队列的资源占 root 队列总资源的百分比  \nAbsolute Configured Max Capacity: 表示当前的队列的资源最大能占 root 队列的百分比  \nUsed Resources: 表示当前队列已使用的资源总量(资源以内存和虚拟核形态表示,基本调度单位)  \nNum Schedulable Applications: 表示当前队列调度的应用个数\nNum Non-Schedulable Applications: 表示当前队列没有调度(积压,pending)的应用个数  \nNum Containers: 表示当前队列已经启动的 container 个数  \nMax Applications: 表示当前队列最大并发调度应用个数  \nMax Applications Per User: 表示当前队列对每个用户最大并发调度应用个数  \nMax Application Master Resources: 表示所有 Application 的 AM 可使用资源量之和的最大值    \nUsed Application Master Resources: 表示当前队列中所有 Application 的 AM 使用资源量之和  \nMax Application Master Resources Per User: 表示当前队列中每个用户的 Application 的 AM 使用资源量之和的最大值  \nConfigured Minimum User Limit Percent: 表示队列每个用户分配的最低资源百分比(资源保障)  \nConfigured User Limit Factor: 表示每个用户能占用的队列资源的百分比  \nAccessible Node Labels:  表示当前队列可在哪些节点上分配资源 (*为全部节点)  \nPreemption: 是否允许资源抢占  \n\n指标可分为两类:配置型指标,静态数据 不会变化,如 Configured Capacity;状态型指标,动态数据 随应用的提交运行结束而变化,如 Used Capacity.  \n运行时异常指标如下图,无 Application 运行的情况下,low 队列状态型指标为负\n\t\n![](/img/pictures/negative/negative_e7a766275896.png)\n\n\n## 先期判断\n\n### 指标关联的变量定位\n\n线上 hadoop 版本: hadoop 2.7.3  \nYarn ResourceManager 管理界面启动的入口是org.apache.hadoop.yarn.server.resourcemanager.ResourceManager#startWebApp()  \n[ResourceManager.java]\n\n```\nprotected void startWepApp() {\n\t....\n    Builder<ApplicationMasterService> builder = \n        WebApps\n            .$for(\"cluster\", ApplicationMasterService.class, masterService,\n                \"ws\")\n            .with(conf)\n            .withHttpSpnegoPrincipalKey(\n                YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY)\n            .withHttpSpnegoKeytabKey(\n                YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)\n            .at(webAppAddress);\n   \t....\n    webApp = builder.start(new RMWebApp(this));\n  }\n```\n\nRMWebApp 主要逻辑是使用 Google Guice 做依赖注入,并分发请求绑定后台逻辑,大致相当于 SpringMVC 系统中的 Dispatcher 的角色  \n对 Yarn UI界面左侧边栏 scheduler 的请求会转发给 RmController#scheduler()方法处理  \n[RMWebApp.java]\n\n```\npublic void setup() {\n    ...\n    bind(RMWebApp.class).toInstance(this);\n    if (rm != null) {\n      bind(ResourceManager.class).toInstance(rm);\n      bind(ApplicationBaseProtocol.class).toInstance(rm.getClientRMService());\n    }\n    ...\n    route(\"/scheduler\", RmController.class, \"scheduler\");\n    route(pajoin(\"/queue\", QUEUE_NAME), RmController.class, \"queue\");\n    ...\n  }\n```\n\nRmController#scheduler() 先获取到 Guice 注入的 ResourceManager,然后依据 RM 使用的调度器做页面渲染.  \n页面渲染逻辑的入口是CapacitySchedulerPage#render()    \n[RmController.java]  \n\n```\npublic void scheduler() {\n    ...\n    ResourceManager rm = getInstance(ResourceManager.class);\n    ResourceScheduler rs = rm.getResourceScheduler();\n    if (rs == null || rs instanceof CapacityScheduler) {\n      setTitle(\"Capacity Scheduler\");\n      //渲染\n      render(CapacitySchedulerPage.class);\n      return;\n    }\n    if (rs instanceof FairScheduler) {\n      setTitle(\"Fair Scheduler\");\n      render(FairSchedulerPage.class);\n      return;\n    }\n    ....\n  }\n protected void render(Class<? extends View> cls) {\n    context().rendered = true;\n    getInstance(cls).render();\n  }\n```\n\n泛型上界是 View,调用栈是:  \nView#render()  \n&ensp;&ensp;->HtmlPage#render()  \n&ensp;&ensp;&ensp;&ensp;&ensp;->TwoColumnLayout#render(html)  \n&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;->CapacitySchedulerPage#content()  \n[CapacitySchedulerPage.java] \n\n```\nprotected Class<? extends SubView> content() {\n    return QueuesBlock.class;\n  }\n```\n\nQueuesBlock 是 CapacitySchedulerPage 的内部类,用于展示CapacityScheduler 的 DashBoard 信息,包含队列、标签、应用概览信息,并构建根队列 root 来做为后续递归渲染的起点    \n[QueuesBlock.java]\n\n```\nclass QueuesBlock extends HtmlBlock {\n    final CapacityScheduler cs;\n    final CSQInfo csqinfo;\n    private List<NodeLabel> nodeLabelsInfo;\n    \n    public void render(Block html) {\n        ...\n        float used = 0;\n        if (null == nodeLabelsInfo\n            || (nodeLabelsInfo.size() == 1 && nodeLabelsInfo.get(0)\n                .getLabelName().isEmpty())) {\n                //创建根队列,作为后续渲染的起点\n          CSQueue root = cs.getRootQueue();\n          CapacitySchedulerInfo sinfo =\n              new CapacitySchedulerInfo(root, new NodeLabel(\n                  RMNodeLabelsManager.NO_LABEL));\n          csqinfo.csinfo = sinfo;\n          csqinfo.qinfo = null;\n\n           ...\n          ul.li().\n            ...\n            _(QueueBlock.class)._();\n        } else {\n          for (NodeLabel label : nodeLabelsInfo) {\n            ....\n            underLabel.li().\n            ...\n            _(QueueBlock.class)._()._();\n          }\n        }\n      }\n      //Application List 界面\n      ul._()._().\n      script().$type(\"text/javascript\").\n          _(\"$('#cs').hide();\")._()._().\n      _(RMAppsBlock.class);\n    }\n  }\n```\n\n从 rootQueue 开始递归,Queue 检查自身有没有子队列 subQueues,若无则其本身为 LeafQueue,使用 LeafQueueInfoBlock#render()渲染叶子队列信息,使用 QueueUsersInfoBlock#render()渲染队列下的用户信息;若有子队列则其本身为ParentQueue,使用 QueueBlock#render()渲染,直至叶子队列  \n[QueuesBlock.java]\n\n```\npublic void render(Block html) {\n      ArrayList<CapacitySchedulerQueueInfo> subQueues =\n          (csqinfo.qinfo == null) ? csqinfo.csinfo.getQueues().getQueueInfoList()\n              : csqinfo.qinfo.getQueues().getQueueInfoList();\n      UL<Hamlet> ul = html.ul(\"#pq\");\n      for (CapacitySchedulerQueueInfo info : subQueues) {\n  \t  ...\n          if (info.getQueues() == null) {\n          li.ul(\"#lq\").li()._(LeafQueueInfoBlock.class)._()._();\n          li.ul(\"#lq\").li()._(QueueUsersInfoBlock.class)._()._();\n        } else {\n          li._(QueueBlock.class);\n        }\n  \t...\n      }\n    }\n```\n\n由于异常指标位于叶子队列信息中,所以暂且不看QueueUsersInfoBlock.java  \n[LeafQueueInfoBlock.java]\n\n```\nprivate String nodeLabel;\nfinal CapacitySchedulerLeafQueueInfo lqinfo;\n@Inject LeafQueueInfoBlock(ViewContext ctx, CSQInfo info) {\n      super(ctx);\n      lqinfo = (CapacitySchedulerLeafQueueInfo) info.qinfo;\n      nodeLabel = info.label;\n    }\nprotected void render(Block html) {\n      if (nodeLabel == null) {\n        renderLeafQueueInfoWithoutParition(html);\n      } else {\n        renderLeafQueueInfoWithPartition(html);\n      }\n    }\n\n虽然按照 nodeLabel 做了分支,但是核心逻辑都是\nrenderQueueCapacityInfo方法和renderCommonLeafQueueInfo方法,\n这两个方法分别展示不同方面的指标,和 Yarn UI 展示的指标相同 \n\nprivate void renderQueueCapacityInfo(final ResponseInfo ri) {\n      ri.\n      //异常指标\n      _(\"Used Capacity:\", percent(lqinfo.getUsedCapacity() / 100)).\n      _(\"Configured Capacity:\", percent(lqinfo.getCapacity() / 100)).\n      _(\"Configured Max Capacity:\", percent(lqinfo.getMaxCapacity() / 100)).\n      //异常指标\n      _(\"Absolute Used Capacity:\", percent(lqinfo.getAbsoluteUsedCapacity() / 100)).\n      _(\"Absolute Configured Capacity:\", percent(lqinfo.getAbsoluteCapacity() / 100)).\n      _(\"Absolute Configured Max Capacity:\", percent(lqinfo.getAbsoluteMaxCapacity() / 100)).\n      //异常指标\n      _(\"Used Resources:\", lqinfo.getResourcesUsed().toString());\n    }\n \n  private void renderCommonLeafQueueInfo(final ResponseInfo ri) {\n      ri.\n      _(\"Num Schedulable Applications:\", Integer.toString(lqinfo.getNumActiveApplications())).\n      _(\"Num Non-Schedulable Applications:\", Integer.toString(lqinfo.getNumPendingApplications())).\n      //异常指标\n      _(\"Num Containers:\", Integer.toString(lqinfo.getNumContainers())).\n      _(\"Max Applications:\", Integer.toString(lqinfo.getMaxApplications())).\n      _(\"Max Applications Per User:\", Integer.toString(lqinfo.getMaxApplicationsPerUser())).\n      _(\"Max Application Master Resources:\", lqinfo.getAMResourceLimit().toString()).\n      _(\"Used Application Master Resources:\", lqinfo.getUsedAMResource().toString()).\n      _(\"Max Application Master Resources Per User:\", lqinfo.getUserAMResourceLimit().toString()).\n      _(\"Configured Minimum User Limit Percent:\", Integer.toString(lqinfo.getUserLimit()) + \"%\").\n      _(\"Configured User Limit Factor:\", StringUtils.format(\n          \"%.1f\", lqinfo.getUserLimitFactor())).\n      _(\"Accessible Node Labels:\", StringUtils.join(\",\", lqinfo.getNodeLabels())).\n      _(\"Preemption:\", lqinfo.getPreemptionDisabled() ? \"disabled\" : \"enabled\");\n    }\n```\n\n### 异常指标计算方式\n\n注:集群没有使用 label 系统,所以下述的 nodeLabel 视为\"\"即可\n\n* Used Capacity\n\n\t[AbstractCSQueue.java]\n\t\n\t```\npublic final synchronized float getUsedCapacity(final String nodeLabel) {\n    //集群所有资源 * 该队列的绝对容量百分比 = 该队列的绝对容量 \n    Resource availableToQueue =\n        Resources.multiply(\n            labelManager.getResourceByLabel(nodeLabel, this.clusterResource),\n            queueCapacities.getAbsoluteCapacity(nodeLabel));\n   //使用 queueUsage 中记录的使用量除以该队列的绝对容量得到队列的 Used Capacity\n   //queueUsage 的类型为 ResourceUsage\n    return\n        Resources.divide(resourceCalculator, this.clusterResource,\n            queueUsage.getUsed(nodeLabel), availableToQueue);\n  }\n\t```\n* Absolute Used Capacity\n\t\n\t算法和Used Capacity算法相似,只是分母不同,AbsoluteUsedCapacity 计算时分母是整个集群的资源  \n\t\n\t[AbstractCSQueue.java]\n\t\n\t```\n\tpublic final synchronized float getAbsoluteUsedCapacity(final String nodeLabel) {\n    Resource labeledResources =\n               labelManager.getResourceByLabel(nodeLabel, this.clusterResource);\n    return Resources.divide(resourceCalculator, this.clusterResource,\n        queueUsage.getUsed(nodeLabel), labeledResources);\n  }\n\t```\n\t\n* Used Resource\n\n\t&ensp;&ensp;使用的是Queue 按 Label 记录的资源信息  \n\tqueueResourceUsage.getUsed(nodeLabel)  \n\t\n* Num Containers\n\n\t&ensp;&ensp;使用的 Queue 本身记录的信息  \n\tnumContainers = leafQueue.getNumContainers();  \n\t\n>\n综上所述:  \t\n四个指标中,队列层面的Used Capacity 、Absolute Used Capacity 、Used Resource 均和 ResourceUsage维护 used 资源信息有关;numContainer 是 LeafQueue 自身维护的 container 数量.这些指标的变化逻辑在 AbstractCSQueue的 assignContainer()和 releaseContainer()中,即和 container 的释放/申请有关.\n而在队列完全空闲的时候,四个指标为负,初步定位可能是 Container 重复释放或无效释放的原因.\n\n## 问题复现\n\n### 猜测重复释放\n\n在已经出现指标异常的私有云上,对出现异常时日期前后的 resourcemanager 日志进行分析,检测其中出现\"Assigned container\"和\"Released  container\"字符串的数量,从日志层面对 container 的申请和释放次数做一个简单的判断.这种方式适用于集群作业周期性较强且没有跨天任务运行的情况.  \n\n```\negrep -o \"Assigned container\" resourcemanager.log | sort | uniq -c\negrep -o \"Released container\" resourcemanager.log | sort | uniq -c \n```\n\n从结果上看,\"Assigned container\"和\"Released container\"字符串出现次数是一致的,可能不是重复释放的问题  \n\n### 猜测无效释放\t\n\n对无效释放的猜想源自于重复释放和 rm 日志中的\"Null container completed...\",猜测可能对于 nullContainer 处理有问题,修改了系统维护的信息, 导致了负值    \n\nUI 的 kill 操作,命令行 yarn application --kill 和 RMApp 的正常结束都会释放 container,由 APP_ATTEMPT_REMOVED 事件触发,\n回收 AppAttemp 持有的runningContainer 和 reservedContainer,核心逻辑在 CapacityScheduler#completedContainer()方法,但是其中对于 RMContainer 和 Application 都做了校验,不会修改维护信息;如果 RMContainer 对象内持有的 contianer 对象为空的话,会抛出 NullPointerException,也不会修改维护信息. 所以无效释放的猜测不合理,此处代码证明重复释放也不合理\n[CapacityScheduler.java]\n\n```\nprotected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) { \n    //rmContaienr 空值校验  \n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    Container container = rmContainer.getContainer();\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n    //如果 RMContainer 映射的 container 为空,则此处会 NullPointerException\n        container.getId().getApplicationAttemptId().getApplicationId();\n    //application 空值校验\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    ....\n  }\n```\n\n### movetoqueue\n\n重复释放和无效释放的猜测证否之后,只好再去仔细研究 系统指标异常前的几个运行的任务日志,对 application 的 attempt 和每个 contianer 状态机的状态和 触发事件按照时间线标注出来,对application 发生的所有事件还原出来,发现了其中一个 application 的以下信息  \n>  \n1.提交到 root.high 队列后,队列资源不足,在某些节点上为该 applicaiton reserve 资源,其他 application 调度时不会再该节点上分配资源  \n2.application 被从 high 队列移动了 low 队列  \n3.container 被移动队列时,源队列和目标队列的资源有变动,两个队列都会被 re-sort,以便优先在资源利用率最低的 queue 调度,所以有每个 container 的移动有四条日志:queueMoveOut 队列的信息,queueMoveOut 的父队列信息(有几个 parentQueue 就会有几条日志),queuMoveIn 队列信息,queueMoveIn 的父队列信息  \n4.在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000002,在 high队列上分配了资源,被 move 到 low 队列后,containerCompleted 之后,释放资源到了 low 队列  \n5.<b>在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000080,在 high 队列上保留了资源,move 操作没有影响到该 container,但是containerCompleted之后释放资源到了 low 队列</b>. \n\n日志如下  \n\n```\nINFO LeafQueue: assignedContainer application attempt=appattempt_1565789460020_5864_000001 container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: null, ] queue=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:420864, vCores:20>, usedCapacity=1.4421053, absoluteUsedCapacity=0.86526316, numApps=3, numContainers=20 clusterResource=<memory:486400, vCores:136> type=OFF_SWITCH  \nINFO RMContainerImpl: container_e08_1565789460020_5864_01_000080 Container Transitioned from NEW to RESERVED\nINFO LeafQueue: Reserved container  application=application_1565789460020_5864 resource=<memory:11264, vCores:1> queue=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:443392, vCores:22>, usedCapacity=1.5192982, absoluteUsedCapacity=0.91157895, numApps=3, numContainers=22 usedCapacity=1.5192982 absoluteUsedCapacity=0.91157895 used=<memory:443392, vCores:22> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: Re-sorting assigned queue: root.high stats: high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:454656, vCores:23>, usedCapacity=1.5578947, absoluteUsedCapacity=0.93473685, numApps=3, numContainers=23\n....moving\nINFO LeafQueue: movedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service:  }, ] resource=<memory:11264, vCores:1> queueMoveOut=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:431104, vCores:20>, usedCapacity=1.477193, absoluteUsedCapacity=0.88631576, numApps=3, numContainers=20 usedCapacity=1.477193 absoluteUsedCapacity=0.88631576 used=<memory:431104, vCores:20> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: movedContainer queueMoveOut=root usedCapacity=0.9768421 absoluteUsedCapacity=0.9768421 used=<memory:475136, vCores:24> cluster=<memory:486400, vCores:136>\nINFO LeafQueue: movedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: }, ] resource=<memory:11264, vCores:1> queueMoveIn=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:-22528, vCores:-3>, usedCapacity=-0.4631579, absoluteUsedCapacity=-0.04631579, numApps=0, numContainers=-3 usedCapacity=-0.4631579 absoluteUsedCapacity=-0.04631579 used=<memory:-22528, vCores:-3> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: movedContainer queueMoveIn=root usedCapacity=1.0 absoluteUsedCapacity=1.0 used=<memory:486400, vCores:25> cluster=<memory:486400, vCores:136>\n....moved\nINFO CapacityScheduler: App: application_1565789460020_5864 successfully moved from high to: low\nINFO LeafQueue: completedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000080, NodeId: hadoop5:8041, NodeHttpAddress: hadoop5:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: null, ] queue=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:0, vCores:-1>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=-1 cluster=<memory:486400, vCores:136>\nINFO LeafQueue: completedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6.:8041, NodeHttpAddress: hadoop6.cn:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: }, ] queue=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:-1024, vCores:-2>, usedCapacity=-0.021052632, absoluteUsedCapacity=-0.002105263, numApps=1, numContainers=-2 cluster=<memory:486400, vCores:136>\n```\n\n>猜测:application 在移动队列后,对 reservedContainer 没有移动或维护信息不同步,导致了源队列的资源泄露给了目标队列,从而目标队列的 UsedCapacity 为负  \n>复现方式  \n&ensp;&ensp;&ensp;&ensp;第一步:提交 application 到资源紧张的 Queue  \n&ensp;&ensp;&ensp;&ensp;第二步:待在 RM 的日志中看到\"Trying to fulfill reservation for application ${APPLICATION_ID} on node ...\"和\"Trying to schedule on node..., available:...\" 表明 CapacityScheduler为该 ApplicationId 保留了资源,跳过在此 nm 上为其他 app 分配 container    \n&ensp;&ensp;&ensp;&ensp;第三步: 使用 yarn application -movetoqueue ${APPLICATION_ID} -queue ${TOQueue}  \n&ensp;&ensp;&ensp;&ensp;第四步: 待在 RM 日志中看到了\"App:${APPLICATION_ID} successfully moved ${FROMQUEUE} to ${TOQUEUE}\"后,使用 yarn application --kill ${APPLICATION_ID} 或等待 app 结束   \n&ensp;&ensp;&ensp;&ensp;第五步: 在 YarnUI上查看被移动的目标队列${TOQUEUE}的信息  \n\n注:  \n&ensp;&ensp;&ensp;&ensp;1.一定要在资源紧张的队列上提交 app,以触发调度系统的保留资源.在空闲队列上提交任务复现不了指标异常的问题  \n&ensp;&ensp;&ensp;&ensp;2.最好移动到一个完全空闲的队列上,否则即使复现了资源泄露,也不易看出来  \n\n## 源码解析\n\n以下代码出自于 hadoop 2.7.3版本  \nYarn 是一个资源调度平台,集群内存资源和 cpu 资源被 Yarn 抽象为 Resource{memory,core},客户端对 Yarn 的资源请求和 Yarn 内部的资源调度都是以 Container 为基本单位的    \n当客户端向资源队列 Queue 提交 Application 时, 客户端申请的 AM 以及 AM 申请新的执行角色(e.g. spark 的 executor)都是在Container 中运行,那么对于 Yarn 调度的 Container,资源信息被多维聚合[app状态|container状态|container用途|用户|标签]统计维护:    \n\n* Queue.包括直接申请的叶子队列及其所有的父队列    \n\t1. Queue(AbstractCSQueue)    \n &ensp;&ensp;主要指标是 container 数量;保存着 QueueMetrics 和 ResourceUsage(ByLabel) 的引用     \n\t\ta. ParentQueue   \n&ensp;&ensp;主要指标是运行的 application数量;保存着所有资源子队列的集合    \n\t\tb. LeafQueue \n&ensp;&ensp; 主要指标是每个用户提交 app 的数量和资源用途用量(ResourceUsageByUser);保存着队列 running 和 pending 的 app[attemp]信息  \t \t\t\n\t2. QueueMetrics  \n&ensp;&ensp;保留着queue 的指标信息,包括[提交|运行|积压|完成|杀死|失败]app 数量、[分配|待分配|积压]的[container|内存|虚拟核]信息、活跃的[app|user]信息;保存着用户级别的 <username,QueueMetrics>映射  \n\t3. ResourceUsage\t\t\t\n&ensp;&ensp;保留着<label,UsageByLabel>信息,分为used、pending、amused、reserved 四类  \n* Application(SchedulerApplicationAttempt). app 中维护着当前正在运行的 container,及 yarn 为其保留的 container 信息\n* NodeManager. container 所在的 NM 维护着自身运行的所有 container,并通过 RM 心跳汇报所有 container 状态机状态,触发 RM 对container 状态的更新或释放    \n\n>\n综上:  \n&ensp;&ensp;&ensp;&ensp;1.指标异常与 container 申请释放有关,所以需要研究 <b>正常申请释放与应用移动队列</b> 对上述维护信息的影响  \n&ensp;&ensp;&ensp;&ensp;2.UI 展示的 numContainers 数据取自 AbstractCSQueue 维护 numContainer 成员变量;usedCapacity,absoluteUsedCapacity,usedResource 均取自AbstractCSQueue$ResourceUsage.used 成员变量.需要关注以上变量在资源申请释放过程中的变化    \n\n### 正常的资源分配和释放过程\n\nRM 的资源分配和资源释放都是被动触发,客户端提交 application 到 RMClientService,app 信息暂存在指定资源队列中,待 NM 向 RM 通过心跳汇报自身信息时 RM 将对该 NM 触发调度,在情况允许的情况下(e.g. 目标队列具有该 NM 的 access 权限,NM 剩余资源满足 app 中一个 ResourceRequest 申请的资源),将在该 NM 上划出 ResourceRequest 要求的资源,并同步 Queue/Application/NodeManager 维护的信息;当 container 完成退出之后,NodeManager 注意到 container 状态机变化,并将其信息附带在心跳中汇报给 RM,触发 Queue/Application/NodeManager 状态变化和信息同步   \n以下代码较为关注资源信息的维护同步,对于其他细节不再赘述    \n\t\n### 资源分配\n\n![](/img/pictures/negative/yarn_node_update_pic.png)\n\nNM 通过心跳触发 CapacityScheduler 调度,CS 首先按照 NM 上报的信息同步 RM 的信息,为新启动的 container 触发 LAUNCHED 事件,为结束的 container 触发FINISHED 事件;并试图在该节点上分配资源  \n[CapacityScheduler.java]\n\n```\npublic void handle(SchedulerEvent event) {\n...\ncase NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      RMNode node = nodeUpdatedEvent.getRMNode();\n      //同步 RM 信息\n      nodeUpdate(node);\n      if (!scheduleAsynchronously) {\n      //试图在 nm 分配资源\n        allocateContainersToNode(getNode(node.getNodeID()));\n      }\n    }\n    break;\n ...\n}\n```\n依据汇报心跳的 NodeManager 是否被保留了资源执行相应的逻辑  \n[CapacityScheduler.java]\n\n```\nprivate synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    ...\n    RMContainer reservedContainer = node.getReservedContainer();\n    //当前 node 被某 applocation 保留了\n    if (reservedContainer != null) {\n      FiCaSchedulerApp reservedApplication =\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment =\n          queue.assignContainers(\n              clusterResource,\n              node,\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      ....\n      }\n    }\n    //当前节点未被保留\n    if (node.getReservedContainer() == null) {\n     \t...\n     \t//交由 root 队列代理分配.root 是资源队列树形结构的根节点,类型一定ParentQueue\n        root.assignContainers(\n            clusterResource,\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }\n```\nParentQueue 将节点委派给子队列,试图分配资源  \n[ParentQueue.java]\n\n```\n  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits resourceLimits) {\n    CSAssignment assignment = \n        new CSAssignment(Resources.createResource(0, 0), NodeType.NODE_LOCAL);\n    Set<String> nodeLabels = node.getLabels();\n    ...校验 queue 对 node 的 access 权限\n    while (canAssign(clusterResource, node)) {\n      ...校验 queue 资源是否超限      \n      // Schedule 递归交由子队列去分配\n      CSAssignment assignedToChild = \n          assignContainersToChildQueues(clusterResource, node, resourceLimits);\n      assignment.setType(assignedToChild.getType());\n      // Done if no child-queue assigned anything\n      // 如果分配到了资源, assignedToChild 大于 Resource<0,0>\n      if (Resources.greaterThan(\n              resourceCalculator, clusterResource, \n              assignedToChild.getResource(), Resources.none())) {\n              //同步维护的信息\n        super.allocateResource(clusterResource, assignedToChild.getResource(),\n            nodeLabels);\n        Resources.addTo(assignment.getResource(), assignedToChild.getResource());\n        ...\n         } else {\n        break;\n      }\n      ...\n      }\n\t...\n        break;\n      }\n    }     \n    return assignment;\n  }\n```\n先看如果分配到资源的话 同步的信息:  \n1.按照 label 去更新 ResourceUsage 中维护的用户使用资源 used(incUsed 方法)    \n2.增加了该队列的 numContainer 数量  \n需要注意的是 <b>无论后续得到的是 allocated container 还是 reserved container,都增加了 numContainer的值,增加了用户 USED 类型的内存和虚拟核数量</b>   \n[AbstractCSQueue.java]\n\n```\nsynchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    // Update usedResources by labels\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n再看对 queueUsage:ResourceUsage 做了什么操作  \nResourceUsage 中维护了一个Map结构 usages,key 是标签类型,value 是 UsageByLabel;UsageByLabel 中只有一个数组 Resource[],数组中的每个值分别表征着 USED,PENDING,AMUSED,RESERVED 用途的 Resource 数量  \n[ResourceUsage.java]\n\n```\n// <labelName,UsageByLabel>\nprivate Map<String, UsageByLabel> usages;\npublic void incUsed(String label, Resource res) {\n    _inc(label, ResourceType.USED, res);\n }\n private void _inc(String label, ResourceType type, Resource res) {\n     ...\n     UsageByLabel usage = getAndAddIfMissing(label);\n     Resources.addTo(usage.resArr[type.idx], res);      \n     ...\n   }\n```\nUsageByLabel 使用一个 Resource[]数组来存储资源的用途和用量  \n[UsageByLabel.java]  \n\n```\n  private static class UsageByLabel {\n    // usage by label, contains all UsageType\n    private Resource[] resArr;\n    public UsageByLabel(String label) {\n      resArr = new Resource[ResourceType.values().length];\n      for (int i = 0; i < resArr.length; i++) {\n        resArr[i] = Resource.newInstance(0, 0);\n      };\n    }\n  }\n  \n   private enum ResourceType {\n    USED(0), PENDING(1), AMUSED(2), RESERVED(3);\n    private int idx;\n    private ResourceType(int value) {\n      this.idx = value;\n    }\n  }\n```\n再回到资源分配,root 队列深度优先遍历所有子队列,尝试在叶子队列上分配资源  \n[LeafQueue.java]\n\n```\npublic synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n\t....\n    // Check for reserved resources\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      FiCaSchedulerApp application = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    //对目前活跃的 application,尝试在当前 NM 上分配资源\n    for (FiCaSchedulerApp application : activeApplications) {\n    \t....\n      synchronized (application) {\n         ....        \n          // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest =\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n         if (null == anyRequest) {\n            continue;\n          }\n          ....校验性工作\n          //校验通过,在此节点上分配资源\n          CSAssignment assignment =  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, currentResourceLimits);\n\t.....\n          // Did we schedule or reserve a container?\n          Resource assigned = assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n            //更新当前[叶子]队列的 numContainer,ResourceUsageByLabel和 ResourceUsageByUser.\n            //比 ParentQueue.allocateResource 多出了一个用户层面的资源统计\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            .....            \n            return assignment;\n          } else {\n            break;\n          }\n        }\n      }\n    }\n    return NULL_ASSIGNMENT;\n  }\n```\n先看LeafQueue#allocateResource 更新信息时更新的指标和内容\nLeafQueue 和ParentQueue  一样,使用抽象父类AbstractCSQueue#allocateResource() 更新当前队列的 numContainer 和 标签层面的 ResourceUsage  \n然后使用 LeafQueue 中维护的Map<String,User>成员变量 users来维护用户标签层面的资源信息 \n[LeafQueue.java]\n\n```\nsynchronized void allocateResource(Resource clusterResource,\n      SchedulerApplicationAttempt application, Resource resource,\n      Set<String> nodeLabels) {\n      //使用的是 AbstractCSQueue#allocateResource,和 ParentQueue更新的指标及内容相同:numContainer,ResourceUsageByLabel\n    super.allocateResource(clusterResource, resource, nodeLabels);\n    //更新用户层面的 ResourceUsage\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.assignContainer(resource, nodeLabels);\n    Resources.subtractFrom(application.getHeadroom(), resource); // headroom\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n  }\n```\nUser 类中有一个 ResourceUsage,维护着在用户层面 各标签的资源用途和用量  \n[User.java]\n\n```\npublic static class User {\n    ResourceUsage userResourceUsage = new ResourceUsage();\n    volatile Resource userResourceLimit = Resource.newInstance(0, 0);\n    int pendingApplications = 0;\n    int activeApplications = 0;\n     public void assignContainer(Resource resource,\n        Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n        userResourceUsage.incUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.incUsed(label, resource);\n        }\n      }\n    }\n  }\n```\n上文 LeafQueue#assignContainers方法 遍历每个活跃的 application,尝试在当前 nodemanager 上分配资源,调用 assignContainersOnNode()方法进行下一步的分配逻辑,并增加 app 在本地化层面分配的 container 数量,此指标逻辑不在此讨论        \n按照本地性优先级,优先分配  NODE_LOCAL(本节点)>RACK_LOCAL(本机架)>OFF_SWITCH(跨机架)  \nYARN 的本地性不同于 MR/SPARK 的本地性,YARN的本地性性体现在 Client/AM申请 container 的时候可以指定 container 所在的节点,此处的 NODE_LOCAL、RACK_LOCAL 和 OFF_SWITCH 是相对于申请 container 时指定的节点而言;数据本地化计算其实更多的是靠计算框架配合,按照数据所处的位置优先分配给计算节点(e.g. RDD#getPreferredLocations)    \n[LeafQueue.java]\n\n```\nprivate CSAssignment assignContainersOnNode(Resource clusterResource,\n      FiCaSchedulerNode node, FiCaSchedulerApp application, Priority priority,\n      RMContainer reservedContainer, ResourceLimits currentResoureLimits) {\n    Resource assigned = Resources.none();\n    NodeType requestType = null;\n    MutableObject allocatedContainer = new MutableObject();\n    // Data-local\n    ResourceRequest nodeLocalResourceRequest =\n        application.getResourceRequest(priority, node.getNodeName());\n    if (nodeLocalResourceRequest != null) {\n      requestType = NodeType.NODE_LOCAL;\n      assigned =\n          assignNodeLocalContainers(clusterResource, nodeLocalResourceRequest, \n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (Resources.greaterThan(resourceCalculator, clusterResource,\n          assigned, Resources.none())) {\n        //update locality statistics\n        if (allocatedContainer.getValue() != null) {\n          application.incNumAllocatedContainers(NodeType.NODE_LOCAL,\n            requestType);\n        }\n        return new CSAssignment(assigned, NodeType.NODE_LOCAL);\n      }\n    }\n    // Rack-local\n    ResourceRequest rackLocalResourceRequest =\n        application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalResourceRequest != null) {\n      if (!rackLocalResourceRequest.getRelaxLocality()) {\n        return SKIP_ASSIGNMENT;\n      }\n      if (requestType != NodeType.NODE_LOCAL) {\n        requestType = NodeType.RACK_LOCAL;\n      }\n      assigned = \n          assignRackLocalContainers(clusterResource, rackLocalResourceRequest, \n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (Resources.greaterThan(resourceCalculator, clusterResource,\n          assigned, Resources.none())) {\n        if (allocatedContainer.getValue() != null) {\n          application.incNumAllocatedContainers(NodeType.RACK_LOCAL,\n            requestType);\n        }\n        return new CSAssignment(assigned, NodeType.RACK_LOCAL);\n      }\n    }\n    // Off-switch\n    ResourceRequest offSwitchResourceRequest =\n        application.getResourceRequest(priority, ResourceRequest.ANY);\n    if (offSwitchResourceRequest != null) {\n      if (!offSwitchResourceRequest.getRelaxLocality()) {\n        return SKIP_ASSIGNMENT;\n      }\n      if (requestType != NodeType.NODE_LOCAL\n          && requestType != NodeType.RACK_LOCAL) {\n        requestType = NodeType.OFF_SWITCH;\n      }\n      assigned =\n          assignOffSwitchContainers(clusterResource, offSwitchResourceRequest,\n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (allocatedContainer.getValue() != null) {\n        application.incNumAllocatedContainers(NodeType.OFF_SWITCH, requestType);\n      }\n      return new CSAssignment(assigned, NodeType.OFF_SWITCH);\n    }\n    return SKIP_ASSIGNMENT;\n  }\n```\nLeafQueue#assignNodeLocalContainers,assignRackLocalContainers,assignOffSwitchContainers的核心逻辑被封装为一处,只是本地化类型 NodeType 不同    \n[LeafQueue.java]\n\n```\nprivate Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, ResourceLimits currentResoureLimits) {\n    ....    \n    Resource capability = request.getCapability();\n    Resource available = node.getAvailableResource();\n    Resource totalResource = node.getTotalResource();\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    // Create the container if necessary\n    Container container = \n        getContainer(rmContainer, application, node, capability, priority);\n  \t...\n      boolean shouldAllocOrReserveNewContainer = shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n    //依据 节点可用资源量与申请的资源量 做除法来判断节点剩余资源能否满足需求 \n    int availableContainers = \n        resourceCalculator.computeAvailableContainers(available, capability);\n    boolean needToUnreserve = Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\t\n    if (availableContainers > 0) {\n      //如果节点上足够分配一个 container 则分配\n      ....\n      //调用 application 和 nodemanager 的方法,触发这两处的信息同步\n      RMContainer allocatedContainer = \n          application.allocate(type, node, priority, request, container);\n          \n      node.allocateContainer(allocatedContainer);\n      \n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n    \t....\n        //节点上没有足够的资源满足需求,则为该 application 保留该节点的资源\n        reserve(application, priority, node, rmContainer, container);\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }\n```\n上文有三处地方需要同步资源信息:reserve()方法,FiCaSchedulerApp#allocate,FiCaSchedulerNode#allocateContainer  \n先看FiCaSchedulerApp#allocate方法,在 application 层面修改了什么信息    \n[FiCaSchedulerApp.java]\n\n```\n synchronized public RMContainer allocate(NodeType type, FiCaSchedulerNode node,\n      Priority priority, ResourceRequest request, \n      Container container) {\n     ....\n    //将 container 封装成 RMContainer 的形式,记录在 application 的所有 container 集合 newlyAllocatedContainers 和 运行态(相对 reserved)的 container 集合 liveContainers 中  \n    RMContainer rmContainer = new RMContainerImpl(container, this\n        .getApplicationAttemptId(), node.getNodeID(),\n        appSchedulingInfo.getUser(), this.rmContext);\n    newlyAllocatedContainers.add(rmContainer);\n    liveContainers.put(container.getId(), rmContainer);    \n    ....\n    //更新 metrics 信息\n    List<ResourceRequest> resourceRequestList = appSchedulingInfo.allocate(\n        type, node, priority, request, container);\n     //当前 app 消费的资源量\n    Resources.addTo(currentConsumption, container.getResource());\n    ...\n    //触发 RMContainer 状态机变化,container 可以准备运行时环境,下载依赖等\n    rmContainer.handle(\n        new RMContainerEvent(container.getId(), RMContainerEventType.START));\n    RMAuditLogger.logSuccess(getUser(), \n        AuditConstants.ALLOC_CONTAINER, \"SchedulerApp\", \n        getApplicationId(), container.getId());\n    return rmContainer;\n```\nAppSchedulingInfo#allocate 更新 QueueMetrics 信息  \n[AppSchedulingInfo.java]\n\n```\nsynchronized public List<ResourceRequest> allocate(NodeType type,\n      SchedulerNode node, Priority priority, ResourceRequest request,\n      Container container) {\n    List<ResourceRequest> resourceRequests = new ArrayList<ResourceRequest>();\n    //本地化层面的统计信息\n    if (type == NodeType.NODE_LOCAL) {\n      allocateNodeLocal(node, priority, request, container, resourceRequests);\n    } else if (type == NodeType.RACK_LOCAL) {\n      allocateRackLocal(node, priority, request, container, resourceRequests);\n    } else {\n      allocateOffSwitch(node, priority, request, container, resourceRequests);\n    }\n    QueueMetrics metrics = queue.getMetrics();\n    ...\n    //对QueueMetrics 做已经分配的资源的统计\n    metrics.allocateResources(user, 1, request.getCapability(), true);\n    metrics.incrNodeTypeAggregations(user, type);\n    return resourceRequests;\n  }\n```\n\nQueueMetrics#allocateResource 对 userMetrics 做判断;对 parent 也做了判断 如果parent 非空那么会递归更新 parent 的信息,对userMetrics 和 parent 执行的方法都是 QueueMetrics#allocateResources,更新的指标一样:增加 allocate 的 container 数量,内存量,虚拟核数量  \n[QueueMetrics.java] \n\n```\npublic void allocateResources(String user, int containers, Resource res,\n      boolean decrPending) {\n    allocatedContainers.incr(containers);\n    aggregateContainersAllocated.incr(containers);\n    allocatedMB.incr(res.getMemory() * containers);\n    allocatedVCores.incr(res.getVirtualCores() * containers);\n    if (decrPending) {\n      _decrPendingResources(containers, res);\n    }\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.allocateResources(user, containers, res, decrPending);\n    }\n    if (parent != null) {\n      parent.allocateResources(user, containers, res, decrPending);\n    }\n  }\n```\nFiCaSchedulerNode#allocateContainer, NodeManager 同步 allocate 信息 \n[FiCaSchedulerNode.java]\n\n```\npublic synchronized void allocateContainer(RMContainer rmContainer) {\n    Container container = rmContainer.getContainer();\n     //减少当前 nm 的可用资源,增加当前 nm 的已分配资源\n    deductAvailableResource(container.getResource());\n    //增加自身维护的 numContainer 数量,不同于 AbstractCSQueue.numContainers,维护Queue维度和 NodeManager 维度的 container 数量\n    ++numContainers;\n    //启动的 container 列表\n    launchedContainers.put(container.getId(), rmContainer);\n    ...\n  }\n  private synchronized void deductAvailableResource(Resource resource) {\n    Resources.subtractFrom(availableResource, resource);\n    Resources.addTo(usedResource, resource);\n  }\n```\n如果在节点上 reserve 了资源,同步的信息和 allocate 的方式相似,会更新 QueueMetrics 对 reserved[containr|memory|core]的信息,更新application reservedContainer 及 currentReservationMemroy 信息,更新 NodeManager 当前 reservedContainer 信息  \n[LeafQueue.java]\n\n```\nprivate void reserve(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, RMContainer rmContainer, Container container) {\n    //传入的 rmContainer 为 null\n    if (rmContainer == null) {\n      getMetrics().reserveResource(\n          application.getUser(), container.getResource());\n    }\n    // Inform the application \n    rmContainer = application.reserve(node, priority, rmContainer, container);\n    // Update the node\n    node.reserveResource(application, priority, rmContainer);\n  }\n```\nQueueMetrics#reserveResource在下方有一个对 parent 的判断,递归调用依次更新 ParentQueue 的信息;有一个对 userMetrics 的判断,对 QueueMetrics 中维护的 users:Map[String, QueueMetrics] 同步用户层面的统计信息,执行的方法都是当前方法QueueMetrics#reserveResource(),同步reserved 资源信息:增加 reserved container 数量,内存量,虚拟核数量  \n\n[QueueMetrics.java]\n\n```\npublic void reserveResource(String user, Resource res) {\n    reservedContainers.incr();\n    reservedMB.incr(res.getMemory());\n    reservedVCores.incr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.reserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.reserveResource(user, res);\n    }\n  }\n```\napplication 同步 reserved container 信息  \n[SchedulerApplicationAttempt.java]\n\n```\n public synchronized RMContainer reserve(SchedulerNode node, Priority priority,\n      RMContainer rmContainer, Container container) {\n     //上游传入的 rmContainer 为 null\n    if (rmContainer == null) {\n      rmContainer = \n          new RMContainerImpl(container, getApplicationAttemptId(), \n              node.getNodeID(), appSchedulingInfo.getUser(), rmContext);\n      //增加当前 application 保留的资源信息\n      Resources.addTo(currentReservation, container.getResource());\n      resetReReservations(priority);\n    } else {\n      // Note down the re-reservation\n      addReReservation(priority);\n    }\n    //在 container 信息中设置了绑定节点的信息....\n    rmContainer.handle(new RMContainerReservedEvent(container.getId(), \n        container.getResource(), node.getNodeID(), priority));\n        //加入到维护的 reservedContainer 信息中\n    Map<NodeId, RMContainer> reservedContainers = \n        this.reservedContainers.get(priority);\n    if (reservedContainers == null) {\n      reservedContainers = new HashMap<NodeId, RMContainer>();\n      this.reservedContainers.put(priority, reservedContainers);\n    }\n    reservedContainers.put(node.getNodeID(), rmContainer);\n    return rmContainer;\n  }\n```\nFicaSchedulerNode#reserveResource(),NodeManager 同步 reserved container 信息  \n[FicaSchedulerNode.java]\n\n```\npublic synchronized void reserveResource(\n      SchedulerApplicationAttempt application, Priority priority,\n      RMContainer container) {\n    // Check if it's already reserved\n    RMContainer reservedContainer = getReservedContainer();\n    ....空值校验性,重复保留校验等操作\n    //设置当前节点保留的 container 为传入值\n    setReservedContainer(container);\n  }\n\n```\n\n\n> 综上:  \n        CS 调度器在 NM 上为 APP 分配 container 时,会同步 [Parent | Leaf] Queue/APP/NM 三个位置维护的信息    \n\t1.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, 其 <b>ParentQueue</b>都会维护:  \n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ParentQueue <b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n\t2.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, <b>LeafQueue</b> 本身都会维护:  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n\t3.在节点上申请资源时,按照节点剩余可用资源和资源需求量做除法,若满足需求则分配为 allocatedContainer,不满足则分配 reservedContainer  \n\t4.若在节点上分配 allocatedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp; Application 方面:newlyAllocatedContainers和 livingContainers 列表,app 已占用资源量    \n&ensp;&ensp;&ensp;&ensp; NodeManager 方面:numContainer 数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表  \n&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身QueueMetrics</b>的allocated [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度 QueueMetrics</b> 的 allocated [containerNum | MB | cores]信息  \n\t5.若在节点上分配 reservedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量  \n&ensp;&ensp;&ensp;&ensp;NodeManager 方面:设置 NM 保留的 container 为本次的 reservedContainer  \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  \n\t\n\n### 资源释放\n\n以客户端在命令行执行 \"yarn application --kill ${APPLICATION_ID}\"为例  \nRM 中响应请求 方法调用栈为:  \n->ClientRMService#forceKillApplication   \n&ensp;&ensp;->RMAppImpl$KillAttemptTransition#transition  \n&ensp;&ensp;&ensp;&ensp;->RMAppAttemptImpl$BaseFinalTransition#transition  \n&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;->CapacityScheduler#handle()   \n\napplication 结束时,主要做了三件事情,  \n1. Container: 释放 APP 持有的<b> living,reserved </b> container  \n2. Application(QueueMetrcis): 清理 LeafQueue 及其 ParentQueue 在 <b>APP</b> 层面的统计(e.g. appRunnings);清理 LeafQueue 在<b>用户APP</b> 层面的统计  \n3. AM(ResourceUsage): 释放 LeafQueue 及其 ParentQueue 在 <b>AM</b> 层面的资源用量;  释放 LeafQueue 在<b>用户AM</b> 层面的统计(AMUSED)  \n\n由于复现指标异常成功后,发现移动到目标队列后,目标队列增加的资源总量是 spark.executor.memory 的整数倍(executor-mem 11G,driver-memory 2G),所以对上述的第二点和第三点不多分析,主要关注 <b>非 AM 的container</b> 的释放,livingContainers 和 reservedContainer 的释放代码是同一个,仅仅是释放 container 的文字性说明(原文: diagnostics 意为诊断)不同      \n[CapactiyScheduler.java]\n\n```\npublic void handle(SchedulerEvent event) {\n....\n case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),\n        appAttemptRemovedEvent.getFinalAttemptState(),\n        appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());\n    }\n    break;\n    ....\n    }\n    \nprivate synchronized void doneApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {\n    \n    FiCaSchedulerApp attempt = getApplicationAttempt(applicationAttemptId);\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n        ....\n    //从 app 维护的 livingContainer 列表中,释放掉 allocated acquired running 状态的 container\n    for (RMContainer rmContainer : attempt.getLiveContainers()) {\n    //在 kill 时keepContainer 为 false;在 failed 时,keepContainer 按场景可为 true\n      if (keepContainers\n          && rmContainer.getState().equals(RMContainerState.RUNNING)) {\n        continue;\n      }\n      //1.释放 container\n      completedContainer(\n        rmContainer,\n        SchedulerUtils.createAbnormalContainerStatus(\n        //COMPLETED_APPLICATION:Container of a completed application\n          rmContainer.getContainerId(), SchedulerUtils.COMPLETED_APPLICATION),\n        RMContainerEventType.KILL);\n    }\n\t\n    //从 app 维护的 reservedContainer 列表中,释放掉 reserved 状态的 container\n    for (RMContainer rmContainer : attempt.getReservedContainers()) {\n    //1.释放 container\n      completedContainer(\n        rmContainer,\n        SchedulerUtils.createAbnormalContainerStatus(\n          rmContainer.getContainerId(), \"Application Complete\"),\n        RMContainerEventType.KILL);\n    }\n    //2. 清理 pending 的 resourceRequest,并同步 [LeafQueue | ParentQueue] [本身 | 用户]维度的 QueueMetrics appRunnings | appPendings \n    attempt.stop(rmAppAttemptFinalState);\n    String queueName = attempt.getQueue().getQueueName();\n    CSQueue queue = queues.get(queueName);\n    if (!(queue instanceof LeafQueue)) {\n      LOG.error(\"Cannot finish application \" + \"from non-leaf queue: \"\n          + queueName);\n    } else {\n    //3. 同步 [LeafQueue | ParentQueue] [本身 | 用户]维度 QueueUsage 的 AMUSED 指标\n      queue.finishApplicationAttempt(attempt, queue.getQueueName());\n    }\n  }\n```\nCapacityScheduler#completedContainer主要做了一些校验,避免无效释放  \n[CapacityScheduler.java]\n\n```\n   protected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    Container container = rmContainer.getContainer();\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n     //container 所在的 NM视图\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n     //代理 container 的 LeafQueue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null, true);\n  }\n```\n先按照 RMContainer 状态机状态判断后续将当前 container 按照 reservedContainer处理或者 allocatedContainer 处理  \n然后若释放 container 成功,则同步 用户和标签维度 QueueUsage的 USED 指标,减少 Queue 运行的 container 数量    \n最后若当前 Queue 非 rootQueue,则递归同步当前 Queue 的所有 ParentQueue做上述处理  \n[LeafQueue.java]\n\n```\npublic void completedContainer(Resource clusterResource, \n      FiCaSchedulerApp application, FiCaSchedulerNode node, RMContainer rmContainer, \n      ContainerStatus containerStatus, RMContainerEventType event, CSQueue childQueue,\n      boolean sortQueues) {\n    if (application != null) {\n      boolean removed = false;\n      synchronized (this) {\n        Container container = rmContainer.getContainer();\n\t// 1. 若 contaienr 状态机为 reserved 状态,在节点上解除 reserve\n        if (rmContainer.getState() == RMContainerState.RESERVED) {\n          removed = unreserve(application, rmContainer.getReservedPriority(),\n              node, rmContainer);\n        } else {\n        // 2.若 container 状态机不为 reserved 状态,在节点上解除 allocate,并使 NM 释放 container\n          removed =\n            application.containerCompleted(rmContainer, containerStatus, event);\n          node.releaseContainer(container);\n        }\n        //3.释放 同步 QueueUsage\n        if (removed) {\n          releaseResource(clusterResource, application,\n              container.getResource(), node.getLabels());\n          LOG.info(\"completedContainer\" +\n              \" container=\" + container +\n              \" queue=\" + this +\n              \" cluster=\" + clusterResource);\n        }\n      }\n      //4.递归执行 Parent的释放逻辑\n      if (removed) {\n        getParent().completedContainer(clusterResource, application, node,\n          rmContainer, null, event, this, sortQueues);\n      }\n    }\n  }\n```\n先看第 4 处对 ParentQueue 的处理  \n[ParentQueue.java]\n\n```\npublic void completedContainer(Resource clusterResource,\n      FiCaSchedulerApp application, FiCaSchedulerNode node, \n      RMContainer rmContainer, ContainerStatus containerStatus, \n      RMContainerEventType event, CSQueue completedChildQueue,\n      boolean sortQueues) {\n    if (application != null) {\n      synchronized (this) {\n      //使用 AbstractCSQueue 中releaseResource方法\n        super.releaseResource(clusterResource, rmContainer.getContainer()\n            .getResource(), node.getLabels());\n\t...\n        //resort sub-queue\n     \t....\n      }\n      // Inform the parent\n      if (parent != null) {\n \t//递归 ParentQueue#completedContainer 方法\n        parent.completedContainer(clusterResource, application, \n            node, rmContainer, null, event, this, sortQueues);\n      }    \n    }\n  }\n```\nParentQueue 在 container 释放时,无论是 allocatedContainer 还是 reservedContainer,都更新: \n1.按标签更新 QueueUsage 中 USED 用途的资源量  \n2.ParentQueue 中运行 container 的数量 \n[AbstractCSQueue.java]\n\n```\nprotected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n      //1.QueueUsageByLabel\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n        //2.减少当前 Queue 的 container 的数量\n    --numContainers;\n  }\n```\n再看第 3 处对 LeafQueue 的处理, 需要注意的是:无论 container 是 allocatedContainer 还是 reversedContainer 在此处都是作为ResourceUsage中 <b>USED</b> 类型的资源来减少的(虽然 ResourceType 枚举中有 RESERVED 类型),且 numContainer 都减少了 1个单位    \n[LeafQueue.java]\n\n```\nsynchronized void releaseResource(Resource clusterResource, \n      FiCaSchedulerApp application, Resource resource, Set<String> nodeLabels) {\n    //1.同步当前队列在 标签维度 ResourceUsage 中 USED 类型的资源,并减少 Queue 中 numContainer\n    super.releaseResource(clusterResource, resource, nodeLabels);\n    //2.同步 app 所属用户在标签维度上的度量信息  \n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.releaseContainer(resource, nodeLabels);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n  }\n```\n[AbstractCSQueue.java]  \n\n```\nprotected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n     //1.同步 label 层面的 QueueUsage 的资源量;无论 reservedContainer 还是 allocatedContainer 都是使用 #decUsed(resource)\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n      //2.减少当前队列启动的 container 数量.无论 reservedContainer 还是 allocatedContainer  \n    --numContainers;\n  }\n```\nLeafQueue 中维护了一个 users:Map[String,User]以保存当前各 user 在 USED,PENDING,AMUSED,RESERVED 用途使用资源的度量  \n[LeafQueue.java]\n\n```\npublic void releaseContainer(Resource resource, Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n      //还是 decUsed(resource),无论 reservedContainer 还是 allocatedContainer\n        userResourceUsage.decUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.decUsed(label, resource);\n        }\n      }\n    }\n```\n在回到LeafQueue#completedContainer方法,按照 RMContainer状态机状态来区分 reservedContainer 还是 allocatedContainer 做处理  \n先看对 allocated 的处理逻辑    \n[FicaSchedulerApp.java]\n\n```\nsynchronized public boolean containerCompleted(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    //维护相应的引用列表\n    if (null == liveContainers.remove(rmContainer.getContainerId())) {\n      return false;\n    }\n    newlyAllocatedContainers.remove(rmContainer);\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n\n    //1.RM 端准备 container 的 结束和清理等任务,待 NM 心跳通过 NodeHeartbeatResponse 交于 NM 做清理\n    rmContainer.handle(\n        new RMContainerFinishedEvent(\n            containerId,\n            containerStatus, \n            event)\n        );\n    ... \n    //2.同步 QueueMetrics\n    Resource containerResource = rmContainer.getContainer().getResource();\n    queue.getMetrics().releaseResources(getUser(), 1, containerResource);\n    //3.减少当前 app 消费的资源\n    Resources.subtractFrom(currentConsumption, containerResource);\n    ....\n    return true;\n  }\n```\n主要看第2步 释放 contaienr 同步 QueueMetrics 的指标和内容,  \n1.同步 QueueMetrics 及其 ParentQueue QueueMetrics 的指标: allocate [containerNum | MB | core]  \n2.同步 QueueMetrics 及其 ParentQueue QueueMetrics 用户维度的 QueueMetrics 指标: allocate [containerNum | MB | core]  \n[QueueMetrics.java] \n\n```\npublic void releaseResources(String user, int containers, Resource res) {\n    //当前资源队列指标\n    allocatedContainers.decr(containers);\n    aggregateContainersReleased.incr(containers);\n    allocatedMB.decr(res.getMemory() * containers);\n    allocatedVCores.decr(res.getVirtualCores() * containers);\n    QueueMetrics userMetrics = getUserMetrics(user);\n    //资源队列中用户指标\n    if (userMetrics != null) {\n      userMetrics.releaseResources(user, containers, res);\n    }\n    //递归 父资源队列\n    if (parent != null) {\n      parent.releaseResources(user, containers, res);\n    }\n  }\n```\n再看释放allocatedContainer 过程中,NM 对该 container 的处理  \n1.删掉 launchedContainer 对该 container 的引用  \n2.增加该节点的可用资源,减少已经资源,减少启动的 numContainer 指标    \n[SchedulerNode.java]\n\n```\npublic synchronized void releaseContainer(Container container) {\n    ..\n    //1.删除引用\n    if (null != launchedContainers.remove(container.getId())) {\n     //2.指标维护\n      updateResource(container);\n    }\n    LOG.info(\"Released container \" + container.getId() + \" of capacity \"\n        + container.getResource() + \" on host \" + rmNode.getNodeAddress()\n        + \", which currently has \" + numContainers + \" containers, \"\n        + getUsedResource() + \" used and \" + getAvailableResource()\n        + \" available\" + \", release resources=\" + true);\n  }\n  private synchronized void updateResource(Container container) {\n    addAvailableResource(container.getResource());\n    --numContainers;\n  }\n  private synchronized void addAvailableResource(Resource resource) {\n     ...\n    Resources.addTo(availableResource, resource);\n    Resources.subtractFrom(usedResource, resource);\n  }\n```\n释放 reservedContainer,对 reversedContainer 的处理  \n1.application 方面:删除对 reservedContainer 的引用,减少 currentReservation 资源数  \n2.nodemanager方面: 设置当前 nm 的reservedContainer 为 null\n3.QueueMetrics 方面: 同步[ParentQueue | LeafQueue] [本身 | 用户]维度的指标 reserved [containerNum | MB | core]  \n[LeafQueue.java]\n\n```\nprivate boolean unreserve(FiCaSchedulerApp application, Priority priority,\n      FiCaSchedulerNode node, RMContainer rmContainer) {\n      //1.application: unreserve\n    if (application.unreserve(node, priority)) {\n     //2. nodemanager: unreserveResource\n      node.unreserveResource(application);\n     //3.QueueMetrics\n      getMetrics().unreserveResource(application.getUser(),\n          rmContainer.getContainer().getResource());\n      return true;\n    }\n    return false;\n  }\n```\napplication 和 nodemanager 层面的逻辑比较简单,且看 QueueMetrics 方面的同步  \n[QueueMetrics.java]\n\n```\npublic void unreserveResource(String user, Resource res) {\n  //当前资源队列 QueueMetrics的 reserved [container | MB | core] 指标\n    reservedContainers.decr();\n    reservedMB.decr(res.getMemory());\n    reservedVCores.decr(res.getVirtualCores());\n    //当前资源队列 在用户维度的 reserved [container | MB | core] 指标\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.unreserveResource(user, res);\n    }\n    //父资源队列在 其本身和用户维度的 reserved [container | MB | core] 指标\n    if (parent != null) {\n      parent.unreserveResource(user, res);\n    }\n  }\n```\n\n>\n综上:   \nCS 调度器在为 APP 释放container 时,会同步 [Parent | Leaf]Queue/APP/NM 三个位置维护的信息  \n1. 释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>ParentQueue</b> 都会维护:\n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的<b>标签维度</b>的QueueUsage 中 USED 用途的资源量  \n2. 释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>LeafQueue</b> 都会维护:   \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n3.释放 container 时,按照该 container 的状态机状态做细分,分为 reservedContainer 和 allocatedContainer 两类做处理  \n4.若在节点上释放 allocatedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:newlyAllocatedContainers 和 reservedContainers 列表,app已占用资源量    \n&ensp;&ensp;&ensp;&ensp;NodeManager方面:numContainer数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表    \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 和所有 ParentQueue <b>自身 QueueMetrics</b> 的 reserved [containerNum | MB | core]信息;LeafQueue 和所有 ParentQueue 在<b>用户维度 QueueMetrics </b> 的  reserved [containerNum | MB | core]信息  \n5.若在节点上释放 reservedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量  \n&ensp;&ensp;&ensp;&ensp; NodeManager 方面:设置 NM 保留的 container 为 null    \n&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  \n\n\nContainer 释放过程和申请过程 对于 Queue/Application/NM三个位置维护的 Queue/QueueUsageByLabel/QueueUsageByUser/QueueMetrics信息,恰好是一一对应的  \n \n对于 YarnUI 指标异常问题,比较关注的 Queue.numContainer 和 QueueUsageByLabel.resArr[ResourceType.USED]两个指标,一次 allocated/reversed/Container 的申请对应着 numContainer++和 ResourceUsage.incUsed;一次 container 的释放对应着 numContainer--和 ResourceUsage.decUsed   \n\n### movetoqueue 时资源转移过程\n\n\n使用命令 \"yarn application -movetoqueue ${APPID} -queue ${TO}\"\n可以将 ${APPID}的 APP 移动到目标资源队列 ${TO}  \nClientRMService 相应请求的调用栈是:  \n->ClientRMService#moveApplicationAcrossQueues  \n&ensp;&ensp;->RMAppImpl$RMAppMoveTransition#transition  \n&ensp;&ensp;&ensp;&ensp;->CapacityScheduler#moveApplication  \n\napplication 从源资源队列移动到目标资源队列分为几步:  \n1. 向目标队列提交application 并做校验,包括 access 权限、目标队列的 app 数量超限、目标队列中用户提交 app 数量超限,若提交成功则增加所有 ParentQueue 的 numApplication 数量(但是没有增加自身的 numApplication 数量)    \n2. 转移源队列中所有非 reserved 的 container 到目标队列,源队列(LeafQueue)释放 container,减少<b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并减少源队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量;增加目标队列(LeafQueue) <b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并增加目标队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量.指标数量维护在Queue/ResourceUsage/QueueMetrics 中,此处没有处理 livingContainers 的 QueueMetrics ,在第 5 步中统一处理 QueueMetrics    \n3. 源队列中移除 appAttemptd,并同步 QueueUsageByUser 和 QueueUsageByLabel 中 AMUSED类型的资源量  \n4. 在源队列的所有 ParentQueue 中移除此 Application    \n5. 同步源队列和目标队列中所有非 AM container 移动导致的 QueueMetrics 变化  \n6. 同步用户维度的 appAttempt 统计信息   \n[CapacityScheduler.java]  \n\n```\n public synchronized String moveApplication(ApplicationId appId,\n      String targetQueueName) throws YarnException {\n    FiCaSchedulerApp app =\n        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));\n    String sourceQueueName = app.getQueue().getQueueName();\n    LeafQueue source = getAndCheckLeafQueue(sourceQueueName);\n    String destQueueName = handleMoveToPlanQueue(targetQueueName);\n    LeafQueue dest = getAndCheckLeafQueue(destQueueName);\n    String user = app.getUser();\n    try {\n      //1. 转移 Application\n      dest.submitApplication(appId, user, destQueueName);\n    } catch (AccessControlException e) {\n      throw new YarnException(e);\n    }\n    //2.转移 livingContainer\n    for (RMContainer rmContainer : app.getLiveContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      // attach the Container to another queue\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //3.源资源队列移除attempt,并同步QueueUsage\n    source.finishApplicationAttempt(app, sourceQueueName);\n    //4.源队列的父队列移除 application\n    source.getParent().finishApplication(appId, app.getUser());\n   //5.同步源队列和目标队列的 QueueMetrics\n    app.move(dest);\n    //6.提交attemp\n    dest.submitApplicationAttempt(app, user);\n    applications.get(appId).setQueue(dest);\n    LOG.info(\"App: \" + app.getApplicationId() + \" successfully moved from \"\n        + sourceQueueName + \" to: \" + destQueueName);\n    return targetQueueName;\n  }\n```\n第 2 步中,遍历 app 的 livingContainer,把每个 container 从源队列及父队列释放,减少源队列及父队列的指标  \n先看源队列 LeafQueue 的释放逻辑  \n[LeafQueue.java]\n\n```\npublic void detachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n      //AbstractCSQueue#releaseResource\n      releaseResource(clusterResource, application, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" container=\" + rmContainer.getContainer()\n          + \" resource=\" + rmContainer.getContainer().getResource()\n          + \" queueMoveOut=\" + this + \" usedCapacity=\" + getUsedCapacity()\n          + \" absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() + \" used=\"\n          + queueUsage.getUsed() + \" cluster=\" + clusterResource);\n      //ParentQueue#detachContainer\n      getParent().detachContainer(clusterResource, application, rmContainer);\n    }\n  }\n  \n  synchronized void releaseResource(Resource clusterResource, \n      FiCaSchedulerApp application, Resource resource, Set<String> nodeLabels) {\n      //AbstractCSQueue#releaseResource\n     //1.减少 标签维度的 ResourceUsage USED 用途的资源;并减少当前 Queue 的 numContainer 数量\n    super.releaseResource(clusterResource, resource, nodeLabels);\n    //2.减少 提交用户在 标签维度的 ResourceUsage USED 用途的资源\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.releaseContainer(resource, nodeLabels);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    ...\n  }\n  \n  //AbstractCSQueue#releaseResource\n  //ResourceUsageByLabel USED 用途的资源量\n  protected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n        //Queue 的运行 container 数量\n    --numContainers;\n  }\n  //第 2 处,ResourceUsageByUser USED 用途的资源量\npublic void releaseContainer(Resource resource, Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n        userResourceUsage.decUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.decUsed(label, resource);\n        }\n      }\n```\nParentQueue 释放 container 时,减少了用户维度的 ResourceUsage 在 USED 用途上资源量  \n[ParentQueue.java]\n\n```\npublic void detachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n      //AbstractCSQueue#releaseResource\n      super.releaseResource(clusterResource,\n          rmContainer.getContainer().getResource(),\n          node.getLabels());\n      LOG.info(\"movedContainer\" + \" queueMoveOut=\" + getQueueName()\n          + \" usedCapacity=\" + getUsedCapacity() + \" absoluteUsedCapacity=\"\n          + getAbsoluteUsedCapacity() + \" used=\" + queueUsage.getUsed() + \" cluster=\"\n          + clusterResource);\n      // Inform the parent\n      if (parent != null) {\n        //递归父队列\n        parent.detachContainer(clusterResource, application, rmContainer);\n      }\n    }\n  }\n  //AbstractCSQueue#releaseResource\n  protected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n    // Update usedResources by labels\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n    --numContainers;\n  }\n```\n将 container 从源队列转移到目标队列时,会增加目标队列及其父队列的指标   \n先看目标队列(LeafQueue)增加的指标   \n[LeafQueue.java]\n\n```\npublic void attachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n          //1.增加 LeafQueue 标签维度的 ResourceUsage 和用户维度的 ResourceUsage, USED 类型的资源量;并增加numContainer\n      allocateResource(clusterResource, application, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" container=\" + rmContainer.getContainer()\n              //add by jiulong.zhu@20190903\n              +\" containerState=\"+rmContainer.getState()\n          + \" resource=\" + rmContainer.getContainer().getResource()\n          + \" queueMoveIn=\" + this + \" usedCapacity=\" + getUsedCapacity()\n          + \" absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() + \" used=\"\n          + queueUsage.getUsed() + \" cluster=\" + clusterResource);\n      //2.增加 ParentQueue 标签维度的 ResourceUsage USED 类型的资源量;并增加 ParentQueue 的 numContainer 数量\n      getParent().attachContainer(clusterResource, application, rmContainer);\n    }\n  }\n  \n  synchronized void allocateResource(Resource clusterResource,\n      SchedulerApplicationAttempt application, Resource resource,\n      Set<String> nodeLabels) {\n      //AbstractCSQueue#allocateResource 增加标签维度的 ResourceUsage USED 用途的用量;增加 Queue 的numContainers  \n    super.allocateResource(clusterResource, resource, nodeLabels);\n    String userName = application.getUser();\n    //增加当前用户 标签维度的 ResourceUsage USED 用途的用量  \n    User user = getUser(userName);\n    user.assignContainer(resource, nodeLabels);\n    \n    Resources.subtractFrom(application.getHeadroom(), resource);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    if (LOG.isDebugEnabled()) {\n      LOG.info(getQueueName() + \n          \" user=\" + userName + \n          \" used=\" + queueUsage.getUsed() + \" numContainers=\" + numContainers +\n          \" headroom = \" + application.getHeadroom() +\n          \" user-resources=\" + user.getUsed()\n          );\n    }\n  }\n    \n    //AbstractCSQueue#allocateResource\n   synchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n再看将 container 移动到目标队列时,目标队列的 ParentQueue 增加的逻辑  \n[ParentQueue.java]\n\n```\npublic void attachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n          //1.增加 ParentQueue 在标签维度的 ResourceUsage USED 用量 \n          //AbstractCSQueue#allocateResource\n      super.allocateResource(clusterResource, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" queueMoveIn=\" + getQueueName()\n          + \" usedCapacity=\" + getUsedCapacity() + \" absoluteUsedCapacity=\"\n          + getAbsoluteUsedCapacity() + \" used=\" + queueUsage.getUsed() + \" cluster=\"\n          + clusterResource);\n      //2.递归增加 ParentQueue 的所有 ParentQueue 的 ResourceUsage USED 用量  \n      if (parent != null) {\n        parent.attachContainer(clusterResource, application, rmContainer);\n      }\n    }\n  }\n  \n  //AbstractCSQueue#allocateResource\n  synchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n第 3 步:删除源队列中该 app 的 currentApplicationAttempt,并同步 LeafQueue 中用户提交的 app 数量变化     \n[LeafQueue.java]\n\n```\npublic void finishApplicationAttempt(FiCaSchedulerApp application, String queue) {\n    synchronized (this) {\n      removeApplicationAttempt(application, getUser(application.getUser()));\n    }\n    //空实现\n    getParent().finishApplicationAttempt(application, queue);\n  }\n  \n  public synchronized void removeApplicationAttempt(\n      FiCaSchedulerApp application, User user) {\n    boolean wasActive = activeApplications.remove(application);\n    if (!wasActive) {\n      pendingApplications.remove(application);\n    } else {\n      //正在运行的 app_attempt,则减少 LeafQueue 的 [ResourceUsage | ResourceUsageByUser] 中AMUSED 占用的资源量\n      queueUsage.decAMUsed(application.getAMResource());\n      user.getResourceUsage().decAMUsed(application.getAMResource());\n    }\n    applicationAttemptMap.remove(application.getApplicationAttemptId());\n     // 在 ResourceUsageByUser 同步 numApplication 指标\n    user.finishApplication(wasActive);\n    if (user.getTotalApplications() == 0) {\n      users.remove(application.getUser());\n    }\n    // Check if we can activate more applications\n    activateApplications();\n    LOG.info(\"Application removed -\" +\n        \" appId: \" + application.getApplicationId() + \n        \" user: \" + application.getUser() + \n        \" queue: \" + getQueueName() +\n        \" #user-pending-applications: \" + user.getPendingApplications() +\n        \" #user-active-applications: \" + user.getActiveApplications() +\n        \" #queue-pending-applications: \" + getNumPendingApplications() +\n        \" #queue-active-applications: \" + getNumActiveApplications()\n        );\n  }\n```\n第 4 步:对于源队列的所有 ParentQueue,移除 app 的 currentApplicationAttempt,并减少资源队列的运行 numApplication 值    \n[ParentQueue.java]\n\n```\npublic void finishApplication(ApplicationId application, String user) {\n    synchronized (this) {\n      removeApplication(application, user);\n    }\n    //递归 父队列\n    if (parent != null) {\n      parent.finishApplication(application, user);\n    }\n  }\n  \n  private synchronized void removeApplication(ApplicationId applicationId, \n      String user) {\n    --numApplications;\n    LOG.info(\"Application removed -\" +\n        \" appId: \" + applicationId + \n        \" user: \" + user + \n        \" leaf-queue of parent: \" + getQueueName() + \n        \" #applications: \" + getNumApplications());\n  }\n```\n第 5 步: 将 app 持有的所有 allocatedContainer(包括 AM container)和 reservedContainer,移动到目标队列,并修改 QueueMetrics  \n[SchedulerApplicationAttempt.java]\n\n```\npublic synchronized void move(Queue newQueue) {\n    QueueMetrics oldMetrics = queue.getMetrics();\n    QueueMetrics newMetrics = newQueue.getMetrics();\n    String user = getUser();\n    //修改 livingContainer 的 QueueMetrics\n    for (RMContainer liveContainer : liveContainers.values()) {\n      Resource resource = liveContainer.getContainer().getResource();\n      oldMetrics.releaseResources(user, 1, resource);\n      newMetrics.allocateResources(user, 1, resource, false);\n    }\n    //修改 reservedContainer的 QueueMetrics\n    for (Map<NodeId, RMContainer> map : reservedContainers.values()) {\n      for (RMContainer reservedContainer : map.values()) {\n        Resource resource = reservedContainer.getReservedResource();\n        oldMetrics.unreserveResource(user, resource);\n        newMetrics.reserveResource(user, resource);\n      }\n    }\n    //移动 pending ResourceRequest,同步信息:QueueMetrics.appsRunning,Queue下的用户列表    \n    appSchedulingInfo.move(newQueue);\n    this.queue = newQueue;\n  }\n```\n修改 livingContainers 和 reservedContainers 的所有逻辑都在 QueueMetrics中,其中  \n1.releaseResources,释放 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标  \n2.allocateResource,申请 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标  \n3.unreserveResource,释放 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Containers| MB | Core]指标  \n4.reserveResource,申请 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  \n[QueueMetrics.java]\n\n```\npublic void releaseResources(String user, int containers, Resource res) {\n    allocatedContainers.decr(containers);\n    aggregateContainersReleased.incr(containers);\n    allocatedMB.decr(res.getMemory() * containers);\n    allocatedVCores.decr(res.getVirtualCores() * containers);\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.releaseResources(user, containers, res);\n    }\n    if (parent != null) {\n      parent.releaseResources(user, containers, res);\n    }\n  }\n  \n public void allocateResources(String user, int containers, Resource res,\n      boolean decrPending) {\n    allocatedContainers.incr(containers);\n    aggregateContainersAllocated.incr(containers);\n    allocatedMB.incr(res.getMemory() * containers);\n    allocatedVCores.incr(res.getVirtualCores() * containers);\n    if (decrPending) {\n      _decrPendingResources(containers, res);\n    }\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.allocateResources(user, containers, res, decrPending);\n    }\n    if (parent != null) {\n      parent.allocateResources(user, containers, res, decrPending);\n    }\n  }\n  \n  public void unreserveResource(String user, Resource res) {\n    reservedContainers.decr();\n    reservedMB.decr(res.getMemory());\n    reservedVCores.decr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.unreserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.unreserveResource(user, res);\n    }\n  }\n  \n  public void reserveResource(String user, Resource res) {\n    reservedContainers.incr();\n    reservedMB.incr(res.getMemory());\n    reservedVCores.incr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.reserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.reserveResource(user, res);\n    }\n  }\n  \n```\n第 6 步:同步目标队列 application 和 applicationAttempt信息    \n[LeafQueue.java]\n\n```\npublic void submitApplicationAttempt(FiCaSchedulerApp application,\n      String userName) {\n    synchronized (this) {\n      User user = getUser(userName);\n      //增加 Queue 和 User 维度 application / applicationAttempt 的值\n      addApplicationAttempt(application, user);\n    }\n    // We don't want to update metrics for move app\n    if (application.isPending()) {\n     //增加 Queue 和 User 维度 QueueMetrics 中 appsPending 的值\n      metrics.submitAppAttempt(userName);\n    }\n    //空实现\n    getParent().submitApplicationAttempt(application, userName);\n  }\n```\n\n>综上:  \n把 application 从源资源队列到目标队列的过程中,在 Queue/ResourceUsageByLabel/ResourceUsageByUser/QueueMetrics 中的变动    \n1.对于 livingContainers,  \n&ensp;&ensp;&ensp;&ensp;Queue 方面:减少源队列及其父队列的 numContainer 值;增加目标队列及其父队列的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ResourceUsage 方面:减少源队列及其父队列的 ResourceUsageByLabel USED 类型的资源量,增加目标队列及其父队列的 ResourceUsageByLabel USED 类型的资源量; 减少源队列(LeafQueue)的 ResourceUsageByUser USED 类型的资源量,增加目标队列(LeafQueue) 的 ResourceUsageByUser USED 类型的资源量;  \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:减少源队列及其父队列的 allocated[Container | MB | Core]指标,增加目标队列及其父队列的 allocated [Container | MB | Core]  \n2.对于 reservedContainers,  \n&ensp;&ensp;&ensp;&ensp;ResourceUsage:减少源队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的reserved [Container | MB | Core]指标,增加目标队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  \n\n\n### 结论  \n\n结合 \"正常的资源分配和释放过程\"和\"movetoqueue 过程中的资源转移过程 \",可以还原一个被转移队列的 application 的资源变动过程  \napplication 中非 reservedContianer(状态机:allocated,acquired,running) 的资源变动过程如下表  \n\n| allocatedContainer  | 分配 | movetoqueue | 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n|ResourceUsageByLabel(USED)|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n\napplication 中 reservedContianer(状态机:reversed) 的资源变动过程如下表   \n\n| reversedContainer  | 分配 | movetoqueue| 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|ResourceUsageByLabel(USED)|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|  \n\n>\n从上述两表中可知:  \n1.在没有 movetoqueue 操作的情况下, 源队列和目标队列一致,无论 allocatedContainer 或 reservedContainer 的分配和释放都是守恒的,一次指标的增加对应着一次指标的减少  \n2.在源队列资源比较充裕,且有 movetoqueue 操作的情况下,application 没有 reservedContainer 的情况下(上述表一),在分配时源队列指标增加,movetoqueue 时,源队列减少指标且目标队列增加指标,在释放时减少目标队列(app 当前归属队列)的指标.指标的增减也是平衡的,这也就是在资源充裕的队列上无法复现该问题的原因  \n3.在源队列资源比较紧张,且有 movetoqueue 操作的情况下,application 有 reservedContainer 和 allocatedContainer(上述表一和表二),对于 allocatedContainer 来说,资源在分配->movetoque->释放的过程中是平衡的(表一);但是对于 reversedContainer 来说,在源队列中分配到资源时,增加了源队列的 numContainer 和 ResourceUsageByLabel 中 USED 用途的资源量,movetoqueue 时没有相应的操作,仅仅是修改了 QueueMetrcis 内的指标,在释放时 container 资源被加入到目标队列的指标中(上述表二).<b>源队列分配出去的资源没有回收,目标队列得到了不是自身分配出去的资源</b>.这样导致了即使源队列和目标队列中完全无任务时,源队列的 numContainer 和 ResourceUsageByLabel 值为正值,目标队列的 numContainer 和 ResourceUsage 值为负值,Yarn UI ->scheduler->queue 内的 Num Containers/Used Capacity/Absolute Used Capacity/Used Resources 正是基于 队列的 numContainer 和 ResourceUsageByLabel(USED) 展示和计算的,也就是指标异常的问题     \n\n指标异常(无任务时 为负)\n![](/img/pictures/negative/negative_e7a766275896.png)\n\n指标异常(无任务时 为正)\n![](/img/pictures/negative/negative_aead-04d8e67b1357.png)\n\n### 危害\n\n1.对于源队列来说,类似于资源泄露,自身资源未能回收,释放到了目标队列.这样对于后续提交到源队列的任务来说,不能分配到资源运行(USED 泄露趋于 100%),但实际上源队列没有任务在运行    \n2.对于目标队列来说,多出了一些资源,导致自身一些指标为负,可能会导致一些未知的问题.以下纯属个人猜测: 例如 同步问题,Queue 有很多的剩余资源,但是 NodeManager 上没有 available 资源;指标负值可能对内部的运算产生未预期的问题  \n\n### 解决方法\n\n针对 reservedContainer 在 movetoqueue 操作过程中,资源释放申请不守恒的漏洞,如下表  \n\n| reversedContainer  | 分配 | movetoqueue| 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|ResourceUsageByLabel(USED)|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|  \n\n>\n问题:    \n&ensp;&ensp;&ensp;&ensp;YarnUI 中指标 Num Container、Used Capacity、Absolute Used Capacity、Used Resource 异常和 Queue成员变量numContainer及 QueueUsageByLabel 中 USED 用途的资源量有关    \n目标:  \n&ensp;&ensp;&ensp;&ensp;解决 numContainer 和 ResourceUsageByLabel(USED)在 movetoqueue 时,没有\"减少源队列的指标\"且没有\"增加目标队列指标\"的问题  \n途径:  \n&ensp;&ensp;&ensp;&ensp;1.需要在 movetoqueue 时,每个 reservedContainer 都应触发 \"源队列减少指标\"和\"目标队列增加指标\"的行为 \n&ensp;&ensp;&ensp;&ensp;源码中有封装好的代码且上文多次提到,即 LeafQueue#releaseResource方法和 LeafQueue.allocateResource方法,但是其中有额外的 对 headroom 的同步逻辑.一方面,在\"正常的分配和释放\"过程中,无论申请和分配的 container 是 allocated 或是 reversed 都会执行 LeafQueue#releaseResource 和 LeafQueue.allocateResource 修改 numContainer 和 ResourceUsageByLabel 的同时修改 headroom值;另一方面 move container中对 headroom 增加和减少 同一个Resource 是幂等的.所以 movetoqueue 操作对于每一个 container 的移动都可以选择触发这两个方法来增加对 numContainer 和 ResourceUsageByLabel 的操作.LeafQueue#detachContainer()和 LeafQueueattachContainer()  中有递归本队列及当前队列此逻辑的过程.直接用即可       \n修改方案:  \n&ensp;&ensp;&ensp;&ensp;CapacitySchedule#moveApplication(ApplicationId appId,String targetQueueName) 方法修改如下  \n\n[CapacityScheduler.java]\n\n```\n@Override\n  public synchronized String moveApplication(ApplicationId appId,\n      String targetQueueName) throws YarnException {\n    FiCaSchedulerApp app =\n        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));\n    String sourceQueueName = app.getQueue().getQueueName();\n    LeafQueue source = getAndCheckLeafQueue(sourceQueueName);\n    String destQueueName = handleMoveToPlanQueue(targetQueueName);\n    LeafQueue dest = getAndCheckLeafQueue(destQueueName);\n    // Validation check - ACLs, submission limits for user & queue\n    String user = app.getUser();\n    try {\n      dest.submitApplication(appId, user, destQueueName);\n    } catch (AccessControlException e) {\n      throw new YarnException(e);\n    }\n    // Move all live containers\n    for (RMContainer rmContainer : app.getLiveContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      // attach the Container to another queue\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //description:解决 reservedContainer 在 movetoqueue 操作过程中,Queue.numContainer 指标和 ResourceUsageByLabel.USED 资源量\n    //释放申请不守恒,导致 Yarn UI \"Num Container\",\"Used Capacity\",\"Absolute Used Capacity\",\"Used Resource\"\n    //指标在队列无任务运行时不为 0 的问题\n    //modify by jiulong.zhu@20190903\n    //++add start\n    for (RMContainer rmContainer : app.getReservedContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //++add end\n\n    // Detach the application..\n    source.finishApplicationAttempt(app, sourceQueueName);\n    source.getParent().finishApplication(appId, app.getUser());\n    // Finish app & update metrics\n    app.move(dest);\n    // Submit to a new queue\n    dest.submitApplicationAttempt(app, user);\n    applications.get(appId).setQueue(dest);\n    LOG.info(\"App: \" + app.getApplicationId() + \" successfully moved from \"\n        + sourceQueueName + \" to: \" + destQueueName);\n    return targetQueueName;\n  }\n```\n\n## 冒烟测试用例\n\n[TestCapacityScheduler.java]\n\n```\n  @Test\n  public void testReservedContainerLeakWhenMoveApplication() throws Exception {\n    CapacitySchedulerConfiguration csConf\n            = new CapacitySchedulerConfiguration();\n    csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {\"a\", \"b\"});\n    csConf.setCapacity(\"root.a\",50);\n    csConf.setMaximumCapacity(\"root.a\",100);\n    csConf.setUserLimitFactor(\"root.a\",100);\n    csConf.setCapacity(\"root.b\",50);\n    csConf.setMaximumCapacity(\"root.b\",100);\n    csConf.setUserLimitFactor(\"root.b\",100);\n\n    YarnConfiguration conf=new YarnConfiguration(csConf);\n    conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n            ResourceScheduler.class);\n    RMNodeLabelsManager mgr=new NullRMNodeLabelsManager();\n    mgr.init(conf);\n    MockRM rm1 = new MockRM(csConf);\n    CapacityScheduler scheduler=(CapacityScheduler) rm1.getResourceScheduler();\n    rm1.getRMContext().setNodeLabelManager(mgr);\n    rm1.start();\n    MockNM nm1 = rm1.registerNode(\"127.0.0.1:1234\", 8 * GB);\n    MockNM nm2 = rm1.registerNode(\"127.0.0.2:1234\", 8 * GB);\n    /**\n     * simulation\n     * app1: (1 AM,1 running container)\n     * app2: (1 AM,1 reserved container)\n     */\n    // launch an app to queue, AM container should be launched in nm1\n    RMApp app1 = rm1.submitApp(1 * GB, \"app_1\", \"user_1\", null, \"a\");\n    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm1, nm1);\n\n    // launch another app to queue, AM container should be launched in nm1\n    RMApp app2 = rm1.submitApp(1 * GB, \"app_2\", \"user_1\", null, \"a\");\n    MockAM am2 = MockRM.launchAndRegisterAM(app2, rm1, nm1);\n\n    am1.allocate(\"*\", 4 * GB, 1, new ArrayList<ContainerId>());\n    //this containerRequest should be reserved\n    am2.allocate(\"*\", 4 * GB, 1, new ArrayList<ContainerId>());\n\n    RMNode rmNode1 = rm1.getRMContext().getRMNodes().get(nm1.getNodeId());\n    // Do node heartbeats 2 times\n    // First time will allocate container for app1, second time will reserve\n    // container for app2\n    scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));\n    scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));\n\n    FiCaSchedulerApp schedulerApp1 =\n            scheduler.getApplicationAttempt(am1.getApplicationAttemptId());\n    FiCaSchedulerApp schedulerApp2 =\n            scheduler.getApplicationAttempt(am2.getApplicationAttemptId());\n    // APP1:  1 AM, 1 allocatedContainer\n    Assert.assertEquals(2, schedulerApp1.getLiveContainers().size());\n    // APP2:  1 AM,1 reservedContainer\n    Assert.assertEquals(1,schedulerApp2.getLiveContainers().size());\n    Assert.assertEquals(1,schedulerApp2.getReservedContainers().size());\n    /**\n     * before,move app2 which has one reservedContainer\n     */\n    LeafQueue srcQueue = (LeafQueue) scheduler.getQueue(\"a\");\n    LeafQueue desQueue = (LeafQueue) scheduler.getQueue(\"b\");\n    Assert.assertEquals(4,srcQueue.getNumContainers());\n    Assert.assertEquals(10*GB,srcQueue.getUsedResources().getMemorySize());// AM: 2*1GB   container: 4GB running,4GB reserved\n    Assert.assertEquals(0,desQueue.getNumContainers());\n    Assert.assertEquals(0,desQueue.getUsedResources().getMemorySize());\n    //app1 ResourceUsage (0 reserved)\n    Assert.assertEquals(5*GB,schedulerApp1.getAppAttemptResourceUsage().getAllUsed().getMemorySize());\n    Assert.assertEquals(0,schedulerApp1.getCurrentReservation().getMemorySize());\n    //app2  ResourceUsage (4GB reserved)\n    Assert.assertEquals(1*GB,schedulerApp2.getAppAttemptResourceUsage().getAllUsed().getMemorySize());\n    Assert.assertEquals(4*GB,schedulerApp2.getCurrentReservation().getMemorySize());\n    /**\n     * move app2 which has one reservedContainer\n     */\n    scheduler.moveApplication(app2.getApplicationId(),\"b\");\n    // finish.keep the order,if killing app1 first,the reservedContainer of app2 will be allocated\n    rm1.killApp(app2.getApplicationId());\n    rm1.killApp(app1.getApplicationId());\n    /**\n     * after,moved app2 which has one reservedContainer\n     */\n    {\n      // after fixed\n      Assert.assertEquals(0, srcQueue.getNumContainers());\n      Assert.assertEquals(0, desQueue.getNumContainers());\n      Assert.assertEquals(0, srcQueue.getUsedResources().getMemorySize());\n      Assert.assertEquals(0, desQueue.getUsedResources().getMemorySize());\n    }\n    /*{\n      // before fixed\n      // <b> the reserved container borrowed from srcQueue and returned to desQueue,\n      // but the numContainer and UsedResource did not sync when moving app to another queue </b>\n      Assert.assertEquals(+1,srcQueue.getNumContainers());    //true\n      Assert.assertEquals(-1,desQueue.getNumContainers());    //true\n      Assert.assertEquals(+4*GB, srcQueue.getUsedResources().getMemorySize());    //true\n      Assert.assertEquals(-4*GB, desQueue.getUsedResources().getMemorySize());    //true\n    }*/\n    rm1.close();\n  }\n```\n\n## 线上测试用例\n\n\n","source":"_posts/2019-09-15-Yarn:movetoqueue导致的UIcontainer泄露问题的修复思路.md","raw":"---\nlayout:     post                                    # 使用的布局（不需要改）\ntitle:      Yarn movetoqueue 导致的UI指标错误的修复思路                             # 标题\nsubtitle:   Yarn 源码解析              #副标题\ndate:       2019-09-15                          # 时间\nauthor:     jiulongzhu                                          # 作者\ncatalog: true                                           # 是否归档\ntags:                                                           #标签\n    - Yarn 2.7.3    \n    - 源码解析\n---\n\n## 问题背景 \n\n多个线上运行状态私有云的某些资源队列,在无任何 Application 提交及运行、资源队列完全空闲的情况下, \nUsed Capacity、Absolute Used Capacity、Used Resource、Num Containers指标非零异常  \n\n<!-- more -->\n\n>\n在Yarn ResourceManager管理界面中,有 scheduler 选项卡,展示了Yarn 当前使用的调度器及各资源队列的信息(YarnUI->scheduler->Application Queue),其中每项指标代表的含义是:  \nQueue State: 表示当前队列的状态,有 RUNNING/STOPPED 两种状态  \nUsed Capacity: 表示当前队列已使用的资源占当前队列总资源的百分比  \nConfigured Capacity: 表示当前队列的资源占父队列资源的百分比  \nConfigured Max Capacity: 表示当前队列资源最大能占父队列资源的百分比  \nAbsolute Used Capacity: 表示当前队列已使用的资源占 root 队列资源(整个集群)的百分比  \nAbsolute Configured Capacity: 表示当前队列的资源占 root 队列总资源的百分比  \nAbsolute Configured Max Capacity: 表示当前的队列的资源最大能占 root 队列的百分比  \nUsed Resources: 表示当前队列已使用的资源总量(资源以内存和虚拟核形态表示,基本调度单位)  \nNum Schedulable Applications: 表示当前队列调度的应用个数\nNum Non-Schedulable Applications: 表示当前队列没有调度(积压,pending)的应用个数  \nNum Containers: 表示当前队列已经启动的 container 个数  \nMax Applications: 表示当前队列最大并发调度应用个数  \nMax Applications Per User: 表示当前队列对每个用户最大并发调度应用个数  \nMax Application Master Resources: 表示所有 Application 的 AM 可使用资源量之和的最大值    \nUsed Application Master Resources: 表示当前队列中所有 Application 的 AM 使用资源量之和  \nMax Application Master Resources Per User: 表示当前队列中每个用户的 Application 的 AM 使用资源量之和的最大值  \nConfigured Minimum User Limit Percent: 表示队列每个用户分配的最低资源百分比(资源保障)  \nConfigured User Limit Factor: 表示每个用户能占用的队列资源的百分比  \nAccessible Node Labels:  表示当前队列可在哪些节点上分配资源 (*为全部节点)  \nPreemption: 是否允许资源抢占  \n\n指标可分为两类:配置型指标,静态数据 不会变化,如 Configured Capacity;状态型指标,动态数据 随应用的提交运行结束而变化,如 Used Capacity.  \n运行时异常指标如下图,无 Application 运行的情况下,low 队列状态型指标为负\n\t\n![](/img/pictures/negative/negative_e7a766275896.png)\n\n\n## 先期判断\n\n### 指标关联的变量定位\n\n线上 hadoop 版本: hadoop 2.7.3  \nYarn ResourceManager 管理界面启动的入口是org.apache.hadoop.yarn.server.resourcemanager.ResourceManager#startWebApp()  \n[ResourceManager.java]\n\n```\nprotected void startWepApp() {\n\t....\n    Builder<ApplicationMasterService> builder = \n        WebApps\n            .$for(\"cluster\", ApplicationMasterService.class, masterService,\n                \"ws\")\n            .with(conf)\n            .withHttpSpnegoPrincipalKey(\n                YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY)\n            .withHttpSpnegoKeytabKey(\n                YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)\n            .at(webAppAddress);\n   \t....\n    webApp = builder.start(new RMWebApp(this));\n  }\n```\n\nRMWebApp 主要逻辑是使用 Google Guice 做依赖注入,并分发请求绑定后台逻辑,大致相当于 SpringMVC 系统中的 Dispatcher 的角色  \n对 Yarn UI界面左侧边栏 scheduler 的请求会转发给 RmController#scheduler()方法处理  \n[RMWebApp.java]\n\n```\npublic void setup() {\n    ...\n    bind(RMWebApp.class).toInstance(this);\n    if (rm != null) {\n      bind(ResourceManager.class).toInstance(rm);\n      bind(ApplicationBaseProtocol.class).toInstance(rm.getClientRMService());\n    }\n    ...\n    route(\"/scheduler\", RmController.class, \"scheduler\");\n    route(pajoin(\"/queue\", QUEUE_NAME), RmController.class, \"queue\");\n    ...\n  }\n```\n\nRmController#scheduler() 先获取到 Guice 注入的 ResourceManager,然后依据 RM 使用的调度器做页面渲染.  \n页面渲染逻辑的入口是CapacitySchedulerPage#render()    \n[RmController.java]  \n\n```\npublic void scheduler() {\n    ...\n    ResourceManager rm = getInstance(ResourceManager.class);\n    ResourceScheduler rs = rm.getResourceScheduler();\n    if (rs == null || rs instanceof CapacityScheduler) {\n      setTitle(\"Capacity Scheduler\");\n      //渲染\n      render(CapacitySchedulerPage.class);\n      return;\n    }\n    if (rs instanceof FairScheduler) {\n      setTitle(\"Fair Scheduler\");\n      render(FairSchedulerPage.class);\n      return;\n    }\n    ....\n  }\n protected void render(Class<? extends View> cls) {\n    context().rendered = true;\n    getInstance(cls).render();\n  }\n```\n\n泛型上界是 View,调用栈是:  \nView#render()  \n&ensp;&ensp;->HtmlPage#render()  \n&ensp;&ensp;&ensp;&ensp;&ensp;->TwoColumnLayout#render(html)  \n&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;->CapacitySchedulerPage#content()  \n[CapacitySchedulerPage.java] \n\n```\nprotected Class<? extends SubView> content() {\n    return QueuesBlock.class;\n  }\n```\n\nQueuesBlock 是 CapacitySchedulerPage 的内部类,用于展示CapacityScheduler 的 DashBoard 信息,包含队列、标签、应用概览信息,并构建根队列 root 来做为后续递归渲染的起点    \n[QueuesBlock.java]\n\n```\nclass QueuesBlock extends HtmlBlock {\n    final CapacityScheduler cs;\n    final CSQInfo csqinfo;\n    private List<NodeLabel> nodeLabelsInfo;\n    \n    public void render(Block html) {\n        ...\n        float used = 0;\n        if (null == nodeLabelsInfo\n            || (nodeLabelsInfo.size() == 1 && nodeLabelsInfo.get(0)\n                .getLabelName().isEmpty())) {\n                //创建根队列,作为后续渲染的起点\n          CSQueue root = cs.getRootQueue();\n          CapacitySchedulerInfo sinfo =\n              new CapacitySchedulerInfo(root, new NodeLabel(\n                  RMNodeLabelsManager.NO_LABEL));\n          csqinfo.csinfo = sinfo;\n          csqinfo.qinfo = null;\n\n           ...\n          ul.li().\n            ...\n            _(QueueBlock.class)._();\n        } else {\n          for (NodeLabel label : nodeLabelsInfo) {\n            ....\n            underLabel.li().\n            ...\n            _(QueueBlock.class)._()._();\n          }\n        }\n      }\n      //Application List 界面\n      ul._()._().\n      script().$type(\"text/javascript\").\n          _(\"$('#cs').hide();\")._()._().\n      _(RMAppsBlock.class);\n    }\n  }\n```\n\n从 rootQueue 开始递归,Queue 检查自身有没有子队列 subQueues,若无则其本身为 LeafQueue,使用 LeafQueueInfoBlock#render()渲染叶子队列信息,使用 QueueUsersInfoBlock#render()渲染队列下的用户信息;若有子队列则其本身为ParentQueue,使用 QueueBlock#render()渲染,直至叶子队列  \n[QueuesBlock.java]\n\n```\npublic void render(Block html) {\n      ArrayList<CapacitySchedulerQueueInfo> subQueues =\n          (csqinfo.qinfo == null) ? csqinfo.csinfo.getQueues().getQueueInfoList()\n              : csqinfo.qinfo.getQueues().getQueueInfoList();\n      UL<Hamlet> ul = html.ul(\"#pq\");\n      for (CapacitySchedulerQueueInfo info : subQueues) {\n  \t  ...\n          if (info.getQueues() == null) {\n          li.ul(\"#lq\").li()._(LeafQueueInfoBlock.class)._()._();\n          li.ul(\"#lq\").li()._(QueueUsersInfoBlock.class)._()._();\n        } else {\n          li._(QueueBlock.class);\n        }\n  \t...\n      }\n    }\n```\n\n由于异常指标位于叶子队列信息中,所以暂且不看QueueUsersInfoBlock.java  \n[LeafQueueInfoBlock.java]\n\n```\nprivate String nodeLabel;\nfinal CapacitySchedulerLeafQueueInfo lqinfo;\n@Inject LeafQueueInfoBlock(ViewContext ctx, CSQInfo info) {\n      super(ctx);\n      lqinfo = (CapacitySchedulerLeafQueueInfo) info.qinfo;\n      nodeLabel = info.label;\n    }\nprotected void render(Block html) {\n      if (nodeLabel == null) {\n        renderLeafQueueInfoWithoutParition(html);\n      } else {\n        renderLeafQueueInfoWithPartition(html);\n      }\n    }\n\n虽然按照 nodeLabel 做了分支,但是核心逻辑都是\nrenderQueueCapacityInfo方法和renderCommonLeafQueueInfo方法,\n这两个方法分别展示不同方面的指标,和 Yarn UI 展示的指标相同 \n\nprivate void renderQueueCapacityInfo(final ResponseInfo ri) {\n      ri.\n      //异常指标\n      _(\"Used Capacity:\", percent(lqinfo.getUsedCapacity() / 100)).\n      _(\"Configured Capacity:\", percent(lqinfo.getCapacity() / 100)).\n      _(\"Configured Max Capacity:\", percent(lqinfo.getMaxCapacity() / 100)).\n      //异常指标\n      _(\"Absolute Used Capacity:\", percent(lqinfo.getAbsoluteUsedCapacity() / 100)).\n      _(\"Absolute Configured Capacity:\", percent(lqinfo.getAbsoluteCapacity() / 100)).\n      _(\"Absolute Configured Max Capacity:\", percent(lqinfo.getAbsoluteMaxCapacity() / 100)).\n      //异常指标\n      _(\"Used Resources:\", lqinfo.getResourcesUsed().toString());\n    }\n \n  private void renderCommonLeafQueueInfo(final ResponseInfo ri) {\n      ri.\n      _(\"Num Schedulable Applications:\", Integer.toString(lqinfo.getNumActiveApplications())).\n      _(\"Num Non-Schedulable Applications:\", Integer.toString(lqinfo.getNumPendingApplications())).\n      //异常指标\n      _(\"Num Containers:\", Integer.toString(lqinfo.getNumContainers())).\n      _(\"Max Applications:\", Integer.toString(lqinfo.getMaxApplications())).\n      _(\"Max Applications Per User:\", Integer.toString(lqinfo.getMaxApplicationsPerUser())).\n      _(\"Max Application Master Resources:\", lqinfo.getAMResourceLimit().toString()).\n      _(\"Used Application Master Resources:\", lqinfo.getUsedAMResource().toString()).\n      _(\"Max Application Master Resources Per User:\", lqinfo.getUserAMResourceLimit().toString()).\n      _(\"Configured Minimum User Limit Percent:\", Integer.toString(lqinfo.getUserLimit()) + \"%\").\n      _(\"Configured User Limit Factor:\", StringUtils.format(\n          \"%.1f\", lqinfo.getUserLimitFactor())).\n      _(\"Accessible Node Labels:\", StringUtils.join(\",\", lqinfo.getNodeLabels())).\n      _(\"Preemption:\", lqinfo.getPreemptionDisabled() ? \"disabled\" : \"enabled\");\n    }\n```\n\n### 异常指标计算方式\n\n注:集群没有使用 label 系统,所以下述的 nodeLabel 视为\"\"即可\n\n* Used Capacity\n\n\t[AbstractCSQueue.java]\n\t\n\t```\npublic final synchronized float getUsedCapacity(final String nodeLabel) {\n    //集群所有资源 * 该队列的绝对容量百分比 = 该队列的绝对容量 \n    Resource availableToQueue =\n        Resources.multiply(\n            labelManager.getResourceByLabel(nodeLabel, this.clusterResource),\n            queueCapacities.getAbsoluteCapacity(nodeLabel));\n   //使用 queueUsage 中记录的使用量除以该队列的绝对容量得到队列的 Used Capacity\n   //queueUsage 的类型为 ResourceUsage\n    return\n        Resources.divide(resourceCalculator, this.clusterResource,\n            queueUsage.getUsed(nodeLabel), availableToQueue);\n  }\n\t```\n* Absolute Used Capacity\n\t\n\t算法和Used Capacity算法相似,只是分母不同,AbsoluteUsedCapacity 计算时分母是整个集群的资源  \n\t\n\t[AbstractCSQueue.java]\n\t\n\t```\n\tpublic final synchronized float getAbsoluteUsedCapacity(final String nodeLabel) {\n    Resource labeledResources =\n               labelManager.getResourceByLabel(nodeLabel, this.clusterResource);\n    return Resources.divide(resourceCalculator, this.clusterResource,\n        queueUsage.getUsed(nodeLabel), labeledResources);\n  }\n\t```\n\t\n* Used Resource\n\n\t&ensp;&ensp;使用的是Queue 按 Label 记录的资源信息  \n\tqueueResourceUsage.getUsed(nodeLabel)  \n\t\n* Num Containers\n\n\t&ensp;&ensp;使用的 Queue 本身记录的信息  \n\tnumContainers = leafQueue.getNumContainers();  \n\t\n>\n综上所述:  \t\n四个指标中,队列层面的Used Capacity 、Absolute Used Capacity 、Used Resource 均和 ResourceUsage维护 used 资源信息有关;numContainer 是 LeafQueue 自身维护的 container 数量.这些指标的变化逻辑在 AbstractCSQueue的 assignContainer()和 releaseContainer()中,即和 container 的释放/申请有关.\n而在队列完全空闲的时候,四个指标为负,初步定位可能是 Container 重复释放或无效释放的原因.\n\n## 问题复现\n\n### 猜测重复释放\n\n在已经出现指标异常的私有云上,对出现异常时日期前后的 resourcemanager 日志进行分析,检测其中出现\"Assigned container\"和\"Released  container\"字符串的数量,从日志层面对 container 的申请和释放次数做一个简单的判断.这种方式适用于集群作业周期性较强且没有跨天任务运行的情况.  \n\n```\negrep -o \"Assigned container\" resourcemanager.log | sort | uniq -c\negrep -o \"Released container\" resourcemanager.log | sort | uniq -c \n```\n\n从结果上看,\"Assigned container\"和\"Released container\"字符串出现次数是一致的,可能不是重复释放的问题  \n\n### 猜测无效释放\t\n\n对无效释放的猜想源自于重复释放和 rm 日志中的\"Null container completed...\",猜测可能对于 nullContainer 处理有问题,修改了系统维护的信息, 导致了负值    \n\nUI 的 kill 操作,命令行 yarn application --kill 和 RMApp 的正常结束都会释放 container,由 APP_ATTEMPT_REMOVED 事件触发,\n回收 AppAttemp 持有的runningContainer 和 reservedContainer,核心逻辑在 CapacityScheduler#completedContainer()方法,但是其中对于 RMContainer 和 Application 都做了校验,不会修改维护信息;如果 RMContainer 对象内持有的 contianer 对象为空的话,会抛出 NullPointerException,也不会修改维护信息. 所以无效释放的猜测不合理,此处代码证明重复释放也不合理\n[CapacityScheduler.java]\n\n```\nprotected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) { \n    //rmContaienr 空值校验  \n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    Container container = rmContainer.getContainer();\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n    //如果 RMContainer 映射的 container 为空,则此处会 NullPointerException\n        container.getId().getApplicationAttemptId().getApplicationId();\n    //application 空值校验\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    ....\n  }\n```\n\n### movetoqueue\n\n重复释放和无效释放的猜测证否之后,只好再去仔细研究 系统指标异常前的几个运行的任务日志,对 application 的 attempt 和每个 contianer 状态机的状态和 触发事件按照时间线标注出来,对application 发生的所有事件还原出来,发现了其中一个 application 的以下信息  \n>  \n1.提交到 root.high 队列后,队列资源不足,在某些节点上为该 applicaiton reserve 资源,其他 application 调度时不会再该节点上分配资源  \n2.application 被从 high 队列移动了 low 队列  \n3.container 被移动队列时,源队列和目标队列的资源有变动,两个队列都会被 re-sort,以便优先在资源利用率最低的 queue 调度,所以有每个 container 的移动有四条日志:queueMoveOut 队列的信息,queueMoveOut 的父队列信息(有几个 parentQueue 就会有几条日志),queuMoveIn 队列信息,queueMoveIn 的父队列信息  \n4.在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000002,在 high队列上分配了资源,被 move 到 low 队列后,containerCompleted 之后,释放资源到了 low 队列  \n5.<b>在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000080,在 high 队列上保留了资源,move 操作没有影响到该 container,但是containerCompleted之后释放资源到了 low 队列</b>. \n\n日志如下  \n\n```\nINFO LeafQueue: assignedContainer application attempt=appattempt_1565789460020_5864_000001 container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: null, ] queue=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:420864, vCores:20>, usedCapacity=1.4421053, absoluteUsedCapacity=0.86526316, numApps=3, numContainers=20 clusterResource=<memory:486400, vCores:136> type=OFF_SWITCH  \nINFO RMContainerImpl: container_e08_1565789460020_5864_01_000080 Container Transitioned from NEW to RESERVED\nINFO LeafQueue: Reserved container  application=application_1565789460020_5864 resource=<memory:11264, vCores:1> queue=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:443392, vCores:22>, usedCapacity=1.5192982, absoluteUsedCapacity=0.91157895, numApps=3, numContainers=22 usedCapacity=1.5192982 absoluteUsedCapacity=0.91157895 used=<memory:443392, vCores:22> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: Re-sorting assigned queue: root.high stats: high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:454656, vCores:23>, usedCapacity=1.5578947, absoluteUsedCapacity=0.93473685, numApps=3, numContainers=23\n....moving\nINFO LeafQueue: movedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service:  }, ] resource=<memory:11264, vCores:1> queueMoveOut=high: capacity=0.6, absoluteCapacity=0.6, usedResources=<memory:431104, vCores:20>, usedCapacity=1.477193, absoluteUsedCapacity=0.88631576, numApps=3, numContainers=20 usedCapacity=1.477193 absoluteUsedCapacity=0.88631576 used=<memory:431104, vCores:20> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: movedContainer queueMoveOut=root usedCapacity=0.9768421 absoluteUsedCapacity=0.9768421 used=<memory:475136, vCores:24> cluster=<memory:486400, vCores:136>\nINFO LeafQueue: movedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: }, ] resource=<memory:11264, vCores:1> queueMoveIn=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:-22528, vCores:-3>, usedCapacity=-0.4631579, absoluteUsedCapacity=-0.04631579, numApps=0, numContainers=-3 usedCapacity=-0.4631579 absoluteUsedCapacity=-0.04631579 used=<memory:-22528, vCores:-3> cluster=<memory:486400, vCores:136>\nINFO ParentQueue: movedContainer queueMoveIn=root usedCapacity=1.0 absoluteUsedCapacity=1.0 used=<memory:486400, vCores:25> cluster=<memory:486400, vCores:136>\n....moved\nINFO CapacityScheduler: App: application_1565789460020_5864 successfully moved from high to: low\nINFO LeafQueue: completedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000080, NodeId: hadoop5:8041, NodeHttpAddress: hadoop5:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: null, ] queue=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:0, vCores:-1>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=-1 cluster=<memory:486400, vCores:136>\nINFO LeafQueue: completedContainer container=Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6.:8041, NodeHttpAddress: hadoop6.cn:8042, Resource: <memory:11264, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: }, ] queue=low: capacity=0.1, absoluteCapacity=0.1, usedResources=<memory:-1024, vCores:-2>, usedCapacity=-0.021052632, absoluteUsedCapacity=-0.002105263, numApps=1, numContainers=-2 cluster=<memory:486400, vCores:136>\n```\n\n>猜测:application 在移动队列后,对 reservedContainer 没有移动或维护信息不同步,导致了源队列的资源泄露给了目标队列,从而目标队列的 UsedCapacity 为负  \n>复现方式  \n&ensp;&ensp;&ensp;&ensp;第一步:提交 application 到资源紧张的 Queue  \n&ensp;&ensp;&ensp;&ensp;第二步:待在 RM 的日志中看到\"Trying to fulfill reservation for application ${APPLICATION_ID} on node ...\"和\"Trying to schedule on node..., available:...\" 表明 CapacityScheduler为该 ApplicationId 保留了资源,跳过在此 nm 上为其他 app 分配 container    \n&ensp;&ensp;&ensp;&ensp;第三步: 使用 yarn application -movetoqueue ${APPLICATION_ID} -queue ${TOQueue}  \n&ensp;&ensp;&ensp;&ensp;第四步: 待在 RM 日志中看到了\"App:${APPLICATION_ID} successfully moved ${FROMQUEUE} to ${TOQUEUE}\"后,使用 yarn application --kill ${APPLICATION_ID} 或等待 app 结束   \n&ensp;&ensp;&ensp;&ensp;第五步: 在 YarnUI上查看被移动的目标队列${TOQUEUE}的信息  \n\n注:  \n&ensp;&ensp;&ensp;&ensp;1.一定要在资源紧张的队列上提交 app,以触发调度系统的保留资源.在空闲队列上提交任务复现不了指标异常的问题  \n&ensp;&ensp;&ensp;&ensp;2.最好移动到一个完全空闲的队列上,否则即使复现了资源泄露,也不易看出来  \n\n## 源码解析\n\n以下代码出自于 hadoop 2.7.3版本  \nYarn 是一个资源调度平台,集群内存资源和 cpu 资源被 Yarn 抽象为 Resource{memory,core},客户端对 Yarn 的资源请求和 Yarn 内部的资源调度都是以 Container 为基本单位的    \n当客户端向资源队列 Queue 提交 Application 时, 客户端申请的 AM 以及 AM 申请新的执行角色(e.g. spark 的 executor)都是在Container 中运行,那么对于 Yarn 调度的 Container,资源信息被多维聚合[app状态|container状态|container用途|用户|标签]统计维护:    \n\n* Queue.包括直接申请的叶子队列及其所有的父队列    \n\t1. Queue(AbstractCSQueue)    \n &ensp;&ensp;主要指标是 container 数量;保存着 QueueMetrics 和 ResourceUsage(ByLabel) 的引用     \n\t\ta. ParentQueue   \n&ensp;&ensp;主要指标是运行的 application数量;保存着所有资源子队列的集合    \n\t\tb. LeafQueue \n&ensp;&ensp; 主要指标是每个用户提交 app 的数量和资源用途用量(ResourceUsageByUser);保存着队列 running 和 pending 的 app[attemp]信息  \t \t\t\n\t2. QueueMetrics  \n&ensp;&ensp;保留着queue 的指标信息,包括[提交|运行|积压|完成|杀死|失败]app 数量、[分配|待分配|积压]的[container|内存|虚拟核]信息、活跃的[app|user]信息;保存着用户级别的 <username,QueueMetrics>映射  \n\t3. ResourceUsage\t\t\t\n&ensp;&ensp;保留着<label,UsageByLabel>信息,分为used、pending、amused、reserved 四类  \n* Application(SchedulerApplicationAttempt). app 中维护着当前正在运行的 container,及 yarn 为其保留的 container 信息\n* NodeManager. container 所在的 NM 维护着自身运行的所有 container,并通过 RM 心跳汇报所有 container 状态机状态,触发 RM 对container 状态的更新或释放    \n\n>\n综上:  \n&ensp;&ensp;&ensp;&ensp;1.指标异常与 container 申请释放有关,所以需要研究 <b>正常申请释放与应用移动队列</b> 对上述维护信息的影响  \n&ensp;&ensp;&ensp;&ensp;2.UI 展示的 numContainers 数据取自 AbstractCSQueue 维护 numContainer 成员变量;usedCapacity,absoluteUsedCapacity,usedResource 均取自AbstractCSQueue$ResourceUsage.used 成员变量.需要关注以上变量在资源申请释放过程中的变化    \n\n### 正常的资源分配和释放过程\n\nRM 的资源分配和资源释放都是被动触发,客户端提交 application 到 RMClientService,app 信息暂存在指定资源队列中,待 NM 向 RM 通过心跳汇报自身信息时 RM 将对该 NM 触发调度,在情况允许的情况下(e.g. 目标队列具有该 NM 的 access 权限,NM 剩余资源满足 app 中一个 ResourceRequest 申请的资源),将在该 NM 上划出 ResourceRequest 要求的资源,并同步 Queue/Application/NodeManager 维护的信息;当 container 完成退出之后,NodeManager 注意到 container 状态机变化,并将其信息附带在心跳中汇报给 RM,触发 Queue/Application/NodeManager 状态变化和信息同步   \n以下代码较为关注资源信息的维护同步,对于其他细节不再赘述    \n\t\n### 资源分配\n\n![](/img/pictures/negative/yarn_node_update_pic.png)\n\nNM 通过心跳触发 CapacityScheduler 调度,CS 首先按照 NM 上报的信息同步 RM 的信息,为新启动的 container 触发 LAUNCHED 事件,为结束的 container 触发FINISHED 事件;并试图在该节点上分配资源  \n[CapacityScheduler.java]\n\n```\npublic void handle(SchedulerEvent event) {\n...\ncase NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      RMNode node = nodeUpdatedEvent.getRMNode();\n      //同步 RM 信息\n      nodeUpdate(node);\n      if (!scheduleAsynchronously) {\n      //试图在 nm 分配资源\n        allocateContainersToNode(getNode(node.getNodeID()));\n      }\n    }\n    break;\n ...\n}\n```\n依据汇报心跳的 NodeManager 是否被保留了资源执行相应的逻辑  \n[CapacityScheduler.java]\n\n```\nprivate synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n    ...\n    RMContainer reservedContainer = node.getReservedContainer();\n    //当前 node 被某 applocation 保留了\n    if (reservedContainer != null) {\n      FiCaSchedulerApp reservedApplication =\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + \n          node.getNodeID());\n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment =\n          queue.assignContainers(\n              clusterResource,\n              node,\n              new ResourceLimits(labelManager.getResourceByLabel(\n                  RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      ....\n      }\n    }\n    //当前节点未被保留\n    if (node.getReservedContainer() == null) {\n     \t...\n     \t//交由 root 队列代理分配.root 是资源队列树形结构的根节点,类型一定ParentQueue\n        root.assignContainers(\n            clusterResource,\n            node,\n            new ResourceLimits(labelManager.getResourceByLabel(\n                RMNodeLabelsManager.NO_LABEL, clusterResource)));\n      }\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + node.getNodeID() + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }\n```\nParentQueue 将节点委派给子队列,试图分配资源  \n[ParentQueue.java]\n\n```\n  public synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits resourceLimits) {\n    CSAssignment assignment = \n        new CSAssignment(Resources.createResource(0, 0), NodeType.NODE_LOCAL);\n    Set<String> nodeLabels = node.getLabels();\n    ...校验 queue 对 node 的 access 权限\n    while (canAssign(clusterResource, node)) {\n      ...校验 queue 资源是否超限      \n      // Schedule 递归交由子队列去分配\n      CSAssignment assignedToChild = \n          assignContainersToChildQueues(clusterResource, node, resourceLimits);\n      assignment.setType(assignedToChild.getType());\n      // Done if no child-queue assigned anything\n      // 如果分配到了资源, assignedToChild 大于 Resource<0,0>\n      if (Resources.greaterThan(\n              resourceCalculator, clusterResource, \n              assignedToChild.getResource(), Resources.none())) {\n              //同步维护的信息\n        super.allocateResource(clusterResource, assignedToChild.getResource(),\n            nodeLabels);\n        Resources.addTo(assignment.getResource(), assignedToChild.getResource());\n        ...\n         } else {\n        break;\n      }\n      ...\n      }\n\t...\n        break;\n      }\n    }     \n    return assignment;\n  }\n```\n先看如果分配到资源的话 同步的信息:  \n1.按照 label 去更新 ResourceUsage 中维护的用户使用资源 used(incUsed 方法)    \n2.增加了该队列的 numContainer 数量  \n需要注意的是 <b>无论后续得到的是 allocated container 还是 reserved container,都增加了 numContainer的值,增加了用户 USED 类型的内存和虚拟核数量</b>   \n[AbstractCSQueue.java]\n\n```\nsynchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    // Update usedResources by labels\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n再看对 queueUsage:ResourceUsage 做了什么操作  \nResourceUsage 中维护了一个Map结构 usages,key 是标签类型,value 是 UsageByLabel;UsageByLabel 中只有一个数组 Resource[],数组中的每个值分别表征着 USED,PENDING,AMUSED,RESERVED 用途的 Resource 数量  \n[ResourceUsage.java]\n\n```\n// <labelName,UsageByLabel>\nprivate Map<String, UsageByLabel> usages;\npublic void incUsed(String label, Resource res) {\n    _inc(label, ResourceType.USED, res);\n }\n private void _inc(String label, ResourceType type, Resource res) {\n     ...\n     UsageByLabel usage = getAndAddIfMissing(label);\n     Resources.addTo(usage.resArr[type.idx], res);      \n     ...\n   }\n```\nUsageByLabel 使用一个 Resource[]数组来存储资源的用途和用量  \n[UsageByLabel.java]  \n\n```\n  private static class UsageByLabel {\n    // usage by label, contains all UsageType\n    private Resource[] resArr;\n    public UsageByLabel(String label) {\n      resArr = new Resource[ResourceType.values().length];\n      for (int i = 0; i < resArr.length; i++) {\n        resArr[i] = Resource.newInstance(0, 0);\n      };\n    }\n  }\n  \n   private enum ResourceType {\n    USED(0), PENDING(1), AMUSED(2), RESERVED(3);\n    private int idx;\n    private ResourceType(int value) {\n      this.idx = value;\n    }\n  }\n```\n再回到资源分配,root 队列深度优先遍历所有子队列,尝试在叶子队列上分配资源  \n[LeafQueue.java]\n\n```\npublic synchronized CSAssignment assignContainers(Resource clusterResource,\n      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) {\n\t....\n    // Check for reserved resources\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      FiCaSchedulerApp application = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      synchronized (application) {\n        return assignReservedContainer(application, node, reservedContainer,\n            clusterResource);\n      }\n    }\n    //对目前活跃的 application,尝试在当前 NM 上分配资源\n    for (FiCaSchedulerApp application : activeApplications) {\n    \t....\n      synchronized (application) {\n         ....        \n          // Schedule in priority order\n        for (Priority priority : application.getPriorities()) {\n          ResourceRequest anyRequest =\n              application.getResourceRequest(priority, ResourceRequest.ANY);\n         if (null == anyRequest) {\n            continue;\n          }\n          ....校验性工作\n          //校验通过,在此节点上分配资源\n          CSAssignment assignment =  \n            assignContainersOnNode(clusterResource, node, application, priority, \n                null, currentResourceLimits);\n\t.....\n          // Did we schedule or reserve a container?\n          Resource assigned = assignment.getResource();\n          if (Resources.greaterThan(\n              resourceCalculator, clusterResource, assigned, Resources.none())) {\n            //更新当前[叶子]队列的 numContainer,ResourceUsageByLabel和 ResourceUsageByUser.\n            //比 ParentQueue.allocateResource 多出了一个用户层面的资源统计\n            allocateResource(clusterResource, application, assigned,\n                node.getLabels());\n            .....            \n            return assignment;\n          } else {\n            break;\n          }\n        }\n      }\n    }\n    return NULL_ASSIGNMENT;\n  }\n```\n先看LeafQueue#allocateResource 更新信息时更新的指标和内容\nLeafQueue 和ParentQueue  一样,使用抽象父类AbstractCSQueue#allocateResource() 更新当前队列的 numContainer 和 标签层面的 ResourceUsage  \n然后使用 LeafQueue 中维护的Map<String,User>成员变量 users来维护用户标签层面的资源信息 \n[LeafQueue.java]\n\n```\nsynchronized void allocateResource(Resource clusterResource,\n      SchedulerApplicationAttempt application, Resource resource,\n      Set<String> nodeLabels) {\n      //使用的是 AbstractCSQueue#allocateResource,和 ParentQueue更新的指标及内容相同:numContainer,ResourceUsageByLabel\n    super.allocateResource(clusterResource, resource, nodeLabels);\n    //更新用户层面的 ResourceUsage\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.assignContainer(resource, nodeLabels);\n    Resources.subtractFrom(application.getHeadroom(), resource); // headroom\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n  }\n```\nUser 类中有一个 ResourceUsage,维护着在用户层面 各标签的资源用途和用量  \n[User.java]\n\n```\npublic static class User {\n    ResourceUsage userResourceUsage = new ResourceUsage();\n    volatile Resource userResourceLimit = Resource.newInstance(0, 0);\n    int pendingApplications = 0;\n    int activeApplications = 0;\n     public void assignContainer(Resource resource,\n        Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n        userResourceUsage.incUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.incUsed(label, resource);\n        }\n      }\n    }\n  }\n```\n上文 LeafQueue#assignContainers方法 遍历每个活跃的 application,尝试在当前 nodemanager 上分配资源,调用 assignContainersOnNode()方法进行下一步的分配逻辑,并增加 app 在本地化层面分配的 container 数量,此指标逻辑不在此讨论        \n按照本地性优先级,优先分配  NODE_LOCAL(本节点)>RACK_LOCAL(本机架)>OFF_SWITCH(跨机架)  \nYARN 的本地性不同于 MR/SPARK 的本地性,YARN的本地性性体现在 Client/AM申请 container 的时候可以指定 container 所在的节点,此处的 NODE_LOCAL、RACK_LOCAL 和 OFF_SWITCH 是相对于申请 container 时指定的节点而言;数据本地化计算其实更多的是靠计算框架配合,按照数据所处的位置优先分配给计算节点(e.g. RDD#getPreferredLocations)    \n[LeafQueue.java]\n\n```\nprivate CSAssignment assignContainersOnNode(Resource clusterResource,\n      FiCaSchedulerNode node, FiCaSchedulerApp application, Priority priority,\n      RMContainer reservedContainer, ResourceLimits currentResoureLimits) {\n    Resource assigned = Resources.none();\n    NodeType requestType = null;\n    MutableObject allocatedContainer = new MutableObject();\n    // Data-local\n    ResourceRequest nodeLocalResourceRequest =\n        application.getResourceRequest(priority, node.getNodeName());\n    if (nodeLocalResourceRequest != null) {\n      requestType = NodeType.NODE_LOCAL;\n      assigned =\n          assignNodeLocalContainers(clusterResource, nodeLocalResourceRequest, \n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (Resources.greaterThan(resourceCalculator, clusterResource,\n          assigned, Resources.none())) {\n        //update locality statistics\n        if (allocatedContainer.getValue() != null) {\n          application.incNumAllocatedContainers(NodeType.NODE_LOCAL,\n            requestType);\n        }\n        return new CSAssignment(assigned, NodeType.NODE_LOCAL);\n      }\n    }\n    // Rack-local\n    ResourceRequest rackLocalResourceRequest =\n        application.getResourceRequest(priority, node.getRackName());\n    if (rackLocalResourceRequest != null) {\n      if (!rackLocalResourceRequest.getRelaxLocality()) {\n        return SKIP_ASSIGNMENT;\n      }\n      if (requestType != NodeType.NODE_LOCAL) {\n        requestType = NodeType.RACK_LOCAL;\n      }\n      assigned = \n          assignRackLocalContainers(clusterResource, rackLocalResourceRequest, \n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (Resources.greaterThan(resourceCalculator, clusterResource,\n          assigned, Resources.none())) {\n        if (allocatedContainer.getValue() != null) {\n          application.incNumAllocatedContainers(NodeType.RACK_LOCAL,\n            requestType);\n        }\n        return new CSAssignment(assigned, NodeType.RACK_LOCAL);\n      }\n    }\n    // Off-switch\n    ResourceRequest offSwitchResourceRequest =\n        application.getResourceRequest(priority, ResourceRequest.ANY);\n    if (offSwitchResourceRequest != null) {\n      if (!offSwitchResourceRequest.getRelaxLocality()) {\n        return SKIP_ASSIGNMENT;\n      }\n      if (requestType != NodeType.NODE_LOCAL\n          && requestType != NodeType.RACK_LOCAL) {\n        requestType = NodeType.OFF_SWITCH;\n      }\n      assigned =\n          assignOffSwitchContainers(clusterResource, offSwitchResourceRequest,\n            node, application, priority, reservedContainer,\n            allocatedContainer, currentResoureLimits);\n      if (allocatedContainer.getValue() != null) {\n        application.incNumAllocatedContainers(NodeType.OFF_SWITCH, requestType);\n      }\n      return new CSAssignment(assigned, NodeType.OFF_SWITCH);\n    }\n    return SKIP_ASSIGNMENT;\n  }\n```\nLeafQueue#assignNodeLocalContainers,assignRackLocalContainers,assignOffSwitchContainers的核心逻辑被封装为一处,只是本地化类型 NodeType 不同    \n[LeafQueue.java]\n\n```\nprivate Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, ResourceLimits currentResoureLimits) {\n    ....    \n    Resource capability = request.getCapability();\n    Resource available = node.getAvailableResource();\n    Resource totalResource = node.getTotalResource();\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    // Create the container if necessary\n    Container container = \n        getContainer(rmContainer, application, node, capability, priority);\n  \t...\n      boolean shouldAllocOrReserveNewContainer = shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n    //依据 节点可用资源量与申请的资源量 做除法来判断节点剩余资源能否满足需求 \n    int availableContainers = \n        resourceCalculator.computeAvailableContainers(available, capability);\n    boolean needToUnreserve = Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\t\n    if (availableContainers > 0) {\n      //如果节点上足够分配一个 container 则分配\n      ....\n      //调用 application 和 nodemanager 的方法,触发这两处的信息同步\n      RMContainer allocatedContainer = \n          application.allocate(type, node, priority, request, container);\n          \n      node.allocateContainer(allocatedContainer);\n      \n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n    \t....\n        //节点上没有足够的资源满足需求,则为该 application 保留该节点的资源\n        reserve(application, priority, node, rmContainer, container);\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }\n```\n上文有三处地方需要同步资源信息:reserve()方法,FiCaSchedulerApp#allocate,FiCaSchedulerNode#allocateContainer  \n先看FiCaSchedulerApp#allocate方法,在 application 层面修改了什么信息    \n[FiCaSchedulerApp.java]\n\n```\n synchronized public RMContainer allocate(NodeType type, FiCaSchedulerNode node,\n      Priority priority, ResourceRequest request, \n      Container container) {\n     ....\n    //将 container 封装成 RMContainer 的形式,记录在 application 的所有 container 集合 newlyAllocatedContainers 和 运行态(相对 reserved)的 container 集合 liveContainers 中  \n    RMContainer rmContainer = new RMContainerImpl(container, this\n        .getApplicationAttemptId(), node.getNodeID(),\n        appSchedulingInfo.getUser(), this.rmContext);\n    newlyAllocatedContainers.add(rmContainer);\n    liveContainers.put(container.getId(), rmContainer);    \n    ....\n    //更新 metrics 信息\n    List<ResourceRequest> resourceRequestList = appSchedulingInfo.allocate(\n        type, node, priority, request, container);\n     //当前 app 消费的资源量\n    Resources.addTo(currentConsumption, container.getResource());\n    ...\n    //触发 RMContainer 状态机变化,container 可以准备运行时环境,下载依赖等\n    rmContainer.handle(\n        new RMContainerEvent(container.getId(), RMContainerEventType.START));\n    RMAuditLogger.logSuccess(getUser(), \n        AuditConstants.ALLOC_CONTAINER, \"SchedulerApp\", \n        getApplicationId(), container.getId());\n    return rmContainer;\n```\nAppSchedulingInfo#allocate 更新 QueueMetrics 信息  \n[AppSchedulingInfo.java]\n\n```\nsynchronized public List<ResourceRequest> allocate(NodeType type,\n      SchedulerNode node, Priority priority, ResourceRequest request,\n      Container container) {\n    List<ResourceRequest> resourceRequests = new ArrayList<ResourceRequest>();\n    //本地化层面的统计信息\n    if (type == NodeType.NODE_LOCAL) {\n      allocateNodeLocal(node, priority, request, container, resourceRequests);\n    } else if (type == NodeType.RACK_LOCAL) {\n      allocateRackLocal(node, priority, request, container, resourceRequests);\n    } else {\n      allocateOffSwitch(node, priority, request, container, resourceRequests);\n    }\n    QueueMetrics metrics = queue.getMetrics();\n    ...\n    //对QueueMetrics 做已经分配的资源的统计\n    metrics.allocateResources(user, 1, request.getCapability(), true);\n    metrics.incrNodeTypeAggregations(user, type);\n    return resourceRequests;\n  }\n```\n\nQueueMetrics#allocateResource 对 userMetrics 做判断;对 parent 也做了判断 如果parent 非空那么会递归更新 parent 的信息,对userMetrics 和 parent 执行的方法都是 QueueMetrics#allocateResources,更新的指标一样:增加 allocate 的 container 数量,内存量,虚拟核数量  \n[QueueMetrics.java] \n\n```\npublic void allocateResources(String user, int containers, Resource res,\n      boolean decrPending) {\n    allocatedContainers.incr(containers);\n    aggregateContainersAllocated.incr(containers);\n    allocatedMB.incr(res.getMemory() * containers);\n    allocatedVCores.incr(res.getVirtualCores() * containers);\n    if (decrPending) {\n      _decrPendingResources(containers, res);\n    }\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.allocateResources(user, containers, res, decrPending);\n    }\n    if (parent != null) {\n      parent.allocateResources(user, containers, res, decrPending);\n    }\n  }\n```\nFiCaSchedulerNode#allocateContainer, NodeManager 同步 allocate 信息 \n[FiCaSchedulerNode.java]\n\n```\npublic synchronized void allocateContainer(RMContainer rmContainer) {\n    Container container = rmContainer.getContainer();\n     //减少当前 nm 的可用资源,增加当前 nm 的已分配资源\n    deductAvailableResource(container.getResource());\n    //增加自身维护的 numContainer 数量,不同于 AbstractCSQueue.numContainers,维护Queue维度和 NodeManager 维度的 container 数量\n    ++numContainers;\n    //启动的 container 列表\n    launchedContainers.put(container.getId(), rmContainer);\n    ...\n  }\n  private synchronized void deductAvailableResource(Resource resource) {\n    Resources.subtractFrom(availableResource, resource);\n    Resources.addTo(usedResource, resource);\n  }\n```\n如果在节点上 reserve 了资源,同步的信息和 allocate 的方式相似,会更新 QueueMetrics 对 reserved[containr|memory|core]的信息,更新application reservedContainer 及 currentReservationMemroy 信息,更新 NodeManager 当前 reservedContainer 信息  \n[LeafQueue.java]\n\n```\nprivate void reserve(FiCaSchedulerApp application, Priority priority, \n      FiCaSchedulerNode node, RMContainer rmContainer, Container container) {\n    //传入的 rmContainer 为 null\n    if (rmContainer == null) {\n      getMetrics().reserveResource(\n          application.getUser(), container.getResource());\n    }\n    // Inform the application \n    rmContainer = application.reserve(node, priority, rmContainer, container);\n    // Update the node\n    node.reserveResource(application, priority, rmContainer);\n  }\n```\nQueueMetrics#reserveResource在下方有一个对 parent 的判断,递归调用依次更新 ParentQueue 的信息;有一个对 userMetrics 的判断,对 QueueMetrics 中维护的 users:Map[String, QueueMetrics] 同步用户层面的统计信息,执行的方法都是当前方法QueueMetrics#reserveResource(),同步reserved 资源信息:增加 reserved container 数量,内存量,虚拟核数量  \n\n[QueueMetrics.java]\n\n```\npublic void reserveResource(String user, Resource res) {\n    reservedContainers.incr();\n    reservedMB.incr(res.getMemory());\n    reservedVCores.incr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.reserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.reserveResource(user, res);\n    }\n  }\n```\napplication 同步 reserved container 信息  \n[SchedulerApplicationAttempt.java]\n\n```\n public synchronized RMContainer reserve(SchedulerNode node, Priority priority,\n      RMContainer rmContainer, Container container) {\n     //上游传入的 rmContainer 为 null\n    if (rmContainer == null) {\n      rmContainer = \n          new RMContainerImpl(container, getApplicationAttemptId(), \n              node.getNodeID(), appSchedulingInfo.getUser(), rmContext);\n      //增加当前 application 保留的资源信息\n      Resources.addTo(currentReservation, container.getResource());\n      resetReReservations(priority);\n    } else {\n      // Note down the re-reservation\n      addReReservation(priority);\n    }\n    //在 container 信息中设置了绑定节点的信息....\n    rmContainer.handle(new RMContainerReservedEvent(container.getId(), \n        container.getResource(), node.getNodeID(), priority));\n        //加入到维护的 reservedContainer 信息中\n    Map<NodeId, RMContainer> reservedContainers = \n        this.reservedContainers.get(priority);\n    if (reservedContainers == null) {\n      reservedContainers = new HashMap<NodeId, RMContainer>();\n      this.reservedContainers.put(priority, reservedContainers);\n    }\n    reservedContainers.put(node.getNodeID(), rmContainer);\n    return rmContainer;\n  }\n```\nFicaSchedulerNode#reserveResource(),NodeManager 同步 reserved container 信息  \n[FicaSchedulerNode.java]\n\n```\npublic synchronized void reserveResource(\n      SchedulerApplicationAttempt application, Priority priority,\n      RMContainer container) {\n    // Check if it's already reserved\n    RMContainer reservedContainer = getReservedContainer();\n    ....空值校验性,重复保留校验等操作\n    //设置当前节点保留的 container 为传入值\n    setReservedContainer(container);\n  }\n\n```\n\n\n> 综上:  \n        CS 调度器在 NM 上为 APP 分配 container 时,会同步 [Parent | Leaf] Queue/APP/NM 三个位置维护的信息    \n\t1.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, 其 <b>ParentQueue</b>都会维护:  \n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ParentQueue <b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n\t2.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, <b>LeafQueue</b> 本身都会维护:  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n\t3.在节点上申请资源时,按照节点剩余可用资源和资源需求量做除法,若满足需求则分配为 allocatedContainer,不满足则分配 reservedContainer  \n\t4.若在节点上分配 allocatedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp; Application 方面:newlyAllocatedContainers和 livingContainers 列表,app 已占用资源量    \n&ensp;&ensp;&ensp;&ensp; NodeManager 方面:numContainer 数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表  \n&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身QueueMetrics</b>的allocated [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度 QueueMetrics</b> 的 allocated [containerNum | MB | cores]信息  \n\t5.若在节点上分配 reservedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量  \n&ensp;&ensp;&ensp;&ensp;NodeManager 方面:设置 NM 保留的 container 为本次的 reservedContainer  \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  \n\t\n\n### 资源释放\n\n以客户端在命令行执行 \"yarn application --kill ${APPLICATION_ID}\"为例  \nRM 中响应请求 方法调用栈为:  \n->ClientRMService#forceKillApplication   \n&ensp;&ensp;->RMAppImpl$KillAttemptTransition#transition  \n&ensp;&ensp;&ensp;&ensp;->RMAppAttemptImpl$BaseFinalTransition#transition  \n&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;->CapacityScheduler#handle()   \n\napplication 结束时,主要做了三件事情,  \n1. Container: 释放 APP 持有的<b> living,reserved </b> container  \n2. Application(QueueMetrcis): 清理 LeafQueue 及其 ParentQueue 在 <b>APP</b> 层面的统计(e.g. appRunnings);清理 LeafQueue 在<b>用户APP</b> 层面的统计  \n3. AM(ResourceUsage): 释放 LeafQueue 及其 ParentQueue 在 <b>AM</b> 层面的资源用量;  释放 LeafQueue 在<b>用户AM</b> 层面的统计(AMUSED)  \n\n由于复现指标异常成功后,发现移动到目标队列后,目标队列增加的资源总量是 spark.executor.memory 的整数倍(executor-mem 11G,driver-memory 2G),所以对上述的第二点和第三点不多分析,主要关注 <b>非 AM 的container</b> 的释放,livingContainers 和 reservedContainer 的释放代码是同一个,仅仅是释放 container 的文字性说明(原文: diagnostics 意为诊断)不同      \n[CapactiyScheduler.java]\n\n```\npublic void handle(SchedulerEvent event) {\n....\n case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),\n        appAttemptRemovedEvent.getFinalAttemptState(),\n        appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());\n    }\n    break;\n    ....\n    }\n    \nprivate synchronized void doneApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {\n    \n    FiCaSchedulerApp attempt = getApplicationAttempt(applicationAttemptId);\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n        ....\n    //从 app 维护的 livingContainer 列表中,释放掉 allocated acquired running 状态的 container\n    for (RMContainer rmContainer : attempt.getLiveContainers()) {\n    //在 kill 时keepContainer 为 false;在 failed 时,keepContainer 按场景可为 true\n      if (keepContainers\n          && rmContainer.getState().equals(RMContainerState.RUNNING)) {\n        continue;\n      }\n      //1.释放 container\n      completedContainer(\n        rmContainer,\n        SchedulerUtils.createAbnormalContainerStatus(\n        //COMPLETED_APPLICATION:Container of a completed application\n          rmContainer.getContainerId(), SchedulerUtils.COMPLETED_APPLICATION),\n        RMContainerEventType.KILL);\n    }\n\t\n    //从 app 维护的 reservedContainer 列表中,释放掉 reserved 状态的 container\n    for (RMContainer rmContainer : attempt.getReservedContainers()) {\n    //1.释放 container\n      completedContainer(\n        rmContainer,\n        SchedulerUtils.createAbnormalContainerStatus(\n          rmContainer.getContainerId(), \"Application Complete\"),\n        RMContainerEventType.KILL);\n    }\n    //2. 清理 pending 的 resourceRequest,并同步 [LeafQueue | ParentQueue] [本身 | 用户]维度的 QueueMetrics appRunnings | appPendings \n    attempt.stop(rmAppAttemptFinalState);\n    String queueName = attempt.getQueue().getQueueName();\n    CSQueue queue = queues.get(queueName);\n    if (!(queue instanceof LeafQueue)) {\n      LOG.error(\"Cannot finish application \" + \"from non-leaf queue: \"\n          + queueName);\n    } else {\n    //3. 同步 [LeafQueue | ParentQueue] [本身 | 用户]维度 QueueUsage 的 AMUSED 指标\n      queue.finishApplicationAttempt(attempt, queue.getQueueName());\n    }\n  }\n```\nCapacityScheduler#completedContainer主要做了一些校验,避免无效释放  \n[CapacityScheduler.java]\n\n```\n   protected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    Container container = rmContainer.getContainer();\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n     //container 所在的 NM视图\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n     //代理 container 的 LeafQueue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null, true);\n  }\n```\n先按照 RMContainer 状态机状态判断后续将当前 container 按照 reservedContainer处理或者 allocatedContainer 处理  \n然后若释放 container 成功,则同步 用户和标签维度 QueueUsage的 USED 指标,减少 Queue 运行的 container 数量    \n最后若当前 Queue 非 rootQueue,则递归同步当前 Queue 的所有 ParentQueue做上述处理  \n[LeafQueue.java]\n\n```\npublic void completedContainer(Resource clusterResource, \n      FiCaSchedulerApp application, FiCaSchedulerNode node, RMContainer rmContainer, \n      ContainerStatus containerStatus, RMContainerEventType event, CSQueue childQueue,\n      boolean sortQueues) {\n    if (application != null) {\n      boolean removed = false;\n      synchronized (this) {\n        Container container = rmContainer.getContainer();\n\t// 1. 若 contaienr 状态机为 reserved 状态,在节点上解除 reserve\n        if (rmContainer.getState() == RMContainerState.RESERVED) {\n          removed = unreserve(application, rmContainer.getReservedPriority(),\n              node, rmContainer);\n        } else {\n        // 2.若 container 状态机不为 reserved 状态,在节点上解除 allocate,并使 NM 释放 container\n          removed =\n            application.containerCompleted(rmContainer, containerStatus, event);\n          node.releaseContainer(container);\n        }\n        //3.释放 同步 QueueUsage\n        if (removed) {\n          releaseResource(clusterResource, application,\n              container.getResource(), node.getLabels());\n          LOG.info(\"completedContainer\" +\n              \" container=\" + container +\n              \" queue=\" + this +\n              \" cluster=\" + clusterResource);\n        }\n      }\n      //4.递归执行 Parent的释放逻辑\n      if (removed) {\n        getParent().completedContainer(clusterResource, application, node,\n          rmContainer, null, event, this, sortQueues);\n      }\n    }\n  }\n```\n先看第 4 处对 ParentQueue 的处理  \n[ParentQueue.java]\n\n```\npublic void completedContainer(Resource clusterResource,\n      FiCaSchedulerApp application, FiCaSchedulerNode node, \n      RMContainer rmContainer, ContainerStatus containerStatus, \n      RMContainerEventType event, CSQueue completedChildQueue,\n      boolean sortQueues) {\n    if (application != null) {\n      synchronized (this) {\n      //使用 AbstractCSQueue 中releaseResource方法\n        super.releaseResource(clusterResource, rmContainer.getContainer()\n            .getResource(), node.getLabels());\n\t...\n        //resort sub-queue\n     \t....\n      }\n      // Inform the parent\n      if (parent != null) {\n \t//递归 ParentQueue#completedContainer 方法\n        parent.completedContainer(clusterResource, application, \n            node, rmContainer, null, event, this, sortQueues);\n      }    \n    }\n  }\n```\nParentQueue 在 container 释放时,无论是 allocatedContainer 还是 reservedContainer,都更新: \n1.按标签更新 QueueUsage 中 USED 用途的资源量  \n2.ParentQueue 中运行 container 的数量 \n[AbstractCSQueue.java]\n\n```\nprotected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n      //1.QueueUsageByLabel\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n        //2.减少当前 Queue 的 container 的数量\n    --numContainers;\n  }\n```\n再看第 3 处对 LeafQueue 的处理, 需要注意的是:无论 container 是 allocatedContainer 还是 reversedContainer 在此处都是作为ResourceUsage中 <b>USED</b> 类型的资源来减少的(虽然 ResourceType 枚举中有 RESERVED 类型),且 numContainer 都减少了 1个单位    \n[LeafQueue.java]\n\n```\nsynchronized void releaseResource(Resource clusterResource, \n      FiCaSchedulerApp application, Resource resource, Set<String> nodeLabels) {\n    //1.同步当前队列在 标签维度 ResourceUsage 中 USED 类型的资源,并减少 Queue 中 numContainer\n    super.releaseResource(clusterResource, resource, nodeLabels);\n    //2.同步 app 所属用户在标签维度上的度量信息  \n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.releaseContainer(resource, nodeLabels);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n  }\n```\n[AbstractCSQueue.java]  \n\n```\nprotected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n     //1.同步 label 层面的 QueueUsage 的资源量;无论 reservedContainer 还是 allocatedContainer 都是使用 #decUsed(resource)\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n      //2.减少当前队列启动的 container 数量.无论 reservedContainer 还是 allocatedContainer  \n    --numContainers;\n  }\n```\nLeafQueue 中维护了一个 users:Map[String,User]以保存当前各 user 在 USED,PENDING,AMUSED,RESERVED 用途使用资源的度量  \n[LeafQueue.java]\n\n```\npublic void releaseContainer(Resource resource, Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n      //还是 decUsed(resource),无论 reservedContainer 还是 allocatedContainer\n        userResourceUsage.decUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.decUsed(label, resource);\n        }\n      }\n    }\n```\n在回到LeafQueue#completedContainer方法,按照 RMContainer状态机状态来区分 reservedContainer 还是 allocatedContainer 做处理  \n先看对 allocated 的处理逻辑    \n[FicaSchedulerApp.java]\n\n```\nsynchronized public boolean containerCompleted(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    //维护相应的引用列表\n    if (null == liveContainers.remove(rmContainer.getContainerId())) {\n      return false;\n    }\n    newlyAllocatedContainers.remove(rmContainer);\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n\n    //1.RM 端准备 container 的 结束和清理等任务,待 NM 心跳通过 NodeHeartbeatResponse 交于 NM 做清理\n    rmContainer.handle(\n        new RMContainerFinishedEvent(\n            containerId,\n            containerStatus, \n            event)\n        );\n    ... \n    //2.同步 QueueMetrics\n    Resource containerResource = rmContainer.getContainer().getResource();\n    queue.getMetrics().releaseResources(getUser(), 1, containerResource);\n    //3.减少当前 app 消费的资源\n    Resources.subtractFrom(currentConsumption, containerResource);\n    ....\n    return true;\n  }\n```\n主要看第2步 释放 contaienr 同步 QueueMetrics 的指标和内容,  \n1.同步 QueueMetrics 及其 ParentQueue QueueMetrics 的指标: allocate [containerNum | MB | core]  \n2.同步 QueueMetrics 及其 ParentQueue QueueMetrics 用户维度的 QueueMetrics 指标: allocate [containerNum | MB | core]  \n[QueueMetrics.java] \n\n```\npublic void releaseResources(String user, int containers, Resource res) {\n    //当前资源队列指标\n    allocatedContainers.decr(containers);\n    aggregateContainersReleased.incr(containers);\n    allocatedMB.decr(res.getMemory() * containers);\n    allocatedVCores.decr(res.getVirtualCores() * containers);\n    QueueMetrics userMetrics = getUserMetrics(user);\n    //资源队列中用户指标\n    if (userMetrics != null) {\n      userMetrics.releaseResources(user, containers, res);\n    }\n    //递归 父资源队列\n    if (parent != null) {\n      parent.releaseResources(user, containers, res);\n    }\n  }\n```\n再看释放allocatedContainer 过程中,NM 对该 container 的处理  \n1.删掉 launchedContainer 对该 container 的引用  \n2.增加该节点的可用资源,减少已经资源,减少启动的 numContainer 指标    \n[SchedulerNode.java]\n\n```\npublic synchronized void releaseContainer(Container container) {\n    ..\n    //1.删除引用\n    if (null != launchedContainers.remove(container.getId())) {\n     //2.指标维护\n      updateResource(container);\n    }\n    LOG.info(\"Released container \" + container.getId() + \" of capacity \"\n        + container.getResource() + \" on host \" + rmNode.getNodeAddress()\n        + \", which currently has \" + numContainers + \" containers, \"\n        + getUsedResource() + \" used and \" + getAvailableResource()\n        + \" available\" + \", release resources=\" + true);\n  }\n  private synchronized void updateResource(Container container) {\n    addAvailableResource(container.getResource());\n    --numContainers;\n  }\n  private synchronized void addAvailableResource(Resource resource) {\n     ...\n    Resources.addTo(availableResource, resource);\n    Resources.subtractFrom(usedResource, resource);\n  }\n```\n释放 reservedContainer,对 reversedContainer 的处理  \n1.application 方面:删除对 reservedContainer 的引用,减少 currentReservation 资源数  \n2.nodemanager方面: 设置当前 nm 的reservedContainer 为 null\n3.QueueMetrics 方面: 同步[ParentQueue | LeafQueue] [本身 | 用户]维度的指标 reserved [containerNum | MB | core]  \n[LeafQueue.java]\n\n```\nprivate boolean unreserve(FiCaSchedulerApp application, Priority priority,\n      FiCaSchedulerNode node, RMContainer rmContainer) {\n      //1.application: unreserve\n    if (application.unreserve(node, priority)) {\n     //2. nodemanager: unreserveResource\n      node.unreserveResource(application);\n     //3.QueueMetrics\n      getMetrics().unreserveResource(application.getUser(),\n          rmContainer.getContainer().getResource());\n      return true;\n    }\n    return false;\n  }\n```\napplication 和 nodemanager 层面的逻辑比较简单,且看 QueueMetrics 方面的同步  \n[QueueMetrics.java]\n\n```\npublic void unreserveResource(String user, Resource res) {\n  //当前资源队列 QueueMetrics的 reserved [container | MB | core] 指标\n    reservedContainers.decr();\n    reservedMB.decr(res.getMemory());\n    reservedVCores.decr(res.getVirtualCores());\n    //当前资源队列 在用户维度的 reserved [container | MB | core] 指标\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.unreserveResource(user, res);\n    }\n    //父资源队列在 其本身和用户维度的 reserved [container | MB | core] 指标\n    if (parent != null) {\n      parent.unreserveResource(user, res);\n    }\n  }\n```\n\n>\n综上:   \nCS 调度器在为 APP 释放container 时,会同步 [Parent | Leaf]Queue/APP/NM 三个位置维护的信息  \n1. 释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>ParentQueue</b> 都会维护:\n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的<b>标签维度</b>的QueueUsage 中 USED 用途的资源量  \n2. 释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>LeafQueue</b> 都会维护:   \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的 numContainer  \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  \n3.释放 container 时,按照该 container 的状态机状态做细分,分为 reservedContainer 和 allocatedContainer 两类做处理  \n4.若在节点上释放 allocatedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:newlyAllocatedContainers 和 reservedContainers 列表,app已占用资源量    \n&ensp;&ensp;&ensp;&ensp;NodeManager方面:numContainer数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表    \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 和所有 ParentQueue <b>自身 QueueMetrics</b> 的 reserved [containerNum | MB | core]信息;LeafQueue 和所有 ParentQueue 在<b>用户维度 QueueMetrics </b> 的  reserved [containerNum | MB | core]信息  \n5.若在节点上释放 reservedContainer,则维护:  \n&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量  \n&ensp;&ensp;&ensp;&ensp; NodeManager 方面:设置 NM 保留的 container 为 null    \n&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  \n\n\nContainer 释放过程和申请过程 对于 Queue/Application/NM三个位置维护的 Queue/QueueUsageByLabel/QueueUsageByUser/QueueMetrics信息,恰好是一一对应的  \n \n对于 YarnUI 指标异常问题,比较关注的 Queue.numContainer 和 QueueUsageByLabel.resArr[ResourceType.USED]两个指标,一次 allocated/reversed/Container 的申请对应着 numContainer++和 ResourceUsage.incUsed;一次 container 的释放对应着 numContainer--和 ResourceUsage.decUsed   \n\n### movetoqueue 时资源转移过程\n\n\n使用命令 \"yarn application -movetoqueue ${APPID} -queue ${TO}\"\n可以将 ${APPID}的 APP 移动到目标资源队列 ${TO}  \nClientRMService 相应请求的调用栈是:  \n->ClientRMService#moveApplicationAcrossQueues  \n&ensp;&ensp;->RMAppImpl$RMAppMoveTransition#transition  \n&ensp;&ensp;&ensp;&ensp;->CapacityScheduler#moveApplication  \n\napplication 从源资源队列移动到目标资源队列分为几步:  \n1. 向目标队列提交application 并做校验,包括 access 权限、目标队列的 app 数量超限、目标队列中用户提交 app 数量超限,若提交成功则增加所有 ParentQueue 的 numApplication 数量(但是没有增加自身的 numApplication 数量)    \n2. 转移源队列中所有非 reserved 的 container 到目标队列,源队列(LeafQueue)释放 container,减少<b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并减少源队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量;增加目标队列(LeafQueue) <b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并增加目标队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量.指标数量维护在Queue/ResourceUsage/QueueMetrics 中,此处没有处理 livingContainers 的 QueueMetrics ,在第 5 步中统一处理 QueueMetrics    \n3. 源队列中移除 appAttemptd,并同步 QueueUsageByUser 和 QueueUsageByLabel 中 AMUSED类型的资源量  \n4. 在源队列的所有 ParentQueue 中移除此 Application    \n5. 同步源队列和目标队列中所有非 AM container 移动导致的 QueueMetrics 变化  \n6. 同步用户维度的 appAttempt 统计信息   \n[CapacityScheduler.java]  \n\n```\n public synchronized String moveApplication(ApplicationId appId,\n      String targetQueueName) throws YarnException {\n    FiCaSchedulerApp app =\n        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));\n    String sourceQueueName = app.getQueue().getQueueName();\n    LeafQueue source = getAndCheckLeafQueue(sourceQueueName);\n    String destQueueName = handleMoveToPlanQueue(targetQueueName);\n    LeafQueue dest = getAndCheckLeafQueue(destQueueName);\n    String user = app.getUser();\n    try {\n      //1. 转移 Application\n      dest.submitApplication(appId, user, destQueueName);\n    } catch (AccessControlException e) {\n      throw new YarnException(e);\n    }\n    //2.转移 livingContainer\n    for (RMContainer rmContainer : app.getLiveContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      // attach the Container to another queue\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //3.源资源队列移除attempt,并同步QueueUsage\n    source.finishApplicationAttempt(app, sourceQueueName);\n    //4.源队列的父队列移除 application\n    source.getParent().finishApplication(appId, app.getUser());\n   //5.同步源队列和目标队列的 QueueMetrics\n    app.move(dest);\n    //6.提交attemp\n    dest.submitApplicationAttempt(app, user);\n    applications.get(appId).setQueue(dest);\n    LOG.info(\"App: \" + app.getApplicationId() + \" successfully moved from \"\n        + sourceQueueName + \" to: \" + destQueueName);\n    return targetQueueName;\n  }\n```\n第 2 步中,遍历 app 的 livingContainer,把每个 container 从源队列及父队列释放,减少源队列及父队列的指标  \n先看源队列 LeafQueue 的释放逻辑  \n[LeafQueue.java]\n\n```\npublic void detachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n      //AbstractCSQueue#releaseResource\n      releaseResource(clusterResource, application, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" container=\" + rmContainer.getContainer()\n          + \" resource=\" + rmContainer.getContainer().getResource()\n          + \" queueMoveOut=\" + this + \" usedCapacity=\" + getUsedCapacity()\n          + \" absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() + \" used=\"\n          + queueUsage.getUsed() + \" cluster=\" + clusterResource);\n      //ParentQueue#detachContainer\n      getParent().detachContainer(clusterResource, application, rmContainer);\n    }\n  }\n  \n  synchronized void releaseResource(Resource clusterResource, \n      FiCaSchedulerApp application, Resource resource, Set<String> nodeLabels) {\n      //AbstractCSQueue#releaseResource\n     //1.减少 标签维度的 ResourceUsage USED 用途的资源;并减少当前 Queue 的 numContainer 数量\n    super.releaseResource(clusterResource, resource, nodeLabels);\n    //2.减少 提交用户在 标签维度的 ResourceUsage USED 用途的资源\n    String userName = application.getUser();\n    User user = getUser(userName);\n    user.releaseContainer(resource, nodeLabels);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    ...\n  }\n  \n  //AbstractCSQueue#releaseResource\n  //ResourceUsageByLabel USED 用途的资源量\n  protected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n        //Queue 的运行 container 数量\n    --numContainers;\n  }\n  //第 2 处,ResourceUsageByUser USED 用途的资源量\npublic void releaseContainer(Resource resource, Set<String> nodeLabels) {\n      if (nodeLabels == null || nodeLabels.isEmpty()) {\n        userResourceUsage.decUsed(resource);\n      } else {\n        for (String label : nodeLabels) {\n          userResourceUsage.decUsed(label, resource);\n        }\n      }\n```\nParentQueue 释放 container 时,减少了用户维度的 ResourceUsage 在 USED 用途上资源量  \n[ParentQueue.java]\n\n```\npublic void detachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n      //AbstractCSQueue#releaseResource\n      super.releaseResource(clusterResource,\n          rmContainer.getContainer().getResource(),\n          node.getLabels());\n      LOG.info(\"movedContainer\" + \" queueMoveOut=\" + getQueueName()\n          + \" usedCapacity=\" + getUsedCapacity() + \" absoluteUsedCapacity=\"\n          + getAbsoluteUsedCapacity() + \" used=\" + queueUsage.getUsed() + \" cluster=\"\n          + clusterResource);\n      // Inform the parent\n      if (parent != null) {\n        //递归父队列\n        parent.detachContainer(clusterResource, application, rmContainer);\n      }\n    }\n  }\n  //AbstractCSQueue#releaseResource\n  protected synchronized void releaseResource(Resource clusterResource,\n      Resource resource, Set<String> nodeLabels) {\n    // Update usedResources by labels\n    if (null == nodeLabels || nodeLabels.isEmpty()) {\n      queueUsage.decUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.decUsed(label, resource);\n      }\n    }\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n    --numContainers;\n  }\n```\n将 container 从源队列转移到目标队列时,会增加目标队列及其父队列的指标   \n先看目标队列(LeafQueue)增加的指标   \n[LeafQueue.java]\n\n```\npublic void attachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n          //1.增加 LeafQueue 标签维度的 ResourceUsage 和用户维度的 ResourceUsage, USED 类型的资源量;并增加numContainer\n      allocateResource(clusterResource, application, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" container=\" + rmContainer.getContainer()\n              //add by jiulong.zhu@20190903\n              +\" containerState=\"+rmContainer.getState()\n          + \" resource=\" + rmContainer.getContainer().getResource()\n          + \" queueMoveIn=\" + this + \" usedCapacity=\" + getUsedCapacity()\n          + \" absoluteUsedCapacity=\" + getAbsoluteUsedCapacity() + \" used=\"\n          + queueUsage.getUsed() + \" cluster=\" + clusterResource);\n      //2.增加 ParentQueue 标签维度的 ResourceUsage USED 类型的资源量;并增加 ParentQueue 的 numContainer 数量\n      getParent().attachContainer(clusterResource, application, rmContainer);\n    }\n  }\n  \n  synchronized void allocateResource(Resource clusterResource,\n      SchedulerApplicationAttempt application, Resource resource,\n      Set<String> nodeLabels) {\n      //AbstractCSQueue#allocateResource 增加标签维度的 ResourceUsage USED 用途的用量;增加 Queue 的numContainers  \n    super.allocateResource(clusterResource, resource, nodeLabels);\n    String userName = application.getUser();\n    //增加当前用户 标签维度的 ResourceUsage USED 用途的用量  \n    User user = getUser(userName);\n    user.assignContainer(resource, nodeLabels);\n    \n    Resources.subtractFrom(application.getHeadroom(), resource);\n    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());\n    if (LOG.isDebugEnabled()) {\n      LOG.info(getQueueName() + \n          \" user=\" + userName + \n          \" used=\" + queueUsage.getUsed() + \" numContainers=\" + numContainers +\n          \" headroom = \" + application.getHeadroom() +\n          \" user-resources=\" + user.getUsed()\n          );\n    }\n  }\n    \n    //AbstractCSQueue#allocateResource\n   synchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n再看将 container 移动到目标队列时,目标队列的 ParentQueue 增加的逻辑  \n[ParentQueue.java]\n\n```\npublic void attachContainer(Resource clusterResource,\n      FiCaSchedulerApp application, RMContainer rmContainer) {\n    if (application != null) {\n      FiCaSchedulerNode node =\n          scheduler.getNode(rmContainer.getContainer().getNodeId());\n          //1.增加 ParentQueue 在标签维度的 ResourceUsage USED 用量 \n          //AbstractCSQueue#allocateResource\n      super.allocateResource(clusterResource, rmContainer.getContainer()\n          .getResource(), node.getLabels());\n      LOG.info(\"movedContainer\" + \" queueMoveIn=\" + getQueueName()\n          + \" usedCapacity=\" + getUsedCapacity() + \" absoluteUsedCapacity=\"\n          + getAbsoluteUsedCapacity() + \" used=\" + queueUsage.getUsed() + \" cluster=\"\n          + clusterResource);\n      //2.递归增加 ParentQueue 的所有 ParentQueue 的 ResourceUsage USED 用量  \n      if (parent != null) {\n        parent.attachContainer(clusterResource, application, rmContainer);\n      }\n    }\n  }\n  \n  //AbstractCSQueue#allocateResource\n  synchronized void allocateResource(Resource clusterResource, \n      Resource resource, Set<String> nodeLabels) {\n    if (nodeLabels == null || nodeLabels.isEmpty()) {\n      queueUsage.incUsed(resource);\n    } else {\n      Set<String> anls = (accessibleLabels.contains(RMNodeLabelsManager.ANY))\n          ? labelManager.getClusterNodeLabels() : accessibleLabels;\n      for (String label : Sets.intersection(anls, nodeLabels)) {\n        queueUsage.incUsed(label, resource);\n      }\n    }\n    ++numContainers;\n    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),\n        clusterResource, minimumAllocation);\n  }\n```\n第 3 步:删除源队列中该 app 的 currentApplicationAttempt,并同步 LeafQueue 中用户提交的 app 数量变化     \n[LeafQueue.java]\n\n```\npublic void finishApplicationAttempt(FiCaSchedulerApp application, String queue) {\n    synchronized (this) {\n      removeApplicationAttempt(application, getUser(application.getUser()));\n    }\n    //空实现\n    getParent().finishApplicationAttempt(application, queue);\n  }\n  \n  public synchronized void removeApplicationAttempt(\n      FiCaSchedulerApp application, User user) {\n    boolean wasActive = activeApplications.remove(application);\n    if (!wasActive) {\n      pendingApplications.remove(application);\n    } else {\n      //正在运行的 app_attempt,则减少 LeafQueue 的 [ResourceUsage | ResourceUsageByUser] 中AMUSED 占用的资源量\n      queueUsage.decAMUsed(application.getAMResource());\n      user.getResourceUsage().decAMUsed(application.getAMResource());\n    }\n    applicationAttemptMap.remove(application.getApplicationAttemptId());\n     // 在 ResourceUsageByUser 同步 numApplication 指标\n    user.finishApplication(wasActive);\n    if (user.getTotalApplications() == 0) {\n      users.remove(application.getUser());\n    }\n    // Check if we can activate more applications\n    activateApplications();\n    LOG.info(\"Application removed -\" +\n        \" appId: \" + application.getApplicationId() + \n        \" user: \" + application.getUser() + \n        \" queue: \" + getQueueName() +\n        \" #user-pending-applications: \" + user.getPendingApplications() +\n        \" #user-active-applications: \" + user.getActiveApplications() +\n        \" #queue-pending-applications: \" + getNumPendingApplications() +\n        \" #queue-active-applications: \" + getNumActiveApplications()\n        );\n  }\n```\n第 4 步:对于源队列的所有 ParentQueue,移除 app 的 currentApplicationAttempt,并减少资源队列的运行 numApplication 值    \n[ParentQueue.java]\n\n```\npublic void finishApplication(ApplicationId application, String user) {\n    synchronized (this) {\n      removeApplication(application, user);\n    }\n    //递归 父队列\n    if (parent != null) {\n      parent.finishApplication(application, user);\n    }\n  }\n  \n  private synchronized void removeApplication(ApplicationId applicationId, \n      String user) {\n    --numApplications;\n    LOG.info(\"Application removed -\" +\n        \" appId: \" + applicationId + \n        \" user: \" + user + \n        \" leaf-queue of parent: \" + getQueueName() + \n        \" #applications: \" + getNumApplications());\n  }\n```\n第 5 步: 将 app 持有的所有 allocatedContainer(包括 AM container)和 reservedContainer,移动到目标队列,并修改 QueueMetrics  \n[SchedulerApplicationAttempt.java]\n\n```\npublic synchronized void move(Queue newQueue) {\n    QueueMetrics oldMetrics = queue.getMetrics();\n    QueueMetrics newMetrics = newQueue.getMetrics();\n    String user = getUser();\n    //修改 livingContainer 的 QueueMetrics\n    for (RMContainer liveContainer : liveContainers.values()) {\n      Resource resource = liveContainer.getContainer().getResource();\n      oldMetrics.releaseResources(user, 1, resource);\n      newMetrics.allocateResources(user, 1, resource, false);\n    }\n    //修改 reservedContainer的 QueueMetrics\n    for (Map<NodeId, RMContainer> map : reservedContainers.values()) {\n      for (RMContainer reservedContainer : map.values()) {\n        Resource resource = reservedContainer.getReservedResource();\n        oldMetrics.unreserveResource(user, resource);\n        newMetrics.reserveResource(user, resource);\n      }\n    }\n    //移动 pending ResourceRequest,同步信息:QueueMetrics.appsRunning,Queue下的用户列表    \n    appSchedulingInfo.move(newQueue);\n    this.queue = newQueue;\n  }\n```\n修改 livingContainers 和 reservedContainers 的所有逻辑都在 QueueMetrics中,其中  \n1.releaseResources,释放 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标  \n2.allocateResource,申请 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标  \n3.unreserveResource,释放 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Containers| MB | Core]指标  \n4.reserveResource,申请 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  \n[QueueMetrics.java]\n\n```\npublic void releaseResources(String user, int containers, Resource res) {\n    allocatedContainers.decr(containers);\n    aggregateContainersReleased.incr(containers);\n    allocatedMB.decr(res.getMemory() * containers);\n    allocatedVCores.decr(res.getVirtualCores() * containers);\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.releaseResources(user, containers, res);\n    }\n    if (parent != null) {\n      parent.releaseResources(user, containers, res);\n    }\n  }\n  \n public void allocateResources(String user, int containers, Resource res,\n      boolean decrPending) {\n    allocatedContainers.incr(containers);\n    aggregateContainersAllocated.incr(containers);\n    allocatedMB.incr(res.getMemory() * containers);\n    allocatedVCores.incr(res.getVirtualCores() * containers);\n    if (decrPending) {\n      _decrPendingResources(containers, res);\n    }\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.allocateResources(user, containers, res, decrPending);\n    }\n    if (parent != null) {\n      parent.allocateResources(user, containers, res, decrPending);\n    }\n  }\n  \n  public void unreserveResource(String user, Resource res) {\n    reservedContainers.decr();\n    reservedMB.decr(res.getMemory());\n    reservedVCores.decr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.unreserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.unreserveResource(user, res);\n    }\n  }\n  \n  public void reserveResource(String user, Resource res) {\n    reservedContainers.incr();\n    reservedMB.incr(res.getMemory());\n    reservedVCores.incr(res.getVirtualCores());\n    QueueMetrics userMetrics = getUserMetrics(user);\n    if (userMetrics != null) {\n      userMetrics.reserveResource(user, res);\n    }\n    if (parent != null) {\n      parent.reserveResource(user, res);\n    }\n  }\n  \n```\n第 6 步:同步目标队列 application 和 applicationAttempt信息    \n[LeafQueue.java]\n\n```\npublic void submitApplicationAttempt(FiCaSchedulerApp application,\n      String userName) {\n    synchronized (this) {\n      User user = getUser(userName);\n      //增加 Queue 和 User 维度 application / applicationAttempt 的值\n      addApplicationAttempt(application, user);\n    }\n    // We don't want to update metrics for move app\n    if (application.isPending()) {\n     //增加 Queue 和 User 维度 QueueMetrics 中 appsPending 的值\n      metrics.submitAppAttempt(userName);\n    }\n    //空实现\n    getParent().submitApplicationAttempt(application, userName);\n  }\n```\n\n>综上:  \n把 application 从源资源队列到目标队列的过程中,在 Queue/ResourceUsageByLabel/ResourceUsageByUser/QueueMetrics 中的变动    \n1.对于 livingContainers,  \n&ensp;&ensp;&ensp;&ensp;Queue 方面:减少源队列及其父队列的 numContainer 值;增加目标队列及其父队列的 numContainer  \n&ensp;&ensp;&ensp;&ensp;ResourceUsage 方面:减少源队列及其父队列的 ResourceUsageByLabel USED 类型的资源量,增加目标队列及其父队列的 ResourceUsageByLabel USED 类型的资源量; 减少源队列(LeafQueue)的 ResourceUsageByUser USED 类型的资源量,增加目标队列(LeafQueue) 的 ResourceUsageByUser USED 类型的资源量;  \n&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:减少源队列及其父队列的 allocated[Container | MB | Core]指标,增加目标队列及其父队列的 allocated [Container | MB | Core]  \n2.对于 reservedContainers,  \n&ensp;&ensp;&ensp;&ensp;ResourceUsage:减少源队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的reserved [Container | MB | Core]指标,增加目标队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  \n\n\n### 结论  \n\n结合 \"正常的资源分配和释放过程\"和\"movetoqueue 过程中的资源转移过程 \",可以还原一个被转移队列的 application 的资源变动过程  \napplication 中非 reservedContianer(状态机:allocated,acquired,running) 的资源变动过程如下表  \n\n| allocatedContainer  | 分配 | movetoqueue | 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n|ResourceUsageByLabel(USED)|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|\n\napplication 中 reservedContianer(状态机:reversed) 的资源变动过程如下表   \n\n| reversedContainer  | 分配 | movetoqueue| 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|ResourceUsageByLabel(USED)|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|  \n\n>\n从上述两表中可知:  \n1.在没有 movetoqueue 操作的情况下, 源队列和目标队列一致,无论 allocatedContainer 或 reservedContainer 的分配和释放都是守恒的,一次指标的增加对应着一次指标的减少  \n2.在源队列资源比较充裕,且有 movetoqueue 操作的情况下,application 没有 reservedContainer 的情况下(上述表一),在分配时源队列指标增加,movetoqueue 时,源队列减少指标且目标队列增加指标,在释放时减少目标队列(app 当前归属队列)的指标.指标的增减也是平衡的,这也就是在资源充裕的队列上无法复现该问题的原因  \n3.在源队列资源比较紧张,且有 movetoqueue 操作的情况下,application 有 reservedContainer 和 allocatedContainer(上述表一和表二),对于 allocatedContainer 来说,资源在分配->movetoque->释放的过程中是平衡的(表一);但是对于 reversedContainer 来说,在源队列中分配到资源时,增加了源队列的 numContainer 和 ResourceUsageByLabel 中 USED 用途的资源量,movetoqueue 时没有相应的操作,仅仅是修改了 QueueMetrcis 内的指标,在释放时 container 资源被加入到目标队列的指标中(上述表二).<b>源队列分配出去的资源没有回收,目标队列得到了不是自身分配出去的资源</b>.这样导致了即使源队列和目标队列中完全无任务时,源队列的 numContainer 和 ResourceUsageByLabel 值为正值,目标队列的 numContainer 和 ResourceUsage 值为负值,Yarn UI ->scheduler->queue 内的 Num Containers/Used Capacity/Absolute Used Capacity/Used Resources 正是基于 队列的 numContainer 和 ResourceUsageByLabel(USED) 展示和计算的,也就是指标异常的问题     \n\n指标异常(无任务时 为负)\n![](/img/pictures/negative/negative_e7a766275896.png)\n\n指标异常(无任务时 为正)\n![](/img/pictures/negative/negative_aead-04d8e67b1357.png)\n\n### 危害\n\n1.对于源队列来说,类似于资源泄露,自身资源未能回收,释放到了目标队列.这样对于后续提交到源队列的任务来说,不能分配到资源运行(USED 泄露趋于 100%),但实际上源队列没有任务在运行    \n2.对于目标队列来说,多出了一些资源,导致自身一些指标为负,可能会导致一些未知的问题.以下纯属个人猜测: 例如 同步问题,Queue 有很多的剩余资源,但是 NodeManager 上没有 available 资源;指标负值可能对内部的运算产生未预期的问题  \n\n### 解决方法\n\n针对 reservedContainer 在 movetoqueue 操作过程中,资源释放申请不守恒的漏洞,如下表  \n\n| reversedContainer  | 分配 | movetoqueue| 释放 | \n|:------------- |:---------------:| :---------------:|:---------------:|\n|numContainer|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|ResourceUsageByLabel(USED)|源队列值增加|<font color=\"#660000\">源队列不变,目标队列不变</font>|目标队列值减少|  \n|QueueMetrics|源队列值增加|源队列减少,目标队列增加|目标队列值减少|  \n\n>\n问题:    \n&ensp;&ensp;&ensp;&ensp;YarnUI 中指标 Num Container、Used Capacity、Absolute Used Capacity、Used Resource 异常和 Queue成员变量numContainer及 QueueUsageByLabel 中 USED 用途的资源量有关    \n目标:  \n&ensp;&ensp;&ensp;&ensp;解决 numContainer 和 ResourceUsageByLabel(USED)在 movetoqueue 时,没有\"减少源队列的指标\"且没有\"增加目标队列指标\"的问题  \n途径:  \n&ensp;&ensp;&ensp;&ensp;1.需要在 movetoqueue 时,每个 reservedContainer 都应触发 \"源队列减少指标\"和\"目标队列增加指标\"的行为 \n&ensp;&ensp;&ensp;&ensp;源码中有封装好的代码且上文多次提到,即 LeafQueue#releaseResource方法和 LeafQueue.allocateResource方法,但是其中有额外的 对 headroom 的同步逻辑.一方面,在\"正常的分配和释放\"过程中,无论申请和分配的 container 是 allocated 或是 reversed 都会执行 LeafQueue#releaseResource 和 LeafQueue.allocateResource 修改 numContainer 和 ResourceUsageByLabel 的同时修改 headroom值;另一方面 move container中对 headroom 增加和减少 同一个Resource 是幂等的.所以 movetoqueue 操作对于每一个 container 的移动都可以选择触发这两个方法来增加对 numContainer 和 ResourceUsageByLabel 的操作.LeafQueue#detachContainer()和 LeafQueueattachContainer()  中有递归本队列及当前队列此逻辑的过程.直接用即可       \n修改方案:  \n&ensp;&ensp;&ensp;&ensp;CapacitySchedule#moveApplication(ApplicationId appId,String targetQueueName) 方法修改如下  \n\n[CapacityScheduler.java]\n\n```\n@Override\n  public synchronized String moveApplication(ApplicationId appId,\n      String targetQueueName) throws YarnException {\n    FiCaSchedulerApp app =\n        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));\n    String sourceQueueName = app.getQueue().getQueueName();\n    LeafQueue source = getAndCheckLeafQueue(sourceQueueName);\n    String destQueueName = handleMoveToPlanQueue(targetQueueName);\n    LeafQueue dest = getAndCheckLeafQueue(destQueueName);\n    // Validation check - ACLs, submission limits for user & queue\n    String user = app.getUser();\n    try {\n      dest.submitApplication(appId, user, destQueueName);\n    } catch (AccessControlException e) {\n      throw new YarnException(e);\n    }\n    // Move all live containers\n    for (RMContainer rmContainer : app.getLiveContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      // attach the Container to another queue\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //description:解决 reservedContainer 在 movetoqueue 操作过程中,Queue.numContainer 指标和 ResourceUsageByLabel.USED 资源量\n    //释放申请不守恒,导致 Yarn UI \"Num Container\",\"Used Capacity\",\"Absolute Used Capacity\",\"Used Resource\"\n    //指标在队列无任务运行时不为 0 的问题\n    //modify by jiulong.zhu@20190903\n    //++add start\n    for (RMContainer rmContainer : app.getReservedContainers()) {\n      source.detachContainer(clusterResource, app, rmContainer);\n      dest.attachContainer(clusterResource, app, rmContainer);\n    }\n    //++add end\n\n    // Detach the application..\n    source.finishApplicationAttempt(app, sourceQueueName);\n    source.getParent().finishApplication(appId, app.getUser());\n    // Finish app & update metrics\n    app.move(dest);\n    // Submit to a new queue\n    dest.submitApplicationAttempt(app, user);\n    applications.get(appId).setQueue(dest);\n    LOG.info(\"App: \" + app.getApplicationId() + \" successfully moved from \"\n        + sourceQueueName + \" to: \" + destQueueName);\n    return targetQueueName;\n  }\n```\n\n## 冒烟测试用例\n\n[TestCapacityScheduler.java]\n\n```\n  @Test\n  public void testReservedContainerLeakWhenMoveApplication() throws Exception {\n    CapacitySchedulerConfiguration csConf\n            = new CapacitySchedulerConfiguration();\n    csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] {\"a\", \"b\"});\n    csConf.setCapacity(\"root.a\",50);\n    csConf.setMaximumCapacity(\"root.a\",100);\n    csConf.setUserLimitFactor(\"root.a\",100);\n    csConf.setCapacity(\"root.b\",50);\n    csConf.setMaximumCapacity(\"root.b\",100);\n    csConf.setUserLimitFactor(\"root.b\",100);\n\n    YarnConfiguration conf=new YarnConfiguration(csConf);\n    conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n            ResourceScheduler.class);\n    RMNodeLabelsManager mgr=new NullRMNodeLabelsManager();\n    mgr.init(conf);\n    MockRM rm1 = new MockRM(csConf);\n    CapacityScheduler scheduler=(CapacityScheduler) rm1.getResourceScheduler();\n    rm1.getRMContext().setNodeLabelManager(mgr);\n    rm1.start();\n    MockNM nm1 = rm1.registerNode(\"127.0.0.1:1234\", 8 * GB);\n    MockNM nm2 = rm1.registerNode(\"127.0.0.2:1234\", 8 * GB);\n    /**\n     * simulation\n     * app1: (1 AM,1 running container)\n     * app2: (1 AM,1 reserved container)\n     */\n    // launch an app to queue, AM container should be launched in nm1\n    RMApp app1 = rm1.submitApp(1 * GB, \"app_1\", \"user_1\", null, \"a\");\n    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm1, nm1);\n\n    // launch another app to queue, AM container should be launched in nm1\n    RMApp app2 = rm1.submitApp(1 * GB, \"app_2\", \"user_1\", null, \"a\");\n    MockAM am2 = MockRM.launchAndRegisterAM(app2, rm1, nm1);\n\n    am1.allocate(\"*\", 4 * GB, 1, new ArrayList<ContainerId>());\n    //this containerRequest should be reserved\n    am2.allocate(\"*\", 4 * GB, 1, new ArrayList<ContainerId>());\n\n    RMNode rmNode1 = rm1.getRMContext().getRMNodes().get(nm1.getNodeId());\n    // Do node heartbeats 2 times\n    // First time will allocate container for app1, second time will reserve\n    // container for app2\n    scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));\n    scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));\n\n    FiCaSchedulerApp schedulerApp1 =\n            scheduler.getApplicationAttempt(am1.getApplicationAttemptId());\n    FiCaSchedulerApp schedulerApp2 =\n            scheduler.getApplicationAttempt(am2.getApplicationAttemptId());\n    // APP1:  1 AM, 1 allocatedContainer\n    Assert.assertEquals(2, schedulerApp1.getLiveContainers().size());\n    // APP2:  1 AM,1 reservedContainer\n    Assert.assertEquals(1,schedulerApp2.getLiveContainers().size());\n    Assert.assertEquals(1,schedulerApp2.getReservedContainers().size());\n    /**\n     * before,move app2 which has one reservedContainer\n     */\n    LeafQueue srcQueue = (LeafQueue) scheduler.getQueue(\"a\");\n    LeafQueue desQueue = (LeafQueue) scheduler.getQueue(\"b\");\n    Assert.assertEquals(4,srcQueue.getNumContainers());\n    Assert.assertEquals(10*GB,srcQueue.getUsedResources().getMemorySize());// AM: 2*1GB   container: 4GB running,4GB reserved\n    Assert.assertEquals(0,desQueue.getNumContainers());\n    Assert.assertEquals(0,desQueue.getUsedResources().getMemorySize());\n    //app1 ResourceUsage (0 reserved)\n    Assert.assertEquals(5*GB,schedulerApp1.getAppAttemptResourceUsage().getAllUsed().getMemorySize());\n    Assert.assertEquals(0,schedulerApp1.getCurrentReservation().getMemorySize());\n    //app2  ResourceUsage (4GB reserved)\n    Assert.assertEquals(1*GB,schedulerApp2.getAppAttemptResourceUsage().getAllUsed().getMemorySize());\n    Assert.assertEquals(4*GB,schedulerApp2.getCurrentReservation().getMemorySize());\n    /**\n     * move app2 which has one reservedContainer\n     */\n    scheduler.moveApplication(app2.getApplicationId(),\"b\");\n    // finish.keep the order,if killing app1 first,the reservedContainer of app2 will be allocated\n    rm1.killApp(app2.getApplicationId());\n    rm1.killApp(app1.getApplicationId());\n    /**\n     * after,moved app2 which has one reservedContainer\n     */\n    {\n      // after fixed\n      Assert.assertEquals(0, srcQueue.getNumContainers());\n      Assert.assertEquals(0, desQueue.getNumContainers());\n      Assert.assertEquals(0, srcQueue.getUsedResources().getMemorySize());\n      Assert.assertEquals(0, desQueue.getUsedResources().getMemorySize());\n    }\n    /*{\n      // before fixed\n      // <b> the reserved container borrowed from srcQueue and returned to desQueue,\n      // but the numContainer and UsedResource did not sync when moving app to another queue </b>\n      Assert.assertEquals(+1,srcQueue.getNumContainers());    //true\n      Assert.assertEquals(-1,desQueue.getNumContainers());    //true\n      Assert.assertEquals(+4*GB, srcQueue.getUsedResources().getMemorySize());    //true\n      Assert.assertEquals(-4*GB, desQueue.getUsedResources().getMemorySize());    //true\n    }*/\n    rm1.close();\n  }\n```\n\n## 线上测试用例\n\n\n","slug":"2019-09-15-Yarn:movetoqueue导致的UIcontainer泄露问题的修复思路","published":1,"updated":"2020-06-16T06:40:36.884Z","comments":1,"photos":[],"link":"","_id":"cke2rms8o002fy7c7eloc4gjr","content":"<h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>多个线上运行状态私有云的某些资源队列,在无任何 Application 提交及运行、资源队列完全空闲的情况下, \nUsed Capacity、Absolute Used Capacity、Used Resource、Num Containers指标非零异常  </p>\n<a id=\"more\"></a>\n\n<blockquote>\n</blockquote>\n<p>在Yarn ResourceManager管理界面中,有 scheduler 选项卡,展示了Yarn 当前使用的调度器及各资源队列的信息(YarnUI-&gt;scheduler-&gt;Application Queue),其中每项指标代表的含义是:<br>Queue State: 表示当前队列的状态,有 RUNNING/STOPPED 两种状态<br>Used Capacity: 表示当前队列已使用的资源占当前队列总资源的百分比<br>Configured Capacity: 表示当前队列的资源占父队列资源的百分比<br>Configured Max Capacity: 表示当前队列资源最大能占父队列资源的百分比<br>Absolute Used Capacity: 表示当前队列已使用的资源占 root 队列资源(整个集群)的百分比<br>Absolute Configured Capacity: 表示当前队列的资源占 root 队列总资源的百分比<br>Absolute Configured Max Capacity: 表示当前的队列的资源最大能占 root 队列的百分比<br>Used Resources: 表示当前队列已使用的资源总量(资源以内存和虚拟核形态表示,基本调度单位)<br>Num Schedulable Applications: 表示当前队列调度的应用个数\nNum Non-Schedulable Applications: 表示当前队列没有调度(积压,pending)的应用个数<br>Num Containers: 表示当前队列已经启动的 container 个数<br>Max Applications: 表示当前队列最大并发调度应用个数<br>Max Applications Per User: 表示当前队列对每个用户最大并发调度应用个数<br>Max Application Master Resources: 表示所有 Application 的 AM 可使用资源量之和的最大值<br>Used Application Master Resources: 表示当前队列中所有 Application 的 AM 使用资源量之和<br>Max Application Master Resources Per User: 表示当前队列中每个用户的 Application 的 AM 使用资源量之和的最大值<br>Configured Minimum User Limit Percent: 表示队列每个用户分配的最低资源百分比(资源保障)<br>Configured User Limit Factor: 表示每个用户能占用的队列资源的百分比<br>Accessible Node Labels:  表示当前队列可在哪些节点上分配资源 (*为全部节点)<br>Preemption: 是否允许资源抢占  </p>\n<p>指标可分为两类:配置型指标,静态数据 不会变化,如 Configured Capacity;状态型指标,动态数据 随应用的提交运行结束而变化,如 Used Capacity.<br>运行时异常指标如下图,无 Application 运行的情况下,low 队列状态型指标为负</p>\n<p><img src=\"/img/pictures/negative/negative_e7a766275896.png\" alt=\"\"></p>\n<h2 id=\"先期判断\"><a href=\"#先期判断\" class=\"headerlink\" title=\"先期判断\"></a>先期判断</h2><h3 id=\"指标关联的变量定位\"><a href=\"#指标关联的变量定位\" class=\"headerlink\" title=\"指标关联的变量定位\"></a>指标关联的变量定位</h3><p>线上 hadoop 版本: hadoop 2.7.3<br>Yarn ResourceManager 管理界面启动的入口是org.apache.hadoop.yarn.server.resourcemanager.ResourceManager#startWebApp()<br>[ResourceManager.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void startWepApp() &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    Builder&lt;ApplicationMasterService&gt; builder &#x3D; </span><br><span class=\"line\">        WebApps</span><br><span class=\"line\">            .$for(&quot;cluster&quot;, ApplicationMasterService.class, masterService,</span><br><span class=\"line\">                &quot;ws&quot;)</span><br><span class=\"line\">            .with(conf)</span><br><span class=\"line\">            .withHttpSpnegoPrincipalKey(</span><br><span class=\"line\">                YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY)</span><br><span class=\"line\">            .withHttpSpnegoKeytabKey(</span><br><span class=\"line\">                YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)</span><br><span class=\"line\">            .at(webAppAddress);</span><br><span class=\"line\">   \t....</span><br><span class=\"line\">    webApp &#x3D; builder.start(new RMWebApp(this));</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>RMWebApp 主要逻辑是使用 Google Guice 做依赖注入,并分发请求绑定后台逻辑,大致相当于 SpringMVC 系统中的 Dispatcher 的角色<br>对 Yarn UI界面左侧边栏 scheduler 的请求会转发给 RmController#scheduler()方法处理<br>[RMWebApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void setup() &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    bind(RMWebApp.class).toInstance(this);</span><br><span class=\"line\">    if (rm !&#x3D; null) &#123;</span><br><span class=\"line\">      bind(ResourceManager.class).toInstance(rm);</span><br><span class=\"line\">      bind(ApplicationBaseProtocol.class).toInstance(rm.getClientRMService());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    route(&quot;&#x2F;scheduler&quot;, RmController.class, &quot;scheduler&quot;);</span><br><span class=\"line\">    route(pajoin(&quot;&#x2F;queue&quot;, QUEUE_NAME), RmController.class, &quot;queue&quot;);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>RmController#scheduler() 先获取到 Guice 注入的 ResourceManager,然后依据 RM 使用的调度器做页面渲染.<br>页面渲染逻辑的入口是CapacitySchedulerPage#render()<br>[RmController.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void scheduler() &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ResourceManager rm &#x3D; getInstance(ResourceManager.class);</span><br><span class=\"line\">    ResourceScheduler rs &#x3D; rm.getResourceScheduler();</span><br><span class=\"line\">    if (rs &#x3D;&#x3D; null || rs instanceof CapacityScheduler) &#123;</span><br><span class=\"line\">      setTitle(&quot;Capacity Scheduler&quot;);</span><br><span class=\"line\">      &#x2F;&#x2F;渲染</span><br><span class=\"line\">      render(CapacitySchedulerPage.class);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (rs instanceof FairScheduler) &#123;</span><br><span class=\"line\">      setTitle(&quot;Fair Scheduler&quot;);</span><br><span class=\"line\">      render(FairSchedulerPage.class);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> protected void render(Class&lt;? extends View&gt; cls) &#123;</span><br><span class=\"line\">    context().rendered &#x3D; true;</span><br><span class=\"line\">    getInstance(cls).render();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>泛型上界是 View,调用栈是:<br>View#render()<br>&ensp;&ensp;-&gt;HtmlPage#render()<br>&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;TwoColumnLayout#render(html)<br>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;CapacitySchedulerPage#content()<br>[CapacitySchedulerPage.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected Class&lt;? extends SubView&gt; content() &#123;</span><br><span class=\"line\">    return QueuesBlock.class;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>QueuesBlock 是 CapacitySchedulerPage 的内部类,用于展示CapacityScheduler 的 DashBoard 信息,包含队列、标签、应用概览信息,并构建根队列 root 来做为后续递归渲染的起点<br>[QueuesBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class QueuesBlock extends HtmlBlock &#123;</span><br><span class=\"line\">    final CapacityScheduler cs;</span><br><span class=\"line\">    final CSQInfo csqinfo;</span><br><span class=\"line\">    private List&lt;NodeLabel&gt; nodeLabelsInfo;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public void render(Block html) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        float used &#x3D; 0;</span><br><span class=\"line\">        if (null &#x3D;&#x3D; nodeLabelsInfo</span><br><span class=\"line\">            || (nodeLabelsInfo.size() &#x3D;&#x3D; 1 &amp;&amp; nodeLabelsInfo.get(0)</span><br><span class=\"line\">                .getLabelName().isEmpty())) &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;创建根队列,作为后续渲染的起点</span><br><span class=\"line\">          CSQueue root &#x3D; cs.getRootQueue();</span><br><span class=\"line\">          CapacitySchedulerInfo sinfo &#x3D;</span><br><span class=\"line\">              new CapacitySchedulerInfo(root, new NodeLabel(</span><br><span class=\"line\">                  RMNodeLabelsManager.NO_LABEL));</span><br><span class=\"line\">          csqinfo.csinfo &#x3D; sinfo;</span><br><span class=\"line\">          csqinfo.qinfo &#x3D; null;</span><br><span class=\"line\"></span><br><span class=\"line\">           ...</span><br><span class=\"line\">          ul.li().</span><br><span class=\"line\">            ...</span><br><span class=\"line\">            _(QueueBlock.class)._();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          for (NodeLabel label : nodeLabelsInfo) &#123;</span><br><span class=\"line\">            ....</span><br><span class=\"line\">            underLabel.li().</span><br><span class=\"line\">            ...</span><br><span class=\"line\">            _(QueueBlock.class)._()._();</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;Application List 界面</span><br><span class=\"line\">      ul._()._().</span><br><span class=\"line\">      script().$type(&quot;text&#x2F;javascript&quot;).</span><br><span class=\"line\">          _(&quot;$(&#39;#cs&#39;).hide();&quot;)._()._().</span><br><span class=\"line\">      _(RMAppsBlock.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>从 rootQueue 开始递归,Queue 检查自身有没有子队列 subQueues,若无则其本身为 LeafQueue,使用 LeafQueueInfoBlock#render()渲染叶子队列信息,使用 QueueUsersInfoBlock#render()渲染队列下的用户信息;若有子队列则其本身为ParentQueue,使用 QueueBlock#render()渲染,直至叶子队列<br>[QueuesBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void render(Block html) &#123;</span><br><span class=\"line\">      ArrayList&lt;CapacitySchedulerQueueInfo&gt; subQueues &#x3D;</span><br><span class=\"line\">          (csqinfo.qinfo &#x3D;&#x3D; null) ? csqinfo.csinfo.getQueues().getQueueInfoList()</span><br><span class=\"line\">              : csqinfo.qinfo.getQueues().getQueueInfoList();</span><br><span class=\"line\">      UL&lt;Hamlet&gt; ul &#x3D; html.ul(&quot;#pq&quot;);</span><br><span class=\"line\">      for (CapacitySchedulerQueueInfo info : subQueues) &#123;</span><br><span class=\"line\">  \t  ...</span><br><span class=\"line\">          if (info.getQueues() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">          li.ul(&quot;#lq&quot;).li()._(LeafQueueInfoBlock.class)._()._();</span><br><span class=\"line\">          li.ul(&quot;#lq&quot;).li()._(QueueUsersInfoBlock.class)._()._();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          li._(QueueBlock.class);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">  \t...</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>由于异常指标位于叶子队列信息中,所以暂且不看QueueUsersInfoBlock.java<br>[LeafQueueInfoBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private String nodeLabel;</span><br><span class=\"line\">final CapacitySchedulerLeafQueueInfo lqinfo;</span><br><span class=\"line\">@Inject LeafQueueInfoBlock(ViewContext ctx, CSQInfo info) &#123;</span><br><span class=\"line\">      super(ctx);</span><br><span class=\"line\">      lqinfo &#x3D; (CapacitySchedulerLeafQueueInfo) info.qinfo;</span><br><span class=\"line\">      nodeLabel &#x3D; info.label;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">protected void render(Block html) &#123;</span><br><span class=\"line\">      if (nodeLabel &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        renderLeafQueueInfoWithoutParition(html);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        renderLeafQueueInfoWithPartition(html);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">虽然按照 nodeLabel 做了分支,但是核心逻辑都是</span><br><span class=\"line\">renderQueueCapacityInfo方法和renderCommonLeafQueueInfo方法,</span><br><span class=\"line\">这两个方法分别展示不同方面的指标,和 Yarn UI 展示的指标相同 </span><br><span class=\"line\"></span><br><span class=\"line\">private void renderQueueCapacityInfo(final ResponseInfo ri) &#123;</span><br><span class=\"line\">      ri.</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Used Capacity:&quot;, percent(lqinfo.getUsedCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Configured Capacity:&quot;, percent(lqinfo.getCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Configured Max Capacity:&quot;, percent(lqinfo.getMaxCapacity() &#x2F; 100)).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Absolute Used Capacity:&quot;, percent(lqinfo.getAbsoluteUsedCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Absolute Configured Capacity:&quot;, percent(lqinfo.getAbsoluteCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Absolute Configured Max Capacity:&quot;, percent(lqinfo.getAbsoluteMaxCapacity() &#x2F; 100)).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Used Resources:&quot;, lqinfo.getResourcesUsed().toString());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  private void renderCommonLeafQueueInfo(final ResponseInfo ri) &#123;</span><br><span class=\"line\">      ri.</span><br><span class=\"line\">      _(&quot;Num Schedulable Applications:&quot;, Integer.toString(lqinfo.getNumActiveApplications())).</span><br><span class=\"line\">      _(&quot;Num Non-Schedulable Applications:&quot;, Integer.toString(lqinfo.getNumPendingApplications())).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Num Containers:&quot;, Integer.toString(lqinfo.getNumContainers())).</span><br><span class=\"line\">      _(&quot;Max Applications:&quot;, Integer.toString(lqinfo.getMaxApplications())).</span><br><span class=\"line\">      _(&quot;Max Applications Per User:&quot;, Integer.toString(lqinfo.getMaxApplicationsPerUser())).</span><br><span class=\"line\">      _(&quot;Max Application Master Resources:&quot;, lqinfo.getAMResourceLimit().toString()).</span><br><span class=\"line\">      _(&quot;Used Application Master Resources:&quot;, lqinfo.getUsedAMResource().toString()).</span><br><span class=\"line\">      _(&quot;Max Application Master Resources Per User:&quot;, lqinfo.getUserAMResourceLimit().toString()).</span><br><span class=\"line\">      _(&quot;Configured Minimum User Limit Percent:&quot;, Integer.toString(lqinfo.getUserLimit()) + &quot;%&quot;).</span><br><span class=\"line\">      _(&quot;Configured User Limit Factor:&quot;, StringUtils.format(</span><br><span class=\"line\">          &quot;%.1f&quot;, lqinfo.getUserLimitFactor())).</span><br><span class=\"line\">      _(&quot;Accessible Node Labels:&quot;, StringUtils.join(&quot;,&quot;, lqinfo.getNodeLabels())).</span><br><span class=\"line\">      _(&quot;Preemption:&quot;, lqinfo.getPreemptionDisabled() ? &quot;disabled&quot; : &quot;enabled&quot;);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"异常指标计算方式\"><a href=\"#异常指标计算方式\" class=\"headerlink\" title=\"异常指标计算方式\"></a>异常指标计算方式</h3><p>注:集群没有使用 label 系统,所以下述的 nodeLabel 视为””即可</p>\n<ul>\n<li><p>Used Capacity</p>\n<p>  [AbstractCSQueue.java]</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final synchronized float getUsedCapacity(final String nodeLabel) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;集群所有资源 * 该队列的绝对容量百分比 &#x3D; 该队列的绝对容量 </span><br><span class=\"line\">    Resource availableToQueue &#x3D;</span><br><span class=\"line\">        Resources.multiply(</span><br><span class=\"line\">            labelManager.getResourceByLabel(nodeLabel, this.clusterResource),</span><br><span class=\"line\">            queueCapacities.getAbsoluteCapacity(nodeLabel));</span><br><span class=\"line\">   &#x2F;&#x2F;使用 queueUsage 中记录的使用量除以该队列的绝对容量得到队列的 Used Capacity</span><br><span class=\"line\">   &#x2F;&#x2F;queueUsage 的类型为 ResourceUsage</span><br><span class=\"line\">    return</span><br><span class=\"line\">        Resources.divide(resourceCalculator, this.clusterResource,</span><br><span class=\"line\">            queueUsage.getUsed(nodeLabel), availableToQueue);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>Absolute Used Capacity</p>\n<p>  算法和Used Capacity算法相似,只是分母不同,AbsoluteUsedCapacity 计算时分母是整个集群的资源  </p>\n<p>  [AbstractCSQueue.java]</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final synchronized float getAbsoluteUsedCapacity(final String nodeLabel) &#123;</span><br><span class=\"line\">   Resource labeledResources &#x3D;</span><br><span class=\"line\">              labelManager.getResourceByLabel(nodeLabel, this.clusterResource);</span><br><span class=\"line\">   return Resources.divide(resourceCalculator, this.clusterResource,</span><br><span class=\"line\">       queueUsage.getUsed(nodeLabel), labeledResources);</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Used Resource</p>\n<p>  &ensp;&ensp;使用的是Queue 按 Label 记录的资源信息<br>  queueResourceUsage.getUsed(nodeLabel)  </p>\n</li>\n<li><p>Num Containers</p>\n<p>  &ensp;&ensp;使用的 Queue 本身记录的信息<br>  numContainers = leafQueue.getNumContainers();  </p>\n</li>\n</ul>\n<blockquote>\n</blockquote>\n<p>综上所述:<br>四个指标中,队列层面的Used Capacity 、Absolute Used Capacity 、Used Resource 均和 ResourceUsage维护 used 资源信息有关;numContainer 是 LeafQueue 自身维护的 container 数量.这些指标的变化逻辑在 AbstractCSQueue的 assignContainer()和 releaseContainer()中,即和 container 的释放/申请有关.\n而在队列完全空闲的时候,四个指标为负,初步定位可能是 Container 重复释放或无效释放的原因.</p>\n<h2 id=\"问题复现\"><a href=\"#问题复现\" class=\"headerlink\" title=\"问题复现\"></a>问题复现</h2><h3 id=\"猜测重复释放\"><a href=\"#猜测重复释放\" class=\"headerlink\" title=\"猜测重复释放\"></a>猜测重复释放</h3><p>在已经出现指标异常的私有云上,对出现异常时日期前后的 resourcemanager 日志进行分析,检测其中出现”Assigned container”和”Released  container”字符串的数量,从日志层面对 container 的申请和释放次数做一个简单的判断.这种方式适用于集群作业周期性较强且没有跨天任务运行的情况.  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">egrep -o &quot;Assigned container&quot; resourcemanager.log | sort | uniq -c</span><br><span class=\"line\">egrep -o &quot;Released container&quot; resourcemanager.log | sort | uniq -c</span><br></pre></td></tr></table></figure>\n\n<p>从结果上看,”Assigned container”和”Released container”字符串出现次数是一致的,可能不是重复释放的问题  </p>\n<h3 id=\"猜测无效释放\"><a href=\"#猜测无效释放\" class=\"headerlink\" title=\"猜测无效释放\"></a>猜测无效释放</h3><p>对无效释放的猜想源自于重复释放和 rm 日志中的”Null container completed…”,猜测可能对于 nullContainer 处理有问题,修改了系统维护的信息, 导致了负值    </p>\n<p>UI 的 kill 操作,命令行 yarn application –kill 和 RMApp 的正常结束都会释放 container,由 APP_ATTEMPT_REMOVED 事件触发,\n回收 AppAttemp 持有的runningContainer 和 reservedContainer,核心逻辑在 CapacityScheduler#completedContainer()方法,但是其中对于 RMContainer 和 Application 都做了校验,不会修改维护信息;如果 RMContainer 对象内持有的 contianer 对象为空的话,会抛出 NullPointerException,也不会修改维护信息. 所以无效释放的猜测不合理,此处代码证明重复释放也不合理\n[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void completedContainer(RMContainer rmContainer,</span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event) &#123; </span><br><span class=\"line\">    &#x2F;&#x2F;rmContaienr 空值校验  </span><br><span class=\"line\">    if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      LOG.info(&quot;Null container completed...&quot;);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">    FiCaSchedulerApp application &#x3D;</span><br><span class=\"line\">        getCurrentAttemptForContainer(container.getId());</span><br><span class=\"line\">    ApplicationId appId &#x3D;</span><br><span class=\"line\">    &#x2F;&#x2F;如果 RMContainer 映射的 container 为空,则此处会 NullPointerException</span><br><span class=\"line\">        container.getId().getApplicationAttemptId().getApplicationId();</span><br><span class=\"line\">    &#x2F;&#x2F;application 空值校验</span><br><span class=\"line\">    if (application &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      LOG.info(&quot;Container &quot; + container + &quot; of&quot; + &quot; unknown application &quot;</span><br><span class=\"line\">          + appId + &quot; completed with event &quot; + event);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"movetoqueue\"><a href=\"#movetoqueue\" class=\"headerlink\" title=\"movetoqueue\"></a>movetoqueue</h3><p>重复释放和无效释放的猜测证否之后,只好再去仔细研究 系统指标异常前的几个运行的任务日志,对 application 的 attempt 和每个 contianer 状态机的状态和 触发事件按照时间线标注出来,对application 发生的所有事件还原出来,发现了其中一个 application 的以下信息  </p>\n<blockquote>\n<p>1.提交到 root.high 队列后,队列资源不足,在某些节点上为该 applicaiton reserve 资源,其他 application 调度时不会再该节点上分配资源<br>2.application 被从 high 队列移动了 low 队列<br>3.container 被移动队列时,源队列和目标队列的资源有变动,两个队列都会被 re-sort,以便优先在资源利用率最低的 queue 调度,所以有每个 container 的移动有四条日志:queueMoveOut 队列的信息,queueMoveOut 的父队列信息(有几个 parentQueue 就会有几条日志),queuMoveIn 队列信息,queueMoveIn 的父队列信息<br>4.在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000002,在 high队列上分配了资源,被 move 到 low 队列后,containerCompleted 之后,释放资源到了 low 队列<br>5.<b>在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000080,在 high 队列上保留了资源,move 操作没有影响到该 container,但是containerCompleted之后释放资源到了 low 队列</b>. </p>\n</blockquote>\n<p>日志如下  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INFO LeafQueue: assignedContainer application attempt&#x3D;appattempt_1565789460020_5864_000001 container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: null, ] queue&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:420864, vCores:20&gt;, usedCapacity&#x3D;1.4421053, absoluteUsedCapacity&#x3D;0.86526316, numApps&#x3D;3, numContainers&#x3D;20 clusterResource&#x3D;&lt;memory:486400, vCores:136&gt; type&#x3D;OFF_SWITCH  </span><br><span class=\"line\">INFO RMContainerImpl: container_e08_1565789460020_5864_01_000080 Container Transitioned from NEW to RESERVED</span><br><span class=\"line\">INFO LeafQueue: Reserved container  application&#x3D;application_1565789460020_5864 resource&#x3D;&lt;memory:11264, vCores:1&gt; queue&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:443392, vCores:22&gt;, usedCapacity&#x3D;1.5192982, absoluteUsedCapacity&#x3D;0.91157895, numApps&#x3D;3, numContainers&#x3D;22 usedCapacity&#x3D;1.5192982 absoluteUsedCapacity&#x3D;0.91157895 used&#x3D;&lt;memory:443392, vCores:22&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: Re-sorting assigned queue: root.high stats: high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:454656, vCores:23&gt;, usedCapacity&#x3D;1.5578947, absoluteUsedCapacity&#x3D;0.93473685, numApps&#x3D;3, numContainers&#x3D;23</span><br><span class=\"line\">....moving</span><br><span class=\"line\">INFO LeafQueue: movedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service:  &#125;, ] resource&#x3D;&lt;memory:11264, vCores:1&gt; queueMoveOut&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:431104, vCores:20&gt;, usedCapacity&#x3D;1.477193, absoluteUsedCapacity&#x3D;0.88631576, numApps&#x3D;3, numContainers&#x3D;20 usedCapacity&#x3D;1.477193 absoluteUsedCapacity&#x3D;0.88631576 used&#x3D;&lt;memory:431104, vCores:20&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: movedContainer queueMoveOut&#x3D;root usedCapacity&#x3D;0.9768421 absoluteUsedCapacity&#x3D;0.9768421 used&#x3D;&lt;memory:475136, vCores:24&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO LeafQueue: movedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service: &#125;, ] resource&#x3D;&lt;memory:11264, vCores:1&gt; queueMoveIn&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:-22528, vCores:-3&gt;, usedCapacity&#x3D;-0.4631579, absoluteUsedCapacity&#x3D;-0.04631579, numApps&#x3D;0, numContainers&#x3D;-3 usedCapacity&#x3D;-0.4631579 absoluteUsedCapacity&#x3D;-0.04631579 used&#x3D;&lt;memory:-22528, vCores:-3&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: movedContainer queueMoveIn&#x3D;root usedCapacity&#x3D;1.0 absoluteUsedCapacity&#x3D;1.0 used&#x3D;&lt;memory:486400, vCores:25&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">....moved</span><br><span class=\"line\">INFO CapacityScheduler: App: application_1565789460020_5864 successfully moved from high to: low</span><br><span class=\"line\">INFO LeafQueue: completedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000080, NodeId: hadoop5:8041, NodeHttpAddress: hadoop5:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: null, ] queue&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:0, vCores:-1&gt;, usedCapacity&#x3D;0.0, absoluteUsedCapacity&#x3D;0.0, numApps&#x3D;1, numContainers&#x3D;-1 cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO LeafQueue: completedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6.:8041, NodeHttpAddress: hadoop6.cn:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service: &#125;, ] queue&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:-1024, vCores:-2&gt;, usedCapacity&#x3D;-0.021052632, absoluteUsedCapacity&#x3D;-0.002105263, numApps&#x3D;1, numContainers&#x3D;-2 cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>猜测:application 在移动队列后,对 reservedContainer 没有移动或维护信息不同步,导致了源队列的资源泄露给了目标队列,从而目标队列的 UsedCapacity 为负<br>复现方式<br>&ensp;&ensp;&ensp;&ensp;第一步:提交 application 到资源紧张的 Queue<br>&ensp;&ensp;&ensp;&ensp;第二步:待在 RM 的日志中看到”Trying to fulfill reservation for application ${APPLICATION_ID} on node …”和”Trying to schedule on node…, available:…” 表明 CapacityScheduler为该 ApplicationId 保留了资源,跳过在此 nm 上为其他 app 分配 container<br>&ensp;&ensp;&ensp;&ensp;第三步: 使用 yarn application -movetoqueue ${APPLICATION_ID} -queue ${TOQueue}<br>&ensp;&ensp;&ensp;&ensp;第四步: 待在 RM 日志中看到了”App:${APPLICATION_ID} successfully moved ${FROMQUEUE} to ${TOQUEUE}”后,使用 yarn application –kill ${APPLICATION_ID} 或等待 app 结束<br>&ensp;&ensp;&ensp;&ensp;第五步: 在 YarnUI上查看被移动的目标队列${TOQUEUE}的信息  </p>\n</blockquote>\n<p>注:<br>&ensp;&ensp;&ensp;&ensp;1.一定要在资源紧张的队列上提交 app,以触发调度系统的保留资源.在空闲队列上提交任务复现不了指标异常的问题<br>&ensp;&ensp;&ensp;&ensp;2.最好移动到一个完全空闲的队列上,否则即使复现了资源泄露,也不易看出来  </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>以下代码出自于 hadoop 2.7.3版本<br>Yarn 是一个资源调度平台,集群内存资源和 cpu 资源被 Yarn 抽象为 Resource{memory,core},客户端对 Yarn 的资源请求和 Yarn 内部的资源调度都是以 Container 为基本单位的<br>当客户端向资源队列 Queue 提交 Application 时, 客户端申请的 AM 以及 AM 申请新的执行角色(e.g. spark 的 executor)都是在Container 中运行,那么对于 Yarn 调度的 Container,资源信息被多维聚合[app状态|container状态|container用途|用户|标签]统计维护:    </p>\n<ul>\n<li>Queue.包括直接申请的叶子队列及其所有的父队列    <ol>\n<li>Queue(AbstractCSQueue)<br>&ensp;&ensp;主要指标是 container 数量;保存着 QueueMetrics 和 ResourceUsage(ByLabel) 的引用<br> a. ParentQueue<br>&ensp;&ensp;主要指标是运行的 application数量;保存着所有资源子队列的集合<br> b. LeafQueue \n&ensp;&ensp; 主要指标是每个用户提交 app 的数量和资源用途用量(ResourceUsageByUser);保存着队列 running 和 pending 的 app[attemp]信息               </li>\n<li>QueueMetrics<br>&ensp;&ensp;保留着queue 的指标信息,包括[提交|运行|积压|完成|杀死|失败]app 数量、[分配|待分配|积压]的[container|内存|虚拟核]信息、活跃的[app|user]信息;保存着用户级别的 &lt;username,QueueMetrics&gt;映射  </li>\n<li>ResourceUsage<br>&ensp;&ensp;保留着&lt;label,UsageByLabel&gt;信息,分为used、pending、amused、reserved 四类  </li>\n</ol>\n</li>\n<li>Application(SchedulerApplicationAttempt). app 中维护着当前正在运行的 container,及 yarn 为其保留的 container 信息</li>\n<li>NodeManager. container 所在的 NM 维护着自身运行的所有 container,并通过 RM 心跳汇报所有 container 状态机状态,触发 RM 对container 状态的更新或释放    </li>\n</ul>\n<blockquote>\n</blockquote>\n<p>综上:<br>&ensp;&ensp;&ensp;&ensp;1.指标异常与 container 申请释放有关,所以需要研究 <b>正常申请释放与应用移动队列</b> 对上述维护信息的影响<br>&ensp;&ensp;&ensp;&ensp;2.UI 展示的 numContainers 数据取自 AbstractCSQueue 维护 numContainer 成员变量;usedCapacity,absoluteUsedCapacity,usedResource 均取自AbstractCSQueue$ResourceUsage.used 成员变量.需要关注以上变量在资源申请释放过程中的变化    </p>\n<h3 id=\"正常的资源分配和释放过程\"><a href=\"#正常的资源分配和释放过程\" class=\"headerlink\" title=\"正常的资源分配和释放过程\"></a>正常的资源分配和释放过程</h3><p>RM 的资源分配和资源释放都是被动触发,客户端提交 application 到 RMClientService,app 信息暂存在指定资源队列中,待 NM 向 RM 通过心跳汇报自身信息时 RM 将对该 NM 触发调度,在情况允许的情况下(e.g. 目标队列具有该 NM 的 access 权限,NM 剩余资源满足 app 中一个 ResourceRequest 申请的资源),将在该 NM 上划出 ResourceRequest 要求的资源,并同步 Queue/Application/NodeManager 维护的信息;当 container 完成退出之后,NodeManager 注意到 container 状态机变化,并将其信息附带在心跳中汇报给 RM,触发 Queue/Application/NodeManager 状态变化和信息同步<br>以下代码较为关注资源信息的维护同步,对于其他细节不再赘述    </p>\n<h3 id=\"资源分配\"><a href=\"#资源分配\" class=\"headerlink\" title=\"资源分配\"></a>资源分配</h3><p><img src=\"/img/pictures/negative/yarn_node_update_pic.png\" alt=\"\"></p>\n<p>NM 通过心跳触发 CapacityScheduler 调度,CS 首先按照 NM 上报的信息同步 RM 的信息,为新启动的 container 触发 LAUNCHED 事件,为结束的 container 触发FINISHED 事件;并试图在该节点上分配资源<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">case NODE_UPDATE:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      NodeUpdateSchedulerEvent nodeUpdatedEvent &#x3D; (NodeUpdateSchedulerEvent)event;</span><br><span class=\"line\">      RMNode node &#x3D; nodeUpdatedEvent.getRMNode();</span><br><span class=\"line\">      &#x2F;&#x2F;同步 RM 信息</span><br><span class=\"line\">      nodeUpdate(node);</span><br><span class=\"line\">      if (!scheduleAsynchronously) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;试图在 nm 分配资源</span><br><span class=\"line\">        allocateContainersToNode(getNode(node.getNodeID()));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\"> ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>依据汇报心跳的 NodeManager 是否被保留了资源执行相应的逻辑<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private synchronized void allocateContainersToNode(FiCaSchedulerNode node) &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; node.getReservedContainer();</span><br><span class=\"line\">    &#x2F;&#x2F;当前 node 被某 applocation 保留了</span><br><span class=\"line\">    if (reservedContainer !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerApp reservedApplication &#x3D;</span><br><span class=\"line\">          getCurrentAttemptForContainer(reservedContainer.getContainerId());</span><br><span class=\"line\">      &#x2F;&#x2F; Try to fulfill the reservation</span><br><span class=\"line\">      LOG.info(&quot;Trying to fulfill reservation for application &quot; + </span><br><span class=\"line\">          reservedApplication.getApplicationId() + &quot; on node: &quot; + </span><br><span class=\"line\">          node.getNodeID());</span><br><span class=\"line\">      LeafQueue queue &#x3D; ((LeafQueue)reservedApplication.getQueue());</span><br><span class=\"line\">      CSAssignment assignment &#x3D;</span><br><span class=\"line\">          queue.assignContainers(</span><br><span class=\"line\">              clusterResource,</span><br><span class=\"line\">              node,</span><br><span class=\"line\">              new ResourceLimits(labelManager.getResourceByLabel(</span><br><span class=\"line\">                  RMNodeLabelsManager.NO_LABEL, clusterResource)));</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;当前节点未被保留</span><br><span class=\"line\">    if (node.getReservedContainer() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     \t...</span><br><span class=\"line\">     \t&#x2F;&#x2F;交由 root 队列代理分配.root 是资源队列树形结构的根节点,类型一定ParentQueue</span><br><span class=\"line\">        root.assignContainers(</span><br><span class=\"line\">            clusterResource,</span><br><span class=\"line\">            node,</span><br><span class=\"line\">            new ResourceLimits(labelManager.getResourceByLabel(</span><br><span class=\"line\">                RMNodeLabelsManager.NO_LABEL, clusterResource)));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      LOG.info(&quot;Skipping scheduling since node &quot; + node.getNodeID() + </span><br><span class=\"line\">          &quot; is reserved by application &quot; + </span><br><span class=\"line\">          node.getReservedContainer().getContainerId().getApplicationAttemptId()</span><br><span class=\"line\">          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 将节点委派给子队列,试图分配资源<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> public synchronized CSAssignment assignContainers(Resource clusterResource,</span><br><span class=\"line\">     FiCaSchedulerNode node, ResourceLimits resourceLimits) &#123;</span><br><span class=\"line\">   CSAssignment assignment &#x3D; </span><br><span class=\"line\">       new CSAssignment(Resources.createResource(0, 0), NodeType.NODE_LOCAL);</span><br><span class=\"line\">   Set&lt;String&gt; nodeLabels &#x3D; node.getLabels();</span><br><span class=\"line\">   ...校验 queue 对 node 的 access 权限</span><br><span class=\"line\">   while (canAssign(clusterResource, node)) &#123;</span><br><span class=\"line\">     ...校验 queue 资源是否超限      </span><br><span class=\"line\">     &#x2F;&#x2F; Schedule 递归交由子队列去分配</span><br><span class=\"line\">     CSAssignment assignedToChild &#x3D; </span><br><span class=\"line\">         assignContainersToChildQueues(clusterResource, node, resourceLimits);</span><br><span class=\"line\">     assignment.setType(assignedToChild.getType());</span><br><span class=\"line\">     &#x2F;&#x2F; Done if no child-queue assigned anything</span><br><span class=\"line\">     &#x2F;&#x2F; 如果分配到了资源, assignedToChild 大于 Resource&lt;0,0&gt;</span><br><span class=\"line\">     if (Resources.greaterThan(</span><br><span class=\"line\">             resourceCalculator, clusterResource, </span><br><span class=\"line\">             assignedToChild.getResource(), Resources.none())) &#123;</span><br><span class=\"line\">             &#x2F;&#x2F;同步维护的信息</span><br><span class=\"line\">       super.allocateResource(clusterResource, assignedToChild.getResource(),</span><br><span class=\"line\">           nodeLabels);</span><br><span class=\"line\">       Resources.addTo(assignment.getResource(), assignedToChild.getResource());</span><br><span class=\"line\">       ...</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">       break;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">       break;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;     </span><br><span class=\"line\">   return assignment;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>先看如果分配到资源的话 同步的信息:<br>1.按照 label 去更新 ResourceUsage 中维护的用户使用资源 used(incUsed 方法)<br>2.增加了该队列的 numContainer 数量<br>需要注意的是 <b>无论后续得到的是 allocated container 还是 reserved container,都增加了 numContainer的值,增加了用户 USED 类型的内存和虚拟核数量</b><br>[AbstractCSQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Update usedResources by labels</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看对 queueUsage:ResourceUsage 做了什么操作<br>ResourceUsage 中维护了一个Map结构 usages,key 是标签类型,value 是 UsageByLabel;UsageByLabel 中只有一个数组 Resource[],数组中的每个值分别表征着 USED,PENDING,AMUSED,RESERVED 用途的 Resource 数量<br>[ResourceUsage.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; &lt;labelName,UsageByLabel&gt;</span><br><span class=\"line\">private Map&lt;String, UsageByLabel&gt; usages;</span><br><span class=\"line\">public void incUsed(String label, Resource res) &#123;</span><br><span class=\"line\">    _inc(label, ResourceType.USED, res);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> private void _inc(String label, ResourceType type, Resource res) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     UsageByLabel usage &#x3D; getAndAddIfMissing(label);</span><br><span class=\"line\">     Resources.addTo(usage.resArr[type.idx], res);      </span><br><span class=\"line\">     ...</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>UsageByLabel 使用一个 Resource[]数组来存储资源的用途和用量<br>[UsageByLabel.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static class UsageByLabel &#123;</span><br><span class=\"line\">  &#x2F;&#x2F; usage by label, contains all UsageType</span><br><span class=\"line\">  private Resource[] resArr;</span><br><span class=\"line\">  public UsageByLabel(String label) &#123;</span><br><span class=\"line\">    resArr &#x3D; new Resource[ResourceType.values().length];</span><br><span class=\"line\">    for (int i &#x3D; 0; i &lt; resArr.length; i++) &#123;</span><br><span class=\"line\">      resArr[i] &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private enum ResourceType &#123;</span><br><span class=\"line\">  USED(0), PENDING(1), AMUSED(2), RESERVED(3);</span><br><span class=\"line\">  private int idx;</span><br><span class=\"line\">  private ResourceType(int value) &#123;</span><br><span class=\"line\">    this.idx &#x3D; value;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再回到资源分配,root 队列深度优先遍历所有子队列,尝试在叶子队列上分配资源<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized CSAssignment assignContainers(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    &#x2F;&#x2F; Check for reserved resources</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; node.getReservedContainer();</span><br><span class=\"line\">    if (reservedContainer !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerApp application &#x3D; </span><br><span class=\"line\">          getApplication(reservedContainer.getApplicationAttemptId());</span><br><span class=\"line\">      synchronized (application) &#123;</span><br><span class=\"line\">        return assignReservedContainer(application, node, reservedContainer,</span><br><span class=\"line\">            clusterResource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;对目前活跃的 application,尝试在当前 NM 上分配资源</span><br><span class=\"line\">    for (FiCaSchedulerApp application : activeApplications) &#123;</span><br><span class=\"line\">    \t....</span><br><span class=\"line\">      synchronized (application) &#123;</span><br><span class=\"line\">         ....        </span><br><span class=\"line\">          &#x2F;&#x2F; Schedule in priority order</span><br><span class=\"line\">        for (Priority priority : application.getPriorities()) &#123;</span><br><span class=\"line\">          ResourceRequest anyRequest &#x3D;</span><br><span class=\"line\">              application.getResourceRequest(priority, ResourceRequest.ANY);</span><br><span class=\"line\">         if (null &#x3D;&#x3D; anyRequest) &#123;</span><br><span class=\"line\">            continue;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          ....校验性工作</span><br><span class=\"line\">          &#x2F;&#x2F;校验通过,在此节点上分配资源</span><br><span class=\"line\">          CSAssignment assignment &#x3D;  </span><br><span class=\"line\">            assignContainersOnNode(clusterResource, node, application, priority, </span><br><span class=\"line\">                null, currentResourceLimits);</span><br><span class=\"line\">\t.....</span><br><span class=\"line\">          &#x2F;&#x2F; Did we schedule or reserve a container?</span><br><span class=\"line\">          Resource assigned &#x3D; assignment.getResource();</span><br><span class=\"line\">          if (Resources.greaterThan(</span><br><span class=\"line\">              resourceCalculator, clusterResource, assigned, Resources.none())) &#123;</span><br><span class=\"line\">            &#x2F;&#x2F;更新当前[叶子]队列的 numContainer,ResourceUsageByLabel和 ResourceUsageByUser.</span><br><span class=\"line\">            &#x2F;&#x2F;比 ParentQueue.allocateResource 多出了一个用户层面的资源统计</span><br><span class=\"line\">            allocateResource(clusterResource, application, assigned,</span><br><span class=\"line\">                node.getLabels());</span><br><span class=\"line\">            .....            </span><br><span class=\"line\">            return assignment;</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            break;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return NULL_ASSIGNMENT;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看LeafQueue#allocateResource 更新信息时更新的指标和内容\nLeafQueue 和ParentQueue  一样,使用抽象父类AbstractCSQueue#allocateResource() 更新当前队列的 numContainer 和 标签层面的 ResourceUsage<br>然后使用 LeafQueue 中维护的Map&lt;String,User&gt;成员变量 users来维护用户标签层面的资源信息 \n[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void allocateResource(Resource clusterResource,</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Resource resource,</span><br><span class=\"line\">      Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;使用的是 AbstractCSQueue#allocateResource,和 ParentQueue更新的指标及内容相同:numContainer,ResourceUsageByLabel</span><br><span class=\"line\">    super.allocateResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;更新用户层面的 ResourceUsage</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.assignContainer(resource, nodeLabels);</span><br><span class=\"line\">    Resources.subtractFrom(application.getHeadroom(), resource); &#x2F;&#x2F; headroom</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>User 类中有一个 ResourceUsage,维护着在用户层面 各标签的资源用途和用量<br>[User.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static class User &#123;</span><br><span class=\"line\">    ResourceUsage userResourceUsage &#x3D; new ResourceUsage();</span><br><span class=\"line\">    volatile Resource userResourceLimit &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    int pendingApplications &#x3D; 0;</span><br><span class=\"line\">    int activeApplications &#x3D; 0;</span><br><span class=\"line\">     public void assignContainer(Resource resource,</span><br><span class=\"line\">        Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">        userResourceUsage.incUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.incUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上文 LeafQueue#assignContainers方法 遍历每个活跃的 application,尝试在当前 nodemanager 上分配资源,调用 assignContainersOnNode()方法进行下一步的分配逻辑,并增加 app 在本地化层面分配的 container 数量,此指标逻辑不在此讨论<br>按照本地性优先级,优先分配  NODE_LOCAL(本节点)&gt;RACK_LOCAL(本机架)&gt;OFF_SWITCH(跨机架)<br>YARN 的本地性不同于 MR/SPARK 的本地性,YARN的本地性性体现在 Client/AM申请 container 的时候可以指定 container 所在的节点,此处的 NODE_LOCAL、RACK_LOCAL 和 OFF_SWITCH 是相对于申请 container 时指定的节点而言;数据本地化计算其实更多的是靠计算框架配合,按照数据所处的位置优先分配给计算节点(e.g. RDD#getPreferredLocations)<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private CSAssignment assignContainersOnNode(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerNode node, FiCaSchedulerApp application, Priority priority,</span><br><span class=\"line\">      RMContainer reservedContainer, ResourceLimits currentResoureLimits) &#123;</span><br><span class=\"line\">    Resource assigned &#x3D; Resources.none();</span><br><span class=\"line\">    NodeType requestType &#x3D; null;</span><br><span class=\"line\">    MutableObject allocatedContainer &#x3D; new MutableObject();</span><br><span class=\"line\">    &#x2F;&#x2F; Data-local</span><br><span class=\"line\">    ResourceRequest nodeLocalResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, node.getNodeName());</span><br><span class=\"line\">    if (nodeLocalResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      requestType &#x3D; NodeType.NODE_LOCAL;</span><br><span class=\"line\">      assigned &#x3D;</span><br><span class=\"line\">          assignNodeLocalContainers(clusterResource, nodeLocalResourceRequest, </span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (Resources.greaterThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">          assigned, Resources.none())) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F;update locality statistics</span><br><span class=\"line\">        if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">          application.incNumAllocatedContainers(NodeType.NODE_LOCAL,</span><br><span class=\"line\">            requestType);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return new CSAssignment(assigned, NodeType.NODE_LOCAL);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Rack-local</span><br><span class=\"line\">    ResourceRequest rackLocalResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, node.getRackName());</span><br><span class=\"line\">    if (rackLocalResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      if (!rackLocalResourceRequest.getRelaxLocality()) &#123;</span><br><span class=\"line\">        return SKIP_ASSIGNMENT;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (requestType !&#x3D; NodeType.NODE_LOCAL) &#123;</span><br><span class=\"line\">        requestType &#x3D; NodeType.RACK_LOCAL;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      assigned &#x3D; </span><br><span class=\"line\">          assignRackLocalContainers(clusterResource, rackLocalResourceRequest, </span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (Resources.greaterThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">          assigned, Resources.none())) &#123;</span><br><span class=\"line\">        if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">          application.incNumAllocatedContainers(NodeType.RACK_LOCAL,</span><br><span class=\"line\">            requestType);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return new CSAssignment(assigned, NodeType.RACK_LOCAL);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Off-switch</span><br><span class=\"line\">    ResourceRequest offSwitchResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, ResourceRequest.ANY);</span><br><span class=\"line\">    if (offSwitchResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      if (!offSwitchResourceRequest.getRelaxLocality()) &#123;</span><br><span class=\"line\">        return SKIP_ASSIGNMENT;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (requestType !&#x3D; NodeType.NODE_LOCAL</span><br><span class=\"line\">          &amp;&amp; requestType !&#x3D; NodeType.RACK_LOCAL) &#123;</span><br><span class=\"line\">        requestType &#x3D; NodeType.OFF_SWITCH;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      assigned &#x3D;</span><br><span class=\"line\">          assignOffSwitchContainers(clusterResource, offSwitchResourceRequest,</span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">        application.incNumAllocatedContainers(NodeType.OFF_SWITCH, requestType);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return new CSAssignment(assigned, NodeType.OFF_SWITCH);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return SKIP_ASSIGNMENT;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>LeafQueue#assignNodeLocalContainers,assignRackLocalContainers,assignOffSwitchContainers的核心逻辑被封装为一处,只是本地化类型 NodeType 不同<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, </span><br><span class=\"line\">      FiCaSchedulerApp application, Priority priority, </span><br><span class=\"line\">      ResourceRequest request, NodeType type, RMContainer rmContainer,</span><br><span class=\"line\">      MutableObject createdContainer, ResourceLimits currentResoureLimits) &#123;</span><br><span class=\"line\">    ....    </span><br><span class=\"line\">    Resource capability &#x3D; request.getCapability();</span><br><span class=\"line\">    Resource available &#x3D; node.getAvailableResource();</span><br><span class=\"line\">    Resource totalResource &#x3D; node.getTotalResource();</span><br><span class=\"line\">    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,</span><br><span class=\"line\">        capability, totalResource)) &#123;</span><br><span class=\"line\">      LOG.warn(&quot;Node : &quot; + node.getNodeID()</span><br><span class=\"line\">          + &quot; does not have sufficient resource for request : &quot; + request</span><br><span class=\"line\">          + &quot; node total capability : &quot; + node.getTotalResource());</span><br><span class=\"line\">      return Resources.none();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Create the container if necessary</span><br><span class=\"line\">    Container container &#x3D; </span><br><span class=\"line\">        getContainer(rmContainer, application, node, capability, priority);</span><br><span class=\"line\">  \t...</span><br><span class=\"line\">      boolean shouldAllocOrReserveNewContainer &#x3D; shouldAllocOrReserveNewContainer(</span><br><span class=\"line\">        application, priority, capability);</span><br><span class=\"line\">    &#x2F;&#x2F;依据 节点可用资源量与申请的资源量 做除法来判断节点剩余资源能否满足需求 </span><br><span class=\"line\">    int availableContainers &#x3D; </span><br><span class=\"line\">        resourceCalculator.computeAvailableContainers(available, capability);</span><br><span class=\"line\">    boolean needToUnreserve &#x3D; Resources.greaterThan(resourceCalculator,clusterResource,</span><br><span class=\"line\">        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());</span><br><span class=\"line\">\t</span><br><span class=\"line\">    if (availableContainers &gt; 0) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;如果节点上足够分配一个 container 则分配</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#x2F;&#x2F;调用 application 和 nodemanager 的方法,触发这两处的信息同步</span><br><span class=\"line\">      RMContainer allocatedContainer &#x3D; </span><br><span class=\"line\">          application.allocate(type, node, priority, request, container);</span><br><span class=\"line\">          </span><br><span class=\"line\">      node.allocateContainer(allocatedContainer);</span><br><span class=\"line\">      </span><br><span class=\"line\">      createdContainer.setValue(allocatedContainer);</span><br><span class=\"line\">      return container.getResource();</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    \t....</span><br><span class=\"line\">        &#x2F;&#x2F;节点上没有足够的资源满足需求,则为该 application 保留该节点的资源</span><br><span class=\"line\">        reserve(application, priority, node, rmContainer, container);</span><br><span class=\"line\">        return request.getCapability();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return Resources.none();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上文有三处地方需要同步资源信息:reserve()方法,FiCaSchedulerApp#allocate,FiCaSchedulerNode#allocateContainer<br>先看FiCaSchedulerApp#allocate方法,在 application 层面修改了什么信息<br>[FiCaSchedulerApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public RMContainer allocate(NodeType type, FiCaSchedulerNode node,</span><br><span class=\"line\">     Priority priority, ResourceRequest request, </span><br><span class=\"line\">     Container container) &#123;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">   &#x2F;&#x2F;将 container 封装成 RMContainer 的形式,记录在 application 的所有 container 集合 newlyAllocatedContainers 和 运行态(相对 reserved)的 container 集合 liveContainers 中  </span><br><span class=\"line\">   RMContainer rmContainer &#x3D; new RMContainerImpl(container, this</span><br><span class=\"line\">       .getApplicationAttemptId(), node.getNodeID(),</span><br><span class=\"line\">       appSchedulingInfo.getUser(), this.rmContext);</span><br><span class=\"line\">   newlyAllocatedContainers.add(rmContainer);</span><br><span class=\"line\">   liveContainers.put(container.getId(), rmContainer);    </span><br><span class=\"line\">   ....</span><br><span class=\"line\">   &#x2F;&#x2F;更新 metrics 信息</span><br><span class=\"line\">   List&lt;ResourceRequest&gt; resourceRequestList &#x3D; appSchedulingInfo.allocate(</span><br><span class=\"line\">       type, node, priority, request, container);</span><br><span class=\"line\">    &#x2F;&#x2F;当前 app 消费的资源量</span><br><span class=\"line\">   Resources.addTo(currentConsumption, container.getResource());</span><br><span class=\"line\">   ...</span><br><span class=\"line\">   &#x2F;&#x2F;触发 RMContainer 状态机变化,container 可以准备运行时环境,下载依赖等</span><br><span class=\"line\">   rmContainer.handle(</span><br><span class=\"line\">       new RMContainerEvent(container.getId(), RMContainerEventType.START));</span><br><span class=\"line\">   RMAuditLogger.logSuccess(getUser(), </span><br><span class=\"line\">       AuditConstants.ALLOC_CONTAINER, &quot;SchedulerApp&quot;, </span><br><span class=\"line\">       getApplicationId(), container.getId());</span><br><span class=\"line\">   return rmContainer;</span><br></pre></td></tr></table></figure>\n<p>AppSchedulingInfo#allocate 更新 QueueMetrics 信息<br>[AppSchedulingInfo.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public List&lt;ResourceRequest&gt; allocate(NodeType type,</span><br><span class=\"line\">      SchedulerNode node, Priority priority, ResourceRequest request,</span><br><span class=\"line\">      Container container) &#123;</span><br><span class=\"line\">    List&lt;ResourceRequest&gt; resourceRequests &#x3D; new ArrayList&lt;ResourceRequest&gt;();</span><br><span class=\"line\">    &#x2F;&#x2F;本地化层面的统计信息</span><br><span class=\"line\">    if (type &#x3D;&#x3D; NodeType.NODE_LOCAL) &#123;</span><br><span class=\"line\">      allocateNodeLocal(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125; else if (type &#x3D;&#x3D; NodeType.RACK_LOCAL) &#123;</span><br><span class=\"line\">      allocateRackLocal(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      allocateOffSwitch(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics metrics &#x3D; queue.getMetrics();</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &#x2F;&#x2F;对QueueMetrics 做已经分配的资源的统计</span><br><span class=\"line\">    metrics.allocateResources(user, 1, request.getCapability(), true);</span><br><span class=\"line\">    metrics.incrNodeTypeAggregations(user, type);</span><br><span class=\"line\">    return resourceRequests;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>QueueMetrics#allocateResource 对 userMetrics 做判断;对 parent 也做了判断 如果parent 非空那么会递归更新 parent 的信息,对userMetrics 和 parent 执行的方法都是 QueueMetrics#allocateResources,更新的指标一样:增加 allocate 的 container 数量,内存量,虚拟核数量<br>[QueueMetrics.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void allocateResources(String user, int containers, Resource res,</span><br><span class=\"line\">      boolean decrPending) &#123;</span><br><span class=\"line\">    allocatedContainers.incr(containers);</span><br><span class=\"line\">    aggregateContainersAllocated.incr(containers);</span><br><span class=\"line\">    allocatedMB.incr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.incr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    if (decrPending) &#123;</span><br><span class=\"line\">      _decrPendingResources(containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>FiCaSchedulerNode#allocateContainer, NodeManager 同步 allocate 信息 \n[FiCaSchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void allocateContainer(RMContainer rmContainer) &#123;</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">     &#x2F;&#x2F;减少当前 nm 的可用资源,增加当前 nm 的已分配资源</span><br><span class=\"line\">    deductAvailableResource(container.getResource());</span><br><span class=\"line\">    &#x2F;&#x2F;增加自身维护的 numContainer 数量,不同于 AbstractCSQueue.numContainers,维护Queue维度和 NodeManager 维度的 container 数量</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    &#x2F;&#x2F;启动的 container 列表</span><br><span class=\"line\">    launchedContainers.put(container.getId(), rmContainer);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void deductAvailableResource(Resource resource) &#123;</span><br><span class=\"line\">    Resources.subtractFrom(availableResource, resource);</span><br><span class=\"line\">    Resources.addTo(usedResource, resource);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>如果在节点上 reserve 了资源,同步的信息和 allocate 的方式相似,会更新 QueueMetrics 对 reserved[containr|memory|core]的信息,更新application reservedContainer 及 currentReservationMemroy 信息,更新 NodeManager 当前 reservedContainer 信息<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private void reserve(FiCaSchedulerApp application, Priority priority, </span><br><span class=\"line\">      FiCaSchedulerNode node, RMContainer rmContainer, Container container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;传入的 rmContainer 为 null</span><br><span class=\"line\">    if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      getMetrics().reserveResource(</span><br><span class=\"line\">          application.getUser(), container.getResource());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Inform the application </span><br><span class=\"line\">    rmContainer &#x3D; application.reserve(node, priority, rmContainer, container);</span><br><span class=\"line\">    &#x2F;&#x2F; Update the node</span><br><span class=\"line\">    node.reserveResource(application, priority, rmContainer);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>QueueMetrics#reserveResource在下方有一个对 parent 的判断,递归调用依次更新 ParentQueue 的信息;有一个对 userMetrics 的判断,对 QueueMetrics 中维护的 users:Map[String, QueueMetrics] 同步用户层面的统计信息,执行的方法都是当前方法QueueMetrics#reserveResource(),同步reserved 资源信息:增加 reserved container 数量,内存量,虚拟核数量  </p>\n<p>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void reserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.incr();</span><br><span class=\"line\">    reservedMB.incr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.incr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>application 同步 reserved container 信息<br>[SchedulerApplicationAttempt.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized RMContainer reserve(SchedulerNode node, Priority priority,</span><br><span class=\"line\">     RMContainer rmContainer, Container container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;上游传入的 rmContainer 为 null</span><br><span class=\"line\">   if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     rmContainer &#x3D; </span><br><span class=\"line\">         new RMContainerImpl(container, getApplicationAttemptId(), </span><br><span class=\"line\">             node.getNodeID(), appSchedulingInfo.getUser(), rmContext);</span><br><span class=\"line\">     &#x2F;&#x2F;增加当前 application 保留的资源信息</span><br><span class=\"line\">     Resources.addTo(currentReservation, container.getResource());</span><br><span class=\"line\">     resetReReservations(priority);</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; Note down the re-reservation</span><br><span class=\"line\">     addReReservation(priority);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;在 container 信息中设置了绑定节点的信息....</span><br><span class=\"line\">   rmContainer.handle(new RMContainerReservedEvent(container.getId(), </span><br><span class=\"line\">       container.getResource(), node.getNodeID(), priority));</span><br><span class=\"line\">       &#x2F;&#x2F;加入到维护的 reservedContainer 信息中</span><br><span class=\"line\">   Map&lt;NodeId, RMContainer&gt; reservedContainers &#x3D; </span><br><span class=\"line\">       this.reservedContainers.get(priority);</span><br><span class=\"line\">   if (reservedContainers &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     reservedContainers &#x3D; new HashMap&lt;NodeId, RMContainer&gt;();</span><br><span class=\"line\">     this.reservedContainers.put(priority, reservedContainers);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   reservedContainers.put(node.getNodeID(), rmContainer);</span><br><span class=\"line\">   return rmContainer;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>FicaSchedulerNode#reserveResource(),NodeManager 同步 reserved container 信息<br>[FicaSchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void reserveResource(</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Priority priority,</span><br><span class=\"line\">      RMContainer container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Check if it&#39;s already reserved</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; getReservedContainer();</span><br><span class=\"line\">    ....空值校验性,重复保留校验等操作</span><br><span class=\"line\">    &#x2F;&#x2F;设置当前节点保留的 container 为传入值</span><br><span class=\"line\">    setReservedContainer(container);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n\n<blockquote>\n<p>综上:<br>        CS 调度器在 NM 上为 APP 分配 container 时,会同步 [Parent | Leaf] Queue/APP/NM 三个位置维护的信息<br>    1.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, 其 <b>ParentQueue</b>都会维护:<br>&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ParentQueue <b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>    2.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, <b>LeafQueue</b> 本身都会维护:<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>    3.在节点上申请资源时,按照节点剩余可用资源和资源需求量做除法,若满足需求则分配为 allocatedContainer,不满足则分配 reservedContainer<br>    4.若在节点上分配 allocatedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp; Application 方面:newlyAllocatedContainers和 livingContainers 列表,app 已占用资源量<br>&ensp;&ensp;&ensp;&ensp; NodeManager 方面:numContainer 数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表<br>&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身QueueMetrics</b>的allocated [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度 QueueMetrics</b> 的 allocated [containerNum | MB | cores]信息<br>    5.若在节点上分配 reservedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量<br>&ensp;&ensp;&ensp;&ensp;NodeManager 方面:设置 NM 保留的 container 为本次的 reservedContainer<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  </p>\n</blockquote>\n<h3 id=\"资源释放\"><a href=\"#资源释放\" class=\"headerlink\" title=\"资源释放\"></a>资源释放</h3><p>以客户端在命令行执行 “yarn application –kill ${APPLICATION_ID}”为例<br>RM 中响应请求 方法调用栈为:<br>-&gt;ClientRMService#forceKillApplication<br>&ensp;&ensp;-&gt;RMAppImpl$KillAttemptTransition#transition<br>&ensp;&ensp;&ensp;&ensp;-&gt;RMAppAttemptImpl$BaseFinalTransition#transition<br>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;CapacityScheduler#handle()   </p>\n<p>application 结束时,主要做了三件事情,  </p>\n<ol>\n<li>Container: 释放 APP 持有的<b> living,reserved </b> container  </li>\n<li>Application(QueueMetrcis): 清理 LeafQueue 及其 ParentQueue 在 <b>APP</b> 层面的统计(e.g. appRunnings);清理 LeafQueue 在<b>用户APP</b> 层面的统计  </li>\n<li>AM(ResourceUsage): 释放 LeafQueue 及其 ParentQueue 在 <b>AM</b> 层面的资源用量;  释放 LeafQueue 在<b>用户AM</b> 层面的统计(AMUSED)  </li>\n</ol>\n<p>由于复现指标异常成功后,发现移动到目标队列后,目标队列增加的资源总量是 spark.executor.memory 的整数倍(executor-mem 11G,driver-memory 2G),所以对上述的第二点和第三点不多分析,主要关注 <b>非 AM 的container</b> 的释放,livingContainers 和 reservedContainer 的释放代码是同一个,仅仅是释放 container 的文字性说明(原文: diagnostics 意为诊断)不同<br>[CapactiyScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">....</span><br><span class=\"line\"> case APP_ATTEMPT_REMOVED:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent &#x3D;</span><br><span class=\"line\">          (AppAttemptRemovedSchedulerEvent) event;</span><br><span class=\"line\">      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),</span><br><span class=\"line\">        appAttemptRemovedEvent.getFinalAttemptState(),</span><br><span class=\"line\">        appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private synchronized void doneApplicationAttempt(</span><br><span class=\"line\">      ApplicationAttemptId applicationAttemptId,</span><br><span class=\"line\">      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    FiCaSchedulerApp attempt &#x3D; getApplicationAttempt(applicationAttemptId);</span><br><span class=\"line\">    SchedulerApplication&lt;FiCaSchedulerApp&gt; application &#x3D;</span><br><span class=\"line\">        applications.get(applicationAttemptId.getApplicationId());</span><br><span class=\"line\">        ....</span><br><span class=\"line\">    &#x2F;&#x2F;从 app 维护的 livingContainer 列表中,释放掉 allocated acquired running 状态的 container</span><br><span class=\"line\">    for (RMContainer rmContainer : attempt.getLiveContainers()) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;在 kill 时keepContainer 为 false;在 failed 时,keepContainer 按场景可为 true</span><br><span class=\"line\">      if (keepContainers</span><br><span class=\"line\">          &amp;&amp; rmContainer.getState().equals(RMContainerState.RUNNING)) &#123;</span><br><span class=\"line\">        continue;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;1.释放 container</span><br><span class=\"line\">      completedContainer(</span><br><span class=\"line\">        rmContainer,</span><br><span class=\"line\">        SchedulerUtils.createAbnormalContainerStatus(</span><br><span class=\"line\">        &#x2F;&#x2F;COMPLETED_APPLICATION:Container of a completed application</span><br><span class=\"line\">          rmContainer.getContainerId(), SchedulerUtils.COMPLETED_APPLICATION),</span><br><span class=\"line\">        RMContainerEventType.KILL);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    &#x2F;&#x2F;从 app 维护的 reservedContainer 列表中,释放掉 reserved 状态的 container</span><br><span class=\"line\">    for (RMContainer rmContainer : attempt.getReservedContainers()) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;1.释放 container</span><br><span class=\"line\">      completedContainer(</span><br><span class=\"line\">        rmContainer,</span><br><span class=\"line\">        SchedulerUtils.createAbnormalContainerStatus(</span><br><span class=\"line\">          rmContainer.getContainerId(), &quot;Application Complete&quot;),</span><br><span class=\"line\">        RMContainerEventType.KILL);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;2. 清理 pending 的 resourceRequest,并同步 [LeafQueue | ParentQueue] [本身 | 用户]维度的 QueueMetrics appRunnings | appPendings </span><br><span class=\"line\">    attempt.stop(rmAppAttemptFinalState);</span><br><span class=\"line\">    String queueName &#x3D; attempt.getQueue().getQueueName();</span><br><span class=\"line\">    CSQueue queue &#x3D; queues.get(queueName);</span><br><span class=\"line\">    if (!(queue instanceof LeafQueue)) &#123;</span><br><span class=\"line\">      LOG.error(&quot;Cannot finish application &quot; + &quot;from non-leaf queue: &quot;</span><br><span class=\"line\">          + queueName);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;3. 同步 [LeafQueue | ParentQueue] [本身 | 用户]维度 QueueUsage 的 AMUSED 指标</span><br><span class=\"line\">      queue.finishApplicationAttempt(attempt, queue.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>CapacityScheduler#completedContainer主要做了一些校验,避免无效释放<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> protected synchronized void completedContainer(RMContainer rmContainer,</span><br><span class=\"line\">    ContainerStatus containerStatus, RMContainerEventType event) &#123;</span><br><span class=\"line\">  if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">    LOG.info(&quot;Null container completed...&quot;);</span><br><span class=\"line\">    return;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">  FiCaSchedulerApp application &#x3D;</span><br><span class=\"line\">      getCurrentAttemptForContainer(container.getId());</span><br><span class=\"line\">  ApplicationId appId &#x3D;</span><br><span class=\"line\">      container.getId().getApplicationAttemptId().getApplicationId();</span><br><span class=\"line\">  if (application &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">    LOG.info(&quot;Container &quot; + container + &quot; of&quot; + &quot; unknown application &quot;</span><br><span class=\"line\">        + appId + &quot; completed with event &quot; + event);</span><br><span class=\"line\">    return;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;container 所在的 NM视图</span><br><span class=\"line\">  FiCaSchedulerNode node &#x3D; getNode(container.getNodeId());</span><br><span class=\"line\">   &#x2F;&#x2F;代理 container 的 LeafQueue</span><br><span class=\"line\">  LeafQueue queue &#x3D; (LeafQueue)application.getQueue();</span><br><span class=\"line\">  queue.completedContainer(clusterResource, application, node, </span><br><span class=\"line\">      rmContainer, containerStatus, event, null, true);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>先按照 RMContainer 状态机状态判断后续将当前 container 按照 reservedContainer处理或者 allocatedContainer 处理<br>然后若释放 container 成功,则同步 用户和标签维度 QueueUsage的 USED 指标,减少 Queue 运行的 container 数量<br>最后若当前 Queue 非 rootQueue,则递归同步当前 Queue 的所有 ParentQueue做上述处理<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void completedContainer(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, FiCaSchedulerNode node, RMContainer rmContainer, </span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event, CSQueue childQueue,</span><br><span class=\"line\">      boolean sortQueues) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      boolean removed &#x3D; false;</span><br><span class=\"line\">      synchronized (this) &#123;</span><br><span class=\"line\">        Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">\t&#x2F;&#x2F; 1. 若 contaienr 状态机为 reserved 状态,在节点上解除 reserve</span><br><span class=\"line\">        if (rmContainer.getState() &#x3D;&#x3D; RMContainerState.RESERVED) &#123;</span><br><span class=\"line\">          removed &#x3D; unreserve(application, rmContainer.getReservedPriority(),</span><br><span class=\"line\">              node, rmContainer);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 2.若 container 状态机不为 reserved 状态,在节点上解除 allocate,并使 NM 释放 container</span><br><span class=\"line\">          removed &#x3D;</span><br><span class=\"line\">            application.containerCompleted(rmContainer, containerStatus, event);</span><br><span class=\"line\">          node.releaseContainer(container);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F;3.释放 同步 QueueUsage</span><br><span class=\"line\">        if (removed) &#123;</span><br><span class=\"line\">          releaseResource(clusterResource, application,</span><br><span class=\"line\">              container.getResource(), node.getLabels());</span><br><span class=\"line\">          LOG.info(&quot;completedContainer&quot; +</span><br><span class=\"line\">              &quot; container&#x3D;&quot; + container +</span><br><span class=\"line\">              &quot; queue&#x3D;&quot; + this +</span><br><span class=\"line\">              &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;4.递归执行 Parent的释放逻辑</span><br><span class=\"line\">      if (removed) &#123;</span><br><span class=\"line\">        getParent().completedContainer(clusterResource, application, node,</span><br><span class=\"line\">          rmContainer, null, event, this, sortQueues);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看第 4 处对 ParentQueue 的处理<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void completedContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, FiCaSchedulerNode node, </span><br><span class=\"line\">      RMContainer rmContainer, ContainerStatus containerStatus, </span><br><span class=\"line\">      RMContainerEventType event, CSQueue completedChildQueue,</span><br><span class=\"line\">      boolean sortQueues) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      synchronized (this) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;使用 AbstractCSQueue 中releaseResource方法</span><br><span class=\"line\">        super.releaseResource(clusterResource, rmContainer.getContainer()</span><br><span class=\"line\">            .getResource(), node.getLabels());</span><br><span class=\"line\">\t...</span><br><span class=\"line\">        &#x2F;&#x2F;resort sub-queue</span><br><span class=\"line\">     \t....</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; Inform the parent</span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\"> \t&#x2F;&#x2F;递归 ParentQueue#completedContainer 方法</span><br><span class=\"line\">        parent.completedContainer(clusterResource, application, </span><br><span class=\"line\">            node, rmContainer, null, event, this, sortQueues);</span><br><span class=\"line\">      &#125;    </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 在 container 释放时,无论是 allocatedContainer 还是 reservedContainer,都更新: \n1.按标签更新 QueueUsage 中 USED 用途的资源量<br>2.ParentQueue 中运行 container 的数量 \n[AbstractCSQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;1.QueueUsageByLabel</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">        &#x2F;&#x2F;2.减少当前 Queue 的 container 的数量</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看第 3 处对 LeafQueue 的处理, 需要注意的是:无论 container 是 allocatedContainer 还是 reversedContainer 在此处都是作为ResourceUsage中 <b>USED</b> 类型的资源来减少的(虽然 ResourceType 枚举中有 RESERVED 类型),且 numContainer 都减少了 1个单位<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void releaseResource(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;1.同步当前队列在 标签维度 ResourceUsage 中 USED 类型的资源,并减少 Queue 中 numContainer</span><br><span class=\"line\">    super.releaseResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;2.同步 app 所属用户在标签维度上的度量信息  </span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.releaseContainer(resource, nodeLabels);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[AbstractCSQueue.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;1.同步 label 层面的 QueueUsage 的资源量;无论 reservedContainer 还是 allocatedContainer 都是使用 #decUsed(resource)</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">      &#x2F;&#x2F;2.减少当前队列启动的 container 数量.无论 reservedContainer 还是 allocatedContainer  </span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>LeafQueue 中维护了一个 users:Map[String,User]以保存当前各 user 在 USED,PENDING,AMUSED,RESERVED 用途使用资源的度量<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseContainer(Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;还是 decUsed(resource),无论 reservedContainer 还是 allocatedContainer</span><br><span class=\"line\">        userResourceUsage.decUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.decUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>在回到LeafQueue#completedContainer方法,按照 RMContainer状态机状态来区分 reservedContainer 还是 allocatedContainer 做处理<br>先看对 allocated 的处理逻辑<br>[FicaSchedulerApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public boolean containerCompleted(RMContainer rmContainer,</span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;维护相应的引用列表</span><br><span class=\"line\">    if (null &#x3D;&#x3D; liveContainers.remove(rmContainer.getContainerId())) &#123;</span><br><span class=\"line\">      return false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    newlyAllocatedContainers.remove(rmContainer);</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">    ContainerId containerId &#x3D; container.getId();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F;1.RM 端准备 container 的 结束和清理等任务,待 NM 心跳通过 NodeHeartbeatResponse 交于 NM 做清理</span><br><span class=\"line\">    rmContainer.handle(</span><br><span class=\"line\">        new RMContainerFinishedEvent(</span><br><span class=\"line\">            containerId,</span><br><span class=\"line\">            containerStatus, </span><br><span class=\"line\">            event)</span><br><span class=\"line\">        );</span><br><span class=\"line\">    ... </span><br><span class=\"line\">    &#x2F;&#x2F;2.同步 QueueMetrics</span><br><span class=\"line\">    Resource containerResource &#x3D; rmContainer.getContainer().getResource();</span><br><span class=\"line\">    queue.getMetrics().releaseResources(getUser(), 1, containerResource);</span><br><span class=\"line\">    &#x2F;&#x2F;3.减少当前 app 消费的资源</span><br><span class=\"line\">    Resources.subtractFrom(currentConsumption, containerResource);</span><br><span class=\"line\">    ....</span><br><span class=\"line\">    return true;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>主要看第2步 释放 contaienr 同步 QueueMetrics 的指标和内容,<br>1.同步 QueueMetrics 及其 ParentQueue QueueMetrics 的指标: allocate [containerNum | MB | core]<br>2.同步 QueueMetrics 及其 ParentQueue QueueMetrics 用户维度的 QueueMetrics 指标: allocate [containerNum | MB | core]<br>[QueueMetrics.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseResources(String user, int containers, Resource res) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;当前资源队列指标</span><br><span class=\"line\">    allocatedContainers.decr(containers);</span><br><span class=\"line\">    aggregateContainersReleased.incr(containers);</span><br><span class=\"line\">    allocatedMB.decr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.decr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    &#x2F;&#x2F;资源队列中用户指标</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;递归 父资源队列</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看释放allocatedContainer 过程中,NM 对该 container 的处理<br>1.删掉 launchedContainer 对该 container 的引用<br>2.增加该节点的可用资源,减少已经资源,减少启动的 numContainer 指标<br>[SchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void releaseContainer(Container container) &#123;</span><br><span class=\"line\">    ..</span><br><span class=\"line\">    &#x2F;&#x2F;1.删除引用</span><br><span class=\"line\">    if (null !&#x3D; launchedContainers.remove(container.getId())) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;2.指标维护</span><br><span class=\"line\">      updateResource(container);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Released container &quot; + container.getId() + &quot; of capacity &quot;</span><br><span class=\"line\">        + container.getResource() + &quot; on host &quot; + rmNode.getNodeAddress()</span><br><span class=\"line\">        + &quot;, which currently has &quot; + numContainers + &quot; containers, &quot;</span><br><span class=\"line\">        + getUsedResource() + &quot; used and &quot; + getAvailableResource()</span><br><span class=\"line\">        + &quot; available&quot; + &quot;, release resources&#x3D;&quot; + true);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void updateResource(Container container) &#123;</span><br><span class=\"line\">    addAvailableResource(container.getResource());</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void addAvailableResource(Resource resource) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    Resources.addTo(availableResource, resource);</span><br><span class=\"line\">    Resources.subtractFrom(usedResource, resource);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>释放 reservedContainer,对 reversedContainer 的处理<br>1.application 方面:删除对 reservedContainer 的引用,减少 currentReservation 资源数<br>2.nodemanager方面: 设置当前 nm 的reservedContainer 为 null\n3.QueueMetrics 方面: 同步[ParentQueue | LeafQueue] [本身 | 用户]维度的指标 reserved [containerNum | MB | core]<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean unreserve(FiCaSchedulerApp application, Priority priority,</span><br><span class=\"line\">      FiCaSchedulerNode node, RMContainer rmContainer) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;1.application: unreserve</span><br><span class=\"line\">    if (application.unreserve(node, priority)) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;2. nodemanager: unreserveResource</span><br><span class=\"line\">      node.unreserveResource(application);</span><br><span class=\"line\">     &#x2F;&#x2F;3.QueueMetrics</span><br><span class=\"line\">      getMetrics().unreserveResource(application.getUser(),</span><br><span class=\"line\">          rmContainer.getContainer().getResource());</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>application 和 nodemanager 层面的逻辑比较简单,且看 QueueMetrics 方面的同步<br>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void unreserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">  &#x2F;&#x2F;当前资源队列 QueueMetrics的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    reservedContainers.decr();</span><br><span class=\"line\">    reservedMB.decr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.decr(res.getVirtualCores());</span><br><span class=\"line\">    &#x2F;&#x2F;当前资源队列 在用户维度的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;父资源队列在 其本身和用户维度的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n</blockquote>\n<p>综上:<br>CS 调度器在为 APP 释放container 时,会同步 [Parent | Leaf]Queue/APP/NM 三个位置维护的信息  </p>\n<ol>\n<li>释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>ParentQueue</b> 都会维护:\n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的<b>标签维度</b>的QueueUsage 中 USED 用途的资源量  </li>\n<li>释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>LeafQueue</b> 都会维护:<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  </li>\n<li>释放 container 时,按照该 container 的状态机状态做细分,分为 reservedContainer 和 allocatedContainer 两类做处理  </li>\n<li>若在节点上释放 allocatedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:newlyAllocatedContainers 和 reservedContainers 列表,app已占用资源量<br>&ensp;&ensp;&ensp;&ensp;NodeManager方面:numContainer数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 和所有 ParentQueue <b>自身 QueueMetrics</b> 的 reserved [containerNum | MB | core]信息;LeafQueue 和所有 ParentQueue 在<b>用户维度 QueueMetrics </b> 的  reserved [containerNum | MB | core]信息  </li>\n<li>若在节点上释放 reservedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量<br>&ensp;&ensp;&ensp;&ensp; NodeManager 方面:设置 NM 保留的 container 为 null<br>&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  </li>\n</ol>\n<p>Container 释放过程和申请过程 对于 Queue/Application/NM三个位置维护的 Queue/QueueUsageByLabel/QueueUsageByUser/QueueMetrics信息,恰好是一一对应的  </p>\n<p>对于 YarnUI 指标异常问题,比较关注的 Queue.numContainer 和 QueueUsageByLabel.resArr[ResourceType.USED]两个指标,一次 allocated/reversed/Container 的申请对应着 numContainer++和 ResourceUsage.incUsed;一次 container 的释放对应着 numContainer–和 ResourceUsage.decUsed   </p>\n<h3 id=\"movetoqueue-时资源转移过程\"><a href=\"#movetoqueue-时资源转移过程\" class=\"headerlink\" title=\"movetoqueue 时资源转移过程\"></a>movetoqueue 时资源转移过程</h3><p>使用命令 “yarn application -movetoqueue ${APPID} -queue ${TO}”\n可以将 ${APPID}的 APP 移动到目标资源队列 ${TO}<br>ClientRMService 相应请求的调用栈是:<br>-&gt;ClientRMService#moveApplicationAcrossQueues<br>&ensp;&ensp;-&gt;RMAppImpl$RMAppMoveTransition#transition<br>&ensp;&ensp;&ensp;&ensp;-&gt;CapacityScheduler#moveApplication  </p>\n<p>application 从源资源队列移动到目标资源队列分为几步:  </p>\n<ol>\n<li>向目标队列提交application 并做校验,包括 access 权限、目标队列的 app 数量超限、目标队列中用户提交 app 数量超限,若提交成功则增加所有 ParentQueue 的 numApplication 数量(但是没有增加自身的 numApplication 数量)    </li>\n<li>转移源队列中所有非 reserved 的 container 到目标队列,源队列(LeafQueue)释放 container,减少<b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并减少源队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量;增加目标队列(LeafQueue) <b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并增加目标队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量.指标数量维护在Queue/ResourceUsage/QueueMetrics 中,此处没有处理 livingContainers 的 QueueMetrics ,在第 5 步中统一处理 QueueMetrics    </li>\n<li>源队列中移除 appAttemptd,并同步 QueueUsageByUser 和 QueueUsageByLabel 中 AMUSED类型的资源量  </li>\n<li>在源队列的所有 ParentQueue 中移除此 Application    </li>\n<li>同步源队列和目标队列中所有非 AM container 移动导致的 QueueMetrics 变化  </li>\n<li>同步用户维度的 appAttempt 统计信息<br>[CapacityScheduler.java]  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized String moveApplication(ApplicationId appId,</span><br><span class=\"line\">     String targetQueueName) throws YarnException &#123;</span><br><span class=\"line\">   FiCaSchedulerApp app &#x3D;</span><br><span class=\"line\">       getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));</span><br><span class=\"line\">   String sourceQueueName &#x3D; app.getQueue().getQueueName();</span><br><span class=\"line\">   LeafQueue source &#x3D; getAndCheckLeafQueue(sourceQueueName);</span><br><span class=\"line\">   String destQueueName &#x3D; handleMoveToPlanQueue(targetQueueName);</span><br><span class=\"line\">   LeafQueue dest &#x3D; getAndCheckLeafQueue(destQueueName);</span><br><span class=\"line\">   String user &#x3D; app.getUser();</span><br><span class=\"line\">   try &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;1. 转移 Application</span><br><span class=\"line\">     dest.submitApplication(appId, user, destQueueName);</span><br><span class=\"line\">   &#125; catch (AccessControlException e) &#123;</span><br><span class=\"line\">     throw new YarnException(e);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;2.转移 livingContainer</span><br><span class=\"line\">   for (RMContainer rmContainer : app.getLiveContainers()) &#123;</span><br><span class=\"line\">     source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">     &#x2F;&#x2F; attach the Container to another queue</span><br><span class=\"line\">     dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;3.源资源队列移除attempt,并同步QueueUsage</span><br><span class=\"line\">   source.finishApplicationAttempt(app, sourceQueueName);</span><br><span class=\"line\">   &#x2F;&#x2F;4.源队列的父队列移除 application</span><br><span class=\"line\">   source.getParent().finishApplication(appId, app.getUser());</span><br><span class=\"line\">  &#x2F;&#x2F;5.同步源队列和目标队列的 QueueMetrics</span><br><span class=\"line\">   app.move(dest);</span><br><span class=\"line\">   &#x2F;&#x2F;6.提交attemp</span><br><span class=\"line\">   dest.submitApplicationAttempt(app, user);</span><br><span class=\"line\">   applications.get(appId).setQueue(dest);</span><br><span class=\"line\">   LOG.info(&quot;App: &quot; + app.getApplicationId() + &quot; successfully moved from &quot;</span><br><span class=\"line\">       + sourceQueueName + &quot; to: &quot; + destQueueName);</span><br><span class=\"line\">   return targetQueueName;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>第 2 步中,遍历 app 的 livingContainer,把每个 container 从源队列及父队列释放,减少源队列及父队列的指标<br>先看源队列 LeafQueue 的释放逻辑<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void detachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">      releaseResource(clusterResource, application, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; container&#x3D;&quot; + rmContainer.getContainer()</span><br><span class=\"line\">          + &quot; resource&#x3D;&quot; + rmContainer.getContainer().getResource()</span><br><span class=\"line\">          + &quot; queueMoveOut&#x3D;&quot; + this + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity()</span><br><span class=\"line\">          + &quot; absoluteUsedCapacity&#x3D;&quot; + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot;</span><br><span class=\"line\">          + queueUsage.getUsed() + &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;ParentQueue#detachContainer</span><br><span class=\"line\">      getParent().detachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  synchronized void releaseResource(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">     &#x2F;&#x2F;1.减少 标签维度的 ResourceUsage USED 用途的资源;并减少当前 Queue 的 numContainer 数量</span><br><span class=\"line\">    super.releaseResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;2.减少 提交用户在 标签维度的 ResourceUsage USED 用途的资源</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.releaseContainer(resource, nodeLabels);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">  &#x2F;&#x2F;ResourceUsageByLabel USED 用途的资源量</span><br><span class=\"line\">  protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">        &#x2F;&#x2F;Queue 的运行 container 数量</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F;第 2 处,ResourceUsageByUser USED 用途的资源量</span><br><span class=\"line\">public void releaseContainer(Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">        userResourceUsage.decUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.decUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 释放 container 时,减少了用户维度的 ResourceUsage 在 USED 用途上资源量<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void detachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">      super.releaseResource(clusterResource,</span><br><span class=\"line\">          rmContainer.getContainer().getResource(),</span><br><span class=\"line\">          node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; queueMoveOut&#x3D;&quot; + getQueueName()</span><br><span class=\"line\">          + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity() + &quot; absoluteUsedCapacity&#x3D;&quot;</span><br><span class=\"line\">          + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; cluster&#x3D;&quot;</span><br><span class=\"line\">          + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F; Inform the parent</span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F;递归父队列</span><br><span class=\"line\">        parent.detachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">  protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Update usedResources by labels</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>将 container 从源队列转移到目标队列时,会增加目标队列及其父队列的指标<br>先看目标队列(LeafQueue)增加的指标<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void attachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">          &#x2F;&#x2F;1.增加 LeafQueue 标签维度的 ResourceUsage 和用户维度的 ResourceUsage, USED 类型的资源量;并增加numContainer</span><br><span class=\"line\">      allocateResource(clusterResource, application, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; container&#x3D;&quot; + rmContainer.getContainer()</span><br><span class=\"line\">              &#x2F;&#x2F;add by jiulong.zhu@20190903</span><br><span class=\"line\">              +&quot; containerState&#x3D;&quot;+rmContainer.getState()</span><br><span class=\"line\">          + &quot; resource&#x3D;&quot; + rmContainer.getContainer().getResource()</span><br><span class=\"line\">          + &quot; queueMoveIn&#x3D;&quot; + this + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity()</span><br><span class=\"line\">          + &quot; absoluteUsedCapacity&#x3D;&quot; + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot;</span><br><span class=\"line\">          + queueUsage.getUsed() + &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;2.增加 ParentQueue 标签维度的 ResourceUsage USED 类型的资源量;并增加 ParentQueue 的 numContainer 数量</span><br><span class=\"line\">      getParent().attachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  synchronized void allocateResource(Resource clusterResource,</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Resource resource,</span><br><span class=\"line\">      Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#allocateResource 增加标签维度的 ResourceUsage USED 用途的用量;增加 Queue 的numContainers  </span><br><span class=\"line\">    super.allocateResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    &#x2F;&#x2F;增加当前用户 标签维度的 ResourceUsage USED 用途的用量  </span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.assignContainer(resource, nodeLabels);</span><br><span class=\"line\">    </span><br><span class=\"line\">    Resources.subtractFrom(application.getHeadroom(), resource);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.info(getQueueName() + </span><br><span class=\"line\">          &quot; user&#x3D;&quot; + userName + </span><br><span class=\"line\">          &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; numContainers&#x3D;&quot; + numContainers +</span><br><span class=\"line\">          &quot; headroom &#x3D; &quot; + application.getHeadroom() +</span><br><span class=\"line\">          &quot; user-resources&#x3D;&quot; + user.getUsed()</span><br><span class=\"line\">          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">   synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看将 container 移动到目标队列时,目标队列的 ParentQueue 增加的逻辑<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void attachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">          &#x2F;&#x2F;1.增加 ParentQueue 在标签维度的 ResourceUsage USED 用量 </span><br><span class=\"line\">          &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">      super.allocateResource(clusterResource, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; queueMoveIn&#x3D;&quot; + getQueueName()</span><br><span class=\"line\">          + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity() + &quot; absoluteUsedCapacity&#x3D;&quot;</span><br><span class=\"line\">          + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; cluster&#x3D;&quot;</span><br><span class=\"line\">          + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;2.递归增加 ParentQueue 的所有 ParentQueue 的 ResourceUsage USED 用量  </span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">        parent.attachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">  synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 3 步:删除源队列中该 app 的 currentApplicationAttempt,并同步 LeafQueue 中用户提交的 app 数量变化<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void finishApplicationAttempt(FiCaSchedulerApp application, String queue) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      removeApplicationAttempt(application, getUser(application.getUser()));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;空实现</span><br><span class=\"line\">    getParent().finishApplicationAttempt(application, queue);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public synchronized void removeApplicationAttempt(</span><br><span class=\"line\">      FiCaSchedulerApp application, User user) &#123;</span><br><span class=\"line\">    boolean wasActive &#x3D; activeApplications.remove(application);</span><br><span class=\"line\">    if (!wasActive) &#123;</span><br><span class=\"line\">      pendingApplications.remove(application);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;正在运行的 app_attempt,则减少 LeafQueue 的 [ResourceUsage | ResourceUsageByUser] 中AMUSED 占用的资源量</span><br><span class=\"line\">      queueUsage.decAMUsed(application.getAMResource());</span><br><span class=\"line\">      user.getResourceUsage().decAMUsed(application.getAMResource());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    applicationAttemptMap.remove(application.getApplicationAttemptId());</span><br><span class=\"line\">     &#x2F;&#x2F; 在 ResourceUsageByUser 同步 numApplication 指标</span><br><span class=\"line\">    user.finishApplication(wasActive);</span><br><span class=\"line\">    if (user.getTotalApplications() &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      users.remove(application.getUser());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Check if we can activate more applications</span><br><span class=\"line\">    activateApplications();</span><br><span class=\"line\">    LOG.info(&quot;Application removed -&quot; +</span><br><span class=\"line\">        &quot; appId: &quot; + application.getApplicationId() + </span><br><span class=\"line\">        &quot; user: &quot; + application.getUser() + </span><br><span class=\"line\">        &quot; queue: &quot; + getQueueName() +</span><br><span class=\"line\">        &quot; #user-pending-applications: &quot; + user.getPendingApplications() +</span><br><span class=\"line\">        &quot; #user-active-applications: &quot; + user.getActiveApplications() +</span><br><span class=\"line\">        &quot; #queue-pending-applications: &quot; + getNumPendingApplications() +</span><br><span class=\"line\">        &quot; #queue-active-applications: &quot; + getNumActiveApplications()</span><br><span class=\"line\">        );</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 4 步:对于源队列的所有 ParentQueue,移除 app 的 currentApplicationAttempt,并减少资源队列的运行 numApplication 值<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void finishApplication(ApplicationId application, String user) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      removeApplication(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;递归 父队列</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.finishApplication(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  private synchronized void removeApplication(ApplicationId applicationId, </span><br><span class=\"line\">      String user) &#123;</span><br><span class=\"line\">    --numApplications;</span><br><span class=\"line\">    LOG.info(&quot;Application removed -&quot; +</span><br><span class=\"line\">        &quot; appId: &quot; + applicationId + </span><br><span class=\"line\">        &quot; user: &quot; + user + </span><br><span class=\"line\">        &quot; leaf-queue of parent: &quot; + getQueueName() + </span><br><span class=\"line\">        &quot; #applications: &quot; + getNumApplications());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 5 步: 将 app 持有的所有 allocatedContainer(包括 AM container)和 reservedContainer,移动到目标队列,并修改 QueueMetrics<br>[SchedulerApplicationAttempt.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void move(Queue newQueue) &#123;</span><br><span class=\"line\">    QueueMetrics oldMetrics &#x3D; queue.getMetrics();</span><br><span class=\"line\">    QueueMetrics newMetrics &#x3D; newQueue.getMetrics();</span><br><span class=\"line\">    String user &#x3D; getUser();</span><br><span class=\"line\">    &#x2F;&#x2F;修改 livingContainer 的 QueueMetrics</span><br><span class=\"line\">    for (RMContainer liveContainer : liveContainers.values()) &#123;</span><br><span class=\"line\">      Resource resource &#x3D; liveContainer.getContainer().getResource();</span><br><span class=\"line\">      oldMetrics.releaseResources(user, 1, resource);</span><br><span class=\"line\">      newMetrics.allocateResources(user, 1, resource, false);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;修改 reservedContainer的 QueueMetrics</span><br><span class=\"line\">    for (Map&lt;NodeId, RMContainer&gt; map : reservedContainers.values()) &#123;</span><br><span class=\"line\">      for (RMContainer reservedContainer : map.values()) &#123;</span><br><span class=\"line\">        Resource resource &#x3D; reservedContainer.getReservedResource();</span><br><span class=\"line\">        oldMetrics.unreserveResource(user, resource);</span><br><span class=\"line\">        newMetrics.reserveResource(user, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;移动 pending ResourceRequest,同步信息:QueueMetrics.appsRunning,Queue下的用户列表    </span><br><span class=\"line\">    appSchedulingInfo.move(newQueue);</span><br><span class=\"line\">    this.queue &#x3D; newQueue;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>修改 livingContainers 和 reservedContainers 的所有逻辑都在 QueueMetrics中,其中<br>1.releaseResources,释放 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标<br>2.allocateResource,申请 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标<br>3.unreserveResource,释放 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Containers| MB | Core]指标<br>4.reserveResource,申请 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标<br>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseResources(String user, int containers, Resource res) &#123;</span><br><span class=\"line\">    allocatedContainers.decr(containers);</span><br><span class=\"line\">    aggregateContainersReleased.incr(containers);</span><br><span class=\"line\">    allocatedMB.decr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.decr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"> public void allocateResources(String user, int containers, Resource res,</span><br><span class=\"line\">      boolean decrPending) &#123;</span><br><span class=\"line\">    allocatedContainers.incr(containers);</span><br><span class=\"line\">    aggregateContainersAllocated.incr(containers);</span><br><span class=\"line\">    allocatedMB.incr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.incr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    if (decrPending) &#123;</span><br><span class=\"line\">      _decrPendingResources(containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public void unreserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.decr();</span><br><span class=\"line\">    reservedMB.decr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.decr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public void reserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.incr();</span><br><span class=\"line\">    reservedMB.incr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.incr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 6 步:同步目标队列 application 和 applicationAttempt信息<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void submitApplicationAttempt(FiCaSchedulerApp application,</span><br><span class=\"line\">      String userName) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      User user &#x3D; getUser(userName);</span><br><span class=\"line\">      &#x2F;&#x2F;增加 Queue 和 User 维度 application &#x2F; applicationAttempt 的值</span><br><span class=\"line\">      addApplicationAttempt(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; We don&#39;t want to update metrics for move app</span><br><span class=\"line\">    if (application.isPending()) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;增加 Queue 和 User 维度 QueueMetrics 中 appsPending 的值</span><br><span class=\"line\">      metrics.submitAppAttempt(userName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;空实现</span><br><span class=\"line\">    getParent().submitApplicationAttempt(application, userName);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>综上:<br>把 application 从源资源队列到目标队列的过程中,在 Queue/ResourceUsageByLabel/ResourceUsageByUser/QueueMetrics 中的变动<br>1.对于 livingContainers,<br>&ensp;&ensp;&ensp;&ensp;Queue 方面:减少源队列及其父队列的 numContainer 值;增加目标队列及其父队列的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ResourceUsage 方面:减少源队列及其父队列的 ResourceUsageByLabel USED 类型的资源量,增加目标队列及其父队列的 ResourceUsageByLabel USED 类型的资源量; 减少源队列(LeafQueue)的 ResourceUsageByUser USED 类型的资源量,增加目标队列(LeafQueue) 的 ResourceUsageByUser USED 类型的资源量;<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:减少源队列及其父队列的 allocated[Container | MB | Core]指标,增加目标队列及其父队列的 allocated [Container | MB | Core]<br>2.对于 reservedContainers,<br>&ensp;&ensp;&ensp;&ensp;ResourceUsage:减少源队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的reserved [Container | MB | Core]指标,增加目标队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  </p>\n</blockquote>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><p>结合 “正常的资源分配和释放过程”和”movetoqueue 过程中的资源转移过程 “,可以还原一个被转移队列的 application 的资源变动过程<br>application 中非 reservedContianer(状态机:allocated,acquired,running) 的资源变动过程如下表  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">allocatedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<p>application 中 reservedContianer(状态机:reversed) 的资源变动过程如下表   </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">reversedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<blockquote>\n</blockquote>\n<p>从上述两表中可知:<br>1.在没有 movetoqueue 操作的情况下, 源队列和目标队列一致,无论 allocatedContainer 或 reservedContainer 的分配和释放都是守恒的,一次指标的增加对应着一次指标的减少<br>2.在源队列资源比较充裕,且有 movetoqueue 操作的情况下,application 没有 reservedContainer 的情况下(上述表一),在分配时源队列指标增加,movetoqueue 时,源队列减少指标且目标队列增加指标,在释放时减少目标队列(app 当前归属队列)的指标.指标的增减也是平衡的,这也就是在资源充裕的队列上无法复现该问题的原因<br>3.在源队列资源比较紧张,且有 movetoqueue 操作的情况下,application 有 reservedContainer 和 allocatedContainer(上述表一和表二),对于 allocatedContainer 来说,资源在分配-&gt;movetoque-&gt;释放的过程中是平衡的(表一);但是对于 reversedContainer 来说,在源队列中分配到资源时,增加了源队列的 numContainer 和 ResourceUsageByLabel 中 USED 用途的资源量,movetoqueue 时没有相应的操作,仅仅是修改了 QueueMetrcis 内的指标,在释放时 container 资源被加入到目标队列的指标中(上述表二).<b>源队列分配出去的资源没有回收,目标队列得到了不是自身分配出去的资源</b>.这样导致了即使源队列和目标队列中完全无任务时,源队列的 numContainer 和 ResourceUsageByLabel 值为正值,目标队列的 numContainer 和 ResourceUsage 值为负值,Yarn UI -&gt;scheduler-&gt;queue 内的 Num Containers/Used Capacity/Absolute Used Capacity/Used Resources 正是基于 队列的 numContainer 和 ResourceUsageByLabel(USED) 展示和计算的,也就是指标异常的问题     </p>\n<p>指标异常(无任务时 为负)\n<img src=\"/img/pictures/negative/negative_e7a766275896.png\" alt=\"\"></p>\n<p>指标异常(无任务时 为正)\n<img src=\"/img/pictures/negative/negative_aead-04d8e67b1357.png\" alt=\"\"></p>\n<h3 id=\"危害\"><a href=\"#危害\" class=\"headerlink\" title=\"危害\"></a>危害</h3><p>1.对于源队列来说,类似于资源泄露,自身资源未能回收,释放到了目标队列.这样对于后续提交到源队列的任务来说,不能分配到资源运行(USED 泄露趋于 100%),但实际上源队列没有任务在运行<br>2.对于目标队列来说,多出了一些资源,导致自身一些指标为负,可能会导致一些未知的问题.以下纯属个人猜测: 例如 同步问题,Queue 有很多的剩余资源,但是 NodeManager 上没有 available 资源;指标负值可能对内部的运算产生未预期的问题  </p>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>针对 reservedContainer 在 movetoqueue 操作过程中,资源释放申请不守恒的漏洞,如下表  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">reversedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<blockquote>\n</blockquote>\n<p>问题:<br>&ensp;&ensp;&ensp;&ensp;YarnUI 中指标 Num Container、Used Capacity、Absolute Used Capacity、Used Resource 异常和 Queue成员变量numContainer及 QueueUsageByLabel 中 USED 用途的资源量有关<br>目标:<br>&ensp;&ensp;&ensp;&ensp;解决 numContainer 和 ResourceUsageByLabel(USED)在 movetoqueue 时,没有”减少源队列的指标”且没有”增加目标队列指标”的问题<br>途径:<br>&ensp;&ensp;&ensp;&ensp;1.需要在 movetoqueue 时,每个 reservedContainer 都应触发 “源队列减少指标”和”目标队列增加指标”的行为 \n&ensp;&ensp;&ensp;&ensp;源码中有封装好的代码且上文多次提到,即 LeafQueue#releaseResource方法和 LeafQueue.allocateResource方法,但是其中有额外的 对 headroom 的同步逻辑.一方面,在”正常的分配和释放”过程中,无论申请和分配的 container 是 allocated 或是 reversed 都会执行 LeafQueue#releaseResource 和 LeafQueue.allocateResource 修改 numContainer 和 ResourceUsageByLabel 的同时修改 headroom值;另一方面 move container中对 headroom 增加和减少 同一个Resource 是幂等的.所以 movetoqueue 操作对于每一个 container 的移动都可以选择触发这两个方法来增加对 numContainer 和 ResourceUsageByLabel 的操作.LeafQueue#detachContainer()和 LeafQueueattachContainer()  中有递归本队列及当前队列此逻辑的过程.直接用即可<br>修改方案:<br>&ensp;&ensp;&ensp;&ensp;CapacitySchedule#moveApplication(ApplicationId appId,String targetQueueName) 方法修改如下  </p>\n<p>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized String moveApplication(ApplicationId appId,</span><br><span class=\"line\">      String targetQueueName) throws YarnException &#123;</span><br><span class=\"line\">    FiCaSchedulerApp app &#x3D;</span><br><span class=\"line\">        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));</span><br><span class=\"line\">    String sourceQueueName &#x3D; app.getQueue().getQueueName();</span><br><span class=\"line\">    LeafQueue source &#x3D; getAndCheckLeafQueue(sourceQueueName);</span><br><span class=\"line\">    String destQueueName &#x3D; handleMoveToPlanQueue(targetQueueName);</span><br><span class=\"line\">    LeafQueue dest &#x3D; getAndCheckLeafQueue(destQueueName);</span><br><span class=\"line\">    &#x2F;&#x2F; Validation check - ACLs, submission limits for user &amp; queue</span><br><span class=\"line\">    String user &#x3D; app.getUser();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      dest.submitApplication(appId, user, destQueueName);</span><br><span class=\"line\">    &#125; catch (AccessControlException e) &#123;</span><br><span class=\"line\">      throw new YarnException(e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Move all live containers</span><br><span class=\"line\">    for (RMContainer rmContainer : app.getLiveContainers()) &#123;</span><br><span class=\"line\">      source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">      &#x2F;&#x2F; attach the Container to another queue</span><br><span class=\"line\">      dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;description:解决 reservedContainer 在 movetoqueue 操作过程中,Queue.numContainer 指标和 ResourceUsageByLabel.USED 资源量</span><br><span class=\"line\">    &#x2F;&#x2F;释放申请不守恒,导致 Yarn UI &quot;Num Container&quot;,&quot;Used Capacity&quot;,&quot;Absolute Used Capacity&quot;,&quot;Used Resource&quot;</span><br><span class=\"line\">    &#x2F;&#x2F;指标在队列无任务运行时不为 0 的问题</span><br><span class=\"line\">    &#x2F;&#x2F;modify by jiulong.zhu@20190903</span><br><span class=\"line\">    &#x2F;&#x2F;++add start</span><br><span class=\"line\">    for (RMContainer rmContainer : app.getReservedContainers()) &#123;</span><br><span class=\"line\">      source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">      dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;++add end</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Detach the application..</span><br><span class=\"line\">    source.finishApplicationAttempt(app, sourceQueueName);</span><br><span class=\"line\">    source.getParent().finishApplication(appId, app.getUser());</span><br><span class=\"line\">    &#x2F;&#x2F; Finish app &amp; update metrics</span><br><span class=\"line\">    app.move(dest);</span><br><span class=\"line\">    &#x2F;&#x2F; Submit to a new queue</span><br><span class=\"line\">    dest.submitApplicationAttempt(app, user);</span><br><span class=\"line\">    applications.get(appId).setQueue(dest);</span><br><span class=\"line\">    LOG.info(&quot;App: &quot; + app.getApplicationId() + &quot; successfully moved from &quot;</span><br><span class=\"line\">        + sourceQueueName + &quot; to: &quot; + destQueueName);</span><br><span class=\"line\">    return targetQueueName;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"冒烟测试用例\"><a href=\"#冒烟测试用例\" class=\"headerlink\" title=\"冒烟测试用例\"></a>冒烟测试用例</h2><p>[TestCapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">public void testReservedContainerLeakWhenMoveApplication() throws Exception &#123;</span><br><span class=\"line\">  CapacitySchedulerConfiguration csConf</span><br><span class=\"line\">          &#x3D; new CapacitySchedulerConfiguration();</span><br><span class=\"line\">  csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] &#123;&quot;a&quot;, &quot;b&quot;&#125;);</span><br><span class=\"line\">  csConf.setCapacity(&quot;root.a&quot;,50);</span><br><span class=\"line\">  csConf.setMaximumCapacity(&quot;root.a&quot;,100);</span><br><span class=\"line\">  csConf.setUserLimitFactor(&quot;root.a&quot;,100);</span><br><span class=\"line\">  csConf.setCapacity(&quot;root.b&quot;,50);</span><br><span class=\"line\">  csConf.setMaximumCapacity(&quot;root.b&quot;,100);</span><br><span class=\"line\">  csConf.setUserLimitFactor(&quot;root.b&quot;,100);</span><br><span class=\"line\"></span><br><span class=\"line\">  YarnConfiguration conf&#x3D;new YarnConfiguration(csConf);</span><br><span class=\"line\">  conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,</span><br><span class=\"line\">          ResourceScheduler.class);</span><br><span class=\"line\">  RMNodeLabelsManager mgr&#x3D;new NullRMNodeLabelsManager();</span><br><span class=\"line\">  mgr.init(conf);</span><br><span class=\"line\">  MockRM rm1 &#x3D; new MockRM(csConf);</span><br><span class=\"line\">  CapacityScheduler scheduler&#x3D;(CapacityScheduler) rm1.getResourceScheduler();</span><br><span class=\"line\">  rm1.getRMContext().setNodeLabelManager(mgr);</span><br><span class=\"line\">  rm1.start();</span><br><span class=\"line\">  MockNM nm1 &#x3D; rm1.registerNode(&quot;127.0.0.1:1234&quot;, 8 * GB);</span><br><span class=\"line\">  MockNM nm2 &#x3D; rm1.registerNode(&quot;127.0.0.2:1234&quot;, 8 * GB);</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * simulation</span><br><span class=\"line\">   * app1: (1 AM,1 running container)</span><br><span class=\"line\">   * app2: (1 AM,1 reserved container)</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  &#x2F;&#x2F; launch an app to queue, AM container should be launched in nm1</span><br><span class=\"line\">  RMApp app1 &#x3D; rm1.submitApp(1 * GB, &quot;app_1&quot;, &quot;user_1&quot;, null, &quot;a&quot;);</span><br><span class=\"line\">  MockAM am1 &#x3D; MockRM.launchAndRegisterAM(app1, rm1, nm1);</span><br><span class=\"line\"></span><br><span class=\"line\">  &#x2F;&#x2F; launch another app to queue, AM container should be launched in nm1</span><br><span class=\"line\">  RMApp app2 &#x3D; rm1.submitApp(1 * GB, &quot;app_2&quot;, &quot;user_1&quot;, null, &quot;a&quot;);</span><br><span class=\"line\">  MockAM am2 &#x3D; MockRM.launchAndRegisterAM(app2, rm1, nm1);</span><br><span class=\"line\"></span><br><span class=\"line\">  am1.allocate(&quot;*&quot;, 4 * GB, 1, new ArrayList&lt;ContainerId&gt;());</span><br><span class=\"line\">  &#x2F;&#x2F;this containerRequest should be reserved</span><br><span class=\"line\">  am2.allocate(&quot;*&quot;, 4 * GB, 1, new ArrayList&lt;ContainerId&gt;());</span><br><span class=\"line\"></span><br><span class=\"line\">  RMNode rmNode1 &#x3D; rm1.getRMContext().getRMNodes().get(nm1.getNodeId());</span><br><span class=\"line\">  &#x2F;&#x2F; Do node heartbeats 2 times</span><br><span class=\"line\">  &#x2F;&#x2F; First time will allocate container for app1, second time will reserve</span><br><span class=\"line\">  &#x2F;&#x2F; container for app2</span><br><span class=\"line\">  scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));</span><br><span class=\"line\">  scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));</span><br><span class=\"line\"></span><br><span class=\"line\">  FiCaSchedulerApp schedulerApp1 &#x3D;</span><br><span class=\"line\">          scheduler.getApplicationAttempt(am1.getApplicationAttemptId());</span><br><span class=\"line\">  FiCaSchedulerApp schedulerApp2 &#x3D;</span><br><span class=\"line\">          scheduler.getApplicationAttempt(am2.getApplicationAttemptId());</span><br><span class=\"line\">  &#x2F;&#x2F; APP1:  1 AM, 1 allocatedContainer</span><br><span class=\"line\">  Assert.assertEquals(2, schedulerApp1.getLiveContainers().size());</span><br><span class=\"line\">  &#x2F;&#x2F; APP2:  1 AM,1 reservedContainer</span><br><span class=\"line\">  Assert.assertEquals(1,schedulerApp2.getLiveContainers().size());</span><br><span class=\"line\">  Assert.assertEquals(1,schedulerApp2.getReservedContainers().size());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * before,move app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  LeafQueue srcQueue &#x3D; (LeafQueue) scheduler.getQueue(&quot;a&quot;);</span><br><span class=\"line\">  LeafQueue desQueue &#x3D; (LeafQueue) scheduler.getQueue(&quot;b&quot;);</span><br><span class=\"line\">  Assert.assertEquals(4,srcQueue.getNumContainers());</span><br><span class=\"line\">  Assert.assertEquals(10*GB,srcQueue.getUsedResources().getMemorySize());&#x2F;&#x2F; AM: 2*1GB   container: 4GB running,4GB reserved</span><br><span class=\"line\">  Assert.assertEquals(0,desQueue.getNumContainers());</span><br><span class=\"line\">  Assert.assertEquals(0,desQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">  &#x2F;&#x2F;app1 ResourceUsage (0 reserved)</span><br><span class=\"line\">  Assert.assertEquals(5*GB,schedulerApp1.getAppAttemptResourceUsage().getAllUsed().getMemorySize());</span><br><span class=\"line\">  Assert.assertEquals(0,schedulerApp1.getCurrentReservation().getMemorySize());</span><br><span class=\"line\">  &#x2F;&#x2F;app2  ResourceUsage (4GB reserved)</span><br><span class=\"line\">  Assert.assertEquals(1*GB,schedulerApp2.getAppAttemptResourceUsage().getAllUsed().getMemorySize());</span><br><span class=\"line\">  Assert.assertEquals(4*GB,schedulerApp2.getCurrentReservation().getMemorySize());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * move app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  scheduler.moveApplication(app2.getApplicationId(),&quot;b&quot;);</span><br><span class=\"line\">  &#x2F;&#x2F; finish.keep the order,if killing app1 first,the reservedContainer of app2 will be allocated</span><br><span class=\"line\">  rm1.killApp(app2.getApplicationId());</span><br><span class=\"line\">  rm1.killApp(app1.getApplicationId());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * after,moved app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; after fixed</span><br><span class=\"line\">    Assert.assertEquals(0, srcQueue.getNumContainers());</span><br><span class=\"line\">    Assert.assertEquals(0, desQueue.getNumContainers());</span><br><span class=\"line\">    Assert.assertEquals(0, srcQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">    Assert.assertEquals(0, desQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;*&#123;</span><br><span class=\"line\">    &#x2F;&#x2F; before fixed</span><br><span class=\"line\">    &#x2F;&#x2F; &lt;b&gt; the reserved container borrowed from srcQueue and returned to desQueue,</span><br><span class=\"line\">    &#x2F;&#x2F; but the numContainer and UsedResource did not sync when moving app to another queue &lt;&#x2F;b&gt;</span><br><span class=\"line\">    Assert.assertEquals(+1,srcQueue.getNumContainers());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(-1,desQueue.getNumContainers());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(+4*GB, srcQueue.getUsedResources().getMemorySize());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(-4*GB, desQueue.getUsedResources().getMemorySize());    &#x2F;&#x2F;true</span><br><span class=\"line\">  &#125;*&#x2F;</span><br><span class=\"line\">  rm1.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"线上测试用例\"><a href=\"#线上测试用例\" class=\"headerlink\" title=\"线上测试用例\"></a>线上测试用例</h2>","site":{"data":{}},"excerpt":"<h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>多个线上运行状态私有云的某些资源队列,在无任何 Application 提交及运行、资源队列完全空闲的情况下, \nUsed Capacity、Absolute Used Capacity、Used Resource、Num Containers指标非零异常  </p>","more":"<blockquote>\n</blockquote>\n<p>在Yarn ResourceManager管理界面中,有 scheduler 选项卡,展示了Yarn 当前使用的调度器及各资源队列的信息(YarnUI-&gt;scheduler-&gt;Application Queue),其中每项指标代表的含义是:<br>Queue State: 表示当前队列的状态,有 RUNNING/STOPPED 两种状态<br>Used Capacity: 表示当前队列已使用的资源占当前队列总资源的百分比<br>Configured Capacity: 表示当前队列的资源占父队列资源的百分比<br>Configured Max Capacity: 表示当前队列资源最大能占父队列资源的百分比<br>Absolute Used Capacity: 表示当前队列已使用的资源占 root 队列资源(整个集群)的百分比<br>Absolute Configured Capacity: 表示当前队列的资源占 root 队列总资源的百分比<br>Absolute Configured Max Capacity: 表示当前的队列的资源最大能占 root 队列的百分比<br>Used Resources: 表示当前队列已使用的资源总量(资源以内存和虚拟核形态表示,基本调度单位)<br>Num Schedulable Applications: 表示当前队列调度的应用个数\nNum Non-Schedulable Applications: 表示当前队列没有调度(积压,pending)的应用个数<br>Num Containers: 表示当前队列已经启动的 container 个数<br>Max Applications: 表示当前队列最大并发调度应用个数<br>Max Applications Per User: 表示当前队列对每个用户最大并发调度应用个数<br>Max Application Master Resources: 表示所有 Application 的 AM 可使用资源量之和的最大值<br>Used Application Master Resources: 表示当前队列中所有 Application 的 AM 使用资源量之和<br>Max Application Master Resources Per User: 表示当前队列中每个用户的 Application 的 AM 使用资源量之和的最大值<br>Configured Minimum User Limit Percent: 表示队列每个用户分配的最低资源百分比(资源保障)<br>Configured User Limit Factor: 表示每个用户能占用的队列资源的百分比<br>Accessible Node Labels:  表示当前队列可在哪些节点上分配资源 (*为全部节点)<br>Preemption: 是否允许资源抢占  </p>\n<p>指标可分为两类:配置型指标,静态数据 不会变化,如 Configured Capacity;状态型指标,动态数据 随应用的提交运行结束而变化,如 Used Capacity.<br>运行时异常指标如下图,无 Application 运行的情况下,low 队列状态型指标为负</p>\n<p><img src=\"/img/pictures/negative/negative_e7a766275896.png\" alt=\"\"></p>\n<h2 id=\"先期判断\"><a href=\"#先期判断\" class=\"headerlink\" title=\"先期判断\"></a>先期判断</h2><h3 id=\"指标关联的变量定位\"><a href=\"#指标关联的变量定位\" class=\"headerlink\" title=\"指标关联的变量定位\"></a>指标关联的变量定位</h3><p>线上 hadoop 版本: hadoop 2.7.3<br>Yarn ResourceManager 管理界面启动的入口是org.apache.hadoop.yarn.server.resourcemanager.ResourceManager#startWebApp()<br>[ResourceManager.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void startWepApp() &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    Builder&lt;ApplicationMasterService&gt; builder &#x3D; </span><br><span class=\"line\">        WebApps</span><br><span class=\"line\">            .$for(&quot;cluster&quot;, ApplicationMasterService.class, masterService,</span><br><span class=\"line\">                &quot;ws&quot;)</span><br><span class=\"line\">            .with(conf)</span><br><span class=\"line\">            .withHttpSpnegoPrincipalKey(</span><br><span class=\"line\">                YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY)</span><br><span class=\"line\">            .withHttpSpnegoKeytabKey(</span><br><span class=\"line\">                YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY)</span><br><span class=\"line\">            .at(webAppAddress);</span><br><span class=\"line\">   \t....</span><br><span class=\"line\">    webApp &#x3D; builder.start(new RMWebApp(this));</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>RMWebApp 主要逻辑是使用 Google Guice 做依赖注入,并分发请求绑定后台逻辑,大致相当于 SpringMVC 系统中的 Dispatcher 的角色<br>对 Yarn UI界面左侧边栏 scheduler 的请求会转发给 RmController#scheduler()方法处理<br>[RMWebApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void setup() &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    bind(RMWebApp.class).toInstance(this);</span><br><span class=\"line\">    if (rm !&#x3D; null) &#123;</span><br><span class=\"line\">      bind(ResourceManager.class).toInstance(rm);</span><br><span class=\"line\">      bind(ApplicationBaseProtocol.class).toInstance(rm.getClientRMService());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    route(&quot;&#x2F;scheduler&quot;, RmController.class, &quot;scheduler&quot;);</span><br><span class=\"line\">    route(pajoin(&quot;&#x2F;queue&quot;, QUEUE_NAME), RmController.class, &quot;queue&quot;);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>RmController#scheduler() 先获取到 Guice 注入的 ResourceManager,然后依据 RM 使用的调度器做页面渲染.<br>页面渲染逻辑的入口是CapacitySchedulerPage#render()<br>[RmController.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void scheduler() &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ResourceManager rm &#x3D; getInstance(ResourceManager.class);</span><br><span class=\"line\">    ResourceScheduler rs &#x3D; rm.getResourceScheduler();</span><br><span class=\"line\">    if (rs &#x3D;&#x3D; null || rs instanceof CapacityScheduler) &#123;</span><br><span class=\"line\">      setTitle(&quot;Capacity Scheduler&quot;);</span><br><span class=\"line\">      &#x2F;&#x2F;渲染</span><br><span class=\"line\">      render(CapacitySchedulerPage.class);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (rs instanceof FairScheduler) &#123;</span><br><span class=\"line\">      setTitle(&quot;Fair Scheduler&quot;);</span><br><span class=\"line\">      render(FairSchedulerPage.class);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> protected void render(Class&lt;? extends View&gt; cls) &#123;</span><br><span class=\"line\">    context().rendered &#x3D; true;</span><br><span class=\"line\">    getInstance(cls).render();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>泛型上界是 View,调用栈是:<br>View#render()<br>&ensp;&ensp;-&gt;HtmlPage#render()<br>&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;TwoColumnLayout#render(html)<br>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;CapacitySchedulerPage#content()<br>[CapacitySchedulerPage.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected Class&lt;? extends SubView&gt; content() &#123;</span><br><span class=\"line\">    return QueuesBlock.class;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>QueuesBlock 是 CapacitySchedulerPage 的内部类,用于展示CapacityScheduler 的 DashBoard 信息,包含队列、标签、应用概览信息,并构建根队列 root 来做为后续递归渲染的起点<br>[QueuesBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class QueuesBlock extends HtmlBlock &#123;</span><br><span class=\"line\">    final CapacityScheduler cs;</span><br><span class=\"line\">    final CSQInfo csqinfo;</span><br><span class=\"line\">    private List&lt;NodeLabel&gt; nodeLabelsInfo;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public void render(Block html) &#123;</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        float used &#x3D; 0;</span><br><span class=\"line\">        if (null &#x3D;&#x3D; nodeLabelsInfo</span><br><span class=\"line\">            || (nodeLabelsInfo.size() &#x3D;&#x3D; 1 &amp;&amp; nodeLabelsInfo.get(0)</span><br><span class=\"line\">                .getLabelName().isEmpty())) &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;创建根队列,作为后续渲染的起点</span><br><span class=\"line\">          CSQueue root &#x3D; cs.getRootQueue();</span><br><span class=\"line\">          CapacitySchedulerInfo sinfo &#x3D;</span><br><span class=\"line\">              new CapacitySchedulerInfo(root, new NodeLabel(</span><br><span class=\"line\">                  RMNodeLabelsManager.NO_LABEL));</span><br><span class=\"line\">          csqinfo.csinfo &#x3D; sinfo;</span><br><span class=\"line\">          csqinfo.qinfo &#x3D; null;</span><br><span class=\"line\"></span><br><span class=\"line\">           ...</span><br><span class=\"line\">          ul.li().</span><br><span class=\"line\">            ...</span><br><span class=\"line\">            _(QueueBlock.class)._();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          for (NodeLabel label : nodeLabelsInfo) &#123;</span><br><span class=\"line\">            ....</span><br><span class=\"line\">            underLabel.li().</span><br><span class=\"line\">            ...</span><br><span class=\"line\">            _(QueueBlock.class)._()._();</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;Application List 界面</span><br><span class=\"line\">      ul._()._().</span><br><span class=\"line\">      script().$type(&quot;text&#x2F;javascript&quot;).</span><br><span class=\"line\">          _(&quot;$(&#39;#cs&#39;).hide();&quot;)._()._().</span><br><span class=\"line\">      _(RMAppsBlock.class);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>从 rootQueue 开始递归,Queue 检查自身有没有子队列 subQueues,若无则其本身为 LeafQueue,使用 LeafQueueInfoBlock#render()渲染叶子队列信息,使用 QueueUsersInfoBlock#render()渲染队列下的用户信息;若有子队列则其本身为ParentQueue,使用 QueueBlock#render()渲染,直至叶子队列<br>[QueuesBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void render(Block html) &#123;</span><br><span class=\"line\">      ArrayList&lt;CapacitySchedulerQueueInfo&gt; subQueues &#x3D;</span><br><span class=\"line\">          (csqinfo.qinfo &#x3D;&#x3D; null) ? csqinfo.csinfo.getQueues().getQueueInfoList()</span><br><span class=\"line\">              : csqinfo.qinfo.getQueues().getQueueInfoList();</span><br><span class=\"line\">      UL&lt;Hamlet&gt; ul &#x3D; html.ul(&quot;#pq&quot;);</span><br><span class=\"line\">      for (CapacitySchedulerQueueInfo info : subQueues) &#123;</span><br><span class=\"line\">  \t  ...</span><br><span class=\"line\">          if (info.getQueues() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">          li.ul(&quot;#lq&quot;).li()._(LeafQueueInfoBlock.class)._()._();</span><br><span class=\"line\">          li.ul(&quot;#lq&quot;).li()._(QueueUsersInfoBlock.class)._()._();</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          li._(QueueBlock.class);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">  \t...</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>由于异常指标位于叶子队列信息中,所以暂且不看QueueUsersInfoBlock.java<br>[LeafQueueInfoBlock.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private String nodeLabel;</span><br><span class=\"line\">final CapacitySchedulerLeafQueueInfo lqinfo;</span><br><span class=\"line\">@Inject LeafQueueInfoBlock(ViewContext ctx, CSQInfo info) &#123;</span><br><span class=\"line\">      super(ctx);</span><br><span class=\"line\">      lqinfo &#x3D; (CapacitySchedulerLeafQueueInfo) info.qinfo;</span><br><span class=\"line\">      nodeLabel &#x3D; info.label;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">protected void render(Block html) &#123;</span><br><span class=\"line\">      if (nodeLabel &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        renderLeafQueueInfoWithoutParition(html);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        renderLeafQueueInfoWithPartition(html);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">虽然按照 nodeLabel 做了分支,但是核心逻辑都是</span><br><span class=\"line\">renderQueueCapacityInfo方法和renderCommonLeafQueueInfo方法,</span><br><span class=\"line\">这两个方法分别展示不同方面的指标,和 Yarn UI 展示的指标相同 </span><br><span class=\"line\"></span><br><span class=\"line\">private void renderQueueCapacityInfo(final ResponseInfo ri) &#123;</span><br><span class=\"line\">      ri.</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Used Capacity:&quot;, percent(lqinfo.getUsedCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Configured Capacity:&quot;, percent(lqinfo.getCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Configured Max Capacity:&quot;, percent(lqinfo.getMaxCapacity() &#x2F; 100)).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Absolute Used Capacity:&quot;, percent(lqinfo.getAbsoluteUsedCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Absolute Configured Capacity:&quot;, percent(lqinfo.getAbsoluteCapacity() &#x2F; 100)).</span><br><span class=\"line\">      _(&quot;Absolute Configured Max Capacity:&quot;, percent(lqinfo.getAbsoluteMaxCapacity() &#x2F; 100)).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Used Resources:&quot;, lqinfo.getResourcesUsed().toString());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  private void renderCommonLeafQueueInfo(final ResponseInfo ri) &#123;</span><br><span class=\"line\">      ri.</span><br><span class=\"line\">      _(&quot;Num Schedulable Applications:&quot;, Integer.toString(lqinfo.getNumActiveApplications())).</span><br><span class=\"line\">      _(&quot;Num Non-Schedulable Applications:&quot;, Integer.toString(lqinfo.getNumPendingApplications())).</span><br><span class=\"line\">      &#x2F;&#x2F;异常指标</span><br><span class=\"line\">      _(&quot;Num Containers:&quot;, Integer.toString(lqinfo.getNumContainers())).</span><br><span class=\"line\">      _(&quot;Max Applications:&quot;, Integer.toString(lqinfo.getMaxApplications())).</span><br><span class=\"line\">      _(&quot;Max Applications Per User:&quot;, Integer.toString(lqinfo.getMaxApplicationsPerUser())).</span><br><span class=\"line\">      _(&quot;Max Application Master Resources:&quot;, lqinfo.getAMResourceLimit().toString()).</span><br><span class=\"line\">      _(&quot;Used Application Master Resources:&quot;, lqinfo.getUsedAMResource().toString()).</span><br><span class=\"line\">      _(&quot;Max Application Master Resources Per User:&quot;, lqinfo.getUserAMResourceLimit().toString()).</span><br><span class=\"line\">      _(&quot;Configured Minimum User Limit Percent:&quot;, Integer.toString(lqinfo.getUserLimit()) + &quot;%&quot;).</span><br><span class=\"line\">      _(&quot;Configured User Limit Factor:&quot;, StringUtils.format(</span><br><span class=\"line\">          &quot;%.1f&quot;, lqinfo.getUserLimitFactor())).</span><br><span class=\"line\">      _(&quot;Accessible Node Labels:&quot;, StringUtils.join(&quot;,&quot;, lqinfo.getNodeLabels())).</span><br><span class=\"line\">      _(&quot;Preemption:&quot;, lqinfo.getPreemptionDisabled() ? &quot;disabled&quot; : &quot;enabled&quot;);</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"异常指标计算方式\"><a href=\"#异常指标计算方式\" class=\"headerlink\" title=\"异常指标计算方式\"></a>异常指标计算方式</h3><p>注:集群没有使用 label 系统,所以下述的 nodeLabel 视为””即可</p>\n<ul>\n<li><p>Used Capacity</p>\n<p>  [AbstractCSQueue.java]</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final synchronized float getUsedCapacity(final String nodeLabel) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;集群所有资源 * 该队列的绝对容量百分比 &#x3D; 该队列的绝对容量 </span><br><span class=\"line\">    Resource availableToQueue &#x3D;</span><br><span class=\"line\">        Resources.multiply(</span><br><span class=\"line\">            labelManager.getResourceByLabel(nodeLabel, this.clusterResource),</span><br><span class=\"line\">            queueCapacities.getAbsoluteCapacity(nodeLabel));</span><br><span class=\"line\">   &#x2F;&#x2F;使用 queueUsage 中记录的使用量除以该队列的绝对容量得到队列的 Used Capacity</span><br><span class=\"line\">   &#x2F;&#x2F;queueUsage 的类型为 ResourceUsage</span><br><span class=\"line\">    return</span><br><span class=\"line\">        Resources.divide(resourceCalculator, this.clusterResource,</span><br><span class=\"line\">            queueUsage.getUsed(nodeLabel), availableToQueue);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>Absolute Used Capacity</p>\n<p>  算法和Used Capacity算法相似,只是分母不同,AbsoluteUsedCapacity 计算时分母是整个集群的资源  </p>\n<p>  [AbstractCSQueue.java]</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public final synchronized float getAbsoluteUsedCapacity(final String nodeLabel) &#123;</span><br><span class=\"line\">   Resource labeledResources &#x3D;</span><br><span class=\"line\">              labelManager.getResourceByLabel(nodeLabel, this.clusterResource);</span><br><span class=\"line\">   return Resources.divide(resourceCalculator, this.clusterResource,</span><br><span class=\"line\">       queueUsage.getUsed(nodeLabel), labeledResources);</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Used Resource</p>\n<p>  &ensp;&ensp;使用的是Queue 按 Label 记录的资源信息<br>  queueResourceUsage.getUsed(nodeLabel)  </p>\n</li>\n<li><p>Num Containers</p>\n<p>  &ensp;&ensp;使用的 Queue 本身记录的信息<br>  numContainers = leafQueue.getNumContainers();  </p>\n</li>\n</ul>\n<blockquote>\n</blockquote>\n<p>综上所述:<br>四个指标中,队列层面的Used Capacity 、Absolute Used Capacity 、Used Resource 均和 ResourceUsage维护 used 资源信息有关;numContainer 是 LeafQueue 自身维护的 container 数量.这些指标的变化逻辑在 AbstractCSQueue的 assignContainer()和 releaseContainer()中,即和 container 的释放/申请有关.\n而在队列完全空闲的时候,四个指标为负,初步定位可能是 Container 重复释放或无效释放的原因.</p>\n<h2 id=\"问题复现\"><a href=\"#问题复现\" class=\"headerlink\" title=\"问题复现\"></a>问题复现</h2><h3 id=\"猜测重复释放\"><a href=\"#猜测重复释放\" class=\"headerlink\" title=\"猜测重复释放\"></a>猜测重复释放</h3><p>在已经出现指标异常的私有云上,对出现异常时日期前后的 resourcemanager 日志进行分析,检测其中出现”Assigned container”和”Released  container”字符串的数量,从日志层面对 container 的申请和释放次数做一个简单的判断.这种方式适用于集群作业周期性较强且没有跨天任务运行的情况.  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">egrep -o &quot;Assigned container&quot; resourcemanager.log | sort | uniq -c</span><br><span class=\"line\">egrep -o &quot;Released container&quot; resourcemanager.log | sort | uniq -c</span><br></pre></td></tr></table></figure>\n\n<p>从结果上看,”Assigned container”和”Released container”字符串出现次数是一致的,可能不是重复释放的问题  </p>\n<h3 id=\"猜测无效释放\"><a href=\"#猜测无效释放\" class=\"headerlink\" title=\"猜测无效释放\"></a>猜测无效释放</h3><p>对无效释放的猜想源自于重复释放和 rm 日志中的”Null container completed…”,猜测可能对于 nullContainer 处理有问题,修改了系统维护的信息, 导致了负值    </p>\n<p>UI 的 kill 操作,命令行 yarn application –kill 和 RMApp 的正常结束都会释放 container,由 APP_ATTEMPT_REMOVED 事件触发,\n回收 AppAttemp 持有的runningContainer 和 reservedContainer,核心逻辑在 CapacityScheduler#completedContainer()方法,但是其中对于 RMContainer 和 Application 都做了校验,不会修改维护信息;如果 RMContainer 对象内持有的 contianer 对象为空的话,会抛出 NullPointerException,也不会修改维护信息. 所以无效释放的猜测不合理,此处代码证明重复释放也不合理\n[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void completedContainer(RMContainer rmContainer,</span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event) &#123; </span><br><span class=\"line\">    &#x2F;&#x2F;rmContaienr 空值校验  </span><br><span class=\"line\">    if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      LOG.info(&quot;Null container completed...&quot;);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">    FiCaSchedulerApp application &#x3D;</span><br><span class=\"line\">        getCurrentAttemptForContainer(container.getId());</span><br><span class=\"line\">    ApplicationId appId &#x3D;</span><br><span class=\"line\">    &#x2F;&#x2F;如果 RMContainer 映射的 container 为空,则此处会 NullPointerException</span><br><span class=\"line\">        container.getId().getApplicationAttemptId().getApplicationId();</span><br><span class=\"line\">    &#x2F;&#x2F;application 空值校验</span><br><span class=\"line\">    if (application &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      LOG.info(&quot;Container &quot; + container + &quot; of&quot; + &quot; unknown application &quot;</span><br><span class=\"line\">          + appId + &quot; completed with event &quot; + event);</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"movetoqueue\"><a href=\"#movetoqueue\" class=\"headerlink\" title=\"movetoqueue\"></a>movetoqueue</h3><p>重复释放和无效释放的猜测证否之后,只好再去仔细研究 系统指标异常前的几个运行的任务日志,对 application 的 attempt 和每个 contianer 状态机的状态和 触发事件按照时间线标注出来,对application 发生的所有事件还原出来,发现了其中一个 application 的以下信息  </p>\n<blockquote>\n<p>1.提交到 root.high 队列后,队列资源不足,在某些节点上为该 applicaiton reserve 资源,其他 application 调度时不会再该节点上分配资源<br>2.application 被从 high 队列移动了 low 队列<br>3.container 被移动队列时,源队列和目标队列的资源有变动,两个队列都会被 re-sort,以便优先在资源利用率最低的 queue 调度,所以有每个 container 的移动有四条日志:queueMoveOut 队列的信息,queueMoveOut 的父队列信息(有几个 parentQueue 就会有几条日志),queuMoveIn 队列信息,queueMoveIn 的父队列信息<br>4.在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000002,在 high队列上分配了资源,被 move 到 low 队列后,containerCompleted 之后,释放资源到了 low 队列<br>5.<b>在 move container 前后有一个 container: container_e08_1565789460020_5864_01_000080,在 high 队列上保留了资源,move 操作没有影响到该 container,但是containerCompleted之后释放资源到了 low 队列</b>. </p>\n</blockquote>\n<p>日志如下  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">INFO LeafQueue: assignedContainer application attempt&#x3D;appattempt_1565789460020_5864_000001 container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: null, ] queue&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:420864, vCores:20&gt;, usedCapacity&#x3D;1.4421053, absoluteUsedCapacity&#x3D;0.86526316, numApps&#x3D;3, numContainers&#x3D;20 clusterResource&#x3D;&lt;memory:486400, vCores:136&gt; type&#x3D;OFF_SWITCH  </span><br><span class=\"line\">INFO RMContainerImpl: container_e08_1565789460020_5864_01_000080 Container Transitioned from NEW to RESERVED</span><br><span class=\"line\">INFO LeafQueue: Reserved container  application&#x3D;application_1565789460020_5864 resource&#x3D;&lt;memory:11264, vCores:1&gt; queue&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:443392, vCores:22&gt;, usedCapacity&#x3D;1.5192982, absoluteUsedCapacity&#x3D;0.91157895, numApps&#x3D;3, numContainers&#x3D;22 usedCapacity&#x3D;1.5192982 absoluteUsedCapacity&#x3D;0.91157895 used&#x3D;&lt;memory:443392, vCores:22&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: Re-sorting assigned queue: root.high stats: high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:454656, vCores:23&gt;, usedCapacity&#x3D;1.5578947, absoluteUsedCapacity&#x3D;0.93473685, numApps&#x3D;3, numContainers&#x3D;23</span><br><span class=\"line\">....moving</span><br><span class=\"line\">INFO LeafQueue: movedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service:  &#125;, ] resource&#x3D;&lt;memory:11264, vCores:1&gt; queueMoveOut&#x3D;high: capacity&#x3D;0.6, absoluteCapacity&#x3D;0.6, usedResources&#x3D;&lt;memory:431104, vCores:20&gt;, usedCapacity&#x3D;1.477193, absoluteUsedCapacity&#x3D;0.88631576, numApps&#x3D;3, numContainers&#x3D;20 usedCapacity&#x3D;1.477193 absoluteUsedCapacity&#x3D;0.88631576 used&#x3D;&lt;memory:431104, vCores:20&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: movedContainer queueMoveOut&#x3D;root usedCapacity&#x3D;0.9768421 absoluteUsedCapacity&#x3D;0.9768421 used&#x3D;&lt;memory:475136, vCores:24&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO LeafQueue: movedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6:8041, NodeHttpAddress: hadoop6:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service: &#125;, ] resource&#x3D;&lt;memory:11264, vCores:1&gt; queueMoveIn&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:-22528, vCores:-3&gt;, usedCapacity&#x3D;-0.4631579, absoluteUsedCapacity&#x3D;-0.04631579, numApps&#x3D;0, numContainers&#x3D;-3 usedCapacity&#x3D;-0.4631579 absoluteUsedCapacity&#x3D;-0.04631579 used&#x3D;&lt;memory:-22528, vCores:-3&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO ParentQueue: movedContainer queueMoveIn&#x3D;root usedCapacity&#x3D;1.0 absoluteUsedCapacity&#x3D;1.0 used&#x3D;&lt;memory:486400, vCores:25&gt; cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">....moved</span><br><span class=\"line\">INFO CapacityScheduler: App: application_1565789460020_5864 successfully moved from high to: low</span><br><span class=\"line\">INFO LeafQueue: completedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000080, NodeId: hadoop5:8041, NodeHttpAddress: hadoop5:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: null, ] queue&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:0, vCores:-1&gt;, usedCapacity&#x3D;0.0, absoluteUsedCapacity&#x3D;0.0, numApps&#x3D;1, numContainers&#x3D;-1 cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br><span class=\"line\">INFO LeafQueue: completedContainer container&#x3D;Container: [ContainerId: container_e08_1565789460020_5864_01_000002, NodeId: hadoop6.:8041, NodeHttpAddress: hadoop6.cn:8042, Resource: &lt;memory:11264, vCores:1&gt;, Priority: 1, Token: Token &#123; kind: ContainerToken, service: &#125;, ] queue&#x3D;low: capacity&#x3D;0.1, absoluteCapacity&#x3D;0.1, usedResources&#x3D;&lt;memory:-1024, vCores:-2&gt;, usedCapacity&#x3D;-0.021052632, absoluteUsedCapacity&#x3D;-0.002105263, numApps&#x3D;1, numContainers&#x3D;-2 cluster&#x3D;&lt;memory:486400, vCores:136&gt;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>猜测:application 在移动队列后,对 reservedContainer 没有移动或维护信息不同步,导致了源队列的资源泄露给了目标队列,从而目标队列的 UsedCapacity 为负<br>复现方式<br>&ensp;&ensp;&ensp;&ensp;第一步:提交 application 到资源紧张的 Queue<br>&ensp;&ensp;&ensp;&ensp;第二步:待在 RM 的日志中看到”Trying to fulfill reservation for application ${APPLICATION_ID} on node …”和”Trying to schedule on node…, available:…” 表明 CapacityScheduler为该 ApplicationId 保留了资源,跳过在此 nm 上为其他 app 分配 container<br>&ensp;&ensp;&ensp;&ensp;第三步: 使用 yarn application -movetoqueue ${APPLICATION_ID} -queue ${TOQueue}<br>&ensp;&ensp;&ensp;&ensp;第四步: 待在 RM 日志中看到了”App:${APPLICATION_ID} successfully moved ${FROMQUEUE} to ${TOQUEUE}”后,使用 yarn application –kill ${APPLICATION_ID} 或等待 app 结束<br>&ensp;&ensp;&ensp;&ensp;第五步: 在 YarnUI上查看被移动的目标队列${TOQUEUE}的信息  </p>\n</blockquote>\n<p>注:<br>&ensp;&ensp;&ensp;&ensp;1.一定要在资源紧张的队列上提交 app,以触发调度系统的保留资源.在空闲队列上提交任务复现不了指标异常的问题<br>&ensp;&ensp;&ensp;&ensp;2.最好移动到一个完全空闲的队列上,否则即使复现了资源泄露,也不易看出来  </p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><p>以下代码出自于 hadoop 2.7.3版本<br>Yarn 是一个资源调度平台,集群内存资源和 cpu 资源被 Yarn 抽象为 Resource{memory,core},客户端对 Yarn 的资源请求和 Yarn 内部的资源调度都是以 Container 为基本单位的<br>当客户端向资源队列 Queue 提交 Application 时, 客户端申请的 AM 以及 AM 申请新的执行角色(e.g. spark 的 executor)都是在Container 中运行,那么对于 Yarn 调度的 Container,资源信息被多维聚合[app状态|container状态|container用途|用户|标签]统计维护:    </p>\n<ul>\n<li>Queue.包括直接申请的叶子队列及其所有的父队列    <ol>\n<li>Queue(AbstractCSQueue)<br>&ensp;&ensp;主要指标是 container 数量;保存着 QueueMetrics 和 ResourceUsage(ByLabel) 的引用<br> a. ParentQueue<br>&ensp;&ensp;主要指标是运行的 application数量;保存着所有资源子队列的集合<br> b. LeafQueue \n&ensp;&ensp; 主要指标是每个用户提交 app 的数量和资源用途用量(ResourceUsageByUser);保存着队列 running 和 pending 的 app[attemp]信息               </li>\n<li>QueueMetrics<br>&ensp;&ensp;保留着queue 的指标信息,包括[提交|运行|积压|完成|杀死|失败]app 数量、[分配|待分配|积压]的[container|内存|虚拟核]信息、活跃的[app|user]信息;保存着用户级别的 &lt;username,QueueMetrics&gt;映射  </li>\n<li>ResourceUsage<br>&ensp;&ensp;保留着&lt;label,UsageByLabel&gt;信息,分为used、pending、amused、reserved 四类  </li>\n</ol>\n</li>\n<li>Application(SchedulerApplicationAttempt). app 中维护着当前正在运行的 container,及 yarn 为其保留的 container 信息</li>\n<li>NodeManager. container 所在的 NM 维护着自身运行的所有 container,并通过 RM 心跳汇报所有 container 状态机状态,触发 RM 对container 状态的更新或释放    </li>\n</ul>\n<blockquote>\n</blockquote>\n<p>综上:<br>&ensp;&ensp;&ensp;&ensp;1.指标异常与 container 申请释放有关,所以需要研究 <b>正常申请释放与应用移动队列</b> 对上述维护信息的影响<br>&ensp;&ensp;&ensp;&ensp;2.UI 展示的 numContainers 数据取自 AbstractCSQueue 维护 numContainer 成员变量;usedCapacity,absoluteUsedCapacity,usedResource 均取自AbstractCSQueue$ResourceUsage.used 成员变量.需要关注以上变量在资源申请释放过程中的变化    </p>\n<h3 id=\"正常的资源分配和释放过程\"><a href=\"#正常的资源分配和释放过程\" class=\"headerlink\" title=\"正常的资源分配和释放过程\"></a>正常的资源分配和释放过程</h3><p>RM 的资源分配和资源释放都是被动触发,客户端提交 application 到 RMClientService,app 信息暂存在指定资源队列中,待 NM 向 RM 通过心跳汇报自身信息时 RM 将对该 NM 触发调度,在情况允许的情况下(e.g. 目标队列具有该 NM 的 access 权限,NM 剩余资源满足 app 中一个 ResourceRequest 申请的资源),将在该 NM 上划出 ResourceRequest 要求的资源,并同步 Queue/Application/NodeManager 维护的信息;当 container 完成退出之后,NodeManager 注意到 container 状态机变化,并将其信息附带在心跳中汇报给 RM,触发 Queue/Application/NodeManager 状态变化和信息同步<br>以下代码较为关注资源信息的维护同步,对于其他细节不再赘述    </p>\n<h3 id=\"资源分配\"><a href=\"#资源分配\" class=\"headerlink\" title=\"资源分配\"></a>资源分配</h3><p><img src=\"/img/pictures/negative/yarn_node_update_pic.png\" alt=\"\"></p>\n<p>NM 通过心跳触发 CapacityScheduler 调度,CS 首先按照 NM 上报的信息同步 RM 的信息,为新启动的 container 触发 LAUNCHED 事件,为结束的 container 触发FINISHED 事件;并试图在该节点上分配资源<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">...</span><br><span class=\"line\">case NODE_UPDATE:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      NodeUpdateSchedulerEvent nodeUpdatedEvent &#x3D; (NodeUpdateSchedulerEvent)event;</span><br><span class=\"line\">      RMNode node &#x3D; nodeUpdatedEvent.getRMNode();</span><br><span class=\"line\">      &#x2F;&#x2F;同步 RM 信息</span><br><span class=\"line\">      nodeUpdate(node);</span><br><span class=\"line\">      if (!scheduleAsynchronously) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;试图在 nm 分配资源</span><br><span class=\"line\">        allocateContainersToNode(getNode(node.getNodeID()));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\"> ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>依据汇报心跳的 NodeManager 是否被保留了资源执行相应的逻辑<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private synchronized void allocateContainersToNode(FiCaSchedulerNode node) &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; node.getReservedContainer();</span><br><span class=\"line\">    &#x2F;&#x2F;当前 node 被某 applocation 保留了</span><br><span class=\"line\">    if (reservedContainer !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerApp reservedApplication &#x3D;</span><br><span class=\"line\">          getCurrentAttemptForContainer(reservedContainer.getContainerId());</span><br><span class=\"line\">      &#x2F;&#x2F; Try to fulfill the reservation</span><br><span class=\"line\">      LOG.info(&quot;Trying to fulfill reservation for application &quot; + </span><br><span class=\"line\">          reservedApplication.getApplicationId() + &quot; on node: &quot; + </span><br><span class=\"line\">          node.getNodeID());</span><br><span class=\"line\">      LeafQueue queue &#x3D; ((LeafQueue)reservedApplication.getQueue());</span><br><span class=\"line\">      CSAssignment assignment &#x3D;</span><br><span class=\"line\">          queue.assignContainers(</span><br><span class=\"line\">              clusterResource,</span><br><span class=\"line\">              node,</span><br><span class=\"line\">              new ResourceLimits(labelManager.getResourceByLabel(</span><br><span class=\"line\">                  RMNodeLabelsManager.NO_LABEL, clusterResource)));</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;当前节点未被保留</span><br><span class=\"line\">    if (node.getReservedContainer() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     \t...</span><br><span class=\"line\">     \t&#x2F;&#x2F;交由 root 队列代理分配.root 是资源队列树形结构的根节点,类型一定ParentQueue</span><br><span class=\"line\">        root.assignContainers(</span><br><span class=\"line\">            clusterResource,</span><br><span class=\"line\">            node,</span><br><span class=\"line\">            new ResourceLimits(labelManager.getResourceByLabel(</span><br><span class=\"line\">                RMNodeLabelsManager.NO_LABEL, clusterResource)));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      LOG.info(&quot;Skipping scheduling since node &quot; + node.getNodeID() + </span><br><span class=\"line\">          &quot; is reserved by application &quot; + </span><br><span class=\"line\">          node.getReservedContainer().getContainerId().getApplicationAttemptId()</span><br><span class=\"line\">          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 将节点委派给子队列,试图分配资源<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> public synchronized CSAssignment assignContainers(Resource clusterResource,</span><br><span class=\"line\">     FiCaSchedulerNode node, ResourceLimits resourceLimits) &#123;</span><br><span class=\"line\">   CSAssignment assignment &#x3D; </span><br><span class=\"line\">       new CSAssignment(Resources.createResource(0, 0), NodeType.NODE_LOCAL);</span><br><span class=\"line\">   Set&lt;String&gt; nodeLabels &#x3D; node.getLabels();</span><br><span class=\"line\">   ...校验 queue 对 node 的 access 权限</span><br><span class=\"line\">   while (canAssign(clusterResource, node)) &#123;</span><br><span class=\"line\">     ...校验 queue 资源是否超限      </span><br><span class=\"line\">     &#x2F;&#x2F; Schedule 递归交由子队列去分配</span><br><span class=\"line\">     CSAssignment assignedToChild &#x3D; </span><br><span class=\"line\">         assignContainersToChildQueues(clusterResource, node, resourceLimits);</span><br><span class=\"line\">     assignment.setType(assignedToChild.getType());</span><br><span class=\"line\">     &#x2F;&#x2F; Done if no child-queue assigned anything</span><br><span class=\"line\">     &#x2F;&#x2F; 如果分配到了资源, assignedToChild 大于 Resource&lt;0,0&gt;</span><br><span class=\"line\">     if (Resources.greaterThan(</span><br><span class=\"line\">             resourceCalculator, clusterResource, </span><br><span class=\"line\">             assignedToChild.getResource(), Resources.none())) &#123;</span><br><span class=\"line\">             &#x2F;&#x2F;同步维护的信息</span><br><span class=\"line\">       super.allocateResource(clusterResource, assignedToChild.getResource(),</span><br><span class=\"line\">           nodeLabels);</span><br><span class=\"line\">       Resources.addTo(assignment.getResource(), assignedToChild.getResource());</span><br><span class=\"line\">       ...</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">       break;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">...</span><br><span class=\"line\">       break;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;     </span><br><span class=\"line\">   return assignment;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>先看如果分配到资源的话 同步的信息:<br>1.按照 label 去更新 ResourceUsage 中维护的用户使用资源 used(incUsed 方法)<br>2.增加了该队列的 numContainer 数量<br>需要注意的是 <b>无论后续得到的是 allocated container 还是 reserved container,都增加了 numContainer的值,增加了用户 USED 类型的内存和虚拟核数量</b><br>[AbstractCSQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Update usedResources by labels</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看对 queueUsage:ResourceUsage 做了什么操作<br>ResourceUsage 中维护了一个Map结构 usages,key 是标签类型,value 是 UsageByLabel;UsageByLabel 中只有一个数组 Resource[],数组中的每个值分别表征着 USED,PENDING,AMUSED,RESERVED 用途的 Resource 数量<br>[ResourceUsage.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F; &lt;labelName,UsageByLabel&gt;</span><br><span class=\"line\">private Map&lt;String, UsageByLabel&gt; usages;</span><br><span class=\"line\">public void incUsed(String label, Resource res) &#123;</span><br><span class=\"line\">    _inc(label, ResourceType.USED, res);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> private void _inc(String label, ResourceType type, Resource res) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">     UsageByLabel usage &#x3D; getAndAddIfMissing(label);</span><br><span class=\"line\">     Resources.addTo(usage.resArr[type.idx], res);      </span><br><span class=\"line\">     ...</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>UsageByLabel 使用一个 Resource[]数组来存储资源的用途和用量<br>[UsageByLabel.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static class UsageByLabel &#123;</span><br><span class=\"line\">  &#x2F;&#x2F; usage by label, contains all UsageType</span><br><span class=\"line\">  private Resource[] resArr;</span><br><span class=\"line\">  public UsageByLabel(String label) &#123;</span><br><span class=\"line\">    resArr &#x3D; new Resource[ResourceType.values().length];</span><br><span class=\"line\">    for (int i &#x3D; 0; i &lt; resArr.length; i++) &#123;</span><br><span class=\"line\">      resArr[i] &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> private enum ResourceType &#123;</span><br><span class=\"line\">  USED(0), PENDING(1), AMUSED(2), RESERVED(3);</span><br><span class=\"line\">  private int idx;</span><br><span class=\"line\">  private ResourceType(int value) &#123;</span><br><span class=\"line\">    this.idx &#x3D; value;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再回到资源分配,root 队列深度优先遍历所有子队列,尝试在叶子队列上分配资源<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized CSAssignment assignContainers(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerNode node, ResourceLimits currentResourceLimits) &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    &#x2F;&#x2F; Check for reserved resources</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; node.getReservedContainer();</span><br><span class=\"line\">    if (reservedContainer !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerApp application &#x3D; </span><br><span class=\"line\">          getApplication(reservedContainer.getApplicationAttemptId());</span><br><span class=\"line\">      synchronized (application) &#123;</span><br><span class=\"line\">        return assignReservedContainer(application, node, reservedContainer,</span><br><span class=\"line\">            clusterResource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;对目前活跃的 application,尝试在当前 NM 上分配资源</span><br><span class=\"line\">    for (FiCaSchedulerApp application : activeApplications) &#123;</span><br><span class=\"line\">    \t....</span><br><span class=\"line\">      synchronized (application) &#123;</span><br><span class=\"line\">         ....        </span><br><span class=\"line\">          &#x2F;&#x2F; Schedule in priority order</span><br><span class=\"line\">        for (Priority priority : application.getPriorities()) &#123;</span><br><span class=\"line\">          ResourceRequest anyRequest &#x3D;</span><br><span class=\"line\">              application.getResourceRequest(priority, ResourceRequest.ANY);</span><br><span class=\"line\">         if (null &#x3D;&#x3D; anyRequest) &#123;</span><br><span class=\"line\">            continue;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          ....校验性工作</span><br><span class=\"line\">          &#x2F;&#x2F;校验通过,在此节点上分配资源</span><br><span class=\"line\">          CSAssignment assignment &#x3D;  </span><br><span class=\"line\">            assignContainersOnNode(clusterResource, node, application, priority, </span><br><span class=\"line\">                null, currentResourceLimits);</span><br><span class=\"line\">\t.....</span><br><span class=\"line\">          &#x2F;&#x2F; Did we schedule or reserve a container?</span><br><span class=\"line\">          Resource assigned &#x3D; assignment.getResource();</span><br><span class=\"line\">          if (Resources.greaterThan(</span><br><span class=\"line\">              resourceCalculator, clusterResource, assigned, Resources.none())) &#123;</span><br><span class=\"line\">            &#x2F;&#x2F;更新当前[叶子]队列的 numContainer,ResourceUsageByLabel和 ResourceUsageByUser.</span><br><span class=\"line\">            &#x2F;&#x2F;比 ParentQueue.allocateResource 多出了一个用户层面的资源统计</span><br><span class=\"line\">            allocateResource(clusterResource, application, assigned,</span><br><span class=\"line\">                node.getLabels());</span><br><span class=\"line\">            .....            </span><br><span class=\"line\">            return assignment;</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">            break;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return NULL_ASSIGNMENT;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看LeafQueue#allocateResource 更新信息时更新的指标和内容\nLeafQueue 和ParentQueue  一样,使用抽象父类AbstractCSQueue#allocateResource() 更新当前队列的 numContainer 和 标签层面的 ResourceUsage<br>然后使用 LeafQueue 中维护的Map&lt;String,User&gt;成员变量 users来维护用户标签层面的资源信息 \n[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void allocateResource(Resource clusterResource,</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Resource resource,</span><br><span class=\"line\">      Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;使用的是 AbstractCSQueue#allocateResource,和 ParentQueue更新的指标及内容相同:numContainer,ResourceUsageByLabel</span><br><span class=\"line\">    super.allocateResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;更新用户层面的 ResourceUsage</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.assignContainer(resource, nodeLabels);</span><br><span class=\"line\">    Resources.subtractFrom(application.getHeadroom(), resource); &#x2F;&#x2F; headroom</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>User 类中有一个 ResourceUsage,维护着在用户层面 各标签的资源用途和用量<br>[User.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static class User &#123;</span><br><span class=\"line\">    ResourceUsage userResourceUsage &#x3D; new ResourceUsage();</span><br><span class=\"line\">    volatile Resource userResourceLimit &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    int pendingApplications &#x3D; 0;</span><br><span class=\"line\">    int activeApplications &#x3D; 0;</span><br><span class=\"line\">     public void assignContainer(Resource resource,</span><br><span class=\"line\">        Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">        userResourceUsage.incUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.incUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上文 LeafQueue#assignContainers方法 遍历每个活跃的 application,尝试在当前 nodemanager 上分配资源,调用 assignContainersOnNode()方法进行下一步的分配逻辑,并增加 app 在本地化层面分配的 container 数量,此指标逻辑不在此讨论<br>按照本地性优先级,优先分配  NODE_LOCAL(本节点)&gt;RACK_LOCAL(本机架)&gt;OFF_SWITCH(跨机架)<br>YARN 的本地性不同于 MR/SPARK 的本地性,YARN的本地性性体现在 Client/AM申请 container 的时候可以指定 container 所在的节点,此处的 NODE_LOCAL、RACK_LOCAL 和 OFF_SWITCH 是相对于申请 container 时指定的节点而言;数据本地化计算其实更多的是靠计算框架配合,按照数据所处的位置优先分配给计算节点(e.g. RDD#getPreferredLocations)<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private CSAssignment assignContainersOnNode(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerNode node, FiCaSchedulerApp application, Priority priority,</span><br><span class=\"line\">      RMContainer reservedContainer, ResourceLimits currentResoureLimits) &#123;</span><br><span class=\"line\">    Resource assigned &#x3D; Resources.none();</span><br><span class=\"line\">    NodeType requestType &#x3D; null;</span><br><span class=\"line\">    MutableObject allocatedContainer &#x3D; new MutableObject();</span><br><span class=\"line\">    &#x2F;&#x2F; Data-local</span><br><span class=\"line\">    ResourceRequest nodeLocalResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, node.getNodeName());</span><br><span class=\"line\">    if (nodeLocalResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      requestType &#x3D; NodeType.NODE_LOCAL;</span><br><span class=\"line\">      assigned &#x3D;</span><br><span class=\"line\">          assignNodeLocalContainers(clusterResource, nodeLocalResourceRequest, </span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (Resources.greaterThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">          assigned, Resources.none())) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F;update locality statistics</span><br><span class=\"line\">        if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">          application.incNumAllocatedContainers(NodeType.NODE_LOCAL,</span><br><span class=\"line\">            requestType);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return new CSAssignment(assigned, NodeType.NODE_LOCAL);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Rack-local</span><br><span class=\"line\">    ResourceRequest rackLocalResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, node.getRackName());</span><br><span class=\"line\">    if (rackLocalResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      if (!rackLocalResourceRequest.getRelaxLocality()) &#123;</span><br><span class=\"line\">        return SKIP_ASSIGNMENT;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (requestType !&#x3D; NodeType.NODE_LOCAL) &#123;</span><br><span class=\"line\">        requestType &#x3D; NodeType.RACK_LOCAL;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      assigned &#x3D; </span><br><span class=\"line\">          assignRackLocalContainers(clusterResource, rackLocalResourceRequest, </span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (Resources.greaterThan(resourceCalculator, clusterResource,</span><br><span class=\"line\">          assigned, Resources.none())) &#123;</span><br><span class=\"line\">        if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">          application.incNumAllocatedContainers(NodeType.RACK_LOCAL,</span><br><span class=\"line\">            requestType);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return new CSAssignment(assigned, NodeType.RACK_LOCAL);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Off-switch</span><br><span class=\"line\">    ResourceRequest offSwitchResourceRequest &#x3D;</span><br><span class=\"line\">        application.getResourceRequest(priority, ResourceRequest.ANY);</span><br><span class=\"line\">    if (offSwitchResourceRequest !&#x3D; null) &#123;</span><br><span class=\"line\">      if (!offSwitchResourceRequest.getRelaxLocality()) &#123;</span><br><span class=\"line\">        return SKIP_ASSIGNMENT;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (requestType !&#x3D; NodeType.NODE_LOCAL</span><br><span class=\"line\">          &amp;&amp; requestType !&#x3D; NodeType.RACK_LOCAL) &#123;</span><br><span class=\"line\">        requestType &#x3D; NodeType.OFF_SWITCH;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      assigned &#x3D;</span><br><span class=\"line\">          assignOffSwitchContainers(clusterResource, offSwitchResourceRequest,</span><br><span class=\"line\">            node, application, priority, reservedContainer,</span><br><span class=\"line\">            allocatedContainer, currentResoureLimits);</span><br><span class=\"line\">      if (allocatedContainer.getValue() !&#x3D; null) &#123;</span><br><span class=\"line\">        application.incNumAllocatedContainers(NodeType.OFF_SWITCH, requestType);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return new CSAssignment(assigned, NodeType.OFF_SWITCH);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return SKIP_ASSIGNMENT;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>LeafQueue#assignNodeLocalContainers,assignRackLocalContainers,assignOffSwitchContainers的核心逻辑被封装为一处,只是本地化类型 NodeType 不同<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, </span><br><span class=\"line\">      FiCaSchedulerApp application, Priority priority, </span><br><span class=\"line\">      ResourceRequest request, NodeType type, RMContainer rmContainer,</span><br><span class=\"line\">      MutableObject createdContainer, ResourceLimits currentResoureLimits) &#123;</span><br><span class=\"line\">    ....    </span><br><span class=\"line\">    Resource capability &#x3D; request.getCapability();</span><br><span class=\"line\">    Resource available &#x3D; node.getAvailableResource();</span><br><span class=\"line\">    Resource totalResource &#x3D; node.getTotalResource();</span><br><span class=\"line\">    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,</span><br><span class=\"line\">        capability, totalResource)) &#123;</span><br><span class=\"line\">      LOG.warn(&quot;Node : &quot; + node.getNodeID()</span><br><span class=\"line\">          + &quot; does not have sufficient resource for request : &quot; + request</span><br><span class=\"line\">          + &quot; node total capability : &quot; + node.getTotalResource());</span><br><span class=\"line\">      return Resources.none();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Create the container if necessary</span><br><span class=\"line\">    Container container &#x3D; </span><br><span class=\"line\">        getContainer(rmContainer, application, node, capability, priority);</span><br><span class=\"line\">  \t...</span><br><span class=\"line\">      boolean shouldAllocOrReserveNewContainer &#x3D; shouldAllocOrReserveNewContainer(</span><br><span class=\"line\">        application, priority, capability);</span><br><span class=\"line\">    &#x2F;&#x2F;依据 节点可用资源量与申请的资源量 做除法来判断节点剩余资源能否满足需求 </span><br><span class=\"line\">    int availableContainers &#x3D; </span><br><span class=\"line\">        resourceCalculator.computeAvailableContainers(available, capability);</span><br><span class=\"line\">    boolean needToUnreserve &#x3D; Resources.greaterThan(resourceCalculator,clusterResource,</span><br><span class=\"line\">        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());</span><br><span class=\"line\">\t</span><br><span class=\"line\">    if (availableContainers &gt; 0) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;如果节点上足够分配一个 container 则分配</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#x2F;&#x2F;调用 application 和 nodemanager 的方法,触发这两处的信息同步</span><br><span class=\"line\">      RMContainer allocatedContainer &#x3D; </span><br><span class=\"line\">          application.allocate(type, node, priority, request, container);</span><br><span class=\"line\">          </span><br><span class=\"line\">      node.allocateContainer(allocatedContainer);</span><br><span class=\"line\">      </span><br><span class=\"line\">      createdContainer.setValue(allocatedContainer);</span><br><span class=\"line\">      return container.getResource();</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    \t....</span><br><span class=\"line\">        &#x2F;&#x2F;节点上没有足够的资源满足需求,则为该 application 保留该节点的资源</span><br><span class=\"line\">        reserve(application, priority, node, rmContainer, container);</span><br><span class=\"line\">        return request.getCapability();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return Resources.none();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上文有三处地方需要同步资源信息:reserve()方法,FiCaSchedulerApp#allocate,FiCaSchedulerNode#allocateContainer<br>先看FiCaSchedulerApp#allocate方法,在 application 层面修改了什么信息<br>[FiCaSchedulerApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public RMContainer allocate(NodeType type, FiCaSchedulerNode node,</span><br><span class=\"line\">     Priority priority, ResourceRequest request, </span><br><span class=\"line\">     Container container) &#123;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">   &#x2F;&#x2F;将 container 封装成 RMContainer 的形式,记录在 application 的所有 container 集合 newlyAllocatedContainers 和 运行态(相对 reserved)的 container 集合 liveContainers 中  </span><br><span class=\"line\">   RMContainer rmContainer &#x3D; new RMContainerImpl(container, this</span><br><span class=\"line\">       .getApplicationAttemptId(), node.getNodeID(),</span><br><span class=\"line\">       appSchedulingInfo.getUser(), this.rmContext);</span><br><span class=\"line\">   newlyAllocatedContainers.add(rmContainer);</span><br><span class=\"line\">   liveContainers.put(container.getId(), rmContainer);    </span><br><span class=\"line\">   ....</span><br><span class=\"line\">   &#x2F;&#x2F;更新 metrics 信息</span><br><span class=\"line\">   List&lt;ResourceRequest&gt; resourceRequestList &#x3D; appSchedulingInfo.allocate(</span><br><span class=\"line\">       type, node, priority, request, container);</span><br><span class=\"line\">    &#x2F;&#x2F;当前 app 消费的资源量</span><br><span class=\"line\">   Resources.addTo(currentConsumption, container.getResource());</span><br><span class=\"line\">   ...</span><br><span class=\"line\">   &#x2F;&#x2F;触发 RMContainer 状态机变化,container 可以准备运行时环境,下载依赖等</span><br><span class=\"line\">   rmContainer.handle(</span><br><span class=\"line\">       new RMContainerEvent(container.getId(), RMContainerEventType.START));</span><br><span class=\"line\">   RMAuditLogger.logSuccess(getUser(), </span><br><span class=\"line\">       AuditConstants.ALLOC_CONTAINER, &quot;SchedulerApp&quot;, </span><br><span class=\"line\">       getApplicationId(), container.getId());</span><br><span class=\"line\">   return rmContainer;</span><br></pre></td></tr></table></figure>\n<p>AppSchedulingInfo#allocate 更新 QueueMetrics 信息<br>[AppSchedulingInfo.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public List&lt;ResourceRequest&gt; allocate(NodeType type,</span><br><span class=\"line\">      SchedulerNode node, Priority priority, ResourceRequest request,</span><br><span class=\"line\">      Container container) &#123;</span><br><span class=\"line\">    List&lt;ResourceRequest&gt; resourceRequests &#x3D; new ArrayList&lt;ResourceRequest&gt;();</span><br><span class=\"line\">    &#x2F;&#x2F;本地化层面的统计信息</span><br><span class=\"line\">    if (type &#x3D;&#x3D; NodeType.NODE_LOCAL) &#123;</span><br><span class=\"line\">      allocateNodeLocal(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125; else if (type &#x3D;&#x3D; NodeType.RACK_LOCAL) &#123;</span><br><span class=\"line\">      allocateRackLocal(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      allocateOffSwitch(node, priority, request, container, resourceRequests);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics metrics &#x3D; queue.getMetrics();</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &#x2F;&#x2F;对QueueMetrics 做已经分配的资源的统计</span><br><span class=\"line\">    metrics.allocateResources(user, 1, request.getCapability(), true);</span><br><span class=\"line\">    metrics.incrNodeTypeAggregations(user, type);</span><br><span class=\"line\">    return resourceRequests;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>QueueMetrics#allocateResource 对 userMetrics 做判断;对 parent 也做了判断 如果parent 非空那么会递归更新 parent 的信息,对userMetrics 和 parent 执行的方法都是 QueueMetrics#allocateResources,更新的指标一样:增加 allocate 的 container 数量,内存量,虚拟核数量<br>[QueueMetrics.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void allocateResources(String user, int containers, Resource res,</span><br><span class=\"line\">      boolean decrPending) &#123;</span><br><span class=\"line\">    allocatedContainers.incr(containers);</span><br><span class=\"line\">    aggregateContainersAllocated.incr(containers);</span><br><span class=\"line\">    allocatedMB.incr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.incr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    if (decrPending) &#123;</span><br><span class=\"line\">      _decrPendingResources(containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>FiCaSchedulerNode#allocateContainer, NodeManager 同步 allocate 信息 \n[FiCaSchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void allocateContainer(RMContainer rmContainer) &#123;</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">     &#x2F;&#x2F;减少当前 nm 的可用资源,增加当前 nm 的已分配资源</span><br><span class=\"line\">    deductAvailableResource(container.getResource());</span><br><span class=\"line\">    &#x2F;&#x2F;增加自身维护的 numContainer 数量,不同于 AbstractCSQueue.numContainers,维护Queue维度和 NodeManager 维度的 container 数量</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    &#x2F;&#x2F;启动的 container 列表</span><br><span class=\"line\">    launchedContainers.put(container.getId(), rmContainer);</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void deductAvailableResource(Resource resource) &#123;</span><br><span class=\"line\">    Resources.subtractFrom(availableResource, resource);</span><br><span class=\"line\">    Resources.addTo(usedResource, resource);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>如果在节点上 reserve 了资源,同步的信息和 allocate 的方式相似,会更新 QueueMetrics 对 reserved[containr|memory|core]的信息,更新application reservedContainer 及 currentReservationMemroy 信息,更新 NodeManager 当前 reservedContainer 信息<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private void reserve(FiCaSchedulerApp application, Priority priority, </span><br><span class=\"line\">      FiCaSchedulerNode node, RMContainer rmContainer, Container container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;传入的 rmContainer 为 null</span><br><span class=\"line\">    if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      getMetrics().reserveResource(</span><br><span class=\"line\">          application.getUser(), container.getResource());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Inform the application </span><br><span class=\"line\">    rmContainer &#x3D; application.reserve(node, priority, rmContainer, container);</span><br><span class=\"line\">    &#x2F;&#x2F; Update the node</span><br><span class=\"line\">    node.reserveResource(application, priority, rmContainer);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>QueueMetrics#reserveResource在下方有一个对 parent 的判断,递归调用依次更新 ParentQueue 的信息;有一个对 userMetrics 的判断,对 QueueMetrics 中维护的 users:Map[String, QueueMetrics] 同步用户层面的统计信息,执行的方法都是当前方法QueueMetrics#reserveResource(),同步reserved 资源信息:增加 reserved container 数量,内存量,虚拟核数量  </p>\n<p>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void reserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.incr();</span><br><span class=\"line\">    reservedMB.incr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.incr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>application 同步 reserved container 信息<br>[SchedulerApplicationAttempt.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized RMContainer reserve(SchedulerNode node, Priority priority,</span><br><span class=\"line\">     RMContainer rmContainer, Container container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;上游传入的 rmContainer 为 null</span><br><span class=\"line\">   if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     rmContainer &#x3D; </span><br><span class=\"line\">         new RMContainerImpl(container, getApplicationAttemptId(), </span><br><span class=\"line\">             node.getNodeID(), appSchedulingInfo.getUser(), rmContext);</span><br><span class=\"line\">     &#x2F;&#x2F;增加当前 application 保留的资源信息</span><br><span class=\"line\">     Resources.addTo(currentReservation, container.getResource());</span><br><span class=\"line\">     resetReReservations(priority);</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; Note down the re-reservation</span><br><span class=\"line\">     addReReservation(priority);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;在 container 信息中设置了绑定节点的信息....</span><br><span class=\"line\">   rmContainer.handle(new RMContainerReservedEvent(container.getId(), </span><br><span class=\"line\">       container.getResource(), node.getNodeID(), priority));</span><br><span class=\"line\">       &#x2F;&#x2F;加入到维护的 reservedContainer 信息中</span><br><span class=\"line\">   Map&lt;NodeId, RMContainer&gt; reservedContainers &#x3D; </span><br><span class=\"line\">       this.reservedContainers.get(priority);</span><br><span class=\"line\">   if (reservedContainers &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     reservedContainers &#x3D; new HashMap&lt;NodeId, RMContainer&gt;();</span><br><span class=\"line\">     this.reservedContainers.put(priority, reservedContainers);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   reservedContainers.put(node.getNodeID(), rmContainer);</span><br><span class=\"line\">   return rmContainer;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>FicaSchedulerNode#reserveResource(),NodeManager 同步 reserved container 信息<br>[FicaSchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void reserveResource(</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Priority priority,</span><br><span class=\"line\">      RMContainer container) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Check if it&#39;s already reserved</span><br><span class=\"line\">    RMContainer reservedContainer &#x3D; getReservedContainer();</span><br><span class=\"line\">    ....空值校验性,重复保留校验等操作</span><br><span class=\"line\">    &#x2F;&#x2F;设置当前节点保留的 container 为传入值</span><br><span class=\"line\">    setReservedContainer(container);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n\n<blockquote>\n<p>综上:<br>        CS 调度器在 NM 上为 APP 分配 container 时,会同步 [Parent | Leaf] Queue/APP/NM 三个位置维护的信息<br>    1.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, 其 <b>ParentQueue</b>都会维护:<br>&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ParentQueue <b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>    2.分配时,无论从叶子队列得到 allocated 或 reserved 类型的 container, <b>LeafQueue</b> 本身都会维护:<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>&ensp;&ensp;&ensp;&ensp; LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>    3.在节点上申请资源时,按照节点剩余可用资源和资源需求量做除法,若满足需求则分配为 allocatedContainer,不满足则分配 reservedContainer<br>    4.若在节点上分配 allocatedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp; Application 方面:newlyAllocatedContainers和 livingContainers 列表,app 已占用资源量<br>&ensp;&ensp;&ensp;&ensp; NodeManager 方面:numContainer 数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表<br>&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身QueueMetrics</b>的allocated [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度 QueueMetrics</b> 的 allocated [containerNum | MB | cores]信息<br>    5.若在节点上分配 reservedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量<br>&ensp;&ensp;&ensp;&ensp;NodeManager 方面:设置 NM 保留的 container 为本次的 reservedContainer<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  </p>\n</blockquote>\n<h3 id=\"资源释放\"><a href=\"#资源释放\" class=\"headerlink\" title=\"资源释放\"></a>资源释放</h3><p>以客户端在命令行执行 “yarn application –kill ${APPLICATION_ID}”为例<br>RM 中响应请求 方法调用栈为:<br>-&gt;ClientRMService#forceKillApplication<br>&ensp;&ensp;-&gt;RMAppImpl$KillAttemptTransition#transition<br>&ensp;&ensp;&ensp;&ensp;-&gt;RMAppAttemptImpl$BaseFinalTransition#transition<br>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;-&gt;CapacityScheduler#handle()   </p>\n<p>application 结束时,主要做了三件事情,  </p>\n<ol>\n<li>Container: 释放 APP 持有的<b> living,reserved </b> container  </li>\n<li>Application(QueueMetrcis): 清理 LeafQueue 及其 ParentQueue 在 <b>APP</b> 层面的统计(e.g. appRunnings);清理 LeafQueue 在<b>用户APP</b> 层面的统计  </li>\n<li>AM(ResourceUsage): 释放 LeafQueue 及其 ParentQueue 在 <b>AM</b> 层面的资源用量;  释放 LeafQueue 在<b>用户AM</b> 层面的统计(AMUSED)  </li>\n</ol>\n<p>由于复现指标异常成功后,发现移动到目标队列后,目标队列增加的资源总量是 spark.executor.memory 的整数倍(executor-mem 11G,driver-memory 2G),所以对上述的第二点和第三点不多分析,主要关注 <b>非 AM 的container</b> 的释放,livingContainers 和 reservedContainer 的释放代码是同一个,仅仅是释放 container 的文字性说明(原文: diagnostics 意为诊断)不同<br>[CapactiyScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">....</span><br><span class=\"line\"> case APP_ATTEMPT_REMOVED:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent &#x3D;</span><br><span class=\"line\">          (AppAttemptRemovedSchedulerEvent) event;</span><br><span class=\"line\">      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),</span><br><span class=\"line\">        appAttemptRemovedEvent.getFinalAttemptState(),</span><br><span class=\"line\">        appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private synchronized void doneApplicationAttempt(</span><br><span class=\"line\">      ApplicationAttemptId applicationAttemptId,</span><br><span class=\"line\">      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    FiCaSchedulerApp attempt &#x3D; getApplicationAttempt(applicationAttemptId);</span><br><span class=\"line\">    SchedulerApplication&lt;FiCaSchedulerApp&gt; application &#x3D;</span><br><span class=\"line\">        applications.get(applicationAttemptId.getApplicationId());</span><br><span class=\"line\">        ....</span><br><span class=\"line\">    &#x2F;&#x2F;从 app 维护的 livingContainer 列表中,释放掉 allocated acquired running 状态的 container</span><br><span class=\"line\">    for (RMContainer rmContainer : attempt.getLiveContainers()) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;在 kill 时keepContainer 为 false;在 failed 时,keepContainer 按场景可为 true</span><br><span class=\"line\">      if (keepContainers</span><br><span class=\"line\">          &amp;&amp; rmContainer.getState().equals(RMContainerState.RUNNING)) &#123;</span><br><span class=\"line\">        continue;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;1.释放 container</span><br><span class=\"line\">      completedContainer(</span><br><span class=\"line\">        rmContainer,</span><br><span class=\"line\">        SchedulerUtils.createAbnormalContainerStatus(</span><br><span class=\"line\">        &#x2F;&#x2F;COMPLETED_APPLICATION:Container of a completed application</span><br><span class=\"line\">          rmContainer.getContainerId(), SchedulerUtils.COMPLETED_APPLICATION),</span><br><span class=\"line\">        RMContainerEventType.KILL);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">    &#x2F;&#x2F;从 app 维护的 reservedContainer 列表中,释放掉 reserved 状态的 container</span><br><span class=\"line\">    for (RMContainer rmContainer : attempt.getReservedContainers()) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;1.释放 container</span><br><span class=\"line\">      completedContainer(</span><br><span class=\"line\">        rmContainer,</span><br><span class=\"line\">        SchedulerUtils.createAbnormalContainerStatus(</span><br><span class=\"line\">          rmContainer.getContainerId(), &quot;Application Complete&quot;),</span><br><span class=\"line\">        RMContainerEventType.KILL);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;2. 清理 pending 的 resourceRequest,并同步 [LeafQueue | ParentQueue] [本身 | 用户]维度的 QueueMetrics appRunnings | appPendings </span><br><span class=\"line\">    attempt.stop(rmAppAttemptFinalState);</span><br><span class=\"line\">    String queueName &#x3D; attempt.getQueue().getQueueName();</span><br><span class=\"line\">    CSQueue queue &#x3D; queues.get(queueName);</span><br><span class=\"line\">    if (!(queue instanceof LeafQueue)) &#123;</span><br><span class=\"line\">      LOG.error(&quot;Cannot finish application &quot; + &quot;from non-leaf queue: &quot;</span><br><span class=\"line\">          + queueName);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;3. 同步 [LeafQueue | ParentQueue] [本身 | 用户]维度 QueueUsage 的 AMUSED 指标</span><br><span class=\"line\">      queue.finishApplicationAttempt(attempt, queue.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>CapacityScheduler#completedContainer主要做了一些校验,避免无效释放<br>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> protected synchronized void completedContainer(RMContainer rmContainer,</span><br><span class=\"line\">    ContainerStatus containerStatus, RMContainerEventType event) &#123;</span><br><span class=\"line\">  if (rmContainer &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">    LOG.info(&quot;Null container completed...&quot;);</span><br><span class=\"line\">    return;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">  FiCaSchedulerApp application &#x3D;</span><br><span class=\"line\">      getCurrentAttemptForContainer(container.getId());</span><br><span class=\"line\">  ApplicationId appId &#x3D;</span><br><span class=\"line\">      container.getId().getApplicationAttemptId().getApplicationId();</span><br><span class=\"line\">  if (application &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">    LOG.info(&quot;Container &quot; + container + &quot; of&quot; + &quot; unknown application &quot;</span><br><span class=\"line\">        + appId + &quot; completed with event &quot; + event);</span><br><span class=\"line\">    return;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;container 所在的 NM视图</span><br><span class=\"line\">  FiCaSchedulerNode node &#x3D; getNode(container.getNodeId());</span><br><span class=\"line\">   &#x2F;&#x2F;代理 container 的 LeafQueue</span><br><span class=\"line\">  LeafQueue queue &#x3D; (LeafQueue)application.getQueue();</span><br><span class=\"line\">  queue.completedContainer(clusterResource, application, node, </span><br><span class=\"line\">      rmContainer, containerStatus, event, null, true);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>先按照 RMContainer 状态机状态判断后续将当前 container 按照 reservedContainer处理或者 allocatedContainer 处理<br>然后若释放 container 成功,则同步 用户和标签维度 QueueUsage的 USED 指标,减少 Queue 运行的 container 数量<br>最后若当前 Queue 非 rootQueue,则递归同步当前 Queue 的所有 ParentQueue做上述处理<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void completedContainer(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, FiCaSchedulerNode node, RMContainer rmContainer, </span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event, CSQueue childQueue,</span><br><span class=\"line\">      boolean sortQueues) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      boolean removed &#x3D; false;</span><br><span class=\"line\">      synchronized (this) &#123;</span><br><span class=\"line\">        Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">\t&#x2F;&#x2F; 1. 若 contaienr 状态机为 reserved 状态,在节点上解除 reserve</span><br><span class=\"line\">        if (rmContainer.getState() &#x3D;&#x3D; RMContainerState.RESERVED) &#123;</span><br><span class=\"line\">          removed &#x3D; unreserve(application, rmContainer.getReservedPriority(),</span><br><span class=\"line\">              node, rmContainer);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 2.若 container 状态机不为 reserved 状态,在节点上解除 allocate,并使 NM 释放 container</span><br><span class=\"line\">          removed &#x3D;</span><br><span class=\"line\">            application.containerCompleted(rmContainer, containerStatus, event);</span><br><span class=\"line\">          node.releaseContainer(container);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F;3.释放 同步 QueueUsage</span><br><span class=\"line\">        if (removed) &#123;</span><br><span class=\"line\">          releaseResource(clusterResource, application,</span><br><span class=\"line\">              container.getResource(), node.getLabels());</span><br><span class=\"line\">          LOG.info(&quot;completedContainer&quot; +</span><br><span class=\"line\">              &quot; container&#x3D;&quot; + container +</span><br><span class=\"line\">              &quot; queue&#x3D;&quot; + this +</span><br><span class=\"line\">              &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;4.递归执行 Parent的释放逻辑</span><br><span class=\"line\">      if (removed) &#123;</span><br><span class=\"line\">        getParent().completedContainer(clusterResource, application, node,</span><br><span class=\"line\">          rmContainer, null, event, this, sortQueues);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>先看第 4 处对 ParentQueue 的处理<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void completedContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, FiCaSchedulerNode node, </span><br><span class=\"line\">      RMContainer rmContainer, ContainerStatus containerStatus, </span><br><span class=\"line\">      RMContainerEventType event, CSQueue completedChildQueue,</span><br><span class=\"line\">      boolean sortQueues) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      synchronized (this) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;使用 AbstractCSQueue 中releaseResource方法</span><br><span class=\"line\">        super.releaseResource(clusterResource, rmContainer.getContainer()</span><br><span class=\"line\">            .getResource(), node.getLabels());</span><br><span class=\"line\">\t...</span><br><span class=\"line\">        &#x2F;&#x2F;resort sub-queue</span><br><span class=\"line\">     \t....</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; Inform the parent</span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\"> \t&#x2F;&#x2F;递归 ParentQueue#completedContainer 方法</span><br><span class=\"line\">        parent.completedContainer(clusterResource, application, </span><br><span class=\"line\">            node, rmContainer, null, event, this, sortQueues);</span><br><span class=\"line\">      &#125;    </span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 在 container 释放时,无论是 allocatedContainer 还是 reservedContainer,都更新: \n1.按标签更新 QueueUsage 中 USED 用途的资源量<br>2.ParentQueue 中运行 container 的数量 \n[AbstractCSQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;1.QueueUsageByLabel</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">        &#x2F;&#x2F;2.减少当前 Queue 的 container 的数量</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看第 3 处对 LeafQueue 的处理, 需要注意的是:无论 container 是 allocatedContainer 还是 reversedContainer 在此处都是作为ResourceUsage中 <b>USED</b> 类型的资源来减少的(虽然 ResourceType 枚举中有 RESERVED 类型),且 numContainer 都减少了 1个单位<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized void releaseResource(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;1.同步当前队列在 标签维度 ResourceUsage 中 USED 类型的资源,并减少 Queue 中 numContainer</span><br><span class=\"line\">    super.releaseResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;2.同步 app 所属用户在标签维度上的度量信息  </span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.releaseContainer(resource, nodeLabels);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[AbstractCSQueue.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;1.同步 label 层面的 QueueUsage 的资源量;无论 reservedContainer 还是 allocatedContainer 都是使用 #decUsed(resource)</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">      &#x2F;&#x2F;2.减少当前队列启动的 container 数量.无论 reservedContainer 还是 allocatedContainer  </span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>LeafQueue 中维护了一个 users:Map[String,User]以保存当前各 user 在 USED,PENDING,AMUSED,RESERVED 用途使用资源的度量<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseContainer(Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;还是 decUsed(resource),无论 reservedContainer 还是 allocatedContainer</span><br><span class=\"line\">        userResourceUsage.decUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.decUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>在回到LeafQueue#completedContainer方法,按照 RMContainer状态机状态来区分 reservedContainer 还是 allocatedContainer 做处理<br>先看对 allocated 的处理逻辑<br>[FicaSchedulerApp.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">synchronized public boolean containerCompleted(RMContainer rmContainer,</span><br><span class=\"line\">      ContainerStatus containerStatus, RMContainerEventType event) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;维护相应的引用列表</span><br><span class=\"line\">    if (null &#x3D;&#x3D; liveContainers.remove(rmContainer.getContainerId())) &#123;</span><br><span class=\"line\">      return false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    newlyAllocatedContainers.remove(rmContainer);</span><br><span class=\"line\">    Container container &#x3D; rmContainer.getContainer();</span><br><span class=\"line\">    ContainerId containerId &#x3D; container.getId();</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F;1.RM 端准备 container 的 结束和清理等任务,待 NM 心跳通过 NodeHeartbeatResponse 交于 NM 做清理</span><br><span class=\"line\">    rmContainer.handle(</span><br><span class=\"line\">        new RMContainerFinishedEvent(</span><br><span class=\"line\">            containerId,</span><br><span class=\"line\">            containerStatus, </span><br><span class=\"line\">            event)</span><br><span class=\"line\">        );</span><br><span class=\"line\">    ... </span><br><span class=\"line\">    &#x2F;&#x2F;2.同步 QueueMetrics</span><br><span class=\"line\">    Resource containerResource &#x3D; rmContainer.getContainer().getResource();</span><br><span class=\"line\">    queue.getMetrics().releaseResources(getUser(), 1, containerResource);</span><br><span class=\"line\">    &#x2F;&#x2F;3.减少当前 app 消费的资源</span><br><span class=\"line\">    Resources.subtractFrom(currentConsumption, containerResource);</span><br><span class=\"line\">    ....</span><br><span class=\"line\">    return true;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>主要看第2步 释放 contaienr 同步 QueueMetrics 的指标和内容,<br>1.同步 QueueMetrics 及其 ParentQueue QueueMetrics 的指标: allocate [containerNum | MB | core]<br>2.同步 QueueMetrics 及其 ParentQueue QueueMetrics 用户维度的 QueueMetrics 指标: allocate [containerNum | MB | core]<br>[QueueMetrics.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseResources(String user, int containers, Resource res) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F;当前资源队列指标</span><br><span class=\"line\">    allocatedContainers.decr(containers);</span><br><span class=\"line\">    aggregateContainersReleased.incr(containers);</span><br><span class=\"line\">    allocatedMB.decr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.decr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    &#x2F;&#x2F;资源队列中用户指标</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;递归 父资源队列</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看释放allocatedContainer 过程中,NM 对该 container 的处理<br>1.删掉 launchedContainer 对该 container 的引用<br>2.增加该节点的可用资源,减少已经资源,减少启动的 numContainer 指标<br>[SchedulerNode.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void releaseContainer(Container container) &#123;</span><br><span class=\"line\">    ..</span><br><span class=\"line\">    &#x2F;&#x2F;1.删除引用</span><br><span class=\"line\">    if (null !&#x3D; launchedContainers.remove(container.getId())) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;2.指标维护</span><br><span class=\"line\">      updateResource(container);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Released container &quot; + container.getId() + &quot; of capacity &quot;</span><br><span class=\"line\">        + container.getResource() + &quot; on host &quot; + rmNode.getNodeAddress()</span><br><span class=\"line\">        + &quot;, which currently has &quot; + numContainers + &quot; containers, &quot;</span><br><span class=\"line\">        + getUsedResource() + &quot; used and &quot; + getAvailableResource()</span><br><span class=\"line\">        + &quot; available&quot; + &quot;, release resources&#x3D;&quot; + true);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void updateResource(Container container) &#123;</span><br><span class=\"line\">    addAvailableResource(container.getResource());</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  private synchronized void addAvailableResource(Resource resource) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    Resources.addTo(availableResource, resource);</span><br><span class=\"line\">    Resources.subtractFrom(usedResource, resource);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>释放 reservedContainer,对 reversedContainer 的处理<br>1.application 方面:删除对 reservedContainer 的引用,减少 currentReservation 资源数<br>2.nodemanager方面: 设置当前 nm 的reservedContainer 为 null\n3.QueueMetrics 方面: 同步[ParentQueue | LeafQueue] [本身 | 用户]维度的指标 reserved [containerNum | MB | core]<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean unreserve(FiCaSchedulerApp application, Priority priority,</span><br><span class=\"line\">      FiCaSchedulerNode node, RMContainer rmContainer) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;1.application: unreserve</span><br><span class=\"line\">    if (application.unreserve(node, priority)) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;2. nodemanager: unreserveResource</span><br><span class=\"line\">      node.unreserveResource(application);</span><br><span class=\"line\">     &#x2F;&#x2F;3.QueueMetrics</span><br><span class=\"line\">      getMetrics().unreserveResource(application.getUser(),</span><br><span class=\"line\">          rmContainer.getContainer().getResource());</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return false;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>application 和 nodemanager 层面的逻辑比较简单,且看 QueueMetrics 方面的同步<br>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void unreserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">  &#x2F;&#x2F;当前资源队列 QueueMetrics的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    reservedContainers.decr();</span><br><span class=\"line\">    reservedMB.decr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.decr(res.getVirtualCores());</span><br><span class=\"line\">    &#x2F;&#x2F;当前资源队列 在用户维度的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;父资源队列在 其本身和用户维度的 reserved [container | MB | core] 指标</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n</blockquote>\n<p>综上:<br>CS 调度器在为 APP 释放container 时,会同步 [Parent | Leaf]Queue/APP/NM 三个位置维护的信息  </p>\n<ol>\n<li>释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>ParentQueue</b> 都会维护:\n&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ParentQueue 维护的<b>标签维度</b>的QueueUsage 中 USED 用途的资源量  </li>\n<li>释放时,无论是释放 allocated 或 reserved 类型的 container,其 <b>LeafQueue</b> 都会维护:<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的 numContainer<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>标签维度</b>的 QueueUsage 中 USED 用途的资源量<br>&ensp;&ensp;&ensp;&ensp;LeafQueue 维护的<b>用户标签维度</b>的 QueueUsage 中 USED 用途的资源量  </li>\n<li>释放 container 时,按照该 container 的状态机状态做细分,分为 reservedContainer 和 allocatedContainer 两类做处理  </li>\n<li>若在节点上释放 allocatedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:newlyAllocatedContainers 和 reservedContainers 列表,app已占用资源量<br>&ensp;&ensp;&ensp;&ensp;NodeManager方面:numContainer数量,NM 可用资源量,NM 已用资源量,NM 启动的 container 列表<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:LeafQueue 和所有 ParentQueue <b>自身 QueueMetrics</b> 的 reserved [containerNum | MB | core]信息;LeafQueue 和所有 ParentQueue 在<b>用户维度 QueueMetrics </b> 的  reserved [containerNum | MB | core]信息  </li>\n<li>若在节点上释放 reservedContainer,则维护:<br>&ensp;&ensp;&ensp;&ensp;Application 方面:reservedContainers 列表,app 已保留资源量<br>&ensp;&ensp;&ensp;&ensp; NodeManager 方面:设置 NM 保留的 container 为 null<br>&ensp;&ensp;&ensp;&ensp; QueueMetrics 方面:LeafQueue 及所有 ParentQueue<b>自身 QueueMetrics</b>的 reserved [containerNum | MB | core]信息;LeafQueue 及其所有 ParentQueue 在<b>用户维度QueueMetrics</b> 的 reserved [containerNum | MB | core ]信息  </li>\n</ol>\n<p>Container 释放过程和申请过程 对于 Queue/Application/NM三个位置维护的 Queue/QueueUsageByLabel/QueueUsageByUser/QueueMetrics信息,恰好是一一对应的  </p>\n<p>对于 YarnUI 指标异常问题,比较关注的 Queue.numContainer 和 QueueUsageByLabel.resArr[ResourceType.USED]两个指标,一次 allocated/reversed/Container 的申请对应着 numContainer++和 ResourceUsage.incUsed;一次 container 的释放对应着 numContainer–和 ResourceUsage.decUsed   </p>\n<h3 id=\"movetoqueue-时资源转移过程\"><a href=\"#movetoqueue-时资源转移过程\" class=\"headerlink\" title=\"movetoqueue 时资源转移过程\"></a>movetoqueue 时资源转移过程</h3><p>使用命令 “yarn application -movetoqueue ${APPID} -queue ${TO}”\n可以将 ${APPID}的 APP 移动到目标资源队列 ${TO}<br>ClientRMService 相应请求的调用栈是:<br>-&gt;ClientRMService#moveApplicationAcrossQueues<br>&ensp;&ensp;-&gt;RMAppImpl$RMAppMoveTransition#transition<br>&ensp;&ensp;&ensp;&ensp;-&gt;CapacityScheduler#moveApplication  </p>\n<p>application 从源资源队列移动到目标资源队列分为几步:  </p>\n<ol>\n<li>向目标队列提交application 并做校验,包括 access 权限、目标队列的 app 数量超限、目标队列中用户提交 app 数量超限,若提交成功则增加所有 ParentQueue 的 numApplication 数量(但是没有增加自身的 numApplication 数量)    </li>\n<li>转移源队列中所有非 reserved 的 container 到目标队列,源队列(LeafQueue)释放 container,减少<b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并减少源队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量;增加目标队列(LeafQueue) <b>标签维度和用户维度</b>的 ResourceUsage USED 类型资源量,并增加目标队列的所有父队列(ParentQueue) <b>标签维度</b>的 ResourceUsage USED 类型资源量.指标数量维护在Queue/ResourceUsage/QueueMetrics 中,此处没有处理 livingContainers 的 QueueMetrics ,在第 5 步中统一处理 QueueMetrics    </li>\n<li>源队列中移除 appAttemptd,并同步 QueueUsageByUser 和 QueueUsageByLabel 中 AMUSED类型的资源量  </li>\n<li>在源队列的所有 ParentQueue 中移除此 Application    </li>\n<li>同步源队列和目标队列中所有非 AM container 移动导致的 QueueMetrics 变化  </li>\n<li>同步用户维度的 appAttempt 统计信息<br>[CapacityScheduler.java]  </li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized String moveApplication(ApplicationId appId,</span><br><span class=\"line\">     String targetQueueName) throws YarnException &#123;</span><br><span class=\"line\">   FiCaSchedulerApp app &#x3D;</span><br><span class=\"line\">       getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));</span><br><span class=\"line\">   String sourceQueueName &#x3D; app.getQueue().getQueueName();</span><br><span class=\"line\">   LeafQueue source &#x3D; getAndCheckLeafQueue(sourceQueueName);</span><br><span class=\"line\">   String destQueueName &#x3D; handleMoveToPlanQueue(targetQueueName);</span><br><span class=\"line\">   LeafQueue dest &#x3D; getAndCheckLeafQueue(destQueueName);</span><br><span class=\"line\">   String user &#x3D; app.getUser();</span><br><span class=\"line\">   try &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;1. 转移 Application</span><br><span class=\"line\">     dest.submitApplication(appId, user, destQueueName);</span><br><span class=\"line\">   &#125; catch (AccessControlException e) &#123;</span><br><span class=\"line\">     throw new YarnException(e);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;2.转移 livingContainer</span><br><span class=\"line\">   for (RMContainer rmContainer : app.getLiveContainers()) &#123;</span><br><span class=\"line\">     source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">     &#x2F;&#x2F; attach the Container to another queue</span><br><span class=\"line\">     dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   &#x2F;&#x2F;3.源资源队列移除attempt,并同步QueueUsage</span><br><span class=\"line\">   source.finishApplicationAttempt(app, sourceQueueName);</span><br><span class=\"line\">   &#x2F;&#x2F;4.源队列的父队列移除 application</span><br><span class=\"line\">   source.getParent().finishApplication(appId, app.getUser());</span><br><span class=\"line\">  &#x2F;&#x2F;5.同步源队列和目标队列的 QueueMetrics</span><br><span class=\"line\">   app.move(dest);</span><br><span class=\"line\">   &#x2F;&#x2F;6.提交attemp</span><br><span class=\"line\">   dest.submitApplicationAttempt(app, user);</span><br><span class=\"line\">   applications.get(appId).setQueue(dest);</span><br><span class=\"line\">   LOG.info(&quot;App: &quot; + app.getApplicationId() + &quot; successfully moved from &quot;</span><br><span class=\"line\">       + sourceQueueName + &quot; to: &quot; + destQueueName);</span><br><span class=\"line\">   return targetQueueName;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>第 2 步中,遍历 app 的 livingContainer,把每个 container 从源队列及父队列释放,减少源队列及父队列的指标<br>先看源队列 LeafQueue 的释放逻辑<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void detachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">      releaseResource(clusterResource, application, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; container&#x3D;&quot; + rmContainer.getContainer()</span><br><span class=\"line\">          + &quot; resource&#x3D;&quot; + rmContainer.getContainer().getResource()</span><br><span class=\"line\">          + &quot; queueMoveOut&#x3D;&quot; + this + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity()</span><br><span class=\"line\">          + &quot; absoluteUsedCapacity&#x3D;&quot; + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot;</span><br><span class=\"line\">          + queueUsage.getUsed() + &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;ParentQueue#detachContainer</span><br><span class=\"line\">      getParent().detachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  synchronized void releaseResource(Resource clusterResource, </span><br><span class=\"line\">      FiCaSchedulerApp application, Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">     &#x2F;&#x2F;1.减少 标签维度的 ResourceUsage USED 用途的资源;并减少当前 Queue 的 numContainer 数量</span><br><span class=\"line\">    super.releaseResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    &#x2F;&#x2F;2.减少 提交用户在 标签维度的 ResourceUsage USED 用途的资源</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.releaseContainer(resource, nodeLabels);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">    ...</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">  &#x2F;&#x2F;ResourceUsageByLabel USED 用途的资源量</span><br><span class=\"line\">  protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">        &#x2F;&#x2F;Queue 的运行 container 数量</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F;第 2 处,ResourceUsageByUser USED 用途的资源量</span><br><span class=\"line\">public void releaseContainer(Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">        userResourceUsage.decUsed(resource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        for (String label : nodeLabels) &#123;</span><br><span class=\"line\">          userResourceUsage.decUsed(label, resource);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br></pre></td></tr></table></figure>\n<p>ParentQueue 释放 container 时,减少了用户维度的 ResourceUsage 在 USED 用途上资源量<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void detachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">      super.releaseResource(clusterResource,</span><br><span class=\"line\">          rmContainer.getContainer().getResource(),</span><br><span class=\"line\">          node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; queueMoveOut&#x3D;&quot; + getQueueName()</span><br><span class=\"line\">          + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity() + &quot; absoluteUsedCapacity&#x3D;&quot;</span><br><span class=\"line\">          + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; cluster&#x3D;&quot;</span><br><span class=\"line\">          + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F; Inform the parent</span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F;递归父队列</span><br><span class=\"line\">        parent.detachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#releaseResource</span><br><span class=\"line\">  protected synchronized void releaseResource(Resource clusterResource,</span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; Update usedResources by labels</span><br><span class=\"line\">    if (null &#x3D;&#x3D; nodeLabels || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.decUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.decUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">    --numContainers;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>将 container 从源队列转移到目标队列时,会增加目标队列及其父队列的指标<br>先看目标队列(LeafQueue)增加的指标<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void attachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">          &#x2F;&#x2F;1.增加 LeafQueue 标签维度的 ResourceUsage 和用户维度的 ResourceUsage, USED 类型的资源量;并增加numContainer</span><br><span class=\"line\">      allocateResource(clusterResource, application, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; container&#x3D;&quot; + rmContainer.getContainer()</span><br><span class=\"line\">              &#x2F;&#x2F;add by jiulong.zhu@20190903</span><br><span class=\"line\">              +&quot; containerState&#x3D;&quot;+rmContainer.getState()</span><br><span class=\"line\">          + &quot; resource&#x3D;&quot; + rmContainer.getContainer().getResource()</span><br><span class=\"line\">          + &quot; queueMoveIn&#x3D;&quot; + this + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity()</span><br><span class=\"line\">          + &quot; absoluteUsedCapacity&#x3D;&quot; + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot;</span><br><span class=\"line\">          + queueUsage.getUsed() + &quot; cluster&#x3D;&quot; + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;2.增加 ParentQueue 标签维度的 ResourceUsage USED 类型的资源量;并增加 ParentQueue 的 numContainer 数量</span><br><span class=\"line\">      getParent().attachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  synchronized void allocateResource(Resource clusterResource,</span><br><span class=\"line\">      SchedulerApplicationAttempt application, Resource resource,</span><br><span class=\"line\">      Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;AbstractCSQueue#allocateResource 增加标签维度的 ResourceUsage USED 用途的用量;增加 Queue 的numContainers  </span><br><span class=\"line\">    super.allocateResource(clusterResource, resource, nodeLabels);</span><br><span class=\"line\">    String userName &#x3D; application.getUser();</span><br><span class=\"line\">    &#x2F;&#x2F;增加当前用户 标签维度的 ResourceUsage USED 用途的用量  </span><br><span class=\"line\">    User user &#x3D; getUser(userName);</span><br><span class=\"line\">    user.assignContainer(resource, nodeLabels);</span><br><span class=\"line\">    </span><br><span class=\"line\">    Resources.subtractFrom(application.getHeadroom(), resource);</span><br><span class=\"line\">    metrics.setAvailableResourcesToUser(userName, application.getHeadroom());</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.info(getQueueName() + </span><br><span class=\"line\">          &quot; user&#x3D;&quot; + userName + </span><br><span class=\"line\">          &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; numContainers&#x3D;&quot; + numContainers +</span><br><span class=\"line\">          &quot; headroom &#x3D; &quot; + application.getHeadroom() +</span><br><span class=\"line\">          &quot; user-resources&#x3D;&quot; + user.getUsed()</span><br><span class=\"line\">          );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">   synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看将 container 移动到目标队列时,目标队列的 ParentQueue 增加的逻辑<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void attachContainer(Resource clusterResource,</span><br><span class=\"line\">      FiCaSchedulerApp application, RMContainer rmContainer) &#123;</span><br><span class=\"line\">    if (application !&#x3D; null) &#123;</span><br><span class=\"line\">      FiCaSchedulerNode node &#x3D;</span><br><span class=\"line\">          scheduler.getNode(rmContainer.getContainer().getNodeId());</span><br><span class=\"line\">          &#x2F;&#x2F;1.增加 ParentQueue 在标签维度的 ResourceUsage USED 用量 </span><br><span class=\"line\">          &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">      super.allocateResource(clusterResource, rmContainer.getContainer()</span><br><span class=\"line\">          .getResource(), node.getLabels());</span><br><span class=\"line\">      LOG.info(&quot;movedContainer&quot; + &quot; queueMoveIn&#x3D;&quot; + getQueueName()</span><br><span class=\"line\">          + &quot; usedCapacity&#x3D;&quot; + getUsedCapacity() + &quot; absoluteUsedCapacity&#x3D;&quot;</span><br><span class=\"line\">          + getAbsoluteUsedCapacity() + &quot; used&#x3D;&quot; + queueUsage.getUsed() + &quot; cluster&#x3D;&quot;</span><br><span class=\"line\">          + clusterResource);</span><br><span class=\"line\">      &#x2F;&#x2F;2.递归增加 ParentQueue 的所有 ParentQueue 的 ResourceUsage USED 用量  </span><br><span class=\"line\">      if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">        parent.attachContainer(clusterResource, application, rmContainer);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  &#x2F;&#x2F;AbstractCSQueue#allocateResource</span><br><span class=\"line\">  synchronized void allocateResource(Resource clusterResource, </span><br><span class=\"line\">      Resource resource, Set&lt;String&gt; nodeLabels) &#123;</span><br><span class=\"line\">    if (nodeLabels &#x3D;&#x3D; null || nodeLabels.isEmpty()) &#123;</span><br><span class=\"line\">      queueUsage.incUsed(resource);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      Set&lt;String&gt; anls &#x3D; (accessibleLabels.contains(RMNodeLabelsManager.ANY))</span><br><span class=\"line\">          ? labelManager.getClusterNodeLabels() : accessibleLabels;</span><br><span class=\"line\">      for (String label : Sets.intersection(anls, nodeLabels)) &#123;</span><br><span class=\"line\">        queueUsage.incUsed(label, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ++numContainers;</span><br><span class=\"line\">    CSQueueUtils.updateQueueStatistics(resourceCalculator, this, getParent(),</span><br><span class=\"line\">        clusterResource, minimumAllocation);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 3 步:删除源队列中该 app 的 currentApplicationAttempt,并同步 LeafQueue 中用户提交的 app 数量变化<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void finishApplicationAttempt(FiCaSchedulerApp application, String queue) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      removeApplicationAttempt(application, getUser(application.getUser()));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;空实现</span><br><span class=\"line\">    getParent().finishApplicationAttempt(application, queue);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public synchronized void removeApplicationAttempt(</span><br><span class=\"line\">      FiCaSchedulerApp application, User user) &#123;</span><br><span class=\"line\">    boolean wasActive &#x3D; activeApplications.remove(application);</span><br><span class=\"line\">    if (!wasActive) &#123;</span><br><span class=\"line\">      pendingApplications.remove(application);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F;正在运行的 app_attempt,则减少 LeafQueue 的 [ResourceUsage | ResourceUsageByUser] 中AMUSED 占用的资源量</span><br><span class=\"line\">      queueUsage.decAMUsed(application.getAMResource());</span><br><span class=\"line\">      user.getResourceUsage().decAMUsed(application.getAMResource());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    applicationAttemptMap.remove(application.getApplicationAttemptId());</span><br><span class=\"line\">     &#x2F;&#x2F; 在 ResourceUsageByUser 同步 numApplication 指标</span><br><span class=\"line\">    user.finishApplication(wasActive);</span><br><span class=\"line\">    if (user.getTotalApplications() &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">      users.remove(application.getUser());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Check if we can activate more applications</span><br><span class=\"line\">    activateApplications();</span><br><span class=\"line\">    LOG.info(&quot;Application removed -&quot; +</span><br><span class=\"line\">        &quot; appId: &quot; + application.getApplicationId() + </span><br><span class=\"line\">        &quot; user: &quot; + application.getUser() + </span><br><span class=\"line\">        &quot; queue: &quot; + getQueueName() +</span><br><span class=\"line\">        &quot; #user-pending-applications: &quot; + user.getPendingApplications() +</span><br><span class=\"line\">        &quot; #user-active-applications: &quot; + user.getActiveApplications() +</span><br><span class=\"line\">        &quot; #queue-pending-applications: &quot; + getNumPendingApplications() +</span><br><span class=\"line\">        &quot; #queue-active-applications: &quot; + getNumActiveApplications()</span><br><span class=\"line\">        );</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 4 步:对于源队列的所有 ParentQueue,移除 app 的 currentApplicationAttempt,并减少资源队列的运行 numApplication 值<br>[ParentQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void finishApplication(ApplicationId application, String user) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      removeApplication(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;递归 父队列</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.finishApplication(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  private synchronized void removeApplication(ApplicationId applicationId, </span><br><span class=\"line\">      String user) &#123;</span><br><span class=\"line\">    --numApplications;</span><br><span class=\"line\">    LOG.info(&quot;Application removed -&quot; +</span><br><span class=\"line\">        &quot; appId: &quot; + applicationId + </span><br><span class=\"line\">        &quot; user: &quot; + user + </span><br><span class=\"line\">        &quot; leaf-queue of parent: &quot; + getQueueName() + </span><br><span class=\"line\">        &quot; #applications: &quot; + getNumApplications());</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 5 步: 将 app 持有的所有 allocatedContainer(包括 AM container)和 reservedContainer,移动到目标队列,并修改 QueueMetrics<br>[SchedulerApplicationAttempt.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void move(Queue newQueue) &#123;</span><br><span class=\"line\">    QueueMetrics oldMetrics &#x3D; queue.getMetrics();</span><br><span class=\"line\">    QueueMetrics newMetrics &#x3D; newQueue.getMetrics();</span><br><span class=\"line\">    String user &#x3D; getUser();</span><br><span class=\"line\">    &#x2F;&#x2F;修改 livingContainer 的 QueueMetrics</span><br><span class=\"line\">    for (RMContainer liveContainer : liveContainers.values()) &#123;</span><br><span class=\"line\">      Resource resource &#x3D; liveContainer.getContainer().getResource();</span><br><span class=\"line\">      oldMetrics.releaseResources(user, 1, resource);</span><br><span class=\"line\">      newMetrics.allocateResources(user, 1, resource, false);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;修改 reservedContainer的 QueueMetrics</span><br><span class=\"line\">    for (Map&lt;NodeId, RMContainer&gt; map : reservedContainers.values()) &#123;</span><br><span class=\"line\">      for (RMContainer reservedContainer : map.values()) &#123;</span><br><span class=\"line\">        Resource resource &#x3D; reservedContainer.getReservedResource();</span><br><span class=\"line\">        oldMetrics.unreserveResource(user, resource);</span><br><span class=\"line\">        newMetrics.reserveResource(user, resource);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;移动 pending ResourceRequest,同步信息:QueueMetrics.appsRunning,Queue下的用户列表    </span><br><span class=\"line\">    appSchedulingInfo.move(newQueue);</span><br><span class=\"line\">    this.queue &#x3D; newQueue;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>修改 livingContainers 和 reservedContainers 的所有逻辑都在 QueueMetrics中,其中<br>1.releaseResources,释放 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标<br>2.allocateResource,申请 livingContainer: [ParentQueue | LeafQueue] [本身 | 用户维度] 的 allocated [Containers | MB | Core]指标<br>3.unreserveResource,释放 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Containers| MB | Core]指标<br>4.reserveResource,申请 reservedContainer:[ParentQueue | LeafQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标<br>[QueueMetrics.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void releaseResources(String user, int containers, Resource res) &#123;</span><br><span class=\"line\">    allocatedContainers.decr(containers);</span><br><span class=\"line\">    aggregateContainersReleased.incr(containers);</span><br><span class=\"line\">    allocatedMB.decr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.decr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.releaseResources(user, containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\"> public void allocateResources(String user, int containers, Resource res,</span><br><span class=\"line\">      boolean decrPending) &#123;</span><br><span class=\"line\">    allocatedContainers.incr(containers);</span><br><span class=\"line\">    aggregateContainersAllocated.incr(containers);</span><br><span class=\"line\">    allocatedMB.incr(res.getMemory() * containers);</span><br><span class=\"line\">    allocatedVCores.incr(res.getVirtualCores() * containers);</span><br><span class=\"line\">    if (decrPending) &#123;</span><br><span class=\"line\">      _decrPendingResources(containers, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.allocateResources(user, containers, res, decrPending);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public void unreserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.decr();</span><br><span class=\"line\">    reservedMB.decr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.decr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.unreserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  </span><br><span class=\"line\">  public void reserveResource(String user, Resource res) &#123;</span><br><span class=\"line\">    reservedContainers.incr();</span><br><span class=\"line\">    reservedMB.incr(res.getMemory());</span><br><span class=\"line\">    reservedVCores.incr(res.getVirtualCores());</span><br><span class=\"line\">    QueueMetrics userMetrics &#x3D; getUserMetrics(user);</span><br><span class=\"line\">    if (userMetrics !&#x3D; null) &#123;</span><br><span class=\"line\">      userMetrics.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (parent !&#x3D; null) &#123;</span><br><span class=\"line\">      parent.reserveResource(user, res);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第 6 步:同步目标队列 application 和 applicationAttempt信息<br>[LeafQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void submitApplicationAttempt(FiCaSchedulerApp application,</span><br><span class=\"line\">      String userName) &#123;</span><br><span class=\"line\">    synchronized (this) &#123;</span><br><span class=\"line\">      User user &#x3D; getUser(userName);</span><br><span class=\"line\">      &#x2F;&#x2F;增加 Queue 和 User 维度 application &#x2F; applicationAttempt 的值</span><br><span class=\"line\">      addApplicationAttempt(application, user);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; We don&#39;t want to update metrics for move app</span><br><span class=\"line\">    if (application.isPending()) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F;增加 Queue 和 User 维度 QueueMetrics 中 appsPending 的值</span><br><span class=\"line\">      metrics.submitAppAttempt(userName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;空实现</span><br><span class=\"line\">    getParent().submitApplicationAttempt(application, userName);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>综上:<br>把 application 从源资源队列到目标队列的过程中,在 Queue/ResourceUsageByLabel/ResourceUsageByUser/QueueMetrics 中的变动<br>1.对于 livingContainers,<br>&ensp;&ensp;&ensp;&ensp;Queue 方面:减少源队列及其父队列的 numContainer 值;增加目标队列及其父队列的 numContainer<br>&ensp;&ensp;&ensp;&ensp;ResourceUsage 方面:减少源队列及其父队列的 ResourceUsageByLabel USED 类型的资源量,增加目标队列及其父队列的 ResourceUsageByLabel USED 类型的资源量; 减少源队列(LeafQueue)的 ResourceUsageByUser USED 类型的资源量,增加目标队列(LeafQueue) 的 ResourceUsageByUser USED 类型的资源量;<br>&ensp;&ensp;&ensp;&ensp;QueueMetrics 方面:减少源队列及其父队列的 allocated[Container | MB | Core]指标,增加目标队列及其父队列的 allocated [Container | MB | Core]<br>2.对于 reservedContainers,<br>&ensp;&ensp;&ensp;&ensp;ResourceUsage:减少源队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的reserved [Container | MB | Core]指标,增加目标队列[LeafQueue | ParentQueue] [本身 | 用户维度] 的 reserved [Container | MB | Core]指标  </p>\n</blockquote>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><p>结合 “正常的资源分配和释放过程”和”movetoqueue 过程中的资源转移过程 “,可以还原一个被转移队列的 application 的资源变动过程<br>application 中非 reservedContianer(状态机:allocated,acquired,running) 的资源变动过程如下表  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">allocatedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<p>application 中 reservedContianer(状态机:reversed) 的资源变动过程如下表   </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">reversedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<blockquote>\n</blockquote>\n<p>从上述两表中可知:<br>1.在没有 movetoqueue 操作的情况下, 源队列和目标队列一致,无论 allocatedContainer 或 reservedContainer 的分配和释放都是守恒的,一次指标的增加对应着一次指标的减少<br>2.在源队列资源比较充裕,且有 movetoqueue 操作的情况下,application 没有 reservedContainer 的情况下(上述表一),在分配时源队列指标增加,movetoqueue 时,源队列减少指标且目标队列增加指标,在释放时减少目标队列(app 当前归属队列)的指标.指标的增减也是平衡的,这也就是在资源充裕的队列上无法复现该问题的原因<br>3.在源队列资源比较紧张,且有 movetoqueue 操作的情况下,application 有 reservedContainer 和 allocatedContainer(上述表一和表二),对于 allocatedContainer 来说,资源在分配-&gt;movetoque-&gt;释放的过程中是平衡的(表一);但是对于 reversedContainer 来说,在源队列中分配到资源时,增加了源队列的 numContainer 和 ResourceUsageByLabel 中 USED 用途的资源量,movetoqueue 时没有相应的操作,仅仅是修改了 QueueMetrcis 内的指标,在释放时 container 资源被加入到目标队列的指标中(上述表二).<b>源队列分配出去的资源没有回收,目标队列得到了不是自身分配出去的资源</b>.这样导致了即使源队列和目标队列中完全无任务时,源队列的 numContainer 和 ResourceUsageByLabel 值为正值,目标队列的 numContainer 和 ResourceUsage 值为负值,Yarn UI -&gt;scheduler-&gt;queue 内的 Num Containers/Used Capacity/Absolute Used Capacity/Used Resources 正是基于 队列的 numContainer 和 ResourceUsageByLabel(USED) 展示和计算的,也就是指标异常的问题     </p>\n<p>指标异常(无任务时 为负)\n<img src=\"/img/pictures/negative/negative_e7a766275896.png\" alt=\"\"></p>\n<p>指标异常(无任务时 为正)\n<img src=\"/img/pictures/negative/negative_aead-04d8e67b1357.png\" alt=\"\"></p>\n<h3 id=\"危害\"><a href=\"#危害\" class=\"headerlink\" title=\"危害\"></a>危害</h3><p>1.对于源队列来说,类似于资源泄露,自身资源未能回收,释放到了目标队列.这样对于后续提交到源队列的任务来说,不能分配到资源运行(USED 泄露趋于 100%),但实际上源队列没有任务在运行<br>2.对于目标队列来说,多出了一些资源,导致自身一些指标为负,可能会导致一些未知的问题.以下纯属个人猜测: 例如 同步问题,Queue 有很多的剩余资源,但是 NodeManager 上没有 available 资源;指标负值可能对内部的运算产生未预期的问题  </p>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>针对 reservedContainer 在 movetoqueue 操作过程中,资源释放申请不守恒的漏洞,如下表  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">reversedContainer</th>\n<th align=\"center\">分配</th>\n<th align=\"center\">movetoqueue</th>\n<th align=\"center\">释放</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">numContainer</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceUsageByLabel(USED)</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\"><font color=\"#660000\">源队列不变,目标队列不变</font></td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n<tr>\n<td align=\"left\">QueueMetrics</td>\n<td align=\"center\">源队列值增加</td>\n<td align=\"center\">源队列减少,目标队列增加</td>\n<td align=\"center\">目标队列值减少</td>\n</tr>\n</tbody></table>\n<blockquote>\n</blockquote>\n<p>问题:<br>&ensp;&ensp;&ensp;&ensp;YarnUI 中指标 Num Container、Used Capacity、Absolute Used Capacity、Used Resource 异常和 Queue成员变量numContainer及 QueueUsageByLabel 中 USED 用途的资源量有关<br>目标:<br>&ensp;&ensp;&ensp;&ensp;解决 numContainer 和 ResourceUsageByLabel(USED)在 movetoqueue 时,没有”减少源队列的指标”且没有”增加目标队列指标”的问题<br>途径:<br>&ensp;&ensp;&ensp;&ensp;1.需要在 movetoqueue 时,每个 reservedContainer 都应触发 “源队列减少指标”和”目标队列增加指标”的行为 \n&ensp;&ensp;&ensp;&ensp;源码中有封装好的代码且上文多次提到,即 LeafQueue#releaseResource方法和 LeafQueue.allocateResource方法,但是其中有额外的 对 headroom 的同步逻辑.一方面,在”正常的分配和释放”过程中,无论申请和分配的 container 是 allocated 或是 reversed 都会执行 LeafQueue#releaseResource 和 LeafQueue.allocateResource 修改 numContainer 和 ResourceUsageByLabel 的同时修改 headroom值;另一方面 move container中对 headroom 增加和减少 同一个Resource 是幂等的.所以 movetoqueue 操作对于每一个 container 的移动都可以选择触发这两个方法来增加对 numContainer 和 ResourceUsageByLabel 的操作.LeafQueue#detachContainer()和 LeafQueueattachContainer()  中有递归本队列及当前队列此逻辑的过程.直接用即可<br>修改方案:<br>&ensp;&ensp;&ensp;&ensp;CapacitySchedule#moveApplication(ApplicationId appId,String targetQueueName) 方法修改如下  </p>\n<p>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized String moveApplication(ApplicationId appId,</span><br><span class=\"line\">      String targetQueueName) throws YarnException &#123;</span><br><span class=\"line\">    FiCaSchedulerApp app &#x3D;</span><br><span class=\"line\">        getApplicationAttempt(ApplicationAttemptId.newInstance(appId, 0));</span><br><span class=\"line\">    String sourceQueueName &#x3D; app.getQueue().getQueueName();</span><br><span class=\"line\">    LeafQueue source &#x3D; getAndCheckLeafQueue(sourceQueueName);</span><br><span class=\"line\">    String destQueueName &#x3D; handleMoveToPlanQueue(targetQueueName);</span><br><span class=\"line\">    LeafQueue dest &#x3D; getAndCheckLeafQueue(destQueueName);</span><br><span class=\"line\">    &#x2F;&#x2F; Validation check - ACLs, submission limits for user &amp; queue</span><br><span class=\"line\">    String user &#x3D; app.getUser();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      dest.submitApplication(appId, user, destQueueName);</span><br><span class=\"line\">    &#125; catch (AccessControlException e) &#123;</span><br><span class=\"line\">      throw new YarnException(e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; Move all live containers</span><br><span class=\"line\">    for (RMContainer rmContainer : app.getLiveContainers()) &#123;</span><br><span class=\"line\">      source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">      &#x2F;&#x2F; attach the Container to another queue</span><br><span class=\"line\">      dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;description:解决 reservedContainer 在 movetoqueue 操作过程中,Queue.numContainer 指标和 ResourceUsageByLabel.USED 资源量</span><br><span class=\"line\">    &#x2F;&#x2F;释放申请不守恒,导致 Yarn UI &quot;Num Container&quot;,&quot;Used Capacity&quot;,&quot;Absolute Used Capacity&quot;,&quot;Used Resource&quot;</span><br><span class=\"line\">    &#x2F;&#x2F;指标在队列无任务运行时不为 0 的问题</span><br><span class=\"line\">    &#x2F;&#x2F;modify by jiulong.zhu@20190903</span><br><span class=\"line\">    &#x2F;&#x2F;++add start</span><br><span class=\"line\">    for (RMContainer rmContainer : app.getReservedContainers()) &#123;</span><br><span class=\"line\">      source.detachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">      dest.attachContainer(clusterResource, app, rmContainer);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F;++add end</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Detach the application..</span><br><span class=\"line\">    source.finishApplicationAttempt(app, sourceQueueName);</span><br><span class=\"line\">    source.getParent().finishApplication(appId, app.getUser());</span><br><span class=\"line\">    &#x2F;&#x2F; Finish app &amp; update metrics</span><br><span class=\"line\">    app.move(dest);</span><br><span class=\"line\">    &#x2F;&#x2F; Submit to a new queue</span><br><span class=\"line\">    dest.submitApplicationAttempt(app, user);</span><br><span class=\"line\">    applications.get(appId).setQueue(dest);</span><br><span class=\"line\">    LOG.info(&quot;App: &quot; + app.getApplicationId() + &quot; successfully moved from &quot;</span><br><span class=\"line\">        + sourceQueueName + &quot; to: &quot; + destQueueName);</span><br><span class=\"line\">    return targetQueueName;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"冒烟测试用例\"><a href=\"#冒烟测试用例\" class=\"headerlink\" title=\"冒烟测试用例\"></a>冒烟测试用例</h2><p>[TestCapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">public void testReservedContainerLeakWhenMoveApplication() throws Exception &#123;</span><br><span class=\"line\">  CapacitySchedulerConfiguration csConf</span><br><span class=\"line\">          &#x3D; new CapacitySchedulerConfiguration();</span><br><span class=\"line\">  csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] &#123;&quot;a&quot;, &quot;b&quot;&#125;);</span><br><span class=\"line\">  csConf.setCapacity(&quot;root.a&quot;,50);</span><br><span class=\"line\">  csConf.setMaximumCapacity(&quot;root.a&quot;,100);</span><br><span class=\"line\">  csConf.setUserLimitFactor(&quot;root.a&quot;,100);</span><br><span class=\"line\">  csConf.setCapacity(&quot;root.b&quot;,50);</span><br><span class=\"line\">  csConf.setMaximumCapacity(&quot;root.b&quot;,100);</span><br><span class=\"line\">  csConf.setUserLimitFactor(&quot;root.b&quot;,100);</span><br><span class=\"line\"></span><br><span class=\"line\">  YarnConfiguration conf&#x3D;new YarnConfiguration(csConf);</span><br><span class=\"line\">  conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,</span><br><span class=\"line\">          ResourceScheduler.class);</span><br><span class=\"line\">  RMNodeLabelsManager mgr&#x3D;new NullRMNodeLabelsManager();</span><br><span class=\"line\">  mgr.init(conf);</span><br><span class=\"line\">  MockRM rm1 &#x3D; new MockRM(csConf);</span><br><span class=\"line\">  CapacityScheduler scheduler&#x3D;(CapacityScheduler) rm1.getResourceScheduler();</span><br><span class=\"line\">  rm1.getRMContext().setNodeLabelManager(mgr);</span><br><span class=\"line\">  rm1.start();</span><br><span class=\"line\">  MockNM nm1 &#x3D; rm1.registerNode(&quot;127.0.0.1:1234&quot;, 8 * GB);</span><br><span class=\"line\">  MockNM nm2 &#x3D; rm1.registerNode(&quot;127.0.0.2:1234&quot;, 8 * GB);</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * simulation</span><br><span class=\"line\">   * app1: (1 AM,1 running container)</span><br><span class=\"line\">   * app2: (1 AM,1 reserved container)</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  &#x2F;&#x2F; launch an app to queue, AM container should be launched in nm1</span><br><span class=\"line\">  RMApp app1 &#x3D; rm1.submitApp(1 * GB, &quot;app_1&quot;, &quot;user_1&quot;, null, &quot;a&quot;);</span><br><span class=\"line\">  MockAM am1 &#x3D; MockRM.launchAndRegisterAM(app1, rm1, nm1);</span><br><span class=\"line\"></span><br><span class=\"line\">  &#x2F;&#x2F; launch another app to queue, AM container should be launched in nm1</span><br><span class=\"line\">  RMApp app2 &#x3D; rm1.submitApp(1 * GB, &quot;app_2&quot;, &quot;user_1&quot;, null, &quot;a&quot;);</span><br><span class=\"line\">  MockAM am2 &#x3D; MockRM.launchAndRegisterAM(app2, rm1, nm1);</span><br><span class=\"line\"></span><br><span class=\"line\">  am1.allocate(&quot;*&quot;, 4 * GB, 1, new ArrayList&lt;ContainerId&gt;());</span><br><span class=\"line\">  &#x2F;&#x2F;this containerRequest should be reserved</span><br><span class=\"line\">  am2.allocate(&quot;*&quot;, 4 * GB, 1, new ArrayList&lt;ContainerId&gt;());</span><br><span class=\"line\"></span><br><span class=\"line\">  RMNode rmNode1 &#x3D; rm1.getRMContext().getRMNodes().get(nm1.getNodeId());</span><br><span class=\"line\">  &#x2F;&#x2F; Do node heartbeats 2 times</span><br><span class=\"line\">  &#x2F;&#x2F; First time will allocate container for app1, second time will reserve</span><br><span class=\"line\">  &#x2F;&#x2F; container for app2</span><br><span class=\"line\">  scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));</span><br><span class=\"line\">  scheduler.handle(new NodeUpdateSchedulerEvent(rmNode1));</span><br><span class=\"line\"></span><br><span class=\"line\">  FiCaSchedulerApp schedulerApp1 &#x3D;</span><br><span class=\"line\">          scheduler.getApplicationAttempt(am1.getApplicationAttemptId());</span><br><span class=\"line\">  FiCaSchedulerApp schedulerApp2 &#x3D;</span><br><span class=\"line\">          scheduler.getApplicationAttempt(am2.getApplicationAttemptId());</span><br><span class=\"line\">  &#x2F;&#x2F; APP1:  1 AM, 1 allocatedContainer</span><br><span class=\"line\">  Assert.assertEquals(2, schedulerApp1.getLiveContainers().size());</span><br><span class=\"line\">  &#x2F;&#x2F; APP2:  1 AM,1 reservedContainer</span><br><span class=\"line\">  Assert.assertEquals(1,schedulerApp2.getLiveContainers().size());</span><br><span class=\"line\">  Assert.assertEquals(1,schedulerApp2.getReservedContainers().size());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * before,move app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  LeafQueue srcQueue &#x3D; (LeafQueue) scheduler.getQueue(&quot;a&quot;);</span><br><span class=\"line\">  LeafQueue desQueue &#x3D; (LeafQueue) scheduler.getQueue(&quot;b&quot;);</span><br><span class=\"line\">  Assert.assertEquals(4,srcQueue.getNumContainers());</span><br><span class=\"line\">  Assert.assertEquals(10*GB,srcQueue.getUsedResources().getMemorySize());&#x2F;&#x2F; AM: 2*1GB   container: 4GB running,4GB reserved</span><br><span class=\"line\">  Assert.assertEquals(0,desQueue.getNumContainers());</span><br><span class=\"line\">  Assert.assertEquals(0,desQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">  &#x2F;&#x2F;app1 ResourceUsage (0 reserved)</span><br><span class=\"line\">  Assert.assertEquals(5*GB,schedulerApp1.getAppAttemptResourceUsage().getAllUsed().getMemorySize());</span><br><span class=\"line\">  Assert.assertEquals(0,schedulerApp1.getCurrentReservation().getMemorySize());</span><br><span class=\"line\">  &#x2F;&#x2F;app2  ResourceUsage (4GB reserved)</span><br><span class=\"line\">  Assert.assertEquals(1*GB,schedulerApp2.getAppAttemptResourceUsage().getAllUsed().getMemorySize());</span><br><span class=\"line\">  Assert.assertEquals(4*GB,schedulerApp2.getCurrentReservation().getMemorySize());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * move app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  scheduler.moveApplication(app2.getApplicationId(),&quot;b&quot;);</span><br><span class=\"line\">  &#x2F;&#x2F; finish.keep the order,if killing app1 first,the reservedContainer of app2 will be allocated</span><br><span class=\"line\">  rm1.killApp(app2.getApplicationId());</span><br><span class=\"line\">  rm1.killApp(app1.getApplicationId());</span><br><span class=\"line\">  &#x2F;**</span><br><span class=\"line\">   * after,moved app2 which has one reservedContainer</span><br><span class=\"line\">   *&#x2F;</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; after fixed</span><br><span class=\"line\">    Assert.assertEquals(0, srcQueue.getNumContainers());</span><br><span class=\"line\">    Assert.assertEquals(0, desQueue.getNumContainers());</span><br><span class=\"line\">    Assert.assertEquals(0, srcQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">    Assert.assertEquals(0, desQueue.getUsedResources().getMemorySize());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;*&#123;</span><br><span class=\"line\">    &#x2F;&#x2F; before fixed</span><br><span class=\"line\">    &#x2F;&#x2F; &lt;b&gt; the reserved container borrowed from srcQueue and returned to desQueue,</span><br><span class=\"line\">    &#x2F;&#x2F; but the numContainer and UsedResource did not sync when moving app to another queue &lt;&#x2F;b&gt;</span><br><span class=\"line\">    Assert.assertEquals(+1,srcQueue.getNumContainers());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(-1,desQueue.getNumContainers());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(+4*GB, srcQueue.getUsedResources().getMemorySize());    &#x2F;&#x2F;true</span><br><span class=\"line\">    Assert.assertEquals(-4*GB, desQueue.getUsedResources().getMemorySize());    &#x2F;&#x2F;true</span><br><span class=\"line\">  &#125;*&#x2F;</span><br><span class=\"line\">  rm1.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"线上测试用例\"><a href=\"#线上测试用例\" class=\"headerlink\" title=\"线上测试用例\"></a>线上测试用例</h2>"},{"layout":"post","title":"Yarn ReservationSystem","subtitle":"Yarn 预订系统, Yarn 预约系统, Yarn 保留系统","date":"2019-10-27T16:00:00.000Z","author":"jiulongzhu","header-img":"img/moon_night.jpg","catalog":true,"_content":"\n\n## 概述\n\nReservationSystem 是 YARN ResourceManager 的组件。YARN 的 ReservationSystem 为用户提供了提前保留资源的能力,以确保重要的生产作业可预测地运行。ReservationSystem 执行仔细的准入控制，并保证绝对资源量(而不是群集大小的百分比)。保留具有组和并发的语义，并且可以有随时间变化的资源需求。  \n\n<!-- more -->\n\n## 工作流程  \n\n![](/img/pictures/reservation/yarn_reservation_system.png)  \n\n参考上图，预订资源的工作流程如下：  \n\n* step 0: 用户提交预订创建请求，并收到包含 ReservationId 的响应。  \n* step 1: 用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。这描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求。这可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败\n* step 2: ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。  \n* step 3: SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  \n* step 4: 成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据    \n* step 5: PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序 \n* step 6: 用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue) \n* step 7: 常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   \n* step 8: 预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用以重建计划  \n\n注: step 8和官网解释不同。官网解释可能不实,参见源码解析-\"step 8\"\n\n## 源码解析\n\n### 涉及的类\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem  \n该类继承了AbstractService,实现了ReservationSystem 并封装了Capacity/Fair 调度器下预订系统的核心实现  \n主要功能是:作为服务启动时加载配置文件中配置的 reservable LeafQueue 转换为 Plan;管理 PlanFollower,确保 Plan与常规调度器的同步  \n\n\t| 方法  | 功能  | 备注 |\n\t|:------------- |:---------------:|:-------------|\n\t| setRMContext     | 保存 rmContext 指针 |    |\n\t| reinitialize     | 重新初始化 ReservationSystem |    |\n\t| getPlan     | 获取已被加载的 Plan |    |\n\t| getAllPlans    | 获取已被加载的所有 Plan  |    |\n\t| synchronizePlan     | 使用 PlanFollower 同步 Plan 与常规调度器 |    |\n\t| getPlanFollowerTimeStep     | PlanFollower 步长 |    |\n\t| getNewReservationId     | 获取一个全局唯一的reservationId |    |\n\t| getQueueForReservation     | 获取reservationId关联的队列 |    |\n\t| setQueueForReservation     | 为reservationId关联队列 |    |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower  \n该类实现了 PlanFollower与 Runnable 接口  \n主要功能是:周期性同步常规调度器与 Plan。通过将计划中的每个预订的当前资源映射到常规调度器(e.g. 队列的调整能力,设置池权重,调整应用优先级)，来影响调度器的资源分配 进而达到 保证作业与Plan 中预订一致的方式来使用资源。一个关键概念是将预订的绝对值式资源转换为队列的优先级和容量。PlanFollower 也会向 Plan 来同步集群总资源的变化使其作出相应的调整。  \n\n\t| 方法  | 功能  | 备注 |\n\t|:------------- |:---------------|:-------------|\n\t| init     | 通过 SystemClock,ResourceSchduler 和Plans 来初始化PlanFollower |    |\n\t| synchronizePlan     | 同步指定 Plan和常规调度器 | 周期性调用;时间紧迫时同步阻塞调用  |\n\t| setPlans     | 重置 PlanFollower 同步的 Plan 集合 |    |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan   \n该接口实现了 Plan 接口,Plan 接口继承了PlanContext, PlanView, PlanEdit。实现只有 InMemoryPlan    \nPlan 代表着预订系统的核心数据结构,维护着集群资源的工作安排(分配或收回)计划。用户将 ReservationRequest 提交给 RM 之后,RM 委托给 ReservationAgent，ReservationAgent 通过PlanView 接口咨询该 Plan 是否能满足 RDL 时间且资源约束。如果可以分配,则通过 PlanEdit 接口将其存储在该 Plan 中。之后便向用户返回 ReservationId 票据,用户可通过该票据在预订的时间范围使用预订的资源。  \nPlanFollower将会周期性的从 Plan 中读取最新工作安排计划(队列瞬时容量),并同步给常规调度器,进而影响正在运行作业占用的资源。  \n接口中有三类方法:  PlanContext 负责配置信息;PlanView 负责对Plan 状态的只读访问;PlanEdit 负责对 Plan 状态写入访问。  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| getStep     | 获取 Plan 的时间步长 | PlanContext,同 PlanFollower 的时间步长   |\n| getReservationAgent  | 获取 Plan关联的 ReservationAgent   | PlanContext |\n| getReplanner     | 使用 Planner 对象来应对Plan 的资源意外减少 | PlanContext   |\n|getSharingPolicy | SharingPolicy 控制多用户共享计划资源| PlanContext | \n|getReservationById|通过 ReservationId 获取 Reservation 的详细信息|PlanView|\n|getReservationsAtTime|获取指定时间点所有活跃的 Reservation 的详细信息|PlanView|\n|getAllReservations|获取 Plan 中所有预订信息|PlanView|\n|getTotalCommittedResources| 获取指定时间点所有预定的总资源量| PlanView|\n|getConsumptionForUser|获取指定时间点指定用户预定的总资源量|PlanView|\n|getEarliestStartTime|获取计划中最早的预订开始时间|PlanView|\n|getLastEndTime|获取计划中最晚的预订结束时间|PlanView|\n|addReservation|增加一个预订|PlanEdit|\n|updateReservation|更新一个预订|PlanEdit|\n|deleteReservation|删除一个预订|PlanEdit|\n|archiveCompletedReservations|清除所有的过期预订|PlanEdit|\n\n　　仅列举出核心方法,实际不限于此  \n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.GreedyReservationAgent  \n实现了ReservationAgent接口。 \n一个简单的贪婪放置策略来满足用户预订的代理。具体方式是:按照 ReservationRequests中的各个 ReservationRequest作为单独的阶段,从deadline 开始向后移动至 arrival 来安排预订请求。该代理不考虑本地性,仅仅考虑容器粒度的验证(e.g. 不能超过最大容器大小)  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| createReservation     | 使用此代理尝试创建一个预订 | |\n| updateReservation  | 使用此代理更新一个已有预订   | |\n| deleteReservation     | 使用此代理删除一个已有预订 | |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy  \n主要功能是校验 Plan 能否接受用户预订请求 。实现类 CapacityOverTimePolicy使用容量的按时间拓展概念：策略会保证该用户的当前预订申请和已生效的预订申请资源不超过瞬时资源限制(e.g. reservable queue capacity * 1),且在 24h 时间窗口内不能超过平均资源限制(e.g. reservable queue capacity * 0.5)。从某种意义来说,预订可以使用 reservable queue 的大部分容量 但只要保证快速归还以保证平均资源限制，这可以防止资源滥用且增加了灵活性。通过配置瞬时资源限制和平均资源限制以及时间窗口,策略可以使 reservable queue 达到即时执行(max=100%,avg=100%)和完全灵活(max=?,avg=?,保留给其他用户或系统)的效果。   \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| init     | 初始化 Policy | 策略必要的配置读取 |\n| validate  | 校验 Plan 能否接受用户预订请求   | |\n| getValidWindow     | 预订资源的过期时间 | 窗口为[-24h,当前)。预订的 deadline 在此之前的记录将被删除|\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.Planner  \n实现类只有SimpleCapacityReplanner,且功能远不及 Planner 的设计(增删改 Plan)，只有删除预订资源的功能：从当前时刻开始直到 min{最后一个预约endtime，一个小时之后}，如果所有用户预订资源总量超过 reservable queue capacity，则删除接受时间较晚的一批预订。 \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| init     | 初始化 Planner |  |\n| plan  | 更新现有的 Plan,或增删改已有预订或增加一个新的预订   | SCR 只支持删除晚的预订以保证预订资源总量不超过队列最低配额 |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation  \n预订系统的核心数据结构，是一种运行长度编码(Run Length Encoded)的稀疏数据结构(TreeMap)，可随着时间的推移维护资源分配。维护者各事件点(预订分配 starttime，endtime)时(Plan/当前预订)全局应有的资源总量。  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| getEarliestStartTime     | 最早的资源分配的时间戳 | treemap  firstKey |\n|getLatestEndTime|最晚的资源分配的时间戳|treemap lastKey |\n|getCapacityAtTime|该时间点的 Plan 已预订资源量|treemap floorKey |\n|addInterval|在treemap 中新增一个时间范围的预约资源记录|维护开始点和结束点的资源量|\n|removeInterval|在 treemap 中删除一个时间范围的预约资源记录|维护开始点和结束点的资源量|\n|addCompositeInterval|在 treemap 中新增一个时间范围的多个预订资源记录| never used |\n\n\n>\n TreeMap 的floorEntry(targetKey) 和 lowerEntry(targetKey)  区别:   \n floorEntry 返回 key 小于等于 targetKey 的键值对,无则 null  \n lowerEntry 返回 key 严格小于 targetKey 的键值对,无则 null  \n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation\n实现自 ResourceAllocation。预订分配的结果，内存数据结构，包含预订的整体开始结束时间，ReservationSystem 校验(GA 校验资源/SharingPolicy 校验用户违规)通过 ReservationSubmissionRequest 的时间，<时间段，资源量>的分配细节和RLESparseResourceAllocation    \n\n| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| compareTo     | 按 acceptTime比较,晚的在前| 比较器, Plan 队列资源骤减时,删除部分晚的的 ResourceAllocation |\n\n\n* org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest  \n客户端封装ReservationSubmissionRequest 请求，通过 client-RM 接口提交给 RM，数据结构如下       \n 　　queueName:String    //支持预订资源的队列  \n　　rd:ReservationDefinition //预订定义      \n　　　　arrival:long  //预订开始时间的 最早时间    \n　　　　deadline:long   //预订结束时间的 最晚时间  \n　　　　name:String  //名称  \n　　　　reservationRequests:ReservationRequests  //预订请求    　\n　　　　　　reservationResources:List\\<ReservationRequest\\>  \n　　　　　　　　capability:Capacity  //每个预订请求的资源量    \n　　　　　　　　numContainers:int  //预订的 container 数量  \n　　　　　　　　concurrency:int //并发度。numContainers 可以分批分配        \n　　　　　　　　duration:long  //使用时间  \n　　　　type:ReservationRequestInterpreter  //多个预订请求之间的依赖关系  \n\n*  org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter  \n枚举类型，用以表示多个预订请求之间的依赖关系(或称: 组关系)      \n\n\t| 类型  | 含义  | 场景 |\n|:------------- |:---------------|:-------------|\n|R_ANY| 仅满足 RDL 中一个预订请求即可|有多种等效的方式满足要求。e.g. 1个<4G,2core>或2个<2G,1core>。Agent 会决定使用最合适的ReservationRequest|\n|R_ALL|需要满足 RDL中所有预订请求,请求的分配没有限制时间先后|事务|\n|R_ORDER|需要满足 RDL 中所有预订请求,且有严格的时间限制。k 位置的分配时间段必须在 k+1位置分配时间段之前(无交集)，且 k 位置分配的结束时间和 k+1位置分配的开始时间可以有任意长的时间间隔|具有固定依赖的阶段性工作流。e.g. 第一个作业需要 1个<4G,2core> 5min，其输出作为第二个作业的输入，第二个作业需要 2 个<2G,1core> 10min，则两个作业预订的分配时间段必然不能重叠|\n|R\\_ORDER\\_NO\\_GAP|R_ORDER 的严格版本，要求 k 位置分配的结束时间和 k+1 位置分配的开始时间相同、不能有任何间隙,即 \"zero-size gap\",\"no\\_gap\"|1.当前一个作业输出规模比较大时,避免保留太长时间 2.实时性要求很高的场景,作业间时间差则增大了工作流累计延时|\t\n\n* org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionResponse  \n客户端提交 ReservationSubmissionRequest 请求后，若RM 校验通过可以分配预订，则返回 reservationId给客户端。数据结构如下  \n　　reservationId:ReservationId  \n　　　　clusterTimestamp:long    \n　　　　id:long   \n\t\t\n### step 0   \n用户提交预订创建请求，并收到包含 ReservationId 的响应  \n\n[ApplicationClientProtocol.java]\n\n```\npublic ReservationSubmissionResponse submitReservation(\n      ReservationSubmissionRequest request) throws YarnException, IOException\n```\n### step 1  \n用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求,可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败  \n以 TestCase 中的代码片段为例  \n\n[TestClientRMService.java]\n\n```\n@Test\npublic void testReservationAPIs() {\n\t....\n    //创建一个预订请求\n    Clock clock = new UTCClock();\n    //预订最早开始时间\n    long arrival = clock.getTime();\t   \n    //使用时长 \n    long duration = 60000;    \n    //预订的最晚结束时间\n    long deadline = (long) (arrival + 1.05 * duration);\n    //RDL: Rervation Define Language \n    //请求 4 个 <1G,1core> container,使用时长是 6000ms,使用开始的最早时间是 arrival,使用结束的最晚内时间是 deadline。\n    ReservationSubmissionRequest sRequest =\n        createSimpleReservationRequest(4, arrival, deadline, duration);\n    ReservationSubmissionResponse sResponse = null;\n    try {\n      sResponse = clientService.submitReservation(sRequest);\n    } catch (Exception e) {\n      Assert.fail(e.getMessage());\n    }\n    ....\n}\n\nprivate ReservationSubmissionRequest createSimpleReservationRequest(\n      int numContainers, long arrival, long deadline, long duration) {\n    ReservationRequest r =\n        ReservationRequest.newInstance(Resource.newInstance(1024, 1),\n            numContainers, 1, duration);\n            //R_ALL 满足所有请求\n    ReservationRequests reqs =\n        ReservationRequests.newInstance(Collections.singletonList(r),\n            ReservationRequestInterpreter.R_ALL);\n    ReservationDefinition rDef =\n        ReservationDefinition.newInstance(arrival, deadline, reqs,\n            \"testClientRMService#reservation\");\n    ReservationSubmissionRequest request =\n        ReservationSubmissionRequest.newInstance(rDef,\n            ReservationSystemTestUtil.reservationQ);\n    return request;\n  }\n```\n### step 2 \nReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。   \n\n[ClientRMService.java]\n\n```\n@Override\n  public ReservationSubmissionResponse submitReservation(\n      ReservationSubmissionRequest request) throws YarnException, IOException {\n    // 检查 ReservactionSystem 是否启用\n    checkReservationSytem(AuditConstants.SUBMIT_RESERVATION_REQUEST);\n    ReservationSubmissionResponse response =\n        recordFactory.newRecordInstance(ReservationSubmissionResponse.class);\n    // 通过 AtmicLong 创建全局唯一的ReservationId\n    ReservationId reservationId = reservationSystem.getNewReservationId();\n    // 第一步:  校验\n    // 1.预订请求指定了 queue; 2.指定的 queue 是 reservable queue,即属于 ReservationSystem 管理  \n    // 2.Reservation Define Language 校验\n    //\t2a. 空值检验  2b. deadline 不能早于当前时间 2c. ReservationRequests 空值及空集判断  \n    //\t2d. 分配所有请求的最短时间(R_ANY,R_ALL:取最大;R_ORDER..取和)不能超过 deadline-arrival \n    //\t2e. 分配所有请求的最大资源量(concurrency * request capacity)不能超过 reservable queue capacity\n    Plan plan =\n        rValidator.validateReservationSubmissionRequest(reservationSystem,\n            request, reservationId);\n    // 校验 ACL\t\n    String queueName = request.getQueue();\n    String user =\n        checkReservationACLs(queueName,\n            AuditConstants.SUBMIT_RESERVATION_REQUEST);\n    try {\n      // 第二步:  使用 ReservationAgent 来尝试放置预订请求\n      boolean result =\n          plan.getReservationAgent().createReservation(reservationId, user,\n              plan, request.getReservationDefinition());\n      if (result) {\n        // 同步 reservationSystem <reservationId,queueName>关系\n        reservationSystem.setQueueForReservation(reservationId, queueName);\n        // create the reservation synchronously if required\n        // 第三步:  如下,若预留的 arrival 时间早于当前(错过) PlanFollower 一个步长,则同步创建\n        refreshScheduler(queueName, request.getReservationDefinition(),\n            reservationId.toString());\n  \t// response 中返回放置预订请求的 reservationId\n        response.setReservationId(reservationId);\n      }\n    } catch (PlanningException e) {\n     ...\n    }\n    ...\n    return response;\n  }\n   private void refreshScheduler(String planName,\n      ReservationDefinition contract, String reservationId) {\n    if ((contract.getArrival() - clock.getTime()) < reservationSystem\n        .getPlanFollowerTimeStep()) {\n      ....\n      //后续再说源码\n      reservationSystem.synchronizePlan(planName);\n      ....\n    }\n```\n在第二步中,RMClientService 委托 Plan(reservable queue 的映射) 绑定的 ReservationAgent 来决定是否放置该预订请求  \nReservationAgent(RA)将预订请求中的每个 ResourceRequest(RR)作为一个单独的 stage 尝试放置,并在放置时考虑 ReservationRequestInterpreter(RRI)组关系的处理。从最后一个 stage 开始向前依次放置，使用 Plan 已有的预订计划累计的数据和当次预订请求的累计中间数据判断能否放置当前 RR来影响当前 RR放置时间策略并存储最终放置策略结果。   \n \n[GreedyReservationAgent.java]\n\n```\nprivate boolean computeAllocation(ReservationId reservationId, String user,\n      Plan plan, ReservationDefinition contract,\n      ReservationAllocation oldReservation) throws PlanningException,\n      ContractValidationException {\n    LOG.info(\"placing the following ReservationRequest: \" + contract);\n    // reservable queue capacity \n    Resource totalCapacity = plan.getTotalCapacity();\n    \n   // 译文:考虑在此增加逻辑来调整\"ResourceDefinition\" 来解决系统的缺陷(e.g. 大型容器的调度延迟) \n   //  TODO: 想不出会在这个位置使用什么样的策略来解决大容器调度延迟问题? 逻辑能转移到常规调度吗\n    \n    // 使用保守策略规整缩短 [arrival,deadline]区间\n    long earliestStart = contract.getArrival();\n    long step = plan.getStep();\n    if (earliestStart % step != 0) {\n      earliestStart = earliestStart + (step - (earliestStart % step));\n    }\n    long deadline =\n        contract.getDeadline() - contract.getDeadline() % plan.getStep();\n        \n    // 将每个 ReservationRequest(RR) 作为一个单独的 stage,设置一些阶段间的临时变量\n    long curDeadline = deadline;\n    long oldDeadline = -1;\n    // 当次预订请求，全部 RR 放置时间策略结果的(中间)数据结构 \n    Map<ReservationInterval, ReservationRequest> allocations =\n        new HashMap<ReservationInterval, ReservationRequest>();\n     // 当次预订请求，全部 RR 放置造成<时间-资源>变动结果的(中间)数据结构  \n    RLESparseResourceAllocation tempAssigned =\n        new RLESparseResourceAllocation(plan.getResourceCalculator(),\n            plan.getMinimumAllocation());\n\n    List<ReservationRequest> stages = contract.getReservationRequests()\n        .getReservationResources();\n    ReservationRequestInterpreter type = contract.getReservationRequests()\n        .getInterpreter();\n    // 使用迭代器 从最后一个元素之后的 null 向前迭代\n    for (ListIterator<ReservationRequest> li = \n        stages.listIterator(stages.size()); li.hasPrevious();) {\n      ReservationRequest currentReservationStage = li.previous();\n       //第一步:  校验 RR\n      // 1.concurrency > 0; 2.numContainer>0; 3. numContainer 必须是 concurrency 的倍数; 4.单容器大小不得超过maximumAllocation\n      validateInput(plan, currentReservationStage, totalCapacity);\n      //第二步:  尝试分配单个 RR\n      Map<ReservationInterval, ReservationRequest> curAlloc =\n          placeSingleStage(plan, tempAssigned, currentReservationStage,\n              earliestStart, curDeadline, oldReservation, totalCapacity);\n\n      if (curAlloc == null) {\n        // 组关系是 R_ALL,R_ORDER,R_NO_GAP 时，一个 RR 分配失败 导致整个事务失败\n        // 组关系是 R_ANY,一个 RR 分配失败可以尝试其他 RR \n        if (type != ReservationRequestInterpreter.R_ANY) {\n          throw new PlanningException(\"The GreedyAgent\"\n              + \" couldn't find a valid allocation for your request\");\n        } else {\n          continue;\n        }\n      } else {\n       // 可以放置当前 RR(stage)\n       \n        allocations.putAll(curAlloc);\n       // 组关系是 R_ANY 的话,可以终止后续 stage 的尝试了\n        if (type == ReservationRequestInterpreter.R_ANY) {\n          break;\n        }\n\n        // 如果组关系是 R_ODER,R_ODER_NO_GAP，则通过设置 curDeadline 来确保 RR 之间的先后关系 \n        // 将下一个 RR 的 deadline 设置为当前 RR 分配中最早的开始时间\n        if (type == ReservationRequestInterpreter.R_ORDER\n            || type == ReservationRequestInterpreter.R_ORDER_NO_GAP) {\n          curDeadline = findEarliestTime(curAlloc.keySet());\n\n          // 对于 R_ORDER_NO_GAP,确认当前分配和前一个(后向)分配之间没有空隙\n          // 空隙并非之前理解的完全相等,[t0,t1) 与 [t1,t2)之间的关系,而是不超过 Plan 的时间步长即可 \n          if (type == ReservationRequestInterpreter.R_ORDER_NO_GAP\n              && oldDeadline > 0) {\n            if (oldDeadline - findLatestTime(curAlloc.keySet()) > plan\n                .getStep()) {\n              throw new PlanningException(\"The GreedyAgent\"\n                  + \" couldn't find a valid allocation for your request\");\n            }\n          }\n          // keep the variable oldDeadline pointing to the last deadline we\n          // found\n          oldDeadline = curDeadline;\n        }\n      }\n    }\n    // 没有为预订请求分配到<时间,资源>,通过 throws exception 的方式来快速失败\n    if (allocations.isEmpty()) {\n      throw new PlanningException(\"The GreedyAgent\"\n          + \" couldn't find a valid allocation for your request\");\n    }\n\n  // 第三步:  创建预订\n  // 在[arrival,earliestTime)之间加入了 \"零填充\",以表明其是从arrival time 开始的预订\n  // TODO:在开头和末尾添加\"零填充\"的意义是什么?没有的话 会有什么问题  \n    ReservationRequest ZERO_RES =\n        ReservationRequest.newInstance(Resource.newInstance(0, 0), 0);\n    long firstStartTime = findEarliestTime(allocations.keySet());\n    if (firstStartTime > earliestStart) {\n      allocations.put(new ReservationInterval(earliestStart,\n          firstStartTime), ZERO_RES);\n      firstStartTime = earliestStart;\n      // 译: 考虑在[lastEndTime,deadline)添加\"零填充\"\n    }\n    // 译:这有待验证,其他代理可能也在放置(同步问题);有代理不知道的 SharingPolicy\n    ReservationAllocation capReservation =\n        new InMemoryReservationAllocation(reservationId, contract, user,\n            plan.getQueueName(), firstStartTime,\n            findLatestTime(allocations.keySet()), allocations,\n            plan.getResourceCalculator(), plan.getMinimumAllocation());\n    // 第四步: 新增或更新已有预订分配 。在 step 3 中解释\n    if (oldReservation != null) {\n      return plan.updateReservation(capReservation);\n    } else {\n      return plan.addReservation(capReservation);\n    }\n  }\n```\n再看上述 第二步:  尝试分配单个 RR    \n这是整个流程中最重要的部分: 决定了用户的每个 RR 能不能放置,放置在哪个时间段,放置几个 container    \n核心思想是: 从 deadline(动态 deadline)开始向后尝试，每次尝试都遍历 duration 时间，判断 duration 时间每个时间步长是否超过了 capacity(即判断 指定大小的资源能不能占用当前时间段，故称\"放置\")。然后向后移动一个时间步长，直至剩余时间不足以满足 duration 或者放置了 RR 中所有 container 结束       \n\n[GreedyReservationAgent.java]  \n\n```\nprivate Map<ReservationInterval, ReservationRequest> placeSingleStage(\n      Plan plan, RLESparseResourceAllocation tempAssigned,\n      ReservationRequest rr, long earliestStart, long curDeadline,\n      ReservationAllocation oldResAllocation, final Resource totalCapacity) {\n\n    Map<ReservationInterval, ReservationRequest> allocationRequests =\n        new HashMap<ReservationInterval, ReservationRequest>();\n\n    Resource gang = Resources.multiply(rr.getCapability(), rr.getConcurrency());\n    long dur = rr.getDuration();\n    long step = plan.getStep();\n    if (dur % step != 0) {\n      dur += (step - (dur % step));\n    }\n    // 将一个并发作为一个原子性的组分配 \n    // 每次分配资源量: gang=capacity * concurrency \n    //     共计分配次数: gangsToPlace= numContainers/concurrency  \n    int gangsToPlace = rr.getNumContainers() / rr.getConcurrency();\n    int maxGang = 0;\n\n    // loop trying to place until we are done, or we are considering\n    // an invalid range of times\n    // 循环尝试分配 直至所有的 gang 原子组都分配完或者剩余时间不足以容纳一个 duration\n    while (gangsToPlace > 0 && curDeadline - dur >= earliestStart) {\n\n      // as we run along we remember how many gangs we can fit, and what\n      // was the most constraining moment in time (we will restart just\n      // after that to place the next batch)\n      maxGang = gangsToPlace;\n      long minPoint = curDeadline;\n      int curMaxGang = maxGang;\n\n      // 尝试在[curDeadline-duration,curDeadline) 长为 duration 的区间内放置未知个 Resource=gang 的原子组\n      for (long t = curDeadline - plan.getStep(); t >= curDeadline - dur\n          && maxGang > 0; t = t - plan.getStep()) {\n\n        // 如果之前的预订存在的话,则删除此预订之前的分配(e.g. 更新预订时)\n        Resource oldResCap = Resource.newInstance(0, 0);\n        if (oldResAllocation != null) {\n          oldResCap = oldResAllocation.getResourcesAtTime(t);\n        }\n\n\t// 计算当前时间点的净可用资源\n        Resource netAvailableRes = Resources.clone(totalCapacity);\n        Resources.addTo(netAvailableRes, oldResCap);\n        // Plan(reservable queue), 最靠近 t 时刻的累计容量\n        Resources.subtractFrom(netAvailableRes,\n            plan.getTotalCommittedResources(t)); \n        //当前预订, 最靠近 t 时刻的累计容量\n        Resources.subtractFrom(netAvailableRes,\n            tempAssigned.getCapacityAtTime(t));\n        \n        //计算当前时刻能满足的最大数量的 gang\n        curMaxGang =\n            (int) Math.floor(Resources.divide(plan.getResourceCalculator(),\n                totalCapacity, netAvailableRes, gang));\n\n        // min{需求的 gang 数量,剩余资源能支撑的 gang 数量}\n        curMaxGang = Math.min(gangsToPlace, curMaxGang);\n\t// 记住当前尝试的 duration 内,最小数量的 gang 点(即 队列+reservation 占用资源量最多的点)，作为下次尝试的右边界\n        if (curMaxGang <= maxGang) {\n          maxGang = curMaxGang;\n          //资源最紧俏的时间点。当资源充裕时,放置尽可能多的 gang,也会成为当前最紧俏的时间点，作为下次放置的右边界    \n          minPoint = t;\n        }\n      }\n\n      // if we were able to place any gang, record this, and decrement\n      // gangsToPlace maxGang=0:没资源了 时间:范围不够了\n      // 退出上一个循环过程有两种情况: \n      // 1.  maxGang=0,这个 duration 内有一个时间点 净剩余资源不足以放下一个 gang,那么整个 duration 都不能用了\n      // 2. t<=curDeadline - dur 整个 duration 已经遍历完了。此时 maxGang > 0\n      // 以上退出都有可能,所以需要中间变量 maxGang来做判断。\n      if (maxGang > 0) {\n      \n      // 能在[curDeadline-duration,curDeadline) 内放置下 maxGang 个gang，即 concurrency * maxGang 个 container\n        gangsToPlace -= maxGang;\n        ReservationInterval reservationInt =\n            new ReservationInterval(curDeadline - dur, curDeadline);\n        ReservationRequest reservationRes =\n            ReservationRequest.newInstance(rr.getCapability(),\n                rr.getConcurrency() * maxGang, rr.getConcurrency(),\n                rr.getDuration());\n        // 记住已占用的空间。后续贴出源码\n        // reservable queue 对应的 Plan 暂时是只读的，除非能将整个 ReservationRequests 放入到计划中。\n        tempAssigned.addInterval(reservationInt, reservationRes);\n        allocationRequests.put(reservationInt, reservationRes);\n      }\n      // 设置下次放置 gang 的右边界。右边界向右无法再安置一个 gang(如上逻辑所述),尝试在右边界向左放置新的 gang\n      curDeadline = minPoint;\n    }\n    \n    if (gangsToPlace == 0) {\n     // 放置了所有的 gang \n      return allocationRequests;\n    } else {\n      // 不能为当前 ReservationRequest(RR) 在时间/容量/并发 限制上放置所有的 gang。\n      // 此次 RR放置失败,如果不是 R_ANY 的话，可以终止整个 预订请求了。\n      for (Map.Entry<ReservationInterval, ReservationRequest> tempAllocation :\n        allocationRequests.entrySet()) {\n        // 清理之前的中间数据。后续贴出源码\n        tempAssigned.removeInterval(tempAllocation.getKey(),\n            tempAllocation.getValue());\n      }\n      return null;\n    }\n  \n```\n\nPlan 中使用 RLESparseResourceAllocation(以 TreeMap 为核心的稀疏数据结构)保存了当前已生效所有Reservation的累积<时间-资源>状态变化图，通过汇总所有 Reservation 的 RR 中每个分配的时间段(startTime,endTime)和资源量 最终得到了 Plan 随时间变化的已分配资源量。对每个 ReservationSubmissionRequest 请求来说 ，在尝试分配所有 RR 的时候，也使用RLESparseResourceAllocation 来保存临时分配好 RR 累积<时间-资源>状态变化图。  \n \n[RLESparseResourceAllocation.java]  \n\n```\nprivate TreeMap<Long, Resource> cumulativeCapacity = new TreeMap<Long, Resource>();\n//向 RLESparseResourcAllocation 中增加一个 <时间范围-容量> 的预订\npublic boolean addInterval(ReservationInterval reservationInterval,\n      ReservationRequest capacity) {\n    Resource totCap =\n        Resources.multiply(capacity.getCapability(),\n            (float) capacity.getNumContainers());\n    if (totCap.equals(ZERO_RESOURCE)) {\n      return true;\n    }\n    writeLock.lock();\n    try {\n      long startKey = reservationInterval.getStartTime();\n      long endKey = reservationInterval.getEndTime();\n      //截取 (?,endKey)之间 一段 TreeMap\n      NavigableMap<Long, Resource> ticks =\n          cumulativeCapacity.headMap(endKey, false);\n      if (ticks != null && !ticks.isEmpty()) {\n        Resource updatedCapacity = Resource.newInstance(0, 0);\n        // 找到时间上小于等于 startKey 的最大 key 的键值对\n        Entry<Long, Resource> lowEntry = ticks.floorEntry(startKey);\n        if (lowEntry == null) {\n\t   // 表明 startKey 即为 RLE 全局最小键(最早)\n          cumulativeCapacity.put(startKey, totCap);\n        } else {\n          updatedCapacity = Resources.add(lowEntry.getValue(), totCap);\n          if ((startKey == lowEntry.getKey())\n              && (isSameAsPrevious(lowEntry.getKey(), updatedCapacity))) {\n             // 通过 remove 达到合并区间的。\n             // e.g. [t0,t1)为 2G, [t1,t2)为 1G;现在在 [t1,t3) 上分配了 1G,那么可以删除 t1 这个点 \n            cumulativeCapacity.remove(lowEntry.getKey());\n          } else {\n             //新增 一个时间点的容量 记录\n            cumulativeCapacity.put(startKey, updatedCapacity);\n          }\n        }\n        // [startKey,endKey)时间段内的所有已有记录都要增加 totCap 作为最终状态 \n        // cumulativeCapacity 维护的是全局状态量,而不是增量或者单个 reservation 的数据\n        Set<Entry<Long, Resource>> overlapSet =\n            ticks.tailMap(startKey, false).entrySet(); //startKey 已经增加过了 \n        for (Entry<Long, Resource> entry : overlapSet) {\n          updatedCapacity = Resources.add(entry.getValue(), totCap);\n          entry.setValue(updatedCapacity);\n        }\n      } else {\n        cumulativeCapacity.put(startKey, totCap);\n      }\n      Resource nextTick = cumulativeCapacity.get(endKey);\n      if (nextTick != null) {\n        if (isSameAsPrevious(endKey, nextTick)) {\n        // 合并 endKey\n          cumulativeCapacity.remove(endKey);\n        }\n      } else {\n\t// endKey 时还回资源,所以在 endKey 前面记录都加了 totCap,此处会减去  \n        cumulativeCapacity.put(endKey, Resources.subtract(cumulativeCapacity\n            .floorEntry(endKey).getValue(), totCap));\n      }\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  }\n  // 在 RLESparseResourcAllocation 中删除一个 <时间范围-容量> 的预订\n  public boolean removeInterval(ReservationInterval reservationInterval,\n      ReservationRequest capacity) {\n    Resource totCap =\n        Resources.multiply(capacity.getCapability(),\n            (float) capacity.getNumContainers());\n    if (totCap.equals(ZERO_RESOURCE)) {\n      return true;\n    }\n    writeLock.lock();\n    try {\n      long startKey = reservationInterval.getStartTime();\n      long endKey = reservationInterval.getEndTime();\n      NavigableMap<Long, Resource> ticks =\n          cumulativeCapacity.headMap(endKey, false);\n      //取出[startKey,endKey)区间内的所有记录\n      SortedMap<Long, Resource> overlapSet = ticks.tailMap(startKey);\n      if (overlapSet != null && !overlapSet.isEmpty()) {\n        Resource updatedCapacity = Resource.newInstance(0, 0);\n        long currentKey = -1;\n        for (Iterator<Entry<Long, Resource>> overlapEntries =\n            overlapSet.entrySet().iterator(); overlapEntries.hasNext();) {\n          Entry<Long, Resource> entry = overlapEntries.next();\n          currentKey = entry.getKey();\n          // 在每个时间点的减去该预订已分配的资源量  \n          updatedCapacity = Resources.subtract(entry.getValue(), totCap);\n          cumulativeCapacity.put(currentKey, updatedCapacity);\n        }\n        // 左右边界点存在性判断\n        Long firstKey = overlapSet.firstKey();\n        if (isSameAsPrevious(firstKey, overlapSet.get(firstKey))) {\n          cumulativeCapacity.remove(firstKey);\n        }\n        if ((currentKey != -1) && (isSameAsNext(currentKey, updatedCapacity))) {\n          cumulativeCapacity.remove(cumulativeCapacity.higherKey(currentKey));\n        }\n      }\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  }\n```\n\n举个简单例子，为便于讨论:  \n1.容量方面:  reservable queue 有 <2G,2core>的 capacity，所有预订请求中 container 容量都是<1G,1core>，即 queue 最多有 2 个container  \n2.时间方面: 从 t0至 t5，时间单位是 PlanFollower 的时间步长 step(默认是 1s)。t0 为当前时刻    \n3.请求方面: 已有的 reservation 不讨论 arrival,deadline，只给出 ReservationSystem 给出的分配<时间范围-容量>  \n4.ReservationSystem 已有的预订如下  \n\n| 预订 Id  | 开始时间  | 结束时间 | 占用时间 | 总资源量|\n|:------------- |:---------------|:-------------|:-------------|:-------------|\n|r0|t3|t4| 1 个单位| 1个 container|\n|r1|t2|t3| 1个单位|2个 container|\n|r2|t1|t2| 1个单位| 1个 container|\n\n由此可以绘出 Plan 对应的 RLESparseResourceAllocation 中维护的累计<时间-资源>状态变化图  \n\n![](/img/pictures/reservation/reservationSystem.png)  \n\n即: r2 在 t1 时刻拿到1 个 container,因此图中 t1 时刻已分配资源状态是 1 个 container。  \n      r2 在 t2 时刻释放 1 个 container,r1 拿到两个 2 个 container,因此 t2 时刻已分配资源状态是 2 个 container \n \n 假设 ReservationSystem接收到一个请求，ReservationDefinition定义的预订请求内容为：\narrival=t0,deadline=t5  \nReservationRequests(RRS) 中有一个 ReservationRequest(RR)   \n　　capacity<1G,1core>,numContainer=2,concurrency=1,duration=2     \nReservationRequestInterpreter=R_ALL\n\n来模拟一下GreedyRerservationAgent#placeSingleStage是如何尝试放置这一个 RR 的  \n初始值:gang=concurrency * capacity=<1G,1core>  gangsToPlace=numContainers/concurrency=2  maxGang=2  \n \n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|2| t5| 2 | t4| 2-0-0=2 contianer|2|2| t4|是|\n|第二次次循环|2| t4| 2 | t3| 2-1-0=1 contianer|1|1| t3|否,duration 超限退出|\n\n内层循环结束时，maxGang=1，即可以在[t3,t5)这个时间段放下 1 个 gang，在此为 1 个 container，外层循环将此分配封装成<ReservationInterval,ReservationRequest>加入到 当前预订累计分配的中间数据结构中(tmpAssigned:RLESparseResourceAllocation)。  \n\n此时: gangsToPlace=1,curDeadline=t3,进行下一次外层循环  \n\n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|1| t3| 1 | t2| 2-2-0=0 contianer|0|0| t2|否,maxGang=0退出|\n\n内层循环结束后，maxGang=0，不能在[t1,t3)时间段放置任何一个 gang。  \n\n此时: gangsToPlace=1,curDeadline=t2，进行下一次外层循环  \n\n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|1| t2| 1 | t1| 2-1-0=1 contianer|1|1| t1|是|\n|第二次循环|1| t1| 1 | t0| 2-0-0=2 contianer|1|1| t0|否,duration 超限退出|  \n\n内层循环结束后，maxGang=1，即可以在[t0,t2)时间段放下一个 gang，即 1 个 container，外层循环将此分配封装好之后再次加入到 tmpAssigned 中。   \n\n此时: gangsToPlace=0终止了外层循环。\n得到的最终结论是: 在[t3,t5)时间段放下一个 container<1G,1core>,在[t0,t2)时间段放下一个 container<1G,1core>。该 RR 整体可以分配，满足组关系 R_ALL，即不考虑用户的情况可以分配该 RR。\n\n### step 3 \n\nSharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  \n\n在 step 2中，ReservationSystem 尝试对预订请求分配时间和资源。若分配成功，则尝试将其加入到 Plan 中或是更新 Plan。    \n\n[GreedyReservationAgent.java] \n\n```\nprivate boolean computeAllocation(ReservationId reservationId, String user,\n      Plan plan, ReservationDefinition contract,\n      ReservationAllocation oldReservation) throws PlanningException,\n      ContractValidationException {\n    LOG.info(\"placing the following ReservationRequest: \" + contract);\n    ......\n    ReservationAllocation capReservation =\n        new InMemoryReservationAllocation(reservationId, contract, user,\n            plan.getQueueName(), firstStartTime,\n            findLatestTime(allocations.keySet()), allocations,\n            plan.getResourceCalculator(), plan.getMinimumAllocation());\n    if (oldReservation != null) {\n     // 更新\n      return plan.updateReservation(capReservation);\n    } else {\n    // 新增\n      return plan.addReservation(capReservation);\n    }\n    }\n```\nupdateReservation 事务性更新 reservation:  先删除旧的reservation，再新增更新的 reservation(当次ReservationSubmissionRequest)。如果新增失败则再把旧的reservation 回滚。 \n所以两处的逻辑的核心在于 addReservation 和 removeReservation。  \n\n[InMemoryPlan.java]  \n\n```\n    //按时间段索引 ReservationAllocation 信息  \n private TreeMap<ReservationInterval, Set<InMemoryReservationAllocation>> currentReservations =\n      new TreeMap<ReservationInterval, Set<InMemoryReservationAllocation>>();\n   // Plan 整体的<时间-资源>状态变化图\n  private RLESparseResourceAllocation rleSparseVector;\n  // 细分用户的<时间-资源>状态变化图\n  private Map<String, RLESparseResourceAllocation> userResourceAlloc =\n      new HashMap<String, RLESparseResourceAllocation>();\n  // 按 reservationId 索引ReservationAllocation 信息\n  private Map<ReservationId, InMemoryReservationAllocation> reservationTable =\n      new HashMap<ReservationId, InMemoryReservationAllocation>();\n      \n public boolean addReservation(ReservationAllocation reservation)\n      throws PlanningException {\n    InMemoryReservationAllocation inMemReservation =\n        (InMemoryReservationAllocation) reservation;\n    if (inMemReservation.getUser() == null) {\n      String errMsg =\n          \"The specified Reservation with ID \"\n              + inMemReservation.getReservationId()\n              + \" is not mapped to any user\";\n      LOG.error(errMsg);\n      throw new IllegalArgumentException(errMsg);\n    }\n    writeLock.lock();\n    try {\n      if (reservationTable.containsKey(inMemReservation.getReservationId())) {\n        String errMsg =\n            \"The specified Reservation with ID \"\n                + inMemReservation.getReservationId() + \" already exists\";\n        LOG.error(errMsg);\n        throw new IllegalArgumentException(errMsg);\n      }\n      // 第一步: 使用 SharingPolicy 校验 Plan 能否接受该 reservation。后续解释\n      policy.validate(this, inMemReservation);\n      // 第二步: 记录接受 reservation 的时间\n      reservation.setAcceptanceTimestamp(clock.getTime());\n      // 第三步: 内存数据结构维护\n      ReservationInterval searchInterval =\n          new ReservationInterval(inMemReservation.getStartTime(),\n              inMemReservation.getEndTime());\n      Set<InMemoryReservationAllocation> reservations =\n          currentReservations.get(searchInterval);\n      if (reservations == null) {\n        reservations = new HashSet<InMemoryReservationAllocation>();\n      }\n      if (!reservations.add(inMemReservation)) {\n        LOG.error(\"Unable to add reservation: {} to plan.\",\n            inMemReservation.getReservationId());\n        return false;\n      }\n      currentReservations.put(searchInterval, reservations);\n      reservationTable.put(inMemReservation.getReservationId(),\n          inMemReservation);\n      // 第四步: 维护Plan 整体的<时间-资源>状态变化图;维护细分用户<时间-资源>状态变化图  \n      incrementAllocation(inMemReservation);\n      LOG.info(\"Sucessfully added reservation: {} to plan.\",\n          inMemReservation.getReservationId());\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  \n  private boolean removeReservation(ReservationAllocation reservation) {\n    assert (readWriteLock.isWriteLockedByCurrentThread());\n    ReservationInterval searchInterval =\n        new ReservationInterval(reservation.getStartTime(),\n            reservation.getEndTime());\n    Set<InMemoryReservationAllocation> reservations =\n        currentReservations.get(searchInterval);\n     // Plan 成员变量维护的信息 维护\n    if (reservations != null) {\n      if (!reservations.remove(reservation)) {\n        LOG.error(\"Unable to remove reservation: {} from plan.\",\n            reservation.getReservationId());\n        return false;\n      }\n      if (reservations.isEmpty()) {\n        currentReservations.remove(searchInterval);\n      }\n    } else {\n      String errMsg =\n          \"The specified Reservation with ID \" + reservation.getReservationId()\n              + \" does not exist in the plan\";\n      LOG.error(errMsg);\n      throw new IllegalArgumentException(errMsg);\n    }\n    reservationTable.remove(reservation.getReservationId());\n    // 1.维护细分用户<时间-资源>状态变化图;维护Plan 整体<时间-资源>状态变化图  \n    decrementAllocation(reservation);\n    LOG.info(\"Sucessfully deleted reservation: {} in plan.\",\n        reservation.getReservationId());\n    return true;\n  }\n```\n\nPlan 在 addReservation 和 updateReservation 时都使用了 SharingPolicy 来校验用户资源使用是否违规。CapacityOverTimePolicy 是默认 SharingPolicy，主要校验两个方面资源使用是否违规：瞬时用量不超过 reservable queue capaciy 的 1%(默认);24h(默认)平均用量不超过reservable queue capacity 的 1%(默认)。上述的 1%和 24h 是源码内常量,无法用配置修改      \n\n[CapacityOverTimePolicy.java]\n\n```\npublic void validate(Plan plan, ReservationAllocation reservation)\n      throws PlanningException {\n    ReservationAllocation oldReservation =\n        plan.getReservationById(reservation.getReservationId());\n    if (oldReservation != null\n        && !oldReservation.getUser().equals(reservation.getUser())) {\n      throw new MismatchedUserException(\n          \"Updating an existing reservation with mismatched user:\"\n              + oldReservation.getUser() + \" != \" + reservation.getUser());\n    }\n\n    long startTime = reservation.getStartTime();\n    long endTime = reservation.getEndTime();\n    long step = plan.getStep();\n    // reservable queue capacity \n    Resource planTotalCapacity = plan.getTotalCapacity();\n    // 最大瞬时容量和最大平均容量\t\n    Resource maxAvgRes = Resources.multiply(planTotalCapacity, maxAvg);\n    Resource maxInsRes = Resources.multiply(planTotalCapacity, maxInst);\n    \t\n    IntegralResource runningTot = new IntegralResource(0L, 0L);\n    // 平均(乘法,即积分面积)容量限制\n    IntegralResource maxAllowed = new IntegralResource(maxAvgRes);\n    maxAllowed.multiplyBy(validWindow / step);\n\n    // 检查与该分配有重叠区域且长度为 validWindow(24h)的任何窗口,\n    //提供给用户的资源是否超过瞬时容量和平均容量限制\n    for (long t = startTime - validWindow; t < endTime + validWindow; t += step) {\n\n      Resource currExistingAllocTot = plan.getTotalCommittedResources(t);\n      Resource currExistingAllocForUser =\n          plan.getConsumptionForUser(reservation.getUser(), t);\n      Resource currNewAlloc = reservation.getResourcesAtTime(t);\n      Resource currOldAlloc = Resources.none();\n      if (oldReservation != null) {\n        currOldAlloc = oldReservation.getResourcesAtTime(t);\n      }\n      // 所有用户累计 reservation 不能超过 reservable queue  capacity\n      Resource inst =\n          Resources.subtract(Resources.add(currExistingAllocTot, currNewAlloc),\n              currOldAlloc);\n      if (Resources.greaterThan(plan.getResourceCalculator(),\n          planTotalCapacity, inst, planTotalCapacity)) {\n        throw new ResourceOverCommitException(\" Resources at time \" + t\n            + \" would be overcommitted (\" + inst + \" over \"\n            + plan.getTotalCapacity() + \") by accepting reservation: \"\n            + reservation.getReservationId());\n      }\n\n      // 细分用户 容量不能超过 瞬时容量限制\n      if (Resources.greaterThan(plan.getResourceCalculator(),\n          planTotalCapacity, Resources.subtract(\n              Resources.add(currExistingAllocForUser, currNewAlloc),\n              currOldAlloc), maxInsRes)) {\n        throw new PlanningQuotaException(\"Instantaneous quota capacity \"\n            + maxInst + \" would be passed at time \" + t\n            + \" by accepting reservation: \" + reservation.getReservationId());\n      }\n      \n      // 相当于增加了 (用户分配容量[已有的+当前reservation更新后的-当前reservation更新前的]) * (一个时间步长)的面积\n      // 当前reservation更新前的 实际上在 已有的 之中,所以需要减去\n      runningTot.add(currExistingAllocForUser);\n      runningTot.add(currNewAlloc);\n      runningTot.subtract(currOldAlloc);\n\t\n      // 老化掉一个24h窗口之前的那个时间点的面积\n      if (t > startTime) {\n        Resource pastOldAlloc =\n            plan.getConsumptionForUser(reservation.getUser(), t - validWindow);\n        Resource pastNewAlloc = reservation.getResourcesAtTime(t - validWindow);\n        runningTot.subtract(pastOldAlloc);\n        runningTot.subtract(pastNewAlloc);\n      }\n      if (maxAllowed.compareTo(runningTot) < 0) {\n        throw new PlanningQuotaException(\n            \"Integral (avg over time) quota capacity \" + maxAvg\n                + \" over a window of \" + validWindow / 1000 + \" seconds, \"\n                + \" would be passed at time \" + t + \"(\" + new Date(t)\n                + \") by accepting reservation: \"\n                + reservation.getReservationId());\n      }\n    }\n  }\n```\n上述 CapacityOverTimePolicy 用积分面积的方式判断 是否超过平均容量限制的方式，很容易和 RLESparseResourceAllocation 混淆。  RLESparseResourceAllocation 存储的是<时间点-资源>的状态量,是累积状态量不是增量。而积分面积用的是 RLESparseResourceAllocation 绘成的状态图算的积分面积，每向后移动一个时间步长则增加 (RLE 那个时间点资源量) * (一个时间步长) 的积分面积，并减去(RLE 24h前那个时间点资源量) * (一个时间步长)的积分面积。从而达到随着时间增加，积分面积始终是 RLESparseResourceAllocation 图中 [now-24h,now]这个区间的面积，然后和(平均资源限制容量) * (24h) 来比较。很巧妙，第一次见定积分可以在程序中这么用           \n\n### step 4\n\n成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据\n参考 \"step 1\" 的接口定义和 \"step 2\" ReservationAgent 分配成功后的处理  \n \n### step 5\n\nPlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序   \n\nPlanFollower(Runnable) 随 ResourceManager 初始化时创建的 ReservationSystem(服务) 初始化而初始化 启动而启动，调用栈如下  \n\n->ResourceManager$RMActiveServices#serviceInit  \n　　->ResourceManager#createReservationSystem  \n　　　　->AbstractReservationSystem#serviceInit  \n　　　　　　->AbstractReservationSystem#createPlanFollower  \n\n[AbstractReservationSystem.java]　\n\n```\n  public void serviceInit(Configuration conf) throws Exception {\n    Configuration configuration = new Configuration(conf);\n    // 第一步: 初始化所有的 reservable queue 。\n    // 指定 SharingPolicy,Planner,capacity,planstep...\n    reinitialize(configuration, rmContext);\n    // 第二步: 创建 PlanFollower。加载类:CS 调度器对应 CapacitySchedulerPlanFollower...\n    planFollower = createPlanFollower();\n    if (planFollower != null) {\n      // 第三步:初始化 PlanFollower \n      planFollower.init(clock, scheduler, plans.values());\n    }\n    super.serviceInit(conf);\n  }\n  \n  public void serviceStart() throws Exception {\n    if (planFollower != null) {\n      scheduledExecutorService = new ScheduledThreadPoolExecutor(1);\n      //定时调度,调度周期是 planStepSize(默认1s)\n      scheduledExecutorService.scheduleWithFixedDelay(planFollower, 0L,\n          planStepSize, TimeUnit.MILLISECONDS);\n    }\n    super.serviceStart();\n  }\n```\nPlanFollower的核心逻辑在 AbstractSchedulerPlanFollower，用于 PlanFollower 与常规调度器同步预订分配的信息，下有 CapacitySchedulerPlanFollower 和 FairCapacityPlanFollower 两个子类。  \n\n[AbstractSchedulerPlanFollower.java] \n\n```\n@Override\n  public synchronized void run() {\n    for (Plan plan : plans) {\n      synchronizePlan(plan);\n    }\n  }\n\n@Override\n  public synchronized void synchronizePlan(Plan plan) {\n     String planQueueName = plan.getQueueName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Running plan follower edit policy for plan: \" + planQueueName);\n    }\n    long step = plan.getStep();\n    long now = clock.getTime();\n    if (now % step != 0) {\n      now += step - (now % step);\n    }\n    Queue planQueue = getPlanQueue(planQueueName);\n    if (planQueue == null) return;\n    \n    Resource clusterResources = scheduler.getClusterResource();\n    // plan 对应的 reservable queue 的容量\n    Resource planResources = getPlanResources(plan, planQueue,\n        clusterResources);\n    // 当前正在生效的预订(当前时间介于 startTime 和 endTime 之间的所有 Reservation) \n    Set<ReservationAllocation> currentReservations =\n        plan.getReservationsAtTime(now);\n    // 当前正在生效的 reservationId 集合\n    Set<String> curReservationNames = new HashSet<String>();\n    // 当前正在生效预订的 resource\n    Resource reservedResources = Resource.newInstance(0, 0);\n    // 当前正在生效的 reservation;\n    int numRes = getReservedResources(now, currentReservations,\n        curReservationNames, reservedResources);\n\n    // 创建 reservable queue 的默认队列: xxx-default\n    String defReservationId = getReservationIdFromQueueName(planQueueName) +\n        ReservationConstants.DEFAULT_QUEUE_SUFFIX;\n    String defReservationQueue = getReservationQueueName(planQueueName,\n        defReservationId);\n    createDefaultReservationQueue(planQueueName, planQueue,\n        defReservationId);\n    curReservationNames.add(defReservationId);\n\n    // 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)\n    // 如果专用于预订资源的队列骤减(或配置或节点宕机),通过调用 Planner 来删除部分已有预约\n    if (arePlanResourcesLessThanReservations(clusterResources, planResources,\n        reservedResources)) {\n      try {\n        plan.getReplanner().plan(plan, null);\n      } catch (PlanningException e) {\n        LOG.warn(\"Exception while trying to replan: {}\", planQueueName, e);\n      }\n    }\n    \n    // 第二步: 标记当前预约中，哪些旧的预约需要过期删除，哪些新的预约需要创建队列  \n    List<? extends Queue> resQueues = getChildReservationQueues(planQueue);\n    Set<String> expired = new HashSet<String>();\n    for (Queue resQueue : resQueues) {\n      String resQueueName = resQueue.getQueueName();\n      String reservationId = getReservationIdFromQueueName(resQueueName);\n      if (curReservationNames.contains(reservationId)) {\n        // 当前活跃的预订包含此此预约，因此不用为它创建对应的队列\n        curReservationNames.remove(reservationId);\n      } else {\n\t// 预约已经结束，准备清除\n        expired.add(reservationId);\n      }\n    }\n    // 第三步: 清除过期预约。 getMoveOnExpiry 默认为 true\n    cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue);\n\n    float totalAssignedCapacity = 0f;\n    if (currentReservations != null) {\n      // 释放默认队列中的所有多余容量  \n      try {\n        setQueueEntitlement(planQueueName, defReservationQueue, 0f, 1.0f);\n      } catch (YarnException e) {\n        LOG.warn(\n            \"Exception while trying to release default queue capacity for plan: {}\",\n            planQueueName, e);\n      }\n       // 第四步: 增加新的预约或更新已有的预约 对应的队列配额\n       // 4.1 首先按照(当前时间待分配的容量 - 已有的容量)从小到大(从负到正)的方式排序一次调整 capacity。\n       // 这种排序方式用以避免分配过程中瞬时容量超出 100%的 capacity(猜测可能是为了减少干扰并发预订和抢占介入)\n      List<ReservationAllocation> sortedAllocations =\n          sortByDelta(\n              new ArrayList<ReservationAllocation>(currentReservations), now,\n              plan);\n      // 4.2 为每个队列设置或更新 capacity\n      for (ReservationAllocation res : sortedAllocations) {\n        String currResId = res.getReservationId().toString();\n        if (curReservationNames.contains(currResId)) {\n          // 每个reservationId 对应着常规调度器中 PlanQueue 下的一个 ReservationQueue\n          addReservationQueue(planQueueName, planQueue, currResId);\n        }\n        // 当前生效的 ResourceAllocation 需要(或RS已分配给Reservation)的 capacity\n        Resource capToAssign = res.getResourcesAtTime(now);\n        float targetCapacity = 0f;\n        if (planResources.getMemory() > 0\n            && planResources.getVirtualCores() > 0) {\n          // 绝对值式的容量相对 PlanQueue capacity 计算相对容量，capToAssign/planResources\n          targetCapacity =\n              calculateReservationToPlanRatio(clusterResources,\n                  planResources,\n                  capToAssign);\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\n              \"Assigning capacity of {} to queue {} with target capacity {}\",\n              capToAssign, currResId, targetCapacity);\n        }\n        // set maxCapacity to 100% unless the job requires gang, in which\n        // case we stick to capacity (as running early/before is likely a\n        // waste of resources)\n        // 设置最大配额。感觉是无效代码，不明白为什么要设置最大配额 都为1不行吗，为什么按有无并发来设置最大配额  \n        float maxCapacity = 1.0f;\n        if (res.containsGangs()) {\n          maxCapacity = targetCapacity;\n        }\n        try {\n         // 设置capacity 和 maxCapacity\n          setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);\n        } catch (YarnException e) {\n          LOG.warn(\"Exception while trying to size reservation for plan: {}\",\n              currResId, planQueueName, e);\n        }\n        totalAssignedCapacity += targetCapacity;\n      }\n    }\n    // 第五步: 设置 default queue 的配额(PlanQueue 的剩余配额)\n    float defQCap = 1.0f - totalAssignedCapacity;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PlanFollowerEditPolicyTask: total Plan Capacity: {} \"\n          + \"currReservation: {} default-queue capacity: {}\", planResources,\n          numRes, defQCap);\n    }\n    try {\n      setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);\n    } catch (YarnException e) {\n      LOG.warn(\n          \"Exception while trying to reclaim default queue capacity for plan: {}\",\n          planQueueName, e);\n    }\n    // 第六步: 清理过期的预订记录(now-endTime>24h)\n    // garbage collect finished reservations from plan\n    try {\n      plan.archiveCompletedReservations(now);\n    } catch (PlanningException e) {\n      LOG.error(\"Exception in archiving completed reservations: \", e);\n    }\n    LOG.info(\"Finished iteration of plan follower edit policy for plan: \"\n        + planQueueName);\n\n    // Extension: update plan with app states,\n    // useful to support smart replanning\n  }\n```\n\n先看 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)  \n可用于兼容集群/队列资源骤减的情况，策略是删除最晚接受的部分预约。  \n[SimpleCapacityReplanner.java] \n\n```\npublic void plan(Plan plan, List<ReservationDefinition> contracts)\n      throws PlanningException {\n    if (contracts != null) {\n      throw new RuntimeException(\n          \"SimpleCapacityReplanner cannot handle new reservation contracts\");\n    }\n    ResourceCalculator resCalc = plan.getResourceCalculator();\n    Resource totCap = plan.getTotalCapacity();\n    long now = clock.getTime();\n    // lengthOfCheckZone=1h \n    for (long t = now; (t < plan.getLastEndTime() && t < (now + lengthOfCheckZone)); t +=\n        plan.getStep()) {\n        // 使用 Plan 所有预订的总资源量 - Plan 的 capacity 来检查是否超限\n      Resource excessCap =\n          Resources.subtract(plan.getTotalCommittedResources(t), totCap);\n      if (Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE)) {\n         // 按照 ReservationAllocation.acceptedAt 做比较，最新接受的在前\n        Set<ReservationAllocation> curReservations =\n            new TreeSet<ReservationAllocation>(plan.getReservationsAtTime(t));\n        for (Iterator<ReservationAllocation> resIter =\n            curReservations.iterator(); resIter.hasNext()\n            && Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE);) {\n          ReservationAllocation reservation = resIter.next();\n          // 使用 deleteReservation 来调用 Plan.removeReservation 来删除预订。\n          // removeReservation 的源码分析 见\"step 3\"\n          plan.deleteReservation(reservation.getReservationId());\n          excessCap =\n              Resources.subtract(excessCap, reservation.getResourcesAtTime(t));\n          LOG.info(\"Removing reservation \" + reservation.getReservationId()\n              + \" to repair physical-resource constraints in the plan: \"\n              + plan.getQueueName());\n        }\n      }\n    }\n  }\n```\n\n第二步略，第三步: 清除过期预约。  \n[AbstractSchedulerPlanFollower.java]  \n\n```\nprotected void cleanupExpiredQueues(String planQueueName,\n      boolean shouldMove, Set<String> toRemove, String defReservationQueue) {\n    for (String expiredReservationId : toRemove) {\n      try {\n        String expiredReservation = getReservationQueueName(planQueueName,\n            expiredReservationId);\n         // 将过期预约对应的 ReservationQueue 的 capacity 和 maxCapacity 设置为 0 \n        setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);\n        // 默认配置为 true。将其中的 APP 移动到同级的 default 队列中(xxx-default)。\n        if (shouldMove) {\n          moveAppsInQueueSync(expiredReservation, defReservationQueue);\n        }\n        if (scheduler.getAppsInQueue(expiredReservation).size() > 0) {\n        // 移走所有已有 APP 的情况,还有新提交的。暂时保留\n          scheduler.killAllAppsInQueue(expiredReservation);\n          LOG.info(\"Killing applications in queue: {}\", expiredReservation);\n        } else {\n        // 删除队列  \n          scheduler.removeQueue(expiredReservation);\n          LOG.info(\"Queue: \" + expiredReservation + \" removed\");\n        }\n      } catch (YarnException e) {\n        LOG.warn(\"Exception while trying to expire reservation: {}\",\n            expiredReservationId, e);\n      }\n    }\n  }\n```\n第四步 先调用 sortByDelta 方法使用 ReservationAllocationComparator 对所有正在生效的预订对应的 ReservationQueue 做排序，然后对所有预订通过 setQueueEntitlement 设置或修正 capacity 和 maxCapacity。   \n第五步 通过 setQueueEntitlement 设置 default queue 的 capacity 和 maxCapacity，占有所有剩余资源  \n[ReservationAllocationComparator.java]  \n\n```\n    @Override\n    public int compare(ReservationAllocation lhs, ReservationAllocation rhs) {\n      Resource lhsRes = getUnallocatedReservedResources(lhs);\n      Resource rhsRes = getUnallocatedReservedResources(rhs);\n      // 两个 Resource 先比内存再比虚拟核，小的在前。\n      return lhsRes.compareTo(rhsRes);\n    }\n    \nprivate Resource getUnallocatedReservedResources(\n        ReservationAllocation reservation) {\n      Resource resResource;\n      // 查询常规调度器，获取现有 capacity\n      Resource reservationResource = planFollower\n          .getReservationQueueResourceIfExists\n              (plan, reservation.getReservationId());\n      if (reservationResource != null) {\n        resResource =\n            Resources.subtract(\n            // 当前时刻需要的 capacity。可能为负\n                reservation.getResourcesAtTime(now),\n                reservationResource);\n      } else {\n        resResource = reservation.getResourcesAtTime(now);\n      }\n      return resResource;\n    }\n```\n\n[AbstractSchedulerPlanFollower.java]  \n\n```\nprotected void setQueueEntitlement(String planQueueName, String currResId,\n      float targetCapacity,\n      float maxCapacity) throws YarnException {\n    String reservationQueueName = getReservationQueueName(planQueueName,\n        currResId);\n    scheduler.setEntitlement(reservationQueueName, new QueueEntitlement(\n        targetCapacity, maxCapacity));\n  }\n```\n[CapacityScheduler.java]\n\n```\npublic synchronized void setEntitlement(String inQueue,\n      QueueEntitlement entitlement) throws SchedulerDynamicEditException,\n      YarnException {\n    LeafQueue queue = getAndCheckLeafQueue(inQueue);\n    ParentQueue parent = (ParentQueue) queue.getParent();\n    if (!(queue instanceof ReservationQueue)) {\n      throw new SchedulerDynamicEditException(\"Entitlement can not be\"\n          + \" modified dynamically since queue \" + inQueue\n          + \" is not a ReservationQueue\");\n    }\n    if (!(parent instanceof PlanQueue)) {\n      throw new SchedulerDynamicEditException(\"The parent of ReservationQueue \"\n          + inQueue + \" must be an PlanQueue\");\n    }\n    ReservationQueue newQueue = (ReservationQueue) queue;\n    float sumChilds = ((PlanQueue) parent).sumOfChildCapacities();\n    // 计算修改后, 所有同级队列的容量之和\n    float newChildCap = sumChilds - queue.getCapacity() + entitlement.getCapacity();\n\n    if (newChildCap >= 0 && newChildCap < 1.0f + CSQueueUtils.EPSILON) {\n      // 设置前后的 capacity 和 maxCapacity 都一样则 不设置\n      if (Math.abs(entitlement.getCapacity() - queue.getCapacity()) == 0\n          && Math.abs(entitlement.getMaxCapacity() - queue.getMaximumCapacity()) == 0) {\n        return;\n      }\n      //设置 ReservationQueue 的 capacity 和 maxCapacity \n      newQueue.setEntitlement(entitlement);\n    } else {\n      throw new SchedulerDynamicEditException(\n          \"Sum of child queues would exceed 100% for PlanQueue: \"\n              + parent.getQueueName());\n    }\n    LOG.info(\"Set entitlement for ReservationQueue \" + inQueue + \"  to \"\n        + queue.getCapacity() + \" request was (\" + entitlement.getCapacity() + \")\");\n  }\n```\n[ReservationQueue.java]\n\n```\npublic synchronized void setEntitlement(QueueEntitlement entitlement)\n      throws SchedulerDynamicEditException {\n    float capacity = entitlement.getCapacity();\n    if (capacity < 0 || capacity > 1.0f) {\n      throw new SchedulerDynamicEditException(\n          \"Capacity demand is not in the [0,1] range: \" + capacity);\n    }\n    setCapacity(capacity);\n    setAbsoluteCapacity(getParent().getAbsoluteCapacity() * getCapacity());\n    setMaxCapacity(entitlement.getMaxCapacity());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"successfully changed to \" + capacity + \" for queue \"\n          + this.getQueueName());\n    }\n  }\n```\n\n第六步: 清理过期的预订记录  \n[InMemoryPlan.java]\n\n```\npublic void archiveCompletedReservations(long tick) {\n    LOG.debug(\"Running archival at time: {}\", tick);\n    List<InMemoryReservationAllocation> expiredReservations =\n        new ArrayList<InMemoryReservationAllocation>();\n    readLock.lock();\n    try {\n    // 删除过期的保留记录  \n      long archivalTime = tick - policy.getValidWindow();\n      ReservationInterval searchInterval =\n          new ReservationInterval(archivalTime, archivalTime);\n      SortedMap<ReservationInterval, Set<InMemoryReservationAllocation>> reservations =\n          currentReservations.headMap(searchInterval, true);\n      if (!reservations.isEmpty()) {\n        for (Set<InMemoryReservationAllocation> reservationEntries : reservations\n            .values()) {\n          for (InMemoryReservationAllocation reservation : reservationEntries) {\n            // 结束时间早于当前时间24h \n            if (reservation.getEndTime() <= archivalTime) {\n              expiredReservations.add(reservation);\n            }\n          }\n        }\n      }\n    } finally {\n      readLock.unlock();\n    }\n    if (expiredReservations.isEmpty()) {\n      return;\n    }\n    writeLock.lock();\n    try {\n      for (InMemoryReservationAllocation expiredReservation : expiredReservations) {\n      // Plan.removeReservation 源码分析见\"step 3\",主要是内存数据结构的维护\n        removeReservation(expiredReservation);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }\n```\n　　　　\n### step 6\n\n用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue)   \n\n参见 ApplicationSubmissionContext 的数据结构  \n\n### step 7\n\n常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   \n\n```\npublic void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    ......\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;\n      // 解析 APP 提交的队列\n      String queueName =\n          resolveReservationQueueName(appAddedEvent.getQueue(),\n              appAddedEvent.getApplicationId(),\n              appAddedEvent.getReservationID());\n      if (queueName != null) {\n        if (!appAddedEvent.getIsAppRecovering()) {\n          addApplication(appAddedEvent.getApplicationId(), queueName,\n              appAddedEvent.getUser());\n        } else {\n          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,\n              appAddedEvent.getUser());\n        }\n      }\n    }\n    break;\n    ......\n    }\n    }\n    \nprivate synchronized String resolveReservationQueueName(String queueName,\n      ApplicationId applicationId, ReservationId reservationID) {\n    CSQueue queue = getQueue(queueName);\n    if ((queue == null) || !(queue instanceof PlanQueue)) {\n      return queueName;\n    }\n    if (reservationID != null) {\n      String resQName = reservationID.toString();\n      // 根据 reservationId 获取对应的叶子队列\n      queue = getQueue(resQName);\n      if (queue == null) {\n        String message =\n            \"Application \"\n                + applicationId\n                + \" submitted to a reservation which is not yet currently active: \"\n                + resQName;\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId,\n                RMAppEventType.APP_REJECTED, message));\n        return null;\n      }\n      if (!queue.getParent().getQueueName().equals(queueName)) {\n        String message =\n            \"Application: \" + applicationId + \" submitted to a reservation \"\n                + resQName + \" which does not belong to the specified queue: \"\n                + queueName;\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId,\n                RMAppEventType.APP_REJECTED, message));\n        return null;\n      }\n     // 使用 ReservationId 对应的 ReservationQueue 来运行任务, 调度容器....\n      queueName = resQName;\n    } else {\n      // 如果指定了 PlanQueue 但是没指定 reservationId, 会提交到 PlanQueue 的 default 队列执行\n      queueName = queueName + ReservationConstants.DEFAULT_QUEUE_SUFFIX;\n    }\n    return queueName;\n  }\n```\n\n### step 8\n\n预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用     \n参考\"step 5\"中 AbstractSchedulerPlanFollower#synchronizePlan 方法中对 plan.getReplanner().plan(plan, null) 的调用和cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue)的逻辑  \n        \n官网解释可能不实，原文如下:  \nThe system includes mechanisms to adapt to drop in cluster capacity. This consists in replanning by “moving” the reservation if possible, or rejecting the smallest amount of previously accepted reservation (to ensure that other reservation will receive their full amount).\n\n但在 hadoop-2.7.3 和 hadoop-3.2.0的 trunk 分支上 AbstractSchedulerPlanFollower/Planner(只有 SimpleReplanner 实现类)逻辑没有改动，且无相应逻辑  \n\n\n## QA\n>\n1. Q: reservable queue 和 leaf queue 区别   \n    A: 其一，只有 leaf queue 才能被设置为 reservable queue，通过对 leaf queue 设置 yarn.scheduler.capacity.\\<queue-path\\>.reservable=true。其二，在资源管理的层级体系中 reservable queue 对应的数据结构是 PlanQueue extend ParentQueue。即实际上，reservable queue 是在 leaf queue上做了配置的 ParentQueue。  \n2. Q: PlanQueue 和 ReservationQueue 区别  \n    A: PlanQueue 对应一个 reservable queue，ReservationQueue 对应一个 ReservationId，PlanQueue 下面可以创建任意个 ReservationQueue; PlanQueue 是 ParentQueue，ReservationQueue 是 LeafQueue。      \n3. Q: 在名称为 X 的 reservable queue 上预订了资源，reservationId=reservationId_001,提交APP 时 Queue 名字填什么？  \n    A: Queue 名字填 X,且需要在 ApplicationSubmissionContext 中设置 reservationId。常规调度器会根据 reservationId 解析出对应的 ReservationQueue(名称与 ReservationId 相同)。  \n4. Q: 在1:00-2:00 预约了<100G,10core>的资源 30 分钟，但是在 1:00前或者 2:00后提交任务 会怎样?   \n   A:  常规调度器会拒绝任务(APP_REJECT)。因为找不到 reservationId 对应的 ReservationQueue,早于1:00时队列还没创建,晚于2:00时队列被删除。最好在1:00提交，可能会有等待时间(RS 调度时从deadline 向 arrival 尝试分配)       \n5. Q: 任务运行在哪里?  \n   A: 任务运行在与 reservationId 同名的 ReservationQueue 中,如果预约时间过去但是 APP 还没运行完成则默认移动 APP 到与 ReservationQueue 同级的 default 队列中  \n6. Q: 能不能把 reservable queue 当做 leaf queue 用? 每个任务都不指定 ReservationId。  \n    A: 可以。提交到 reservable queue 但是不指定 ReservationId 的任务都会被移动到 reservable queue 下的 default 队列运行。\n7. Q: 提交到 reservable queue 中的 app 有的指定了 ReservationId,有的没指定。运行时有什么影响?  \n    A: PlanQueue 优先为有 ReservationId 队列分配资源,其次再为 default 队列分配资源，即只能使用那些被预订后剩余的资源，在资源紧张时 default 队列无资源或被其他任意 ReservationQueue 抢占。(这是由 ReservationSystem 提供预订的逻辑决定的。1.不预约的资源不在ReservationSystem 管理下,请求预约时 ReservationSystem 会认为自身管理 PlanQueue 所有资源都能预约出去  2.为所有预约分配完资源之后剩下的才归属 default 队列所有)  \n8. Q: Spark On Yarn 如何使用 ReservationSystem?  \n    A: 分两个阶段。第一,使用 Client-RM 协议提交 ReservationSubmissionRequest,预订资源并保存回执的 ReservationId。第二，在 Spark On Yarn 程序中指定 ReservationId 字段，但是此阶段 Spark On Yarn 提交程序尚不完善需要修改源代码。目前 Spark On Yarn 程序使用 createApplicationSubmissionContext 方法来封装 ApplicationSubmissionContext 时, 没有针对 ReservationId 的逻辑。如果有需求的话，可以通过 SparkConf 来传递 ReservationId 字符串,在createApplicationSubmissionContext做解析设置 。    \n 9. Q: ReservationSystem 的适用场景  \n     A: 1.资源紧张时需要保证重要生产任务运行,可以使用预订资源的方式 。ReservationSystem 在 PlanQueue 下创建 ReservationQueue，将预订需要的绝对值资源量转化 ReservationQueue 在当前时刻的capacity 和 maxCapacity，将任务在此队列中调度，本质上还是 LeafQueue。绝对资源量转化成百分比资源量擦掉了容器数量和大小，不修改开源抢占调度逻辑 不密集抢占抢占的情况下，不能解决大容器调度的问题。  \n\n\n## 配置预订系统\n目前可以在 yarn-site.xml 中配置开启ReservationSystem,可以在CapacityScheduler 及 FairScheduler中增加对预订的支持，方式是在capacity-scheduler.xml 或者 fair-scheduler.xml中的任何 LeafQueue的\"reservable\"属性标记为 true，然后该队列的配额就可以用于预订。即使没有预订资源，应用程序仍然可以被提交到该队列上，它们将以\"best-effort\"的模式运行在 预订资源中运行的作业 剩下的容量中。  \n\n## 参考\nhttps://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html  \nhttps://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us  \nhttp://gitbook.net/java/util/java_util_treemap.html  \n\n\n","source":"_posts/2019-10-28-Yarn ReservationSystem.md","raw":"---\nlayout:     post\ntitle:      Yarn ReservationSystem\nsubtitle:   Yarn 预订系统, Yarn 预约系统, Yarn 保留系统  \ndate:       2019-10-28 \nauthor:     jiulongzhu\nheader-img: img/moon_night.jpg\ncatalog: true\ntags:\n    - Yarn 2.7.3\n    - 源码解析\n---\n\n\n## 概述\n\nReservationSystem 是 YARN ResourceManager 的组件。YARN 的 ReservationSystem 为用户提供了提前保留资源的能力,以确保重要的生产作业可预测地运行。ReservationSystem 执行仔细的准入控制，并保证绝对资源量(而不是群集大小的百分比)。保留具有组和并发的语义，并且可以有随时间变化的资源需求。  \n\n<!-- more -->\n\n## 工作流程  \n\n![](/img/pictures/reservation/yarn_reservation_system.png)  \n\n参考上图，预订资源的工作流程如下：  \n\n* step 0: 用户提交预订创建请求，并收到包含 ReservationId 的响应。  \n* step 1: 用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。这描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求。这可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败\n* step 2: ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。  \n* step 3: SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  \n* step 4: 成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据    \n* step 5: PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序 \n* step 6: 用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue) \n* step 7: 常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   \n* step 8: 预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用以重建计划  \n\n注: step 8和官网解释不同。官网解释可能不实,参见源码解析-\"step 8\"\n\n## 源码解析\n\n### 涉及的类\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem  \n该类继承了AbstractService,实现了ReservationSystem 并封装了Capacity/Fair 调度器下预订系统的核心实现  \n主要功能是:作为服务启动时加载配置文件中配置的 reservable LeafQueue 转换为 Plan;管理 PlanFollower,确保 Plan与常规调度器的同步  \n\n\t| 方法  | 功能  | 备注 |\n\t|:------------- |:---------------:|:-------------|\n\t| setRMContext     | 保存 rmContext 指针 |    |\n\t| reinitialize     | 重新初始化 ReservationSystem |    |\n\t| getPlan     | 获取已被加载的 Plan |    |\n\t| getAllPlans    | 获取已被加载的所有 Plan  |    |\n\t| synchronizePlan     | 使用 PlanFollower 同步 Plan 与常规调度器 |    |\n\t| getPlanFollowerTimeStep     | PlanFollower 步长 |    |\n\t| getNewReservationId     | 获取一个全局唯一的reservationId |    |\n\t| getQueueForReservation     | 获取reservationId关联的队列 |    |\n\t| setQueueForReservation     | 为reservationId关联队列 |    |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower  \n该类实现了 PlanFollower与 Runnable 接口  \n主要功能是:周期性同步常规调度器与 Plan。通过将计划中的每个预订的当前资源映射到常规调度器(e.g. 队列的调整能力,设置池权重,调整应用优先级)，来影响调度器的资源分配 进而达到 保证作业与Plan 中预订一致的方式来使用资源。一个关键概念是将预订的绝对值式资源转换为队列的优先级和容量。PlanFollower 也会向 Plan 来同步集群总资源的变化使其作出相应的调整。  \n\n\t| 方法  | 功能  | 备注 |\n\t|:------------- |:---------------|:-------------|\n\t| init     | 通过 SystemClock,ResourceSchduler 和Plans 来初始化PlanFollower |    |\n\t| synchronizePlan     | 同步指定 Plan和常规调度器 | 周期性调用;时间紧迫时同步阻塞调用  |\n\t| setPlans     | 重置 PlanFollower 同步的 Plan 集合 |    |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan   \n该接口实现了 Plan 接口,Plan 接口继承了PlanContext, PlanView, PlanEdit。实现只有 InMemoryPlan    \nPlan 代表着预订系统的核心数据结构,维护着集群资源的工作安排(分配或收回)计划。用户将 ReservationRequest 提交给 RM 之后,RM 委托给 ReservationAgent，ReservationAgent 通过PlanView 接口咨询该 Plan 是否能满足 RDL 时间且资源约束。如果可以分配,则通过 PlanEdit 接口将其存储在该 Plan 中。之后便向用户返回 ReservationId 票据,用户可通过该票据在预订的时间范围使用预订的资源。  \nPlanFollower将会周期性的从 Plan 中读取最新工作安排计划(队列瞬时容量),并同步给常规调度器,进而影响正在运行作业占用的资源。  \n接口中有三类方法:  PlanContext 负责配置信息;PlanView 负责对Plan 状态的只读访问;PlanEdit 负责对 Plan 状态写入访问。  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| getStep     | 获取 Plan 的时间步长 | PlanContext,同 PlanFollower 的时间步长   |\n| getReservationAgent  | 获取 Plan关联的 ReservationAgent   | PlanContext |\n| getReplanner     | 使用 Planner 对象来应对Plan 的资源意外减少 | PlanContext   |\n|getSharingPolicy | SharingPolicy 控制多用户共享计划资源| PlanContext | \n|getReservationById|通过 ReservationId 获取 Reservation 的详细信息|PlanView|\n|getReservationsAtTime|获取指定时间点所有活跃的 Reservation 的详细信息|PlanView|\n|getAllReservations|获取 Plan 中所有预订信息|PlanView|\n|getTotalCommittedResources| 获取指定时间点所有预定的总资源量| PlanView|\n|getConsumptionForUser|获取指定时间点指定用户预定的总资源量|PlanView|\n|getEarliestStartTime|获取计划中最早的预订开始时间|PlanView|\n|getLastEndTime|获取计划中最晚的预订结束时间|PlanView|\n|addReservation|增加一个预订|PlanEdit|\n|updateReservation|更新一个预订|PlanEdit|\n|deleteReservation|删除一个预订|PlanEdit|\n|archiveCompletedReservations|清除所有的过期预订|PlanEdit|\n\n　　仅列举出核心方法,实际不限于此  \n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.GreedyReservationAgent  \n实现了ReservationAgent接口。 \n一个简单的贪婪放置策略来满足用户预订的代理。具体方式是:按照 ReservationRequests中的各个 ReservationRequest作为单独的阶段,从deadline 开始向后移动至 arrival 来安排预订请求。该代理不考虑本地性,仅仅考虑容器粒度的验证(e.g. 不能超过最大容器大小)  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| createReservation     | 使用此代理尝试创建一个预订 | |\n| updateReservation  | 使用此代理更新一个已有预订   | |\n| deleteReservation     | 使用此代理删除一个已有预订 | |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy  \n主要功能是校验 Plan 能否接受用户预订请求 。实现类 CapacityOverTimePolicy使用容量的按时间拓展概念：策略会保证该用户的当前预订申请和已生效的预订申请资源不超过瞬时资源限制(e.g. reservable queue capacity * 1),且在 24h 时间窗口内不能超过平均资源限制(e.g. reservable queue capacity * 0.5)。从某种意义来说,预订可以使用 reservable queue 的大部分容量 但只要保证快速归还以保证平均资源限制，这可以防止资源滥用且增加了灵活性。通过配置瞬时资源限制和平均资源限制以及时间窗口,策略可以使 reservable queue 达到即时执行(max=100%,avg=100%)和完全灵活(max=?,avg=?,保留给其他用户或系统)的效果。   \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------:|:-------------|\n| init     | 初始化 Policy | 策略必要的配置读取 |\n| validate  | 校验 Plan 能否接受用户预订请求   | |\n| getValidWindow     | 预订资源的过期时间 | 窗口为[-24h,当前)。预订的 deadline 在此之前的记录将被删除|\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.Planner  \n实现类只有SimpleCapacityReplanner,且功能远不及 Planner 的设计(增删改 Plan)，只有删除预订资源的功能：从当前时刻开始直到 min{最后一个预约endtime，一个小时之后}，如果所有用户预订资源总量超过 reservable queue capacity，则删除接受时间较晚的一批预订。 \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| init     | 初始化 Planner |  |\n| plan  | 更新现有的 Plan,或增删改已有预订或增加一个新的预订   | SCR 只支持删除晚的预订以保证预订资源总量不超过队列最低配额 |\n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation  \n预订系统的核心数据结构，是一种运行长度编码(Run Length Encoded)的稀疏数据结构(TreeMap)，可随着时间的推移维护资源分配。维护者各事件点(预订分配 starttime，endtime)时(Plan/当前预订)全局应有的资源总量。  \n\n\t| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| getEarliestStartTime     | 最早的资源分配的时间戳 | treemap  firstKey |\n|getLatestEndTime|最晚的资源分配的时间戳|treemap lastKey |\n|getCapacityAtTime|该时间点的 Plan 已预订资源量|treemap floorKey |\n|addInterval|在treemap 中新增一个时间范围的预约资源记录|维护开始点和结束点的资源量|\n|removeInterval|在 treemap 中删除一个时间范围的预约资源记录|维护开始点和结束点的资源量|\n|addCompositeInterval|在 treemap 中新增一个时间范围的多个预订资源记录| never used |\n\n\n>\n TreeMap 的floorEntry(targetKey) 和 lowerEntry(targetKey)  区别:   \n floorEntry 返回 key 小于等于 targetKey 的键值对,无则 null  \n lowerEntry 返回 key 严格小于 targetKey 的键值对,无则 null  \n\n* org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation\n实现自 ResourceAllocation。预订分配的结果，内存数据结构，包含预订的整体开始结束时间，ReservationSystem 校验(GA 校验资源/SharingPolicy 校验用户违规)通过 ReservationSubmissionRequest 的时间，<时间段，资源量>的分配细节和RLESparseResourceAllocation    \n\n| 方法  | 功能  | 备注 |\n|:------------- |:---------------|:-------------|\n| compareTo     | 按 acceptTime比较,晚的在前| 比较器, Plan 队列资源骤减时,删除部分晚的的 ResourceAllocation |\n\n\n* org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest  \n客户端封装ReservationSubmissionRequest 请求，通过 client-RM 接口提交给 RM，数据结构如下       \n 　　queueName:String    //支持预订资源的队列  \n　　rd:ReservationDefinition //预订定义      \n　　　　arrival:long  //预订开始时间的 最早时间    \n　　　　deadline:long   //预订结束时间的 最晚时间  \n　　　　name:String  //名称  \n　　　　reservationRequests:ReservationRequests  //预订请求    　\n　　　　　　reservationResources:List\\<ReservationRequest\\>  \n　　　　　　　　capability:Capacity  //每个预订请求的资源量    \n　　　　　　　　numContainers:int  //预订的 container 数量  \n　　　　　　　　concurrency:int //并发度。numContainers 可以分批分配        \n　　　　　　　　duration:long  //使用时间  \n　　　　type:ReservationRequestInterpreter  //多个预订请求之间的依赖关系  \n\n*  org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter  \n枚举类型，用以表示多个预订请求之间的依赖关系(或称: 组关系)      \n\n\t| 类型  | 含义  | 场景 |\n|:------------- |:---------------|:-------------|\n|R_ANY| 仅满足 RDL 中一个预订请求即可|有多种等效的方式满足要求。e.g. 1个<4G,2core>或2个<2G,1core>。Agent 会决定使用最合适的ReservationRequest|\n|R_ALL|需要满足 RDL中所有预订请求,请求的分配没有限制时间先后|事务|\n|R_ORDER|需要满足 RDL 中所有预订请求,且有严格的时间限制。k 位置的分配时间段必须在 k+1位置分配时间段之前(无交集)，且 k 位置分配的结束时间和 k+1位置分配的开始时间可以有任意长的时间间隔|具有固定依赖的阶段性工作流。e.g. 第一个作业需要 1个<4G,2core> 5min，其输出作为第二个作业的输入，第二个作业需要 2 个<2G,1core> 10min，则两个作业预订的分配时间段必然不能重叠|\n|R\\_ORDER\\_NO\\_GAP|R_ORDER 的严格版本，要求 k 位置分配的结束时间和 k+1 位置分配的开始时间相同、不能有任何间隙,即 \"zero-size gap\",\"no\\_gap\"|1.当前一个作业输出规模比较大时,避免保留太长时间 2.实时性要求很高的场景,作业间时间差则增大了工作流累计延时|\t\n\n* org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionResponse  \n客户端提交 ReservationSubmissionRequest 请求后，若RM 校验通过可以分配预订，则返回 reservationId给客户端。数据结构如下  \n　　reservationId:ReservationId  \n　　　　clusterTimestamp:long    \n　　　　id:long   \n\t\t\n### step 0   \n用户提交预订创建请求，并收到包含 ReservationId 的响应  \n\n[ApplicationClientProtocol.java]\n\n```\npublic ReservationSubmissionResponse submitReservation(\n      ReservationSubmissionRequest request) throws YarnException, IOException\n```\n### step 1  \n用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求,可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败  \n以 TestCase 中的代码片段为例  \n\n[TestClientRMService.java]\n\n```\n@Test\npublic void testReservationAPIs() {\n\t....\n    //创建一个预订请求\n    Clock clock = new UTCClock();\n    //预订最早开始时间\n    long arrival = clock.getTime();\t   \n    //使用时长 \n    long duration = 60000;    \n    //预订的最晚结束时间\n    long deadline = (long) (arrival + 1.05 * duration);\n    //RDL: Rervation Define Language \n    //请求 4 个 <1G,1core> container,使用时长是 6000ms,使用开始的最早时间是 arrival,使用结束的最晚内时间是 deadline。\n    ReservationSubmissionRequest sRequest =\n        createSimpleReservationRequest(4, arrival, deadline, duration);\n    ReservationSubmissionResponse sResponse = null;\n    try {\n      sResponse = clientService.submitReservation(sRequest);\n    } catch (Exception e) {\n      Assert.fail(e.getMessage());\n    }\n    ....\n}\n\nprivate ReservationSubmissionRequest createSimpleReservationRequest(\n      int numContainers, long arrival, long deadline, long duration) {\n    ReservationRequest r =\n        ReservationRequest.newInstance(Resource.newInstance(1024, 1),\n            numContainers, 1, duration);\n            //R_ALL 满足所有请求\n    ReservationRequests reqs =\n        ReservationRequests.newInstance(Collections.singletonList(r),\n            ReservationRequestInterpreter.R_ALL);\n    ReservationDefinition rDef =\n        ReservationDefinition.newInstance(arrival, deadline, reqs,\n            \"testClientRMService#reservation\");\n    ReservationSubmissionRequest request =\n        ReservationSubmissionRequest.newInstance(rDef,\n            ReservationSystemTestUtil.reservationQ);\n    return request;\n  }\n```\n### step 2 \nReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。   \n\n[ClientRMService.java]\n\n```\n@Override\n  public ReservationSubmissionResponse submitReservation(\n      ReservationSubmissionRequest request) throws YarnException, IOException {\n    // 检查 ReservactionSystem 是否启用\n    checkReservationSytem(AuditConstants.SUBMIT_RESERVATION_REQUEST);\n    ReservationSubmissionResponse response =\n        recordFactory.newRecordInstance(ReservationSubmissionResponse.class);\n    // 通过 AtmicLong 创建全局唯一的ReservationId\n    ReservationId reservationId = reservationSystem.getNewReservationId();\n    // 第一步:  校验\n    // 1.预订请求指定了 queue; 2.指定的 queue 是 reservable queue,即属于 ReservationSystem 管理  \n    // 2.Reservation Define Language 校验\n    //\t2a. 空值检验  2b. deadline 不能早于当前时间 2c. ReservationRequests 空值及空集判断  \n    //\t2d. 分配所有请求的最短时间(R_ANY,R_ALL:取最大;R_ORDER..取和)不能超过 deadline-arrival \n    //\t2e. 分配所有请求的最大资源量(concurrency * request capacity)不能超过 reservable queue capacity\n    Plan plan =\n        rValidator.validateReservationSubmissionRequest(reservationSystem,\n            request, reservationId);\n    // 校验 ACL\t\n    String queueName = request.getQueue();\n    String user =\n        checkReservationACLs(queueName,\n            AuditConstants.SUBMIT_RESERVATION_REQUEST);\n    try {\n      // 第二步:  使用 ReservationAgent 来尝试放置预订请求\n      boolean result =\n          plan.getReservationAgent().createReservation(reservationId, user,\n              plan, request.getReservationDefinition());\n      if (result) {\n        // 同步 reservationSystem <reservationId,queueName>关系\n        reservationSystem.setQueueForReservation(reservationId, queueName);\n        // create the reservation synchronously if required\n        // 第三步:  如下,若预留的 arrival 时间早于当前(错过) PlanFollower 一个步长,则同步创建\n        refreshScheduler(queueName, request.getReservationDefinition(),\n            reservationId.toString());\n  \t// response 中返回放置预订请求的 reservationId\n        response.setReservationId(reservationId);\n      }\n    } catch (PlanningException e) {\n     ...\n    }\n    ...\n    return response;\n  }\n   private void refreshScheduler(String planName,\n      ReservationDefinition contract, String reservationId) {\n    if ((contract.getArrival() - clock.getTime()) < reservationSystem\n        .getPlanFollowerTimeStep()) {\n      ....\n      //后续再说源码\n      reservationSystem.synchronizePlan(planName);\n      ....\n    }\n```\n在第二步中,RMClientService 委托 Plan(reservable queue 的映射) 绑定的 ReservationAgent 来决定是否放置该预订请求  \nReservationAgent(RA)将预订请求中的每个 ResourceRequest(RR)作为一个单独的 stage 尝试放置,并在放置时考虑 ReservationRequestInterpreter(RRI)组关系的处理。从最后一个 stage 开始向前依次放置，使用 Plan 已有的预订计划累计的数据和当次预订请求的累计中间数据判断能否放置当前 RR来影响当前 RR放置时间策略并存储最终放置策略结果。   \n \n[GreedyReservationAgent.java]\n\n```\nprivate boolean computeAllocation(ReservationId reservationId, String user,\n      Plan plan, ReservationDefinition contract,\n      ReservationAllocation oldReservation) throws PlanningException,\n      ContractValidationException {\n    LOG.info(\"placing the following ReservationRequest: \" + contract);\n    // reservable queue capacity \n    Resource totalCapacity = plan.getTotalCapacity();\n    \n   // 译文:考虑在此增加逻辑来调整\"ResourceDefinition\" 来解决系统的缺陷(e.g. 大型容器的调度延迟) \n   //  TODO: 想不出会在这个位置使用什么样的策略来解决大容器调度延迟问题? 逻辑能转移到常规调度吗\n    \n    // 使用保守策略规整缩短 [arrival,deadline]区间\n    long earliestStart = contract.getArrival();\n    long step = plan.getStep();\n    if (earliestStart % step != 0) {\n      earliestStart = earliestStart + (step - (earliestStart % step));\n    }\n    long deadline =\n        contract.getDeadline() - contract.getDeadline() % plan.getStep();\n        \n    // 将每个 ReservationRequest(RR) 作为一个单独的 stage,设置一些阶段间的临时变量\n    long curDeadline = deadline;\n    long oldDeadline = -1;\n    // 当次预订请求，全部 RR 放置时间策略结果的(中间)数据结构 \n    Map<ReservationInterval, ReservationRequest> allocations =\n        new HashMap<ReservationInterval, ReservationRequest>();\n     // 当次预订请求，全部 RR 放置造成<时间-资源>变动结果的(中间)数据结构  \n    RLESparseResourceAllocation tempAssigned =\n        new RLESparseResourceAllocation(plan.getResourceCalculator(),\n            plan.getMinimumAllocation());\n\n    List<ReservationRequest> stages = contract.getReservationRequests()\n        .getReservationResources();\n    ReservationRequestInterpreter type = contract.getReservationRequests()\n        .getInterpreter();\n    // 使用迭代器 从最后一个元素之后的 null 向前迭代\n    for (ListIterator<ReservationRequest> li = \n        stages.listIterator(stages.size()); li.hasPrevious();) {\n      ReservationRequest currentReservationStage = li.previous();\n       //第一步:  校验 RR\n      // 1.concurrency > 0; 2.numContainer>0; 3. numContainer 必须是 concurrency 的倍数; 4.单容器大小不得超过maximumAllocation\n      validateInput(plan, currentReservationStage, totalCapacity);\n      //第二步:  尝试分配单个 RR\n      Map<ReservationInterval, ReservationRequest> curAlloc =\n          placeSingleStage(plan, tempAssigned, currentReservationStage,\n              earliestStart, curDeadline, oldReservation, totalCapacity);\n\n      if (curAlloc == null) {\n        // 组关系是 R_ALL,R_ORDER,R_NO_GAP 时，一个 RR 分配失败 导致整个事务失败\n        // 组关系是 R_ANY,一个 RR 分配失败可以尝试其他 RR \n        if (type != ReservationRequestInterpreter.R_ANY) {\n          throw new PlanningException(\"The GreedyAgent\"\n              + \" couldn't find a valid allocation for your request\");\n        } else {\n          continue;\n        }\n      } else {\n       // 可以放置当前 RR(stage)\n       \n        allocations.putAll(curAlloc);\n       // 组关系是 R_ANY 的话,可以终止后续 stage 的尝试了\n        if (type == ReservationRequestInterpreter.R_ANY) {\n          break;\n        }\n\n        // 如果组关系是 R_ODER,R_ODER_NO_GAP，则通过设置 curDeadline 来确保 RR 之间的先后关系 \n        // 将下一个 RR 的 deadline 设置为当前 RR 分配中最早的开始时间\n        if (type == ReservationRequestInterpreter.R_ORDER\n            || type == ReservationRequestInterpreter.R_ORDER_NO_GAP) {\n          curDeadline = findEarliestTime(curAlloc.keySet());\n\n          // 对于 R_ORDER_NO_GAP,确认当前分配和前一个(后向)分配之间没有空隙\n          // 空隙并非之前理解的完全相等,[t0,t1) 与 [t1,t2)之间的关系,而是不超过 Plan 的时间步长即可 \n          if (type == ReservationRequestInterpreter.R_ORDER_NO_GAP\n              && oldDeadline > 0) {\n            if (oldDeadline - findLatestTime(curAlloc.keySet()) > plan\n                .getStep()) {\n              throw new PlanningException(\"The GreedyAgent\"\n                  + \" couldn't find a valid allocation for your request\");\n            }\n          }\n          // keep the variable oldDeadline pointing to the last deadline we\n          // found\n          oldDeadline = curDeadline;\n        }\n      }\n    }\n    // 没有为预订请求分配到<时间,资源>,通过 throws exception 的方式来快速失败\n    if (allocations.isEmpty()) {\n      throw new PlanningException(\"The GreedyAgent\"\n          + \" couldn't find a valid allocation for your request\");\n    }\n\n  // 第三步:  创建预订\n  // 在[arrival,earliestTime)之间加入了 \"零填充\",以表明其是从arrival time 开始的预订\n  // TODO:在开头和末尾添加\"零填充\"的意义是什么?没有的话 会有什么问题  \n    ReservationRequest ZERO_RES =\n        ReservationRequest.newInstance(Resource.newInstance(0, 0), 0);\n    long firstStartTime = findEarliestTime(allocations.keySet());\n    if (firstStartTime > earliestStart) {\n      allocations.put(new ReservationInterval(earliestStart,\n          firstStartTime), ZERO_RES);\n      firstStartTime = earliestStart;\n      // 译: 考虑在[lastEndTime,deadline)添加\"零填充\"\n    }\n    // 译:这有待验证,其他代理可能也在放置(同步问题);有代理不知道的 SharingPolicy\n    ReservationAllocation capReservation =\n        new InMemoryReservationAllocation(reservationId, contract, user,\n            plan.getQueueName(), firstStartTime,\n            findLatestTime(allocations.keySet()), allocations,\n            plan.getResourceCalculator(), plan.getMinimumAllocation());\n    // 第四步: 新增或更新已有预订分配 。在 step 3 中解释\n    if (oldReservation != null) {\n      return plan.updateReservation(capReservation);\n    } else {\n      return plan.addReservation(capReservation);\n    }\n  }\n```\n再看上述 第二步:  尝试分配单个 RR    \n这是整个流程中最重要的部分: 决定了用户的每个 RR 能不能放置,放置在哪个时间段,放置几个 container    \n核心思想是: 从 deadline(动态 deadline)开始向后尝试，每次尝试都遍历 duration 时间，判断 duration 时间每个时间步长是否超过了 capacity(即判断 指定大小的资源能不能占用当前时间段，故称\"放置\")。然后向后移动一个时间步长，直至剩余时间不足以满足 duration 或者放置了 RR 中所有 container 结束       \n\n[GreedyReservationAgent.java]  \n\n```\nprivate Map<ReservationInterval, ReservationRequest> placeSingleStage(\n      Plan plan, RLESparseResourceAllocation tempAssigned,\n      ReservationRequest rr, long earliestStart, long curDeadline,\n      ReservationAllocation oldResAllocation, final Resource totalCapacity) {\n\n    Map<ReservationInterval, ReservationRequest> allocationRequests =\n        new HashMap<ReservationInterval, ReservationRequest>();\n\n    Resource gang = Resources.multiply(rr.getCapability(), rr.getConcurrency());\n    long dur = rr.getDuration();\n    long step = plan.getStep();\n    if (dur % step != 0) {\n      dur += (step - (dur % step));\n    }\n    // 将一个并发作为一个原子性的组分配 \n    // 每次分配资源量: gang=capacity * concurrency \n    //     共计分配次数: gangsToPlace= numContainers/concurrency  \n    int gangsToPlace = rr.getNumContainers() / rr.getConcurrency();\n    int maxGang = 0;\n\n    // loop trying to place until we are done, or we are considering\n    // an invalid range of times\n    // 循环尝试分配 直至所有的 gang 原子组都分配完或者剩余时间不足以容纳一个 duration\n    while (gangsToPlace > 0 && curDeadline - dur >= earliestStart) {\n\n      // as we run along we remember how many gangs we can fit, and what\n      // was the most constraining moment in time (we will restart just\n      // after that to place the next batch)\n      maxGang = gangsToPlace;\n      long minPoint = curDeadline;\n      int curMaxGang = maxGang;\n\n      // 尝试在[curDeadline-duration,curDeadline) 长为 duration 的区间内放置未知个 Resource=gang 的原子组\n      for (long t = curDeadline - plan.getStep(); t >= curDeadline - dur\n          && maxGang > 0; t = t - plan.getStep()) {\n\n        // 如果之前的预订存在的话,则删除此预订之前的分配(e.g. 更新预订时)\n        Resource oldResCap = Resource.newInstance(0, 0);\n        if (oldResAllocation != null) {\n          oldResCap = oldResAllocation.getResourcesAtTime(t);\n        }\n\n\t// 计算当前时间点的净可用资源\n        Resource netAvailableRes = Resources.clone(totalCapacity);\n        Resources.addTo(netAvailableRes, oldResCap);\n        // Plan(reservable queue), 最靠近 t 时刻的累计容量\n        Resources.subtractFrom(netAvailableRes,\n            plan.getTotalCommittedResources(t)); \n        //当前预订, 最靠近 t 时刻的累计容量\n        Resources.subtractFrom(netAvailableRes,\n            tempAssigned.getCapacityAtTime(t));\n        \n        //计算当前时刻能满足的最大数量的 gang\n        curMaxGang =\n            (int) Math.floor(Resources.divide(plan.getResourceCalculator(),\n                totalCapacity, netAvailableRes, gang));\n\n        // min{需求的 gang 数量,剩余资源能支撑的 gang 数量}\n        curMaxGang = Math.min(gangsToPlace, curMaxGang);\n\t// 记住当前尝试的 duration 内,最小数量的 gang 点(即 队列+reservation 占用资源量最多的点)，作为下次尝试的右边界\n        if (curMaxGang <= maxGang) {\n          maxGang = curMaxGang;\n          //资源最紧俏的时间点。当资源充裕时,放置尽可能多的 gang,也会成为当前最紧俏的时间点，作为下次放置的右边界    \n          minPoint = t;\n        }\n      }\n\n      // if we were able to place any gang, record this, and decrement\n      // gangsToPlace maxGang=0:没资源了 时间:范围不够了\n      // 退出上一个循环过程有两种情况: \n      // 1.  maxGang=0,这个 duration 内有一个时间点 净剩余资源不足以放下一个 gang,那么整个 duration 都不能用了\n      // 2. t<=curDeadline - dur 整个 duration 已经遍历完了。此时 maxGang > 0\n      // 以上退出都有可能,所以需要中间变量 maxGang来做判断。\n      if (maxGang > 0) {\n      \n      // 能在[curDeadline-duration,curDeadline) 内放置下 maxGang 个gang，即 concurrency * maxGang 个 container\n        gangsToPlace -= maxGang;\n        ReservationInterval reservationInt =\n            new ReservationInterval(curDeadline - dur, curDeadline);\n        ReservationRequest reservationRes =\n            ReservationRequest.newInstance(rr.getCapability(),\n                rr.getConcurrency() * maxGang, rr.getConcurrency(),\n                rr.getDuration());\n        // 记住已占用的空间。后续贴出源码\n        // reservable queue 对应的 Plan 暂时是只读的，除非能将整个 ReservationRequests 放入到计划中。\n        tempAssigned.addInterval(reservationInt, reservationRes);\n        allocationRequests.put(reservationInt, reservationRes);\n      }\n      // 设置下次放置 gang 的右边界。右边界向右无法再安置一个 gang(如上逻辑所述),尝试在右边界向左放置新的 gang\n      curDeadline = minPoint;\n    }\n    \n    if (gangsToPlace == 0) {\n     // 放置了所有的 gang \n      return allocationRequests;\n    } else {\n      // 不能为当前 ReservationRequest(RR) 在时间/容量/并发 限制上放置所有的 gang。\n      // 此次 RR放置失败,如果不是 R_ANY 的话，可以终止整个 预订请求了。\n      for (Map.Entry<ReservationInterval, ReservationRequest> tempAllocation :\n        allocationRequests.entrySet()) {\n        // 清理之前的中间数据。后续贴出源码\n        tempAssigned.removeInterval(tempAllocation.getKey(),\n            tempAllocation.getValue());\n      }\n      return null;\n    }\n  \n```\n\nPlan 中使用 RLESparseResourceAllocation(以 TreeMap 为核心的稀疏数据结构)保存了当前已生效所有Reservation的累积<时间-资源>状态变化图，通过汇总所有 Reservation 的 RR 中每个分配的时间段(startTime,endTime)和资源量 最终得到了 Plan 随时间变化的已分配资源量。对每个 ReservationSubmissionRequest 请求来说 ，在尝试分配所有 RR 的时候，也使用RLESparseResourceAllocation 来保存临时分配好 RR 累积<时间-资源>状态变化图。  \n \n[RLESparseResourceAllocation.java]  \n\n```\nprivate TreeMap<Long, Resource> cumulativeCapacity = new TreeMap<Long, Resource>();\n//向 RLESparseResourcAllocation 中增加一个 <时间范围-容量> 的预订\npublic boolean addInterval(ReservationInterval reservationInterval,\n      ReservationRequest capacity) {\n    Resource totCap =\n        Resources.multiply(capacity.getCapability(),\n            (float) capacity.getNumContainers());\n    if (totCap.equals(ZERO_RESOURCE)) {\n      return true;\n    }\n    writeLock.lock();\n    try {\n      long startKey = reservationInterval.getStartTime();\n      long endKey = reservationInterval.getEndTime();\n      //截取 (?,endKey)之间 一段 TreeMap\n      NavigableMap<Long, Resource> ticks =\n          cumulativeCapacity.headMap(endKey, false);\n      if (ticks != null && !ticks.isEmpty()) {\n        Resource updatedCapacity = Resource.newInstance(0, 0);\n        // 找到时间上小于等于 startKey 的最大 key 的键值对\n        Entry<Long, Resource> lowEntry = ticks.floorEntry(startKey);\n        if (lowEntry == null) {\n\t   // 表明 startKey 即为 RLE 全局最小键(最早)\n          cumulativeCapacity.put(startKey, totCap);\n        } else {\n          updatedCapacity = Resources.add(lowEntry.getValue(), totCap);\n          if ((startKey == lowEntry.getKey())\n              && (isSameAsPrevious(lowEntry.getKey(), updatedCapacity))) {\n             // 通过 remove 达到合并区间的。\n             // e.g. [t0,t1)为 2G, [t1,t2)为 1G;现在在 [t1,t3) 上分配了 1G,那么可以删除 t1 这个点 \n            cumulativeCapacity.remove(lowEntry.getKey());\n          } else {\n             //新增 一个时间点的容量 记录\n            cumulativeCapacity.put(startKey, updatedCapacity);\n          }\n        }\n        // [startKey,endKey)时间段内的所有已有记录都要增加 totCap 作为最终状态 \n        // cumulativeCapacity 维护的是全局状态量,而不是增量或者单个 reservation 的数据\n        Set<Entry<Long, Resource>> overlapSet =\n            ticks.tailMap(startKey, false).entrySet(); //startKey 已经增加过了 \n        for (Entry<Long, Resource> entry : overlapSet) {\n          updatedCapacity = Resources.add(entry.getValue(), totCap);\n          entry.setValue(updatedCapacity);\n        }\n      } else {\n        cumulativeCapacity.put(startKey, totCap);\n      }\n      Resource nextTick = cumulativeCapacity.get(endKey);\n      if (nextTick != null) {\n        if (isSameAsPrevious(endKey, nextTick)) {\n        // 合并 endKey\n          cumulativeCapacity.remove(endKey);\n        }\n      } else {\n\t// endKey 时还回资源,所以在 endKey 前面记录都加了 totCap,此处会减去  \n        cumulativeCapacity.put(endKey, Resources.subtract(cumulativeCapacity\n            .floorEntry(endKey).getValue(), totCap));\n      }\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  }\n  // 在 RLESparseResourcAllocation 中删除一个 <时间范围-容量> 的预订\n  public boolean removeInterval(ReservationInterval reservationInterval,\n      ReservationRequest capacity) {\n    Resource totCap =\n        Resources.multiply(capacity.getCapability(),\n            (float) capacity.getNumContainers());\n    if (totCap.equals(ZERO_RESOURCE)) {\n      return true;\n    }\n    writeLock.lock();\n    try {\n      long startKey = reservationInterval.getStartTime();\n      long endKey = reservationInterval.getEndTime();\n      NavigableMap<Long, Resource> ticks =\n          cumulativeCapacity.headMap(endKey, false);\n      //取出[startKey,endKey)区间内的所有记录\n      SortedMap<Long, Resource> overlapSet = ticks.tailMap(startKey);\n      if (overlapSet != null && !overlapSet.isEmpty()) {\n        Resource updatedCapacity = Resource.newInstance(0, 0);\n        long currentKey = -1;\n        for (Iterator<Entry<Long, Resource>> overlapEntries =\n            overlapSet.entrySet().iterator(); overlapEntries.hasNext();) {\n          Entry<Long, Resource> entry = overlapEntries.next();\n          currentKey = entry.getKey();\n          // 在每个时间点的减去该预订已分配的资源量  \n          updatedCapacity = Resources.subtract(entry.getValue(), totCap);\n          cumulativeCapacity.put(currentKey, updatedCapacity);\n        }\n        // 左右边界点存在性判断\n        Long firstKey = overlapSet.firstKey();\n        if (isSameAsPrevious(firstKey, overlapSet.get(firstKey))) {\n          cumulativeCapacity.remove(firstKey);\n        }\n        if ((currentKey != -1) && (isSameAsNext(currentKey, updatedCapacity))) {\n          cumulativeCapacity.remove(cumulativeCapacity.higherKey(currentKey));\n        }\n      }\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  }\n```\n\n举个简单例子，为便于讨论:  \n1.容量方面:  reservable queue 有 <2G,2core>的 capacity，所有预订请求中 container 容量都是<1G,1core>，即 queue 最多有 2 个container  \n2.时间方面: 从 t0至 t5，时间单位是 PlanFollower 的时间步长 step(默认是 1s)。t0 为当前时刻    \n3.请求方面: 已有的 reservation 不讨论 arrival,deadline，只给出 ReservationSystem 给出的分配<时间范围-容量>  \n4.ReservationSystem 已有的预订如下  \n\n| 预订 Id  | 开始时间  | 结束时间 | 占用时间 | 总资源量|\n|:------------- |:---------------|:-------------|:-------------|:-------------|\n|r0|t3|t4| 1 个单位| 1个 container|\n|r1|t2|t3| 1个单位|2个 container|\n|r2|t1|t2| 1个单位| 1个 container|\n\n由此可以绘出 Plan 对应的 RLESparseResourceAllocation 中维护的累计<时间-资源>状态变化图  \n\n![](/img/pictures/reservation/reservationSystem.png)  \n\n即: r2 在 t1 时刻拿到1 个 container,因此图中 t1 时刻已分配资源状态是 1 个 container。  \n      r2 在 t2 时刻释放 1 个 container,r1 拿到两个 2 个 container,因此 t2 时刻已分配资源状态是 2 个 container \n \n 假设 ReservationSystem接收到一个请求，ReservationDefinition定义的预订请求内容为：\narrival=t0,deadline=t5  \nReservationRequests(RRS) 中有一个 ReservationRequest(RR)   \n　　capacity<1G,1core>,numContainer=2,concurrency=1,duration=2     \nReservationRequestInterpreter=R_ALL\n\n来模拟一下GreedyRerservationAgent#placeSingleStage是如何尝试放置这一个 RR 的  \n初始值:gang=concurrency * capacity=<1G,1core>  gangsToPlace=numContainers/concurrency=2  maxGang=2  \n \n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|2| t5| 2 | t4| 2-0-0=2 contianer|2|2| t4|是|\n|第二次次循环|2| t4| 2 | t3| 2-1-0=1 contianer|1|1| t3|否,duration 超限退出|\n\n内层循环结束时，maxGang=1，即可以在[t3,t5)这个时间段放下 1 个 gang，在此为 1 个 container，外层循环将此分配封装成<ReservationInterval,ReservationRequest>加入到 当前预订累计分配的中间数据结构中(tmpAssigned:RLESparseResourceAllocation)。  \n\n此时: gangsToPlace=1,curDeadline=t3,进行下一次外层循环  \n\n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|1| t3| 1 | t2| 2-2-0=0 contianer|0|0| t2|否,maxGang=0退出|\n\n内层循环结束后，maxGang=0，不能在[t1,t3)时间段放置任何一个 gang。  \n\n此时: gangsToPlace=1,curDeadline=t2，进行下一次外层循环  \n\n| 状态  | 循环开始前 maxGang  | 开始前 minPoint|开始前 curMaxGang| 循环变量 t|净可用资源(队列容量-Plan 累计-当前预订累计)|最终curMaxGang| 最终 maxGang|最终 minPoint| 是否进行下次循环|\n|:------------- |:---------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n|第一次循环|1| t2| 1 | t1| 2-1-0=1 contianer|1|1| t1|是|\n|第二次循环|1| t1| 1 | t0| 2-0-0=2 contianer|1|1| t0|否,duration 超限退出|  \n\n内层循环结束后，maxGang=1，即可以在[t0,t2)时间段放下一个 gang，即 1 个 container，外层循环将此分配封装好之后再次加入到 tmpAssigned 中。   \n\n此时: gangsToPlace=0终止了外层循环。\n得到的最终结论是: 在[t3,t5)时间段放下一个 container<1G,1core>,在[t0,t2)时间段放下一个 container<1G,1core>。该 RR 整体可以分配，满足组关系 R_ALL，即不考虑用户的情况可以分配该 RR。\n\n### step 3 \n\nSharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  \n\n在 step 2中，ReservationSystem 尝试对预订请求分配时间和资源。若分配成功，则尝试将其加入到 Plan 中或是更新 Plan。    \n\n[GreedyReservationAgent.java] \n\n```\nprivate boolean computeAllocation(ReservationId reservationId, String user,\n      Plan plan, ReservationDefinition contract,\n      ReservationAllocation oldReservation) throws PlanningException,\n      ContractValidationException {\n    LOG.info(\"placing the following ReservationRequest: \" + contract);\n    ......\n    ReservationAllocation capReservation =\n        new InMemoryReservationAllocation(reservationId, contract, user,\n            plan.getQueueName(), firstStartTime,\n            findLatestTime(allocations.keySet()), allocations,\n            plan.getResourceCalculator(), plan.getMinimumAllocation());\n    if (oldReservation != null) {\n     // 更新\n      return plan.updateReservation(capReservation);\n    } else {\n    // 新增\n      return plan.addReservation(capReservation);\n    }\n    }\n```\nupdateReservation 事务性更新 reservation:  先删除旧的reservation，再新增更新的 reservation(当次ReservationSubmissionRequest)。如果新增失败则再把旧的reservation 回滚。 \n所以两处的逻辑的核心在于 addReservation 和 removeReservation。  \n\n[InMemoryPlan.java]  \n\n```\n    //按时间段索引 ReservationAllocation 信息  \n private TreeMap<ReservationInterval, Set<InMemoryReservationAllocation>> currentReservations =\n      new TreeMap<ReservationInterval, Set<InMemoryReservationAllocation>>();\n   // Plan 整体的<时间-资源>状态变化图\n  private RLESparseResourceAllocation rleSparseVector;\n  // 细分用户的<时间-资源>状态变化图\n  private Map<String, RLESparseResourceAllocation> userResourceAlloc =\n      new HashMap<String, RLESparseResourceAllocation>();\n  // 按 reservationId 索引ReservationAllocation 信息\n  private Map<ReservationId, InMemoryReservationAllocation> reservationTable =\n      new HashMap<ReservationId, InMemoryReservationAllocation>();\n      \n public boolean addReservation(ReservationAllocation reservation)\n      throws PlanningException {\n    InMemoryReservationAllocation inMemReservation =\n        (InMemoryReservationAllocation) reservation;\n    if (inMemReservation.getUser() == null) {\n      String errMsg =\n          \"The specified Reservation with ID \"\n              + inMemReservation.getReservationId()\n              + \" is not mapped to any user\";\n      LOG.error(errMsg);\n      throw new IllegalArgumentException(errMsg);\n    }\n    writeLock.lock();\n    try {\n      if (reservationTable.containsKey(inMemReservation.getReservationId())) {\n        String errMsg =\n            \"The specified Reservation with ID \"\n                + inMemReservation.getReservationId() + \" already exists\";\n        LOG.error(errMsg);\n        throw new IllegalArgumentException(errMsg);\n      }\n      // 第一步: 使用 SharingPolicy 校验 Plan 能否接受该 reservation。后续解释\n      policy.validate(this, inMemReservation);\n      // 第二步: 记录接受 reservation 的时间\n      reservation.setAcceptanceTimestamp(clock.getTime());\n      // 第三步: 内存数据结构维护\n      ReservationInterval searchInterval =\n          new ReservationInterval(inMemReservation.getStartTime(),\n              inMemReservation.getEndTime());\n      Set<InMemoryReservationAllocation> reservations =\n          currentReservations.get(searchInterval);\n      if (reservations == null) {\n        reservations = new HashSet<InMemoryReservationAllocation>();\n      }\n      if (!reservations.add(inMemReservation)) {\n        LOG.error(\"Unable to add reservation: {} to plan.\",\n            inMemReservation.getReservationId());\n        return false;\n      }\n      currentReservations.put(searchInterval, reservations);\n      reservationTable.put(inMemReservation.getReservationId(),\n          inMemReservation);\n      // 第四步: 维护Plan 整体的<时间-资源>状态变化图;维护细分用户<时间-资源>状态变化图  \n      incrementAllocation(inMemReservation);\n      LOG.info(\"Sucessfully added reservation: {} to plan.\",\n          inMemReservation.getReservationId());\n      return true;\n    } finally {\n      writeLock.unlock();\n    }\n  \n  private boolean removeReservation(ReservationAllocation reservation) {\n    assert (readWriteLock.isWriteLockedByCurrentThread());\n    ReservationInterval searchInterval =\n        new ReservationInterval(reservation.getStartTime(),\n            reservation.getEndTime());\n    Set<InMemoryReservationAllocation> reservations =\n        currentReservations.get(searchInterval);\n     // Plan 成员变量维护的信息 维护\n    if (reservations != null) {\n      if (!reservations.remove(reservation)) {\n        LOG.error(\"Unable to remove reservation: {} from plan.\",\n            reservation.getReservationId());\n        return false;\n      }\n      if (reservations.isEmpty()) {\n        currentReservations.remove(searchInterval);\n      }\n    } else {\n      String errMsg =\n          \"The specified Reservation with ID \" + reservation.getReservationId()\n              + \" does not exist in the plan\";\n      LOG.error(errMsg);\n      throw new IllegalArgumentException(errMsg);\n    }\n    reservationTable.remove(reservation.getReservationId());\n    // 1.维护细分用户<时间-资源>状态变化图;维护Plan 整体<时间-资源>状态变化图  \n    decrementAllocation(reservation);\n    LOG.info(\"Sucessfully deleted reservation: {} in plan.\",\n        reservation.getReservationId());\n    return true;\n  }\n```\n\nPlan 在 addReservation 和 updateReservation 时都使用了 SharingPolicy 来校验用户资源使用是否违规。CapacityOverTimePolicy 是默认 SharingPolicy，主要校验两个方面资源使用是否违规：瞬时用量不超过 reservable queue capaciy 的 1%(默认);24h(默认)平均用量不超过reservable queue capacity 的 1%(默认)。上述的 1%和 24h 是源码内常量,无法用配置修改      \n\n[CapacityOverTimePolicy.java]\n\n```\npublic void validate(Plan plan, ReservationAllocation reservation)\n      throws PlanningException {\n    ReservationAllocation oldReservation =\n        plan.getReservationById(reservation.getReservationId());\n    if (oldReservation != null\n        && !oldReservation.getUser().equals(reservation.getUser())) {\n      throw new MismatchedUserException(\n          \"Updating an existing reservation with mismatched user:\"\n              + oldReservation.getUser() + \" != \" + reservation.getUser());\n    }\n\n    long startTime = reservation.getStartTime();\n    long endTime = reservation.getEndTime();\n    long step = plan.getStep();\n    // reservable queue capacity \n    Resource planTotalCapacity = plan.getTotalCapacity();\n    // 最大瞬时容量和最大平均容量\t\n    Resource maxAvgRes = Resources.multiply(planTotalCapacity, maxAvg);\n    Resource maxInsRes = Resources.multiply(planTotalCapacity, maxInst);\n    \t\n    IntegralResource runningTot = new IntegralResource(0L, 0L);\n    // 平均(乘法,即积分面积)容量限制\n    IntegralResource maxAllowed = new IntegralResource(maxAvgRes);\n    maxAllowed.multiplyBy(validWindow / step);\n\n    // 检查与该分配有重叠区域且长度为 validWindow(24h)的任何窗口,\n    //提供给用户的资源是否超过瞬时容量和平均容量限制\n    for (long t = startTime - validWindow; t < endTime + validWindow; t += step) {\n\n      Resource currExistingAllocTot = plan.getTotalCommittedResources(t);\n      Resource currExistingAllocForUser =\n          plan.getConsumptionForUser(reservation.getUser(), t);\n      Resource currNewAlloc = reservation.getResourcesAtTime(t);\n      Resource currOldAlloc = Resources.none();\n      if (oldReservation != null) {\n        currOldAlloc = oldReservation.getResourcesAtTime(t);\n      }\n      // 所有用户累计 reservation 不能超过 reservable queue  capacity\n      Resource inst =\n          Resources.subtract(Resources.add(currExistingAllocTot, currNewAlloc),\n              currOldAlloc);\n      if (Resources.greaterThan(plan.getResourceCalculator(),\n          planTotalCapacity, inst, planTotalCapacity)) {\n        throw new ResourceOverCommitException(\" Resources at time \" + t\n            + \" would be overcommitted (\" + inst + \" over \"\n            + plan.getTotalCapacity() + \") by accepting reservation: \"\n            + reservation.getReservationId());\n      }\n\n      // 细分用户 容量不能超过 瞬时容量限制\n      if (Resources.greaterThan(plan.getResourceCalculator(),\n          planTotalCapacity, Resources.subtract(\n              Resources.add(currExistingAllocForUser, currNewAlloc),\n              currOldAlloc), maxInsRes)) {\n        throw new PlanningQuotaException(\"Instantaneous quota capacity \"\n            + maxInst + \" would be passed at time \" + t\n            + \" by accepting reservation: \" + reservation.getReservationId());\n      }\n      \n      // 相当于增加了 (用户分配容量[已有的+当前reservation更新后的-当前reservation更新前的]) * (一个时间步长)的面积\n      // 当前reservation更新前的 实际上在 已有的 之中,所以需要减去\n      runningTot.add(currExistingAllocForUser);\n      runningTot.add(currNewAlloc);\n      runningTot.subtract(currOldAlloc);\n\t\n      // 老化掉一个24h窗口之前的那个时间点的面积\n      if (t > startTime) {\n        Resource pastOldAlloc =\n            plan.getConsumptionForUser(reservation.getUser(), t - validWindow);\n        Resource pastNewAlloc = reservation.getResourcesAtTime(t - validWindow);\n        runningTot.subtract(pastOldAlloc);\n        runningTot.subtract(pastNewAlloc);\n      }\n      if (maxAllowed.compareTo(runningTot) < 0) {\n        throw new PlanningQuotaException(\n            \"Integral (avg over time) quota capacity \" + maxAvg\n                + \" over a window of \" + validWindow / 1000 + \" seconds, \"\n                + \" would be passed at time \" + t + \"(\" + new Date(t)\n                + \") by accepting reservation: \"\n                + reservation.getReservationId());\n      }\n    }\n  }\n```\n上述 CapacityOverTimePolicy 用积分面积的方式判断 是否超过平均容量限制的方式，很容易和 RLESparseResourceAllocation 混淆。  RLESparseResourceAllocation 存储的是<时间点-资源>的状态量,是累积状态量不是增量。而积分面积用的是 RLESparseResourceAllocation 绘成的状态图算的积分面积，每向后移动一个时间步长则增加 (RLE 那个时间点资源量) * (一个时间步长) 的积分面积，并减去(RLE 24h前那个时间点资源量) * (一个时间步长)的积分面积。从而达到随着时间增加，积分面积始终是 RLESparseResourceAllocation 图中 [now-24h,now]这个区间的面积，然后和(平均资源限制容量) * (24h) 来比较。很巧妙，第一次见定积分可以在程序中这么用           \n\n### step 4\n\n成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据\n参考 \"step 1\" 的接口定义和 \"step 2\" ReservationAgent 分配成功后的处理  \n \n### step 5\n\nPlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序   \n\nPlanFollower(Runnable) 随 ResourceManager 初始化时创建的 ReservationSystem(服务) 初始化而初始化 启动而启动，调用栈如下  \n\n->ResourceManager$RMActiveServices#serviceInit  \n　　->ResourceManager#createReservationSystem  \n　　　　->AbstractReservationSystem#serviceInit  \n　　　　　　->AbstractReservationSystem#createPlanFollower  \n\n[AbstractReservationSystem.java]　\n\n```\n  public void serviceInit(Configuration conf) throws Exception {\n    Configuration configuration = new Configuration(conf);\n    // 第一步: 初始化所有的 reservable queue 。\n    // 指定 SharingPolicy,Planner,capacity,planstep...\n    reinitialize(configuration, rmContext);\n    // 第二步: 创建 PlanFollower。加载类:CS 调度器对应 CapacitySchedulerPlanFollower...\n    planFollower = createPlanFollower();\n    if (planFollower != null) {\n      // 第三步:初始化 PlanFollower \n      planFollower.init(clock, scheduler, plans.values());\n    }\n    super.serviceInit(conf);\n  }\n  \n  public void serviceStart() throws Exception {\n    if (planFollower != null) {\n      scheduledExecutorService = new ScheduledThreadPoolExecutor(1);\n      //定时调度,调度周期是 planStepSize(默认1s)\n      scheduledExecutorService.scheduleWithFixedDelay(planFollower, 0L,\n          planStepSize, TimeUnit.MILLISECONDS);\n    }\n    super.serviceStart();\n  }\n```\nPlanFollower的核心逻辑在 AbstractSchedulerPlanFollower，用于 PlanFollower 与常规调度器同步预订分配的信息，下有 CapacitySchedulerPlanFollower 和 FairCapacityPlanFollower 两个子类。  \n\n[AbstractSchedulerPlanFollower.java] \n\n```\n@Override\n  public synchronized void run() {\n    for (Plan plan : plans) {\n      synchronizePlan(plan);\n    }\n  }\n\n@Override\n  public synchronized void synchronizePlan(Plan plan) {\n     String planQueueName = plan.getQueueName();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Running plan follower edit policy for plan: \" + planQueueName);\n    }\n    long step = plan.getStep();\n    long now = clock.getTime();\n    if (now % step != 0) {\n      now += step - (now % step);\n    }\n    Queue planQueue = getPlanQueue(planQueueName);\n    if (planQueue == null) return;\n    \n    Resource clusterResources = scheduler.getClusterResource();\n    // plan 对应的 reservable queue 的容量\n    Resource planResources = getPlanResources(plan, planQueue,\n        clusterResources);\n    // 当前正在生效的预订(当前时间介于 startTime 和 endTime 之间的所有 Reservation) \n    Set<ReservationAllocation> currentReservations =\n        plan.getReservationsAtTime(now);\n    // 当前正在生效的 reservationId 集合\n    Set<String> curReservationNames = new HashSet<String>();\n    // 当前正在生效预订的 resource\n    Resource reservedResources = Resource.newInstance(0, 0);\n    // 当前正在生效的 reservation;\n    int numRes = getReservedResources(now, currentReservations,\n        curReservationNames, reservedResources);\n\n    // 创建 reservable queue 的默认队列: xxx-default\n    String defReservationId = getReservationIdFromQueueName(planQueueName) +\n        ReservationConstants.DEFAULT_QUEUE_SUFFIX;\n    String defReservationQueue = getReservationQueueName(planQueueName,\n        defReservationId);\n    createDefaultReservationQueue(planQueueName, planQueue,\n        defReservationId);\n    curReservationNames.add(defReservationId);\n\n    // 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)\n    // 如果专用于预订资源的队列骤减(或配置或节点宕机),通过调用 Planner 来删除部分已有预约\n    if (arePlanResourcesLessThanReservations(clusterResources, planResources,\n        reservedResources)) {\n      try {\n        plan.getReplanner().plan(plan, null);\n      } catch (PlanningException e) {\n        LOG.warn(\"Exception while trying to replan: {}\", planQueueName, e);\n      }\n    }\n    \n    // 第二步: 标记当前预约中，哪些旧的预约需要过期删除，哪些新的预约需要创建队列  \n    List<? extends Queue> resQueues = getChildReservationQueues(planQueue);\n    Set<String> expired = new HashSet<String>();\n    for (Queue resQueue : resQueues) {\n      String resQueueName = resQueue.getQueueName();\n      String reservationId = getReservationIdFromQueueName(resQueueName);\n      if (curReservationNames.contains(reservationId)) {\n        // 当前活跃的预订包含此此预约，因此不用为它创建对应的队列\n        curReservationNames.remove(reservationId);\n      } else {\n\t// 预约已经结束，准备清除\n        expired.add(reservationId);\n      }\n    }\n    // 第三步: 清除过期预约。 getMoveOnExpiry 默认为 true\n    cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue);\n\n    float totalAssignedCapacity = 0f;\n    if (currentReservations != null) {\n      // 释放默认队列中的所有多余容量  \n      try {\n        setQueueEntitlement(planQueueName, defReservationQueue, 0f, 1.0f);\n      } catch (YarnException e) {\n        LOG.warn(\n            \"Exception while trying to release default queue capacity for plan: {}\",\n            planQueueName, e);\n      }\n       // 第四步: 增加新的预约或更新已有的预约 对应的队列配额\n       // 4.1 首先按照(当前时间待分配的容量 - 已有的容量)从小到大(从负到正)的方式排序一次调整 capacity。\n       // 这种排序方式用以避免分配过程中瞬时容量超出 100%的 capacity(猜测可能是为了减少干扰并发预订和抢占介入)\n      List<ReservationAllocation> sortedAllocations =\n          sortByDelta(\n              new ArrayList<ReservationAllocation>(currentReservations), now,\n              plan);\n      // 4.2 为每个队列设置或更新 capacity\n      for (ReservationAllocation res : sortedAllocations) {\n        String currResId = res.getReservationId().toString();\n        if (curReservationNames.contains(currResId)) {\n          // 每个reservationId 对应着常规调度器中 PlanQueue 下的一个 ReservationQueue\n          addReservationQueue(planQueueName, planQueue, currResId);\n        }\n        // 当前生效的 ResourceAllocation 需要(或RS已分配给Reservation)的 capacity\n        Resource capToAssign = res.getResourcesAtTime(now);\n        float targetCapacity = 0f;\n        if (planResources.getMemory() > 0\n            && planResources.getVirtualCores() > 0) {\n          // 绝对值式的容量相对 PlanQueue capacity 计算相对容量，capToAssign/planResources\n          targetCapacity =\n              calculateReservationToPlanRatio(clusterResources,\n                  planResources,\n                  capToAssign);\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\n              \"Assigning capacity of {} to queue {} with target capacity {}\",\n              capToAssign, currResId, targetCapacity);\n        }\n        // set maxCapacity to 100% unless the job requires gang, in which\n        // case we stick to capacity (as running early/before is likely a\n        // waste of resources)\n        // 设置最大配额。感觉是无效代码，不明白为什么要设置最大配额 都为1不行吗，为什么按有无并发来设置最大配额  \n        float maxCapacity = 1.0f;\n        if (res.containsGangs()) {\n          maxCapacity = targetCapacity;\n        }\n        try {\n         // 设置capacity 和 maxCapacity\n          setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);\n        } catch (YarnException e) {\n          LOG.warn(\"Exception while trying to size reservation for plan: {}\",\n              currResId, planQueueName, e);\n        }\n        totalAssignedCapacity += targetCapacity;\n      }\n    }\n    // 第五步: 设置 default queue 的配额(PlanQueue 的剩余配额)\n    float defQCap = 1.0f - totalAssignedCapacity;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"PlanFollowerEditPolicyTask: total Plan Capacity: {} \"\n          + \"currReservation: {} default-queue capacity: {}\", planResources,\n          numRes, defQCap);\n    }\n    try {\n      setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);\n    } catch (YarnException e) {\n      LOG.warn(\n          \"Exception while trying to reclaim default queue capacity for plan: {}\",\n          planQueueName, e);\n    }\n    // 第六步: 清理过期的预订记录(now-endTime>24h)\n    // garbage collect finished reservations from plan\n    try {\n      plan.archiveCompletedReservations(now);\n    } catch (PlanningException e) {\n      LOG.error(\"Exception in archiving completed reservations: \", e);\n    }\n    LOG.info(\"Finished iteration of plan follower edit policy for plan: \"\n        + planQueueName);\n\n    // Extension: update plan with app states,\n    // useful to support smart replanning\n  }\n```\n\n先看 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)  \n可用于兼容集群/队列资源骤减的情况，策略是删除最晚接受的部分预约。  \n[SimpleCapacityReplanner.java] \n\n```\npublic void plan(Plan plan, List<ReservationDefinition> contracts)\n      throws PlanningException {\n    if (contracts != null) {\n      throw new RuntimeException(\n          \"SimpleCapacityReplanner cannot handle new reservation contracts\");\n    }\n    ResourceCalculator resCalc = plan.getResourceCalculator();\n    Resource totCap = plan.getTotalCapacity();\n    long now = clock.getTime();\n    // lengthOfCheckZone=1h \n    for (long t = now; (t < plan.getLastEndTime() && t < (now + lengthOfCheckZone)); t +=\n        plan.getStep()) {\n        // 使用 Plan 所有预订的总资源量 - Plan 的 capacity 来检查是否超限\n      Resource excessCap =\n          Resources.subtract(plan.getTotalCommittedResources(t), totCap);\n      if (Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE)) {\n         // 按照 ReservationAllocation.acceptedAt 做比较，最新接受的在前\n        Set<ReservationAllocation> curReservations =\n            new TreeSet<ReservationAllocation>(plan.getReservationsAtTime(t));\n        for (Iterator<ReservationAllocation> resIter =\n            curReservations.iterator(); resIter.hasNext()\n            && Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE);) {\n          ReservationAllocation reservation = resIter.next();\n          // 使用 deleteReservation 来调用 Plan.removeReservation 来删除预订。\n          // removeReservation 的源码分析 见\"step 3\"\n          plan.deleteReservation(reservation.getReservationId());\n          excessCap =\n              Resources.subtract(excessCap, reservation.getResourcesAtTime(t));\n          LOG.info(\"Removing reservation \" + reservation.getReservationId()\n              + \" to repair physical-resource constraints in the plan: \"\n              + plan.getQueueName());\n        }\n      }\n    }\n  }\n```\n\n第二步略，第三步: 清除过期预约。  \n[AbstractSchedulerPlanFollower.java]  \n\n```\nprotected void cleanupExpiredQueues(String planQueueName,\n      boolean shouldMove, Set<String> toRemove, String defReservationQueue) {\n    for (String expiredReservationId : toRemove) {\n      try {\n        String expiredReservation = getReservationQueueName(planQueueName,\n            expiredReservationId);\n         // 将过期预约对应的 ReservationQueue 的 capacity 和 maxCapacity 设置为 0 \n        setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);\n        // 默认配置为 true。将其中的 APP 移动到同级的 default 队列中(xxx-default)。\n        if (shouldMove) {\n          moveAppsInQueueSync(expiredReservation, defReservationQueue);\n        }\n        if (scheduler.getAppsInQueue(expiredReservation).size() > 0) {\n        // 移走所有已有 APP 的情况,还有新提交的。暂时保留\n          scheduler.killAllAppsInQueue(expiredReservation);\n          LOG.info(\"Killing applications in queue: {}\", expiredReservation);\n        } else {\n        // 删除队列  \n          scheduler.removeQueue(expiredReservation);\n          LOG.info(\"Queue: \" + expiredReservation + \" removed\");\n        }\n      } catch (YarnException e) {\n        LOG.warn(\"Exception while trying to expire reservation: {}\",\n            expiredReservationId, e);\n      }\n    }\n  }\n```\n第四步 先调用 sortByDelta 方法使用 ReservationAllocationComparator 对所有正在生效的预订对应的 ReservationQueue 做排序，然后对所有预订通过 setQueueEntitlement 设置或修正 capacity 和 maxCapacity。   \n第五步 通过 setQueueEntitlement 设置 default queue 的 capacity 和 maxCapacity，占有所有剩余资源  \n[ReservationAllocationComparator.java]  \n\n```\n    @Override\n    public int compare(ReservationAllocation lhs, ReservationAllocation rhs) {\n      Resource lhsRes = getUnallocatedReservedResources(lhs);\n      Resource rhsRes = getUnallocatedReservedResources(rhs);\n      // 两个 Resource 先比内存再比虚拟核，小的在前。\n      return lhsRes.compareTo(rhsRes);\n    }\n    \nprivate Resource getUnallocatedReservedResources(\n        ReservationAllocation reservation) {\n      Resource resResource;\n      // 查询常规调度器，获取现有 capacity\n      Resource reservationResource = planFollower\n          .getReservationQueueResourceIfExists\n              (plan, reservation.getReservationId());\n      if (reservationResource != null) {\n        resResource =\n            Resources.subtract(\n            // 当前时刻需要的 capacity。可能为负\n                reservation.getResourcesAtTime(now),\n                reservationResource);\n      } else {\n        resResource = reservation.getResourcesAtTime(now);\n      }\n      return resResource;\n    }\n```\n\n[AbstractSchedulerPlanFollower.java]  \n\n```\nprotected void setQueueEntitlement(String planQueueName, String currResId,\n      float targetCapacity,\n      float maxCapacity) throws YarnException {\n    String reservationQueueName = getReservationQueueName(planQueueName,\n        currResId);\n    scheduler.setEntitlement(reservationQueueName, new QueueEntitlement(\n        targetCapacity, maxCapacity));\n  }\n```\n[CapacityScheduler.java]\n\n```\npublic synchronized void setEntitlement(String inQueue,\n      QueueEntitlement entitlement) throws SchedulerDynamicEditException,\n      YarnException {\n    LeafQueue queue = getAndCheckLeafQueue(inQueue);\n    ParentQueue parent = (ParentQueue) queue.getParent();\n    if (!(queue instanceof ReservationQueue)) {\n      throw new SchedulerDynamicEditException(\"Entitlement can not be\"\n          + \" modified dynamically since queue \" + inQueue\n          + \" is not a ReservationQueue\");\n    }\n    if (!(parent instanceof PlanQueue)) {\n      throw new SchedulerDynamicEditException(\"The parent of ReservationQueue \"\n          + inQueue + \" must be an PlanQueue\");\n    }\n    ReservationQueue newQueue = (ReservationQueue) queue;\n    float sumChilds = ((PlanQueue) parent).sumOfChildCapacities();\n    // 计算修改后, 所有同级队列的容量之和\n    float newChildCap = sumChilds - queue.getCapacity() + entitlement.getCapacity();\n\n    if (newChildCap >= 0 && newChildCap < 1.0f + CSQueueUtils.EPSILON) {\n      // 设置前后的 capacity 和 maxCapacity 都一样则 不设置\n      if (Math.abs(entitlement.getCapacity() - queue.getCapacity()) == 0\n          && Math.abs(entitlement.getMaxCapacity() - queue.getMaximumCapacity()) == 0) {\n        return;\n      }\n      //设置 ReservationQueue 的 capacity 和 maxCapacity \n      newQueue.setEntitlement(entitlement);\n    } else {\n      throw new SchedulerDynamicEditException(\n          \"Sum of child queues would exceed 100% for PlanQueue: \"\n              + parent.getQueueName());\n    }\n    LOG.info(\"Set entitlement for ReservationQueue \" + inQueue + \"  to \"\n        + queue.getCapacity() + \" request was (\" + entitlement.getCapacity() + \")\");\n  }\n```\n[ReservationQueue.java]\n\n```\npublic synchronized void setEntitlement(QueueEntitlement entitlement)\n      throws SchedulerDynamicEditException {\n    float capacity = entitlement.getCapacity();\n    if (capacity < 0 || capacity > 1.0f) {\n      throw new SchedulerDynamicEditException(\n          \"Capacity demand is not in the [0,1] range: \" + capacity);\n    }\n    setCapacity(capacity);\n    setAbsoluteCapacity(getParent().getAbsoluteCapacity() * getCapacity());\n    setMaxCapacity(entitlement.getMaxCapacity());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"successfully changed to \" + capacity + \" for queue \"\n          + this.getQueueName());\n    }\n  }\n```\n\n第六步: 清理过期的预订记录  \n[InMemoryPlan.java]\n\n```\npublic void archiveCompletedReservations(long tick) {\n    LOG.debug(\"Running archival at time: {}\", tick);\n    List<InMemoryReservationAllocation> expiredReservations =\n        new ArrayList<InMemoryReservationAllocation>();\n    readLock.lock();\n    try {\n    // 删除过期的保留记录  \n      long archivalTime = tick - policy.getValidWindow();\n      ReservationInterval searchInterval =\n          new ReservationInterval(archivalTime, archivalTime);\n      SortedMap<ReservationInterval, Set<InMemoryReservationAllocation>> reservations =\n          currentReservations.headMap(searchInterval, true);\n      if (!reservations.isEmpty()) {\n        for (Set<InMemoryReservationAllocation> reservationEntries : reservations\n            .values()) {\n          for (InMemoryReservationAllocation reservation : reservationEntries) {\n            // 结束时间早于当前时间24h \n            if (reservation.getEndTime() <= archivalTime) {\n              expiredReservations.add(reservation);\n            }\n          }\n        }\n      }\n    } finally {\n      readLock.unlock();\n    }\n    if (expiredReservations.isEmpty()) {\n      return;\n    }\n    writeLock.lock();\n    try {\n      for (InMemoryReservationAllocation expiredReservation : expiredReservations) {\n      // Plan.removeReservation 源码分析见\"step 3\",主要是内存数据结构的维护\n        removeReservation(expiredReservation);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }\n```\n　　　　\n### step 6\n\n用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue)   \n\n参见 ApplicationSubmissionContext 的数据结构  \n\n### step 7\n\n常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   \n\n```\npublic void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    ......\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;\n      // 解析 APP 提交的队列\n      String queueName =\n          resolveReservationQueueName(appAddedEvent.getQueue(),\n              appAddedEvent.getApplicationId(),\n              appAddedEvent.getReservationID());\n      if (queueName != null) {\n        if (!appAddedEvent.getIsAppRecovering()) {\n          addApplication(appAddedEvent.getApplicationId(), queueName,\n              appAddedEvent.getUser());\n        } else {\n          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,\n              appAddedEvent.getUser());\n        }\n      }\n    }\n    break;\n    ......\n    }\n    }\n    \nprivate synchronized String resolveReservationQueueName(String queueName,\n      ApplicationId applicationId, ReservationId reservationID) {\n    CSQueue queue = getQueue(queueName);\n    if ((queue == null) || !(queue instanceof PlanQueue)) {\n      return queueName;\n    }\n    if (reservationID != null) {\n      String resQName = reservationID.toString();\n      // 根据 reservationId 获取对应的叶子队列\n      queue = getQueue(resQName);\n      if (queue == null) {\n        String message =\n            \"Application \"\n                + applicationId\n                + \" submitted to a reservation which is not yet currently active: \"\n                + resQName;\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId,\n                RMAppEventType.APP_REJECTED, message));\n        return null;\n      }\n      if (!queue.getParent().getQueueName().equals(queueName)) {\n        String message =\n            \"Application: \" + applicationId + \" submitted to a reservation \"\n                + resQName + \" which does not belong to the specified queue: \"\n                + queueName;\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId,\n                RMAppEventType.APP_REJECTED, message));\n        return null;\n      }\n     // 使用 ReservationId 对应的 ReservationQueue 来运行任务, 调度容器....\n      queueName = resQName;\n    } else {\n      // 如果指定了 PlanQueue 但是没指定 reservationId, 会提交到 PlanQueue 的 default 队列执行\n      queueName = queueName + ReservationConstants.DEFAULT_QUEUE_SUFFIX;\n    }\n    return queueName;\n  }\n```\n\n### step 8\n\n预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用     \n参考\"step 5\"中 AbstractSchedulerPlanFollower#synchronizePlan 方法中对 plan.getReplanner().plan(plan, null) 的调用和cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue)的逻辑  \n        \n官网解释可能不实，原文如下:  \nThe system includes mechanisms to adapt to drop in cluster capacity. This consists in replanning by “moving” the reservation if possible, or rejecting the smallest amount of previously accepted reservation (to ensure that other reservation will receive their full amount).\n\n但在 hadoop-2.7.3 和 hadoop-3.2.0的 trunk 分支上 AbstractSchedulerPlanFollower/Planner(只有 SimpleReplanner 实现类)逻辑没有改动，且无相应逻辑  \n\n\n## QA\n>\n1. Q: reservable queue 和 leaf queue 区别   \n    A: 其一，只有 leaf queue 才能被设置为 reservable queue，通过对 leaf queue 设置 yarn.scheduler.capacity.\\<queue-path\\>.reservable=true。其二，在资源管理的层级体系中 reservable queue 对应的数据结构是 PlanQueue extend ParentQueue。即实际上，reservable queue 是在 leaf queue上做了配置的 ParentQueue。  \n2. Q: PlanQueue 和 ReservationQueue 区别  \n    A: PlanQueue 对应一个 reservable queue，ReservationQueue 对应一个 ReservationId，PlanQueue 下面可以创建任意个 ReservationQueue; PlanQueue 是 ParentQueue，ReservationQueue 是 LeafQueue。      \n3. Q: 在名称为 X 的 reservable queue 上预订了资源，reservationId=reservationId_001,提交APP 时 Queue 名字填什么？  \n    A: Queue 名字填 X,且需要在 ApplicationSubmissionContext 中设置 reservationId。常规调度器会根据 reservationId 解析出对应的 ReservationQueue(名称与 ReservationId 相同)。  \n4. Q: 在1:00-2:00 预约了<100G,10core>的资源 30 分钟，但是在 1:00前或者 2:00后提交任务 会怎样?   \n   A:  常规调度器会拒绝任务(APP_REJECT)。因为找不到 reservationId 对应的 ReservationQueue,早于1:00时队列还没创建,晚于2:00时队列被删除。最好在1:00提交，可能会有等待时间(RS 调度时从deadline 向 arrival 尝试分配)       \n5. Q: 任务运行在哪里?  \n   A: 任务运行在与 reservationId 同名的 ReservationQueue 中,如果预约时间过去但是 APP 还没运行完成则默认移动 APP 到与 ReservationQueue 同级的 default 队列中  \n6. Q: 能不能把 reservable queue 当做 leaf queue 用? 每个任务都不指定 ReservationId。  \n    A: 可以。提交到 reservable queue 但是不指定 ReservationId 的任务都会被移动到 reservable queue 下的 default 队列运行。\n7. Q: 提交到 reservable queue 中的 app 有的指定了 ReservationId,有的没指定。运行时有什么影响?  \n    A: PlanQueue 优先为有 ReservationId 队列分配资源,其次再为 default 队列分配资源，即只能使用那些被预订后剩余的资源，在资源紧张时 default 队列无资源或被其他任意 ReservationQueue 抢占。(这是由 ReservationSystem 提供预订的逻辑决定的。1.不预约的资源不在ReservationSystem 管理下,请求预约时 ReservationSystem 会认为自身管理 PlanQueue 所有资源都能预约出去  2.为所有预约分配完资源之后剩下的才归属 default 队列所有)  \n8. Q: Spark On Yarn 如何使用 ReservationSystem?  \n    A: 分两个阶段。第一,使用 Client-RM 协议提交 ReservationSubmissionRequest,预订资源并保存回执的 ReservationId。第二，在 Spark On Yarn 程序中指定 ReservationId 字段，但是此阶段 Spark On Yarn 提交程序尚不完善需要修改源代码。目前 Spark On Yarn 程序使用 createApplicationSubmissionContext 方法来封装 ApplicationSubmissionContext 时, 没有针对 ReservationId 的逻辑。如果有需求的话，可以通过 SparkConf 来传递 ReservationId 字符串,在createApplicationSubmissionContext做解析设置 。    \n 9. Q: ReservationSystem 的适用场景  \n     A: 1.资源紧张时需要保证重要生产任务运行,可以使用预订资源的方式 。ReservationSystem 在 PlanQueue 下创建 ReservationQueue，将预订需要的绝对值资源量转化 ReservationQueue 在当前时刻的capacity 和 maxCapacity，将任务在此队列中调度，本质上还是 LeafQueue。绝对资源量转化成百分比资源量擦掉了容器数量和大小，不修改开源抢占调度逻辑 不密集抢占抢占的情况下，不能解决大容器调度的问题。  \n\n\n## 配置预订系统\n目前可以在 yarn-site.xml 中配置开启ReservationSystem,可以在CapacityScheduler 及 FairScheduler中增加对预订的支持，方式是在capacity-scheduler.xml 或者 fair-scheduler.xml中的任何 LeafQueue的\"reservable\"属性标记为 true，然后该队列的配额就可以用于预订。即使没有预订资源，应用程序仍然可以被提交到该队列上，它们将以\"best-effort\"的模式运行在 预订资源中运行的作业 剩下的容量中。  \n\n## 参考\nhttps://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html  \nhttps://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us  \nhttp://gitbook.net/java/util/java_util_treemap.html  \n\n\n","slug":"2019-10-28-Yarn ReservationSystem","published":1,"updated":"2020-06-16T06:41:10.445Z","comments":1,"photos":[],"link":"","_id":"cke2rms8s002gy7c7ayro5wyl","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>ReservationSystem 是 YARN ResourceManager 的组件。YARN 的 ReservationSystem 为用户提供了提前保留资源的能力,以确保重要的生产作业可预测地运行。ReservationSystem 执行仔细的准入控制，并保证绝对资源量(而不是群集大小的百分比)。保留具有组和并发的语义，并且可以有随时间变化的资源需求。  </p>\n<a id=\"more\"></a>\n\n<h2 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h2><p><img src=\"/img/pictures/reservation/yarn_reservation_system.png\" alt=\"\">  </p>\n<p>参考上图，预订资源的工作流程如下：  </p>\n<ul>\n<li>step 0: 用户提交预订创建请求，并收到包含 ReservationId 的响应。  </li>\n<li>step 1: 用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。这描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求。这可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败</li>\n<li>step 2: ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。  </li>\n<li>step 3: SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  </li>\n<li>step 4: 成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据    </li>\n<li>step 5: PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序 </li>\n<li>step 6: 用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue) </li>\n<li>step 7: 常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   </li>\n<li>step 8: 预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用以重建计划  </li>\n</ul>\n<p>注: step 8和官网解释不同。官网解释可能不实,参见源码解析-“step 8”</p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><h3 id=\"涉及的类\"><a href=\"#涉及的类\" class=\"headerlink\" title=\"涉及的类\"></a>涉及的类</h3><ul>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem<br>该类继承了AbstractService,实现了ReservationSystem 并封装了Capacity/Fair 调度器下预订系统的核心实现<br>主要功能是:作为服务启动时加载配置文件中配置的 reservable LeafQueue 转换为 Plan;管理 PlanFollower,确保 Plan与常规调度器的同步  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">setRMContext</td>\n<td align=\"center\">保存 rmContext 指针</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">reinitialize</td>\n<td align=\"center\">重新初始化 ReservationSystem</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getPlan</td>\n<td align=\"center\">获取已被加载的 Plan</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getAllPlans</td>\n<td align=\"center\">获取已被加载的所有 Plan</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">synchronizePlan</td>\n<td align=\"center\">使用 PlanFollower 同步 Plan 与常规调度器</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getPlanFollowerTimeStep</td>\n<td align=\"center\">PlanFollower 步长</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getNewReservationId</td>\n<td align=\"center\">获取一个全局唯一的reservationId</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getQueueForReservation</td>\n<td align=\"center\">获取reservationId关联的队列</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">setQueueForReservation</td>\n<td align=\"center\">为reservationId关联队列</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower<br>该类实现了 PlanFollower与 Runnable 接口<br>主要功能是:周期性同步常规调度器与 Plan。通过将计划中的每个预订的当前资源映射到常规调度器(e.g. 队列的调整能力,设置池权重,调整应用优先级)，来影响调度器的资源分配 进而达到 保证作业与Plan 中预订一致的方式来使用资源。一个关键概念是将预订的绝对值式资源转换为队列的优先级和容量。PlanFollower 也会向 Plan 来同步集群总资源的变化使其作出相应的调整。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"left\">通过 SystemClock,ResourceSchduler 和Plans 来初始化PlanFollower</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">synchronizePlan</td>\n<td align=\"left\">同步指定 Plan和常规调度器</td>\n<td align=\"left\">周期性调用;时间紧迫时同步阻塞调用</td>\n</tr>\n<tr>\n<td align=\"left\">setPlans</td>\n<td align=\"left\">重置 PlanFollower 同步的 Plan 集合</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan<br>该接口实现了 Plan 接口,Plan 接口继承了PlanContext, PlanView, PlanEdit。实现只有 InMemoryPlan<br>Plan 代表着预订系统的核心数据结构,维护着集群资源的工作安排(分配或收回)计划。用户将 ReservationRequest 提交给 RM 之后,RM 委托给 ReservationAgent，ReservationAgent 通过PlanView 接口咨询该 Plan 是否能满足 RDL 时间且资源约束。如果可以分配,则通过 PlanEdit 接口将其存储在该 Plan 中。之后便向用户返回 ReservationId 票据,用户可通过该票据在预订的时间范围使用预订的资源。<br>PlanFollower将会周期性的从 Plan 中读取最新工作安排计划(队列瞬时容量),并同步给常规调度器,进而影响正在运行作业占用的资源。<br>接口中有三类方法:  PlanContext 负责配置信息;PlanView 负责对Plan 状态的只读访问;PlanEdit 负责对 Plan 状态写入访问。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">getStep</td>\n<td align=\"center\">获取 Plan 的时间步长</td>\n<td align=\"left\">PlanContext,同 PlanFollower 的时间步长</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationAgent</td>\n<td align=\"center\">获取 Plan关联的 ReservationAgent</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getReplanner</td>\n<td align=\"center\">使用 Planner 对象来应对Plan 的资源意外减少</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getSharingPolicy</td>\n<td align=\"center\">SharingPolicy 控制多用户共享计划资源</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationById</td>\n<td align=\"center\">通过 ReservationId 获取 Reservation 的详细信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationsAtTime</td>\n<td align=\"center\">获取指定时间点所有活跃的 Reservation 的详细信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getAllReservations</td>\n<td align=\"center\">获取 Plan 中所有预订信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getTotalCommittedResources</td>\n<td align=\"center\">获取指定时间点所有预定的总资源量</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getConsumptionForUser</td>\n<td align=\"center\">获取指定时间点指定用户预定的总资源量</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getEarliestStartTime</td>\n<td align=\"center\">获取计划中最早的预订开始时间</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getLastEndTime</td>\n<td align=\"center\">获取计划中最晚的预订结束时间</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">addReservation</td>\n<td align=\"center\">增加一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">updateReservation</td>\n<td align=\"center\">更新一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">deleteReservation</td>\n<td align=\"center\">删除一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">archiveCompletedReservations</td>\n<td align=\"center\">清除所有的过期预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>　　仅列举出核心方法,实际不限于此  </p>\n<ul>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.GreedyReservationAgent<br>实现了ReservationAgent接口。 \n一个简单的贪婪放置策略来满足用户预订的代理。具体方式是:按照 ReservationRequests中的各个 ReservationRequest作为单独的阶段,从deadline 开始向后移动至 arrival 来安排预订请求。该代理不考虑本地性,仅仅考虑容器粒度的验证(e.g. 不能超过最大容器大小)  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">createReservation</td>\n<td align=\"center\">使用此代理尝试创建一个预订</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">updateReservation</td>\n<td align=\"center\">使用此代理更新一个已有预订</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">deleteReservation</td>\n<td align=\"center\">使用此代理删除一个已有预订</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy<br>主要功能是校验 Plan 能否接受用户预订请求 。实现类 CapacityOverTimePolicy使用容量的按时间拓展概念：策略会保证该用户的当前预订申请和已生效的预订申请资源不超过瞬时资源限制(e.g. reservable queue capacity * 1),且在 24h 时间窗口内不能超过平均资源限制(e.g. reservable queue capacity * 0.5)。从某种意义来说,预订可以使用 reservable queue 的大部分容量 但只要保证快速归还以保证平均资源限制，这可以防止资源滥用且增加了灵活性。通过配置瞬时资源限制和平均资源限制以及时间窗口,策略可以使 reservable queue 达到即时执行(max=100%,avg=100%)和完全灵活(max=?,avg=?,保留给其他用户或系统)的效果。   </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"center\">初始化 Policy</td>\n<td align=\"left\">策略必要的配置读取</td>\n</tr>\n<tr>\n<td align=\"left\">validate</td>\n<td align=\"center\">校验 Plan 能否接受用户预订请求</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getValidWindow</td>\n<td align=\"center\">预订资源的过期时间</td>\n<td align=\"left\">窗口为[-24h,当前)。预订的 deadline 在此之前的记录将被删除</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.Planner<br>实现类只有SimpleCapacityReplanner,且功能远不及 Planner 的设计(增删改 Plan)，只有删除预订资源的功能：从当前时刻开始直到 min{最后一个预约endtime，一个小时之后}，如果所有用户预订资源总量超过 reservable queue capacity，则删除接受时间较晚的一批预订。 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"left\">初始化 Planner</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">plan</td>\n<td align=\"left\">更新现有的 Plan,或增删改已有预订或增加一个新的预订</td>\n<td align=\"left\">SCR 只支持删除晚的预订以保证预订资源总量不超过队列最低配额</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation<br>预订系统的核心数据结构，是一种运行长度编码(Run Length Encoded)的稀疏数据结构(TreeMap)，可随着时间的推移维护资源分配。维护者各事件点(预订分配 starttime，endtime)时(Plan/当前预订)全局应有的资源总量。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">getEarliestStartTime</td>\n<td align=\"left\">最早的资源分配的时间戳</td>\n<td align=\"left\">treemap  firstKey</td>\n</tr>\n<tr>\n<td align=\"left\">getLatestEndTime</td>\n<td align=\"left\">最晚的资源分配的时间戳</td>\n<td align=\"left\">treemap lastKey</td>\n</tr>\n<tr>\n<td align=\"left\">getCapacityAtTime</td>\n<td align=\"left\">该时间点的 Plan 已预订资源量</td>\n<td align=\"left\">treemap floorKey</td>\n</tr>\n<tr>\n<td align=\"left\">addInterval</td>\n<td align=\"left\">在treemap 中新增一个时间范围的预约资源记录</td>\n<td align=\"left\">维护开始点和结束点的资源量</td>\n</tr>\n<tr>\n<td align=\"left\">removeInterval</td>\n<td align=\"left\">在 treemap 中删除一个时间范围的预约资源记录</td>\n<td align=\"left\">维护开始点和结束点的资源量</td>\n</tr>\n<tr>\n<td align=\"left\">addCompositeInterval</td>\n<td align=\"left\">在 treemap 中新增一个时间范围的多个预订资源记录</td>\n<td align=\"left\">never used</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<blockquote>\n</blockquote>\n<p> TreeMap 的floorEntry(targetKey) 和 lowerEntry(targetKey)  区别:<br> floorEntry 返回 key 小于等于 targetKey 的键值对,无则 null<br> lowerEntry 返回 key 严格小于 targetKey 的键值对,无则 null  </p>\n<ul>\n<li>org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation\n实现自 ResourceAllocation。预订分配的结果，内存数据结构，包含预订的整体开始结束时间，ReservationSystem 校验(GA 校验资源/SharingPolicy 校验用户违规)通过 ReservationSubmissionRequest 的时间，&lt;时间段，资源量&gt;的分配细节和RLESparseResourceAllocation    </li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">compareTo</td>\n<td align=\"left\">按 acceptTime比较,晚的在前</td>\n<td align=\"left\">比较器, Plan 队列资源骤减时,删除部分晚的的 ResourceAllocation</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest<br>客户端封装ReservationSubmissionRequest 请求，通过 client-RM 接口提交给 RM，数据结构如下<br>　　queueName:String    //支持预订资源的队列<br>　　rd:ReservationDefinition //预订定义<br>　　　　arrival:long  //预订开始时间的 最早时间<br>　　　　deadline:long   //预订结束时间的 最晚时间<br>　　　　name:String  //名称<br>　　　　reservationRequests:ReservationRequests  //预订请求    　\n　　　　　　reservationResources:List&lt;ReservationRequest&gt;<br>　　　　　　　　capability:Capacity  //每个预订请求的资源量<br>　　　　　　　　numContainers:int  //预订的 container 数量<br>　　　　　　　　concurrency:int //并发度。numContainers 可以分批分配<br>　　　　　　　　duration:long  //使用时间<br>　　　　type:ReservationRequestInterpreter  //多个预订请求之间的依赖关系  </p>\n</li>\n<li><p>org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter<br>枚举类型，用以表示多个预订请求之间的依赖关系(或称: 组关系)      </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">类型</th>\n<th align=\"left\">含义</th>\n<th align=\"left\">场景</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">R_ANY</td>\n<td align=\"left\">仅满足 RDL 中一个预订请求即可</td>\n<td align=\"left\">有多种等效的方式满足要求。e.g. 1个&lt;4G,2core&gt;或2个&lt;2G,1core&gt;。Agent 会决定使用最合适的ReservationRequest</td>\n</tr>\n<tr>\n<td align=\"left\">R_ALL</td>\n<td align=\"left\">需要满足 RDL中所有预订请求,请求的分配没有限制时间先后</td>\n<td align=\"left\">事务</td>\n</tr>\n<tr>\n<td align=\"left\">R_ORDER</td>\n<td align=\"left\">需要满足 RDL 中所有预订请求,且有严格的时间限制。k 位置的分配时间段必须在 k+1位置分配时间段之前(无交集)，且 k 位置分配的结束时间和 k+1位置分配的开始时间可以有任意长的时间间隔</td>\n<td align=\"left\">具有固定依赖的阶段性工作流。e.g. 第一个作业需要 1个&lt;4G,2core&gt; 5min，其输出作为第二个作业的输入，第二个作业需要 2 个&lt;2G,1core&gt; 10min，则两个作业预订的分配时间段必然不能重叠</td>\n</tr>\n<tr>\n<td align=\"left\">R_ORDER_NO_GAP</td>\n<td align=\"left\">R_ORDER 的严格版本，要求 k 位置分配的结束时间和 k+1 位置分配的开始时间相同、不能有任何间隙,即 “zero-size gap”,”no_gap”</td>\n<td align=\"left\">1.当前一个作业输出规模比较大时,避免保留太长时间 2.实时性要求很高的场景,作业间时间差则增大了工作流累计延时</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionResponse<br>客户端提交 ReservationSubmissionRequest 请求后，若RM 校验通过可以分配预订，则返回 reservationId给客户端。数据结构如下<br>　　reservationId:ReservationId<br>　　　　clusterTimestamp:long<br>　　　　id:long   </p>\n</li>\n</ul>\n<h3 id=\"step-0\"><a href=\"#step-0\" class=\"headerlink\" title=\"step 0\"></a>step 0</h3><p>用户提交预订创建请求，并收到包含 ReservationId 的响应  </p>\n<p>[ApplicationClientProtocol.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public ReservationSubmissionResponse submitReservation(</span><br><span class=\"line\">      ReservationSubmissionRequest request) throws YarnException, IOException</span><br></pre></td></tr></table></figure>\n<h3 id=\"step-1\"><a href=\"#step-1\" class=\"headerlink\" title=\"step 1\"></a>step 1</h3><p>用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求,可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败<br>以 TestCase 中的代码片段为例  </p>\n<p>[TestClientRMService.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">public void testReservationAPIs() &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    &#x2F;&#x2F;创建一个预订请求</span><br><span class=\"line\">    Clock clock &#x3D; new UTCClock();</span><br><span class=\"line\">    &#x2F;&#x2F;预订最早开始时间</span><br><span class=\"line\">    long arrival &#x3D; clock.getTime();\t   </span><br><span class=\"line\">    &#x2F;&#x2F;使用时长 </span><br><span class=\"line\">    long duration &#x3D; 60000;    </span><br><span class=\"line\">    &#x2F;&#x2F;预订的最晚结束时间</span><br><span class=\"line\">    long deadline &#x3D; (long) (arrival + 1.05 * duration);</span><br><span class=\"line\">    &#x2F;&#x2F;RDL: Rervation Define Language </span><br><span class=\"line\">    &#x2F;&#x2F;请求 4 个 &lt;1G,1core&gt; container,使用时长是 6000ms,使用开始的最早时间是 arrival,使用结束的最晚内时间是 deadline。</span><br><span class=\"line\">    ReservationSubmissionRequest sRequest &#x3D;</span><br><span class=\"line\">        createSimpleReservationRequest(4, arrival, deadline, duration);</span><br><span class=\"line\">    ReservationSubmissionResponse sResponse &#x3D; null;</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      sResponse &#x3D; clientService.submitReservation(sRequest);</span><br><span class=\"line\">    &#125; catch (Exception e) &#123;</span><br><span class=\"line\">      Assert.fail(e.getMessage());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">private ReservationSubmissionRequest createSimpleReservationRequest(</span><br><span class=\"line\">      int numContainers, long arrival, long deadline, long duration) &#123;</span><br><span class=\"line\">    ReservationRequest r &#x3D;</span><br><span class=\"line\">        ReservationRequest.newInstance(Resource.newInstance(1024, 1),</span><br><span class=\"line\">            numContainers, 1, duration);</span><br><span class=\"line\">            &#x2F;&#x2F;R_ALL 满足所有请求</span><br><span class=\"line\">    ReservationRequests reqs &#x3D;</span><br><span class=\"line\">        ReservationRequests.newInstance(Collections.singletonList(r),</span><br><span class=\"line\">            ReservationRequestInterpreter.R_ALL);</span><br><span class=\"line\">    ReservationDefinition rDef &#x3D;</span><br><span class=\"line\">        ReservationDefinition.newInstance(arrival, deadline, reqs,</span><br><span class=\"line\">            &quot;testClientRMService#reservation&quot;);</span><br><span class=\"line\">    ReservationSubmissionRequest request &#x3D;</span><br><span class=\"line\">        ReservationSubmissionRequest.newInstance(rDef,</span><br><span class=\"line\">            ReservationSystemTestUtil.reservationQ);</span><br><span class=\"line\">    return request;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"step-2\"><a href=\"#step-2\" class=\"headerlink\" title=\"step 2\"></a>step 2</h3><p>ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。   </p>\n<p>[ClientRMService.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public ReservationSubmissionResponse submitReservation(</span><br><span class=\"line\">      ReservationSubmissionRequest request) throws YarnException, IOException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 检查 ReservactionSystem 是否启用</span><br><span class=\"line\">    checkReservationSytem(AuditConstants.SUBMIT_RESERVATION_REQUEST);</span><br><span class=\"line\">    ReservationSubmissionResponse response &#x3D;</span><br><span class=\"line\">        recordFactory.newRecordInstance(ReservationSubmissionResponse.class);</span><br><span class=\"line\">    &#x2F;&#x2F; 通过 AtmicLong 创建全局唯一的ReservationId</span><br><span class=\"line\">    ReservationId reservationId &#x3D; reservationSystem.getNewReservationId();</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步:  校验</span><br><span class=\"line\">    &#x2F;&#x2F; 1.预订请求指定了 queue; 2.指定的 queue 是 reservable queue,即属于 ReservationSystem 管理  </span><br><span class=\"line\">    &#x2F;&#x2F; 2.Reservation Define Language 校验</span><br><span class=\"line\">    &#x2F;&#x2F;\t2a. 空值检验  2b. deadline 不能早于当前时间 2c. ReservationRequests 空值及空集判断  </span><br><span class=\"line\">    &#x2F;&#x2F;\t2d. 分配所有请求的最短时间(R_ANY,R_ALL:取最大;R_ORDER..取和)不能超过 deadline-arrival </span><br><span class=\"line\">    &#x2F;&#x2F;\t2e. 分配所有请求的最大资源量(concurrency * request capacity)不能超过 reservable queue capacity</span><br><span class=\"line\">    Plan plan &#x3D;</span><br><span class=\"line\">        rValidator.validateReservationSubmissionRequest(reservationSystem,</span><br><span class=\"line\">            request, reservationId);</span><br><span class=\"line\">    &#x2F;&#x2F; 校验 ACL\t</span><br><span class=\"line\">    String queueName &#x3D; request.getQueue();</span><br><span class=\"line\">    String user &#x3D;</span><br><span class=\"line\">        checkReservationACLs(queueName,</span><br><span class=\"line\">            AuditConstants.SUBMIT_RESERVATION_REQUEST);</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 第二步:  使用 ReservationAgent 来尝试放置预订请求</span><br><span class=\"line\">      boolean result &#x3D;</span><br><span class=\"line\">          plan.getReservationAgent().createReservation(reservationId, user,</span><br><span class=\"line\">              plan, request.getReservationDefinition());</span><br><span class=\"line\">      if (result) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 同步 reservationSystem &lt;reservationId,queueName&gt;关系</span><br><span class=\"line\">        reservationSystem.setQueueForReservation(reservationId, queueName);</span><br><span class=\"line\">        &#x2F;&#x2F; create the reservation synchronously if required</span><br><span class=\"line\">        &#x2F;&#x2F; 第三步:  如下,若预留的 arrival 时间早于当前(错过) PlanFollower 一个步长,则同步创建</span><br><span class=\"line\">        refreshScheduler(queueName, request.getReservationDefinition(),</span><br><span class=\"line\">            reservationId.toString());</span><br><span class=\"line\">  \t&#x2F;&#x2F; response 中返回放置预订请求的 reservationId</span><br><span class=\"line\">        response.setReservationId(reservationId);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    return response;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   private void refreshScheduler(String planName,</span><br><span class=\"line\">      ReservationDefinition contract, String reservationId) &#123;</span><br><span class=\"line\">    if ((contract.getArrival() - clock.getTime()) &lt; reservationSystem</span><br><span class=\"line\">        .getPlanFollowerTimeStep()) &#123;</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#x2F;&#x2F;后续再说源码</span><br><span class=\"line\">      reservationSystem.synchronizePlan(planName);</span><br><span class=\"line\">      ....</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>在第二步中,RMClientService 委托 Plan(reservable queue 的映射) 绑定的 ReservationAgent 来决定是否放置该预订请求<br>ReservationAgent(RA)将预订请求中的每个 ResourceRequest(RR)作为一个单独的 stage 尝试放置,并在放置时考虑 ReservationRequestInterpreter(RRI)组关系的处理。从最后一个 stage 开始向前依次放置，使用 Plan 已有的预订计划累计的数据和当次预订请求的累计中间数据判断能否放置当前 RR来影响当前 RR放置时间策略并存储最终放置策略结果。   </p>\n<p>[GreedyReservationAgent.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean computeAllocation(ReservationId reservationId, String user,</span><br><span class=\"line\">      Plan plan, ReservationDefinition contract,</span><br><span class=\"line\">      ReservationAllocation oldReservation) throws PlanningException,</span><br><span class=\"line\">      ContractValidationException &#123;</span><br><span class=\"line\">    LOG.info(&quot;placing the following ReservationRequest: &quot; + contract);</span><br><span class=\"line\">    &#x2F;&#x2F; reservable queue capacity </span><br><span class=\"line\">    Resource totalCapacity &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    </span><br><span class=\"line\">   &#x2F;&#x2F; 译文:考虑在此增加逻辑来调整&quot;ResourceDefinition&quot; 来解决系统的缺陷(e.g. 大型容器的调度延迟) </span><br><span class=\"line\">   &#x2F;&#x2F;  TODO: 想不出会在这个位置使用什么样的策略来解决大容器调度延迟问题? 逻辑能转移到常规调度吗</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 使用保守策略规整缩短 [arrival,deadline]区间</span><br><span class=\"line\">    long earliestStart &#x3D; contract.getArrival();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    if (earliestStart % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      earliestStart &#x3D; earliestStart + (step - (earliestStart % step));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    long deadline &#x3D;</span><br><span class=\"line\">        contract.getDeadline() - contract.getDeadline() % plan.getStep();</span><br><span class=\"line\">        </span><br><span class=\"line\">    &#x2F;&#x2F; 将每个 ReservationRequest(RR) 作为一个单独的 stage,设置一些阶段间的临时变量</span><br><span class=\"line\">    long curDeadline &#x3D; deadline;</span><br><span class=\"line\">    long oldDeadline &#x3D; -1;</span><br><span class=\"line\">    &#x2F;&#x2F; 当次预订请求，全部 RR 放置时间策略结果的(中间)数据结构 </span><br><span class=\"line\">    Map&lt;ReservationInterval, ReservationRequest&gt; allocations &#x3D;</span><br><span class=\"line\">        new HashMap&lt;ReservationInterval, ReservationRequest&gt;();</span><br><span class=\"line\">     &#x2F;&#x2F; 当次预订请求，全部 RR 放置造成&lt;时间-资源&gt;变动结果的(中间)数据结构  </span><br><span class=\"line\">    RLESparseResourceAllocation tempAssigned &#x3D;</span><br><span class=\"line\">        new RLESparseResourceAllocation(plan.getResourceCalculator(),</span><br><span class=\"line\">            plan.getMinimumAllocation());</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;ReservationRequest&gt; stages &#x3D; contract.getReservationRequests()</span><br><span class=\"line\">        .getReservationResources();</span><br><span class=\"line\">    ReservationRequestInterpreter type &#x3D; contract.getReservationRequests()</span><br><span class=\"line\">        .getInterpreter();</span><br><span class=\"line\">    &#x2F;&#x2F; 使用迭代器 从最后一个元素之后的 null 向前迭代</span><br><span class=\"line\">    for (ListIterator&lt;ReservationRequest&gt; li &#x3D; </span><br><span class=\"line\">        stages.listIterator(stages.size()); li.hasPrevious();) &#123;</span><br><span class=\"line\">      ReservationRequest currentReservationStage &#x3D; li.previous();</span><br><span class=\"line\">       &#x2F;&#x2F;第一步:  校验 RR</span><br><span class=\"line\">      &#x2F;&#x2F; 1.concurrency &gt; 0; 2.numContainer&gt;0; 3. numContainer 必须是 concurrency 的倍数; 4.单容器大小不得超过maximumAllocation</span><br><span class=\"line\">      validateInput(plan, currentReservationStage, totalCapacity);</span><br><span class=\"line\">      &#x2F;&#x2F;第二步:  尝试分配单个 RR</span><br><span class=\"line\">      Map&lt;ReservationInterval, ReservationRequest&gt; curAlloc &#x3D;</span><br><span class=\"line\">          placeSingleStage(plan, tempAssigned, currentReservationStage,</span><br><span class=\"line\">              earliestStart, curDeadline, oldReservation, totalCapacity);</span><br><span class=\"line\"></span><br><span class=\"line\">      if (curAlloc &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 组关系是 R_ALL,R_ORDER,R_NO_GAP 时，一个 RR 分配失败 导致整个事务失败</span><br><span class=\"line\">        &#x2F;&#x2F; 组关系是 R_ANY,一个 RR 分配失败可以尝试其他 RR </span><br><span class=\"line\">        if (type !&#x3D; ReservationRequestInterpreter.R_ANY) &#123;</span><br><span class=\"line\">          throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">              + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          continue;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">       &#x2F;&#x2F; 可以放置当前 RR(stage)</span><br><span class=\"line\">       </span><br><span class=\"line\">        allocations.putAll(curAlloc);</span><br><span class=\"line\">       &#x2F;&#x2F; 组关系是 R_ANY 的话,可以终止后续 stage 的尝试了</span><br><span class=\"line\">        if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ANY) &#123;</span><br><span class=\"line\">          break;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; 如果组关系是 R_ODER,R_ODER_NO_GAP，则通过设置 curDeadline 来确保 RR 之间的先后关系 </span><br><span class=\"line\">        &#x2F;&#x2F; 将下一个 RR 的 deadline 设置为当前 RR 分配中最早的开始时间</span><br><span class=\"line\">        if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER</span><br><span class=\"line\">            || type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER_NO_GAP) &#123;</span><br><span class=\"line\">          curDeadline &#x3D; findEarliestTime(curAlloc.keySet());</span><br><span class=\"line\"></span><br><span class=\"line\">          &#x2F;&#x2F; 对于 R_ORDER_NO_GAP,确认当前分配和前一个(后向)分配之间没有空隙</span><br><span class=\"line\">          &#x2F;&#x2F; 空隙并非之前理解的完全相等,[t0,t1) 与 [t1,t2)之间的关系,而是不超过 Plan 的时间步长即可 </span><br><span class=\"line\">          if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER_NO_GAP</span><br><span class=\"line\">              &amp;&amp; oldDeadline &gt; 0) &#123;</span><br><span class=\"line\">            if (oldDeadline - findLatestTime(curAlloc.keySet()) &gt; plan</span><br><span class=\"line\">                .getStep()) &#123;</span><br><span class=\"line\">              throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">                  + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          &#x2F;&#x2F; keep the variable oldDeadline pointing to the last deadline we</span><br><span class=\"line\">          &#x2F;&#x2F; found</span><br><span class=\"line\">          oldDeadline &#x3D; curDeadline;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 没有为预订请求分配到&lt;时间,资源&gt;,通过 throws exception 的方式来快速失败</span><br><span class=\"line\">    if (allocations.isEmpty()) &#123;</span><br><span class=\"line\">      throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">          + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#x2F;&#x2F; 第三步:  创建预订</span><br><span class=\"line\">  &#x2F;&#x2F; 在[arrival,earliestTime)之间加入了 &quot;零填充&quot;,以表明其是从arrival time 开始的预订</span><br><span class=\"line\">  &#x2F;&#x2F; TODO:在开头和末尾添加&quot;零填充&quot;的意义是什么?没有的话 会有什么问题  </span><br><span class=\"line\">    ReservationRequest ZERO_RES &#x3D;</span><br><span class=\"line\">        ReservationRequest.newInstance(Resource.newInstance(0, 0), 0);</span><br><span class=\"line\">    long firstStartTime &#x3D; findEarliestTime(allocations.keySet());</span><br><span class=\"line\">    if (firstStartTime &gt; earliestStart) &#123;</span><br><span class=\"line\">      allocations.put(new ReservationInterval(earliestStart,</span><br><span class=\"line\">          firstStartTime), ZERO_RES);</span><br><span class=\"line\">      firstStartTime &#x3D; earliestStart;</span><br><span class=\"line\">      &#x2F;&#x2F; 译: 考虑在[lastEndTime,deadline)添加&quot;零填充&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 译:这有待验证,其他代理可能也在放置(同步问题);有代理不知道的 SharingPolicy</span><br><span class=\"line\">    ReservationAllocation capReservation &#x3D;</span><br><span class=\"line\">        new InMemoryReservationAllocation(reservationId, contract, user,</span><br><span class=\"line\">            plan.getQueueName(), firstStartTime,</span><br><span class=\"line\">            findLatestTime(allocations.keySet()), allocations,</span><br><span class=\"line\">            plan.getResourceCalculator(), plan.getMinimumAllocation());</span><br><span class=\"line\">    &#x2F;&#x2F; 第四步: 新增或更新已有预订分配 。在 step 3 中解释</span><br><span class=\"line\">    if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">      return plan.updateReservation(capReservation);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      return plan.addReservation(capReservation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看上述 第二步:  尝试分配单个 RR<br>这是整个流程中最重要的部分: 决定了用户的每个 RR 能不能放置,放置在哪个时间段,放置几个 container<br>核心思想是: 从 deadline(动态 deadline)开始向后尝试，每次尝试都遍历 duration 时间，判断 duration 时间每个时间步长是否超过了 capacity(即判断 指定大小的资源能不能占用当前时间段，故称”放置”)。然后向后移动一个时间步长，直至剩余时间不足以满足 duration 或者放置了 RR 中所有 container 结束       </p>\n<p>[GreedyReservationAgent.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Map&lt;ReservationInterval, ReservationRequest&gt; placeSingleStage(</span><br><span class=\"line\">      Plan plan, RLESparseResourceAllocation tempAssigned,</span><br><span class=\"line\">      ReservationRequest rr, long earliestStart, long curDeadline,</span><br><span class=\"line\">      ReservationAllocation oldResAllocation, final Resource totalCapacity) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    Map&lt;ReservationInterval, ReservationRequest&gt; allocationRequests &#x3D;</span><br><span class=\"line\">        new HashMap&lt;ReservationInterval, ReservationRequest&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    Resource gang &#x3D; Resources.multiply(rr.getCapability(), rr.getConcurrency());</span><br><span class=\"line\">    long dur &#x3D; rr.getDuration();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    if (dur % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      dur +&#x3D; (step - (dur % step));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 将一个并发作为一个原子性的组分配 </span><br><span class=\"line\">    &#x2F;&#x2F; 每次分配资源量: gang&#x3D;capacity * concurrency </span><br><span class=\"line\">    &#x2F;&#x2F;     共计分配次数: gangsToPlace&#x3D; numContainers&#x2F;concurrency  </span><br><span class=\"line\">    int gangsToPlace &#x3D; rr.getNumContainers() &#x2F; rr.getConcurrency();</span><br><span class=\"line\">    int maxGang &#x3D; 0;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; loop trying to place until we are done, or we are considering</span><br><span class=\"line\">    &#x2F;&#x2F; an invalid range of times</span><br><span class=\"line\">    &#x2F;&#x2F; 循环尝试分配 直至所有的 gang 原子组都分配完或者剩余时间不足以容纳一个 duration</span><br><span class=\"line\">    while (gangsToPlace &gt; 0 &amp;&amp; curDeadline - dur &gt;&#x3D; earliestStart) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; as we run along we remember how many gangs we can fit, and what</span><br><span class=\"line\">      &#x2F;&#x2F; was the most constraining moment in time (we will restart just</span><br><span class=\"line\">      &#x2F;&#x2F; after that to place the next batch)</span><br><span class=\"line\">      maxGang &#x3D; gangsToPlace;</span><br><span class=\"line\">      long minPoint &#x3D; curDeadline;</span><br><span class=\"line\">      int curMaxGang &#x3D; maxGang;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; 尝试在[curDeadline-duration,curDeadline) 长为 duration 的区间内放置未知个 Resource&#x3D;gang 的原子组</span><br><span class=\"line\">      for (long t &#x3D; curDeadline - plan.getStep(); t &gt;&#x3D; curDeadline - dur</span><br><span class=\"line\">          &amp;&amp; maxGang &gt; 0; t &#x3D; t - plan.getStep()) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; 如果之前的预订存在的话,则删除此预订之前的分配(e.g. 更新预订时)</span><br><span class=\"line\">        Resource oldResCap &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        if (oldResAllocation !&#x3D; null) &#123;</span><br><span class=\"line\">          oldResCap &#x3D; oldResAllocation.getResourcesAtTime(t);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#x2F;&#x2F; 计算当前时间点的净可用资源</span><br><span class=\"line\">        Resource netAvailableRes &#x3D; Resources.clone(totalCapacity);</span><br><span class=\"line\">        Resources.addTo(netAvailableRes, oldResCap);</span><br><span class=\"line\">        &#x2F;&#x2F; Plan(reservable queue), 最靠近 t 时刻的累计容量</span><br><span class=\"line\">        Resources.subtractFrom(netAvailableRes,</span><br><span class=\"line\">            plan.getTotalCommittedResources(t)); </span><br><span class=\"line\">        &#x2F;&#x2F;当前预订, 最靠近 t 时刻的累计容量</span><br><span class=\"line\">        Resources.subtractFrom(netAvailableRes,</span><br><span class=\"line\">            tempAssigned.getCapacityAtTime(t));</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#x2F;&#x2F;计算当前时刻能满足的最大数量的 gang</span><br><span class=\"line\">        curMaxGang &#x3D;</span><br><span class=\"line\">            (int) Math.floor(Resources.divide(plan.getResourceCalculator(),</span><br><span class=\"line\">                totalCapacity, netAvailableRes, gang));</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; min&#123;需求的 gang 数量,剩余资源能支撑的 gang 数量&#125;</span><br><span class=\"line\">        curMaxGang &#x3D; Math.min(gangsToPlace, curMaxGang);</span><br><span class=\"line\">\t&#x2F;&#x2F; 记住当前尝试的 duration 内,最小数量的 gang 点(即 队列+reservation 占用资源量最多的点)，作为下次尝试的右边界</span><br><span class=\"line\">        if (curMaxGang &lt;&#x3D; maxGang) &#123;</span><br><span class=\"line\">          maxGang &#x3D; curMaxGang;</span><br><span class=\"line\">          &#x2F;&#x2F;资源最紧俏的时间点。当资源充裕时,放置尽可能多的 gang,也会成为当前最紧俏的时间点，作为下次放置的右边界    </span><br><span class=\"line\">          minPoint &#x3D; t;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; if we were able to place any gang, record this, and decrement</span><br><span class=\"line\">      &#x2F;&#x2F; gangsToPlace maxGang&#x3D;0:没资源了 时间:范围不够了</span><br><span class=\"line\">      &#x2F;&#x2F; 退出上一个循环过程有两种情况: </span><br><span class=\"line\">      &#x2F;&#x2F; 1.  maxGang&#x3D;0,这个 duration 内有一个时间点 净剩余资源不足以放下一个 gang,那么整个 duration 都不能用了</span><br><span class=\"line\">      &#x2F;&#x2F; 2. t&lt;&#x3D;curDeadline - dur 整个 duration 已经遍历完了。此时 maxGang &gt; 0</span><br><span class=\"line\">      &#x2F;&#x2F; 以上退出都有可能,所以需要中间变量 maxGang来做判断。</span><br><span class=\"line\">      if (maxGang &gt; 0) &#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; 能在[curDeadline-duration,curDeadline) 内放置下 maxGang 个gang，即 concurrency * maxGang 个 container</span><br><span class=\"line\">        gangsToPlace -&#x3D; maxGang;</span><br><span class=\"line\">        ReservationInterval reservationInt &#x3D;</span><br><span class=\"line\">            new ReservationInterval(curDeadline - dur, curDeadline);</span><br><span class=\"line\">        ReservationRequest reservationRes &#x3D;</span><br><span class=\"line\">            ReservationRequest.newInstance(rr.getCapability(),</span><br><span class=\"line\">                rr.getConcurrency() * maxGang, rr.getConcurrency(),</span><br><span class=\"line\">                rr.getDuration());</span><br><span class=\"line\">        &#x2F;&#x2F; 记住已占用的空间。后续贴出源码</span><br><span class=\"line\">        &#x2F;&#x2F; reservable queue 对应的 Plan 暂时是只读的，除非能将整个 ReservationRequests 放入到计划中。</span><br><span class=\"line\">        tempAssigned.addInterval(reservationInt, reservationRes);</span><br><span class=\"line\">        allocationRequests.put(reservationInt, reservationRes);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 设置下次放置 gang 的右边界。右边界向右无法再安置一个 gang(如上逻辑所述),尝试在右边界向左放置新的 gang</span><br><span class=\"line\">      curDeadline &#x3D; minPoint;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    if (gangsToPlace &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 放置了所有的 gang </span><br><span class=\"line\">      return allocationRequests;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 不能为当前 ReservationRequest(RR) 在时间&#x2F;容量&#x2F;并发 限制上放置所有的 gang。</span><br><span class=\"line\">      &#x2F;&#x2F; 此次 RR放置失败,如果不是 R_ANY 的话，可以终止整个 预订请求了。</span><br><span class=\"line\">      for (Map.Entry&lt;ReservationInterval, ReservationRequest&gt; tempAllocation :</span><br><span class=\"line\">        allocationRequests.entrySet()) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 清理之前的中间数据。后续贴出源码</span><br><span class=\"line\">        tempAssigned.removeInterval(tempAllocation.getKey(),</span><br><span class=\"line\">            tempAllocation.getValue());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>Plan 中使用 RLESparseResourceAllocation(以 TreeMap 为核心的稀疏数据结构)保存了当前已生效所有Reservation的累积&lt;时间-资源&gt;状态变化图，通过汇总所有 Reservation 的 RR 中每个分配的时间段(startTime,endTime)和资源量 最终得到了 Plan 随时间变化的已分配资源量。对每个 ReservationSubmissionRequest 请求来说 ，在尝试分配所有 RR 的时候，也使用RLESparseResourceAllocation 来保存临时分配好 RR 累积&lt;时间-资源&gt;状态变化图。  </p>\n<p>[RLESparseResourceAllocation.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private TreeMap&lt;Long, Resource&gt; cumulativeCapacity &#x3D; new TreeMap&lt;Long, Resource&gt;();</span><br><span class=\"line\">&#x2F;&#x2F;向 RLESparseResourcAllocation 中增加一个 &lt;时间范围-容量&gt; 的预订</span><br><span class=\"line\">public boolean addInterval(ReservationInterval reservationInterval,</span><br><span class=\"line\">      ReservationRequest capacity) &#123;</span><br><span class=\"line\">    Resource totCap &#x3D;</span><br><span class=\"line\">        Resources.multiply(capacity.getCapability(),</span><br><span class=\"line\">            (float) capacity.getNumContainers());</span><br><span class=\"line\">    if (totCap.equals(ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      long startKey &#x3D; reservationInterval.getStartTime();</span><br><span class=\"line\">      long endKey &#x3D; reservationInterval.getEndTime();</span><br><span class=\"line\">      &#x2F;&#x2F;截取 (?,endKey)之间 一段 TreeMap</span><br><span class=\"line\">      NavigableMap&lt;Long, Resource&gt; ticks &#x3D;</span><br><span class=\"line\">          cumulativeCapacity.headMap(endKey, false);</span><br><span class=\"line\">      if (ticks !&#x3D; null &amp;&amp; !ticks.isEmpty()) &#123;</span><br><span class=\"line\">        Resource updatedCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        &#x2F;&#x2F; 找到时间上小于等于 startKey 的最大 key 的键值对</span><br><span class=\"line\">        Entry&lt;Long, Resource&gt; lowEntry &#x3D; ticks.floorEntry(startKey);</span><br><span class=\"line\">        if (lowEntry &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">\t   &#x2F;&#x2F; 表明 startKey 即为 RLE 全局最小键(最早)</span><br><span class=\"line\">          cumulativeCapacity.put(startKey, totCap);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.add(lowEntry.getValue(), totCap);</span><br><span class=\"line\">          if ((startKey &#x3D;&#x3D; lowEntry.getKey())</span><br><span class=\"line\">              &amp;&amp; (isSameAsPrevious(lowEntry.getKey(), updatedCapacity))) &#123;</span><br><span class=\"line\">             &#x2F;&#x2F; 通过 remove 达到合并区间的。</span><br><span class=\"line\">             &#x2F;&#x2F; e.g. [t0,t1)为 2G, [t1,t2)为 1G;现在在 [t1,t3) 上分配了 1G,那么可以删除 t1 这个点 </span><br><span class=\"line\">            cumulativeCapacity.remove(lowEntry.getKey());</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">             &#x2F;&#x2F;新增 一个时间点的容量 记录</span><br><span class=\"line\">            cumulativeCapacity.put(startKey, updatedCapacity);</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; [startKey,endKey)时间段内的所有已有记录都要增加 totCap 作为最终状态 </span><br><span class=\"line\">        &#x2F;&#x2F; cumulativeCapacity 维护的是全局状态量,而不是增量或者单个 reservation 的数据</span><br><span class=\"line\">        Set&lt;Entry&lt;Long, Resource&gt;&gt; overlapSet &#x3D;</span><br><span class=\"line\">            ticks.tailMap(startKey, false).entrySet(); &#x2F;&#x2F;startKey 已经增加过了 </span><br><span class=\"line\">        for (Entry&lt;Long, Resource&gt; entry : overlapSet) &#123;</span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.add(entry.getValue(), totCap);</span><br><span class=\"line\">          entry.setValue(updatedCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        cumulativeCapacity.put(startKey, totCap);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      Resource nextTick &#x3D; cumulativeCapacity.get(endKey);</span><br><span class=\"line\">      if (nextTick !&#x3D; null) &#123;</span><br><span class=\"line\">        if (isSameAsPrevious(endKey, nextTick)) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 合并 endKey</span><br><span class=\"line\">          cumulativeCapacity.remove(endKey);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; endKey 时还回资源,所以在 endKey 前面记录都加了 totCap,此处会减去  </span><br><span class=\"line\">        cumulativeCapacity.put(endKey, Resources.subtract(cumulativeCapacity</span><br><span class=\"line\">            .floorEntry(endKey).getValue(), totCap));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F; 在 RLESparseResourcAllocation 中删除一个 &lt;时间范围-容量&gt; 的预订</span><br><span class=\"line\">  public boolean removeInterval(ReservationInterval reservationInterval,</span><br><span class=\"line\">      ReservationRequest capacity) &#123;</span><br><span class=\"line\">    Resource totCap &#x3D;</span><br><span class=\"line\">        Resources.multiply(capacity.getCapability(),</span><br><span class=\"line\">            (float) capacity.getNumContainers());</span><br><span class=\"line\">    if (totCap.equals(ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      long startKey &#x3D; reservationInterval.getStartTime();</span><br><span class=\"line\">      long endKey &#x3D; reservationInterval.getEndTime();</span><br><span class=\"line\">      NavigableMap&lt;Long, Resource&gt; ticks &#x3D;</span><br><span class=\"line\">          cumulativeCapacity.headMap(endKey, false);</span><br><span class=\"line\">      &#x2F;&#x2F;取出[startKey,endKey)区间内的所有记录</span><br><span class=\"line\">      SortedMap&lt;Long, Resource&gt; overlapSet &#x3D; ticks.tailMap(startKey);</span><br><span class=\"line\">      if (overlapSet !&#x3D; null &amp;&amp; !overlapSet.isEmpty()) &#123;</span><br><span class=\"line\">        Resource updatedCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        long currentKey &#x3D; -1;</span><br><span class=\"line\">        for (Iterator&lt;Entry&lt;Long, Resource&gt;&gt; overlapEntries &#x3D;</span><br><span class=\"line\">            overlapSet.entrySet().iterator(); overlapEntries.hasNext();) &#123;</span><br><span class=\"line\">          Entry&lt;Long, Resource&gt; entry &#x3D; overlapEntries.next();</span><br><span class=\"line\">          currentKey &#x3D; entry.getKey();</span><br><span class=\"line\">          &#x2F;&#x2F; 在每个时间点的减去该预订已分配的资源量  </span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.subtract(entry.getValue(), totCap);</span><br><span class=\"line\">          cumulativeCapacity.put(currentKey, updatedCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; 左右边界点存在性判断</span><br><span class=\"line\">        Long firstKey &#x3D; overlapSet.firstKey();</span><br><span class=\"line\">        if (isSameAsPrevious(firstKey, overlapSet.get(firstKey))) &#123;</span><br><span class=\"line\">          cumulativeCapacity.remove(firstKey);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if ((currentKey !&#x3D; -1) &amp;&amp; (isSameAsNext(currentKey, updatedCapacity))) &#123;</span><br><span class=\"line\">          cumulativeCapacity.remove(cumulativeCapacity.higherKey(currentKey));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>举个简单例子，为便于讨论:<br>1.容量方面:  reservable queue 有 &lt;2G,2core&gt;的 capacity，所有预订请求中 container 容量都是&lt;1G,1core&gt;，即 queue 最多有 2 个container<br>2.时间方面: 从 t0至 t5，时间单位是 PlanFollower 的时间步长 step(默认是 1s)。t0 为当前时刻<br>3.请求方面: 已有的 reservation 不讨论 arrival,deadline，只给出 ReservationSystem 给出的分配&lt;时间范围-容量&gt;<br>4.ReservationSystem 已有的预订如下  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">预订 Id</th>\n<th align=\"left\">开始时间</th>\n<th align=\"left\">结束时间</th>\n<th align=\"left\">占用时间</th>\n<th align=\"left\">总资源量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">r0</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">1 个单位</td>\n<td align=\"left\">1个 container</td>\n</tr>\n<tr>\n<td align=\"left\">r1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">1个单位</td>\n<td align=\"left\">2个 container</td>\n</tr>\n<tr>\n<td align=\"left\">r2</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">1个单位</td>\n<td align=\"left\">1个 container</td>\n</tr>\n</tbody></table>\n<p>由此可以绘出 Plan 对应的 RLESparseResourceAllocation 中维护的累计&lt;时间-资源&gt;状态变化图  </p>\n<p><img src=\"/img/pictures/reservation/reservationSystem.png\" alt=\"\">  </p>\n<p>即: r2 在 t1 时刻拿到1 个 container,因此图中 t1 时刻已分配资源状态是 1 个 container。<br>      r2 在 t2 时刻释放 1 个 container,r1 拿到两个 2 个 container,因此 t2 时刻已分配资源状态是 2 个 container </p>\n<p> 假设 ReservationSystem接收到一个请求，ReservationDefinition定义的预订请求内容为：\narrival=t0,deadline=t5<br>ReservationRequests(RRS) 中有一个 ReservationRequest(RR)<br>　　capacity&lt;1G,1core&gt;,numContainer=2,concurrency=1,duration=2<br>ReservationRequestInterpreter=R_ALL</p>\n<p>来模拟一下GreedyRerservationAgent#placeSingleStage是如何尝试放置这一个 RR 的<br>初始值:gang=concurrency * capacity=&lt;1G,1core&gt;  gangsToPlace=numContainers/concurrency=2  maxGang=2  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t5</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">2-0-0=2 contianer</td>\n<td align=\"left\">2</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">是</td>\n</tr>\n<tr>\n<td align=\"left\">第二次次循环</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">2-1-0=1 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">否,duration 超限退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束时，maxGang=1，即可以在[t3,t5)这个时间段放下 1 个 gang，在此为 1 个 container，外层循环将此分配封装成&lt;ReservationInterval,ReservationRequest&gt;加入到 当前预订累计分配的中间数据结构中(tmpAssigned:RLESparseResourceAllocation)。  </p>\n<p>此时: gangsToPlace=1,curDeadline=t3,进行下一次外层循环  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">2-2-0=0 contianer</td>\n<td align=\"left\">0</td>\n<td align=\"left\">0</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">否,maxGang=0退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束后，maxGang=0，不能在[t1,t3)时间段放置任何一个 gang。  </p>\n<p>此时: gangsToPlace=1,curDeadline=t2，进行下一次外层循环  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">2-1-0=1 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">是</td>\n</tr>\n<tr>\n<td align=\"left\">第二次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t0</td>\n<td align=\"left\">2-0-0=2 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t0</td>\n<td align=\"left\">否,duration 超限退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束后，maxGang=1，即可以在[t0,t2)时间段放下一个 gang，即 1 个 container，外层循环将此分配封装好之后再次加入到 tmpAssigned 中。   </p>\n<p>此时: gangsToPlace=0终止了外层循环。\n得到的最终结论是: 在[t3,t5)时间段放下一个 container&lt;1G,1core&gt;,在[t0,t2)时间段放下一个 container&lt;1G,1core&gt;。该 RR 整体可以分配，满足组关系 R_ALL，即不考虑用户的情况可以分配该 RR。</p>\n<h3 id=\"step-3\"><a href=\"#step-3\" class=\"headerlink\" title=\"step 3\"></a>step 3</h3><p>SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  </p>\n<p>在 step 2中，ReservationSystem 尝试对预订请求分配时间和资源。若分配成功，则尝试将其加入到 Plan 中或是更新 Plan。    </p>\n<p>[GreedyReservationAgent.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean computeAllocation(ReservationId reservationId, String user,</span><br><span class=\"line\">      Plan plan, ReservationDefinition contract,</span><br><span class=\"line\">      ReservationAllocation oldReservation) throws PlanningException,</span><br><span class=\"line\">      ContractValidationException &#123;</span><br><span class=\"line\">    LOG.info(&quot;placing the following ReservationRequest: &quot; + contract);</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    ReservationAllocation capReservation &#x3D;</span><br><span class=\"line\">        new InMemoryReservationAllocation(reservationId, contract, user,</span><br><span class=\"line\">            plan.getQueueName(), firstStartTime,</span><br><span class=\"line\">            findLatestTime(allocations.keySet()), allocations,</span><br><span class=\"line\">            plan.getResourceCalculator(), plan.getMinimumAllocation());</span><br><span class=\"line\">    if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 更新</span><br><span class=\"line\">      return plan.updateReservation(capReservation);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 新增</span><br><span class=\"line\">      return plan.addReservation(capReservation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>updateReservation 事务性更新 reservation:  先删除旧的reservation，再新增更新的 reservation(当次ReservationSubmissionRequest)。如果新增失败则再把旧的reservation 回滚。 \n所以两处的逻辑的核心在于 addReservation 和 removeReservation。  </p>\n<p>[InMemoryPlan.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   &#x2F;&#x2F;按时间段索引 ReservationAllocation 信息  </span><br><span class=\"line\">private TreeMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt; currentReservations &#x3D;</span><br><span class=\"line\">     new TreeMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt;();</span><br><span class=\"line\">  &#x2F;&#x2F; Plan 整体的&lt;时间-资源&gt;状态变化图</span><br><span class=\"line\"> private RLESparseResourceAllocation rleSparseVector;</span><br><span class=\"line\"> &#x2F;&#x2F; 细分用户的&lt;时间-资源&gt;状态变化图</span><br><span class=\"line\"> private Map&lt;String, RLESparseResourceAllocation&gt; userResourceAlloc &#x3D;</span><br><span class=\"line\">     new HashMap&lt;String, RLESparseResourceAllocation&gt;();</span><br><span class=\"line\"> &#x2F;&#x2F; 按 reservationId 索引ReservationAllocation 信息</span><br><span class=\"line\"> private Map&lt;ReservationId, InMemoryReservationAllocation&gt; reservationTable &#x3D;</span><br><span class=\"line\">     new HashMap&lt;ReservationId, InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">     </span><br><span class=\"line\">public boolean addReservation(ReservationAllocation reservation)</span><br><span class=\"line\">     throws PlanningException &#123;</span><br><span class=\"line\">   InMemoryReservationAllocation inMemReservation &#x3D;</span><br><span class=\"line\">       (InMemoryReservationAllocation) reservation;</span><br><span class=\"line\">   if (inMemReservation.getUser() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     String errMsg &#x3D;</span><br><span class=\"line\">         &quot;The specified Reservation with ID &quot;</span><br><span class=\"line\">             + inMemReservation.getReservationId()</span><br><span class=\"line\">             + &quot; is not mapped to any user&quot;;</span><br><span class=\"line\">     LOG.error(errMsg);</span><br><span class=\"line\">     throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   writeLock.lock();</span><br><span class=\"line\">   try &#123;</span><br><span class=\"line\">     if (reservationTable.containsKey(inMemReservation.getReservationId())) &#123;</span><br><span class=\"line\">       String errMsg &#x3D;</span><br><span class=\"line\">           &quot;The specified Reservation with ID &quot;</span><br><span class=\"line\">               + inMemReservation.getReservationId() + &quot; already exists&quot;;</span><br><span class=\"line\">       LOG.error(errMsg);</span><br><span class=\"line\">       throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     &#x2F;&#x2F; 第一步: 使用 SharingPolicy 校验 Plan 能否接受该 reservation。后续解释</span><br><span class=\"line\">     policy.validate(this, inMemReservation);</span><br><span class=\"line\">     &#x2F;&#x2F; 第二步: 记录接受 reservation 的时间</span><br><span class=\"line\">     reservation.setAcceptanceTimestamp(clock.getTime());</span><br><span class=\"line\">     &#x2F;&#x2F; 第三步: 内存数据结构维护</span><br><span class=\"line\">     ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">         new ReservationInterval(inMemReservation.getStartTime(),</span><br><span class=\"line\">             inMemReservation.getEndTime());</span><br><span class=\"line\">     Set&lt;InMemoryReservationAllocation&gt; reservations &#x3D;</span><br><span class=\"line\">         currentReservations.get(searchInterval);</span><br><span class=\"line\">     if (reservations &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">       reservations &#x3D; new HashSet&lt;InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     if (!reservations.add(inMemReservation)) &#123;</span><br><span class=\"line\">       LOG.error(&quot;Unable to add reservation: &#123;&#125; to plan.&quot;,</span><br><span class=\"line\">           inMemReservation.getReservationId());</span><br><span class=\"line\">       return false;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     currentReservations.put(searchInterval, reservations);</span><br><span class=\"line\">     reservationTable.put(inMemReservation.getReservationId(),</span><br><span class=\"line\">         inMemReservation);</span><br><span class=\"line\">     &#x2F;&#x2F; 第四步: 维护Plan 整体的&lt;时间-资源&gt;状态变化图;维护细分用户&lt;时间-资源&gt;状态变化图  </span><br><span class=\"line\">     incrementAllocation(inMemReservation);</span><br><span class=\"line\">     LOG.info(&quot;Sucessfully added reservation: &#123;&#125; to plan.&quot;,</span><br><span class=\"line\">         inMemReservation.getReservationId());</span><br><span class=\"line\">     return true;</span><br><span class=\"line\">   &#125; finally &#123;</span><br><span class=\"line\">     writeLock.unlock();</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"> private boolean removeReservation(ReservationAllocation reservation) &#123;</span><br><span class=\"line\">   assert (readWriteLock.isWriteLockedByCurrentThread());</span><br><span class=\"line\">   ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">       new ReservationInterval(reservation.getStartTime(),</span><br><span class=\"line\">           reservation.getEndTime());</span><br><span class=\"line\">   Set&lt;InMemoryReservationAllocation&gt; reservations &#x3D;</span><br><span class=\"line\">       currentReservations.get(searchInterval);</span><br><span class=\"line\">    &#x2F;&#x2F; Plan 成员变量维护的信息 维护</span><br><span class=\"line\">   if (reservations !&#x3D; null) &#123;</span><br><span class=\"line\">     if (!reservations.remove(reservation)) &#123;</span><br><span class=\"line\">       LOG.error(&quot;Unable to remove reservation: &#123;&#125; from plan.&quot;,</span><br><span class=\"line\">           reservation.getReservationId());</span><br><span class=\"line\">       return false;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     if (reservations.isEmpty()) &#123;</span><br><span class=\"line\">       currentReservations.remove(searchInterval);</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     String errMsg &#x3D;</span><br><span class=\"line\">         &quot;The specified Reservation with ID &quot; + reservation.getReservationId()</span><br><span class=\"line\">             + &quot; does not exist in the plan&quot;;</span><br><span class=\"line\">     LOG.error(errMsg);</span><br><span class=\"line\">     throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   reservationTable.remove(reservation.getReservationId());</span><br><span class=\"line\">   &#x2F;&#x2F; 1.维护细分用户&lt;时间-资源&gt;状态变化图;维护Plan 整体&lt;时间-资源&gt;状态变化图  </span><br><span class=\"line\">   decrementAllocation(reservation);</span><br><span class=\"line\">   LOG.info(&quot;Sucessfully deleted reservation: &#123;&#125; in plan.&quot;,</span><br><span class=\"line\">       reservation.getReservationId());</span><br><span class=\"line\">   return true;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<p>Plan 在 addReservation 和 updateReservation 时都使用了 SharingPolicy 来校验用户资源使用是否违规。CapacityOverTimePolicy 是默认 SharingPolicy，主要校验两个方面资源使用是否违规：瞬时用量不超过 reservable queue capaciy 的 1%(默认);24h(默认)平均用量不超过reservable queue capacity 的 1%(默认)。上述的 1%和 24h 是源码内常量,无法用配置修改      </p>\n<p>[CapacityOverTimePolicy.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void validate(Plan plan, ReservationAllocation reservation)</span><br><span class=\"line\">      throws PlanningException &#123;</span><br><span class=\"line\">    ReservationAllocation oldReservation &#x3D;</span><br><span class=\"line\">        plan.getReservationById(reservation.getReservationId());</span><br><span class=\"line\">    if (oldReservation !&#x3D; null</span><br><span class=\"line\">        &amp;&amp; !oldReservation.getUser().equals(reservation.getUser())) &#123;</span><br><span class=\"line\">      throw new MismatchedUserException(</span><br><span class=\"line\">          &quot;Updating an existing reservation with mismatched user:&quot;</span><br><span class=\"line\">              + oldReservation.getUser() + &quot; !&#x3D; &quot; + reservation.getUser());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    long startTime &#x3D; reservation.getStartTime();</span><br><span class=\"line\">    long endTime &#x3D; reservation.getEndTime();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    &#x2F;&#x2F; reservable queue capacity </span><br><span class=\"line\">    Resource planTotalCapacity &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    &#x2F;&#x2F; 最大瞬时容量和最大平均容量\t</span><br><span class=\"line\">    Resource maxAvgRes &#x3D; Resources.multiply(planTotalCapacity, maxAvg);</span><br><span class=\"line\">    Resource maxInsRes &#x3D; Resources.multiply(planTotalCapacity, maxInst);</span><br><span class=\"line\">    \t</span><br><span class=\"line\">    IntegralResource runningTot &#x3D; new IntegralResource(0L, 0L);</span><br><span class=\"line\">    &#x2F;&#x2F; 平均(乘法,即积分面积)容量限制</span><br><span class=\"line\">    IntegralResource maxAllowed &#x3D; new IntegralResource(maxAvgRes);</span><br><span class=\"line\">    maxAllowed.multiplyBy(validWindow &#x2F; step);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 检查与该分配有重叠区域且长度为 validWindow(24h)的任何窗口,</span><br><span class=\"line\">    &#x2F;&#x2F;提供给用户的资源是否超过瞬时容量和平均容量限制</span><br><span class=\"line\">    for (long t &#x3D; startTime - validWindow; t &lt; endTime + validWindow; t +&#x3D; step) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">      Resource currExistingAllocTot &#x3D; plan.getTotalCommittedResources(t);</span><br><span class=\"line\">      Resource currExistingAllocForUser &#x3D;</span><br><span class=\"line\">          plan.getConsumptionForUser(reservation.getUser(), t);</span><br><span class=\"line\">      Resource currNewAlloc &#x3D; reservation.getResourcesAtTime(t);</span><br><span class=\"line\">      Resource currOldAlloc &#x3D; Resources.none();</span><br><span class=\"line\">      if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">        currOldAlloc &#x3D; oldReservation.getResourcesAtTime(t);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 所有用户累计 reservation 不能超过 reservable queue  capacity</span><br><span class=\"line\">      Resource inst &#x3D;</span><br><span class=\"line\">          Resources.subtract(Resources.add(currExistingAllocTot, currNewAlloc),</span><br><span class=\"line\">              currOldAlloc);</span><br><span class=\"line\">      if (Resources.greaterThan(plan.getResourceCalculator(),</span><br><span class=\"line\">          planTotalCapacity, inst, planTotalCapacity)) &#123;</span><br><span class=\"line\">        throw new ResourceOverCommitException(&quot; Resources at time &quot; + t</span><br><span class=\"line\">            + &quot; would be overcommitted (&quot; + inst + &quot; over &quot;</span><br><span class=\"line\">            + plan.getTotalCapacity() + &quot;) by accepting reservation: &quot;</span><br><span class=\"line\">            + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; 细分用户 容量不能超过 瞬时容量限制</span><br><span class=\"line\">      if (Resources.greaterThan(plan.getResourceCalculator(),</span><br><span class=\"line\">          planTotalCapacity, Resources.subtract(</span><br><span class=\"line\">              Resources.add(currExistingAllocForUser, currNewAlloc),</span><br><span class=\"line\">              currOldAlloc), maxInsRes)) &#123;</span><br><span class=\"line\">        throw new PlanningQuotaException(&quot;Instantaneous quota capacity &quot;</span><br><span class=\"line\">            + maxInst + &quot; would be passed at time &quot; + t</span><br><span class=\"line\">            + &quot; by accepting reservation: &quot; + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; 相当于增加了 (用户分配容量[已有的+当前reservation更新后的-当前reservation更新前的]) * (一个时间步长)的面积</span><br><span class=\"line\">      &#x2F;&#x2F; 当前reservation更新前的 实际上在 已有的 之中,所以需要减去</span><br><span class=\"line\">      runningTot.add(currExistingAllocForUser);</span><br><span class=\"line\">      runningTot.add(currNewAlloc);</span><br><span class=\"line\">      runningTot.subtract(currOldAlloc);</span><br><span class=\"line\">\t</span><br><span class=\"line\">      &#x2F;&#x2F; 老化掉一个24h窗口之前的那个时间点的面积</span><br><span class=\"line\">      if (t &gt; startTime) &#123;</span><br><span class=\"line\">        Resource pastOldAlloc &#x3D;</span><br><span class=\"line\">            plan.getConsumptionForUser(reservation.getUser(), t - validWindow);</span><br><span class=\"line\">        Resource pastNewAlloc &#x3D; reservation.getResourcesAtTime(t - validWindow);</span><br><span class=\"line\">        runningTot.subtract(pastOldAlloc);</span><br><span class=\"line\">        runningTot.subtract(pastNewAlloc);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (maxAllowed.compareTo(runningTot) &lt; 0) &#123;</span><br><span class=\"line\">        throw new PlanningQuotaException(</span><br><span class=\"line\">            &quot;Integral (avg over time) quota capacity &quot; + maxAvg</span><br><span class=\"line\">                + &quot; over a window of &quot; + validWindow &#x2F; 1000 + &quot; seconds, &quot;</span><br><span class=\"line\">                + &quot; would be passed at time &quot; + t + &quot;(&quot; + new Date(t)</span><br><span class=\"line\">                + &quot;) by accepting reservation: &quot;</span><br><span class=\"line\">                + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上述 CapacityOverTimePolicy 用积分面积的方式判断 是否超过平均容量限制的方式，很容易和 RLESparseResourceAllocation 混淆。  RLESparseResourceAllocation 存储的是&lt;时间点-资源&gt;的状态量,是累积状态量不是增量。而积分面积用的是 RLESparseResourceAllocation 绘成的状态图算的积分面积，每向后移动一个时间步长则增加 (RLE 那个时间点资源量) * (一个时间步长) 的积分面积，并减去(RLE 24h前那个时间点资源量) * (一个时间步长)的积分面积。从而达到随着时间增加，积分面积始终是 RLESparseResourceAllocation 图中 [now-24h,now]这个区间的面积，然后和(平均资源限制容量) * (24h) 来比较。很巧妙，第一次见定积分可以在程序中这么用           </p>\n<h3 id=\"step-4\"><a href=\"#step-4\" class=\"headerlink\" title=\"step 4\"></a>step 4</h3><p>成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据\n参考 “step 1” 的接口定义和 “step 2” ReservationAgent 分配成功后的处理  </p>\n<h3 id=\"step-5\"><a href=\"#step-5\" class=\"headerlink\" title=\"step 5\"></a>step 5</h3><p>PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序   </p>\n<p>PlanFollower(Runnable) 随 ResourceManager 初始化时创建的 ReservationSystem(服务) 初始化而初始化 启动而启动，调用栈如下  </p>\n<p>-&gt;ResourceManager$RMActiveServices#serviceInit<br>　　-&gt;ResourceManager#createReservationSystem<br>　　　　-&gt;AbstractReservationSystem#serviceInit<br>　　　　　　-&gt;AbstractReservationSystem#createPlanFollower  </p>\n<p>[AbstractReservationSystem.java]　</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void serviceInit(Configuration conf) throws Exception &#123;</span><br><span class=\"line\">  Configuration configuration &#x3D; new Configuration(conf);</span><br><span class=\"line\">  &#x2F;&#x2F; 第一步: 初始化所有的 reservable queue 。</span><br><span class=\"line\">  &#x2F;&#x2F; 指定 SharingPolicy,Planner,capacity,planstep...</span><br><span class=\"line\">  reinitialize(configuration, rmContext);</span><br><span class=\"line\">  &#x2F;&#x2F; 第二步: 创建 PlanFollower。加载类:CS 调度器对应 CapacitySchedulerPlanFollower...</span><br><span class=\"line\">  planFollower &#x3D; createPlanFollower();</span><br><span class=\"line\">  if (planFollower !&#x3D; null) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步:初始化 PlanFollower </span><br><span class=\"line\">    planFollower.init(clock, scheduler, plans.values());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  super.serviceInit(conf);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public void serviceStart() throws Exception &#123;</span><br><span class=\"line\">  if (planFollower !&#x3D; null) &#123;</span><br><span class=\"line\">    scheduledExecutorService &#x3D; new ScheduledThreadPoolExecutor(1);</span><br><span class=\"line\">    &#x2F;&#x2F;定时调度,调度周期是 planStepSize(默认1s)</span><br><span class=\"line\">    scheduledExecutorService.scheduleWithFixedDelay(planFollower, 0L,</span><br><span class=\"line\">        planStepSize, TimeUnit.MILLISECONDS);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  super.serviceStart();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>PlanFollower的核心逻辑在 AbstractSchedulerPlanFollower，用于 PlanFollower 与常规调度器同步预订分配的信息，下有 CapacitySchedulerPlanFollower 和 FairCapacityPlanFollower 两个子类。  </p>\n<p>[AbstractSchedulerPlanFollower.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized void run() &#123;</span><br><span class=\"line\">    for (Plan plan : plans) &#123;</span><br><span class=\"line\">      synchronizePlan(plan);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized void synchronizePlan(Plan plan) &#123;</span><br><span class=\"line\">     String planQueueName &#x3D; plan.getQueueName();</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;Running plan follower edit policy for plan: &quot; + planQueueName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    long now &#x3D; clock.getTime();</span><br><span class=\"line\">    if (now % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      now +&#x3D; step - (now % step);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Queue planQueue &#x3D; getPlanQueue(planQueueName);</span><br><span class=\"line\">    if (planQueue &#x3D;&#x3D; null) return;</span><br><span class=\"line\">    </span><br><span class=\"line\">    Resource clusterResources &#x3D; scheduler.getClusterResource();</span><br><span class=\"line\">    &#x2F;&#x2F; plan 对应的 reservable queue 的容量</span><br><span class=\"line\">    Resource planResources &#x3D; getPlanResources(plan, planQueue,</span><br><span class=\"line\">        clusterResources);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的预订(当前时间介于 startTime 和 endTime 之间的所有 Reservation) </span><br><span class=\"line\">    Set&lt;ReservationAllocation&gt; currentReservations &#x3D;</span><br><span class=\"line\">        plan.getReservationsAtTime(now);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的 reservationId 集合</span><br><span class=\"line\">    Set&lt;String&gt; curReservationNames &#x3D; new HashSet&lt;String&gt;();</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效预订的 resource</span><br><span class=\"line\">    Resource reservedResources &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的 reservation;</span><br><span class=\"line\">    int numRes &#x3D; getReservedResources(now, currentReservations,</span><br><span class=\"line\">        curReservationNames, reservedResources);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 创建 reservable queue 的默认队列: xxx-default</span><br><span class=\"line\">    String defReservationId &#x3D; getReservationIdFromQueueName(planQueueName) +</span><br><span class=\"line\">        ReservationConstants.DEFAULT_QUEUE_SUFFIX;</span><br><span class=\"line\">    String defReservationQueue &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">        defReservationId);</span><br><span class=\"line\">    createDefaultReservationQueue(planQueueName, planQueue,</span><br><span class=\"line\">        defReservationId);</span><br><span class=\"line\">    curReservationNames.add(defReservationId);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)</span><br><span class=\"line\">    &#x2F;&#x2F; 如果专用于预订资源的队列骤减(或配置或节点宕机),通过调用 Planner 来删除部分已有预约</span><br><span class=\"line\">    if (arePlanResourcesLessThanReservations(clusterResources, planResources,</span><br><span class=\"line\">        reservedResources)) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        plan.getReplanner().plan(plan, null);</span><br><span class=\"line\">      &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">        LOG.warn(&quot;Exception while trying to replan: &#123;&#125;&quot;, planQueueName, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 第二步: 标记当前预约中，哪些旧的预约需要过期删除，哪些新的预约需要创建队列  </span><br><span class=\"line\">    List&lt;? extends Queue&gt; resQueues &#x3D; getChildReservationQueues(planQueue);</span><br><span class=\"line\">    Set&lt;String&gt; expired &#x3D; new HashSet&lt;String&gt;();</span><br><span class=\"line\">    for (Queue resQueue : resQueues) &#123;</span><br><span class=\"line\">      String resQueueName &#x3D; resQueue.getQueueName();</span><br><span class=\"line\">      String reservationId &#x3D; getReservationIdFromQueueName(resQueueName);</span><br><span class=\"line\">      if (curReservationNames.contains(reservationId)) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 当前活跃的预订包含此此预约，因此不用为它创建对应的队列</span><br><span class=\"line\">        curReservationNames.remove(reservationId);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; 预约已经结束，准备清除</span><br><span class=\"line\">        expired.add(reservationId);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步: 清除过期预约。 getMoveOnExpiry 默认为 true</span><br><span class=\"line\">    cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,</span><br><span class=\"line\">        defReservationQueue);</span><br><span class=\"line\"></span><br><span class=\"line\">    float totalAssignedCapacity &#x3D; 0f;</span><br><span class=\"line\">    if (currentReservations !&#x3D; null) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 释放默认队列中的所有多余容量  </span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        setQueueEntitlement(planQueueName, defReservationQueue, 0f, 1.0f);</span><br><span class=\"line\">      &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">        LOG.warn(</span><br><span class=\"line\">            &quot;Exception while trying to release default queue capacity for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">            planQueueName, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; 第四步: 增加新的预约或更新已有的预约 对应的队列配额</span><br><span class=\"line\">       &#x2F;&#x2F; 4.1 首先按照(当前时间待分配的容量 - 已有的容量)从小到大(从负到正)的方式排序一次调整 capacity。</span><br><span class=\"line\">       &#x2F;&#x2F; 这种排序方式用以避免分配过程中瞬时容量超出 100%的 capacity(猜测可能是为了减少干扰并发预订和抢占介入)</span><br><span class=\"line\">      List&lt;ReservationAllocation&gt; sortedAllocations &#x3D;</span><br><span class=\"line\">          sortByDelta(</span><br><span class=\"line\">              new ArrayList&lt;ReservationAllocation&gt;(currentReservations), now,</span><br><span class=\"line\">              plan);</span><br><span class=\"line\">      &#x2F;&#x2F; 4.2 为每个队列设置或更新 capacity</span><br><span class=\"line\">      for (ReservationAllocation res : sortedAllocations) &#123;</span><br><span class=\"line\">        String currResId &#x3D; res.getReservationId().toString();</span><br><span class=\"line\">        if (curReservationNames.contains(currResId)) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; 每个reservationId 对应着常规调度器中 PlanQueue 下的一个 ReservationQueue</span><br><span class=\"line\">          addReservationQueue(planQueueName, planQueue, currResId);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; 当前生效的 ResourceAllocation 需要(或RS已分配给Reservation)的 capacity</span><br><span class=\"line\">        Resource capToAssign &#x3D; res.getResourcesAtTime(now);</span><br><span class=\"line\">        float targetCapacity &#x3D; 0f;</span><br><span class=\"line\">        if (planResources.getMemory() &gt; 0</span><br><span class=\"line\">            &amp;&amp; planResources.getVirtualCores() &gt; 0) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; 绝对值式的容量相对 PlanQueue capacity 计算相对容量，capToAssign&#x2F;planResources</span><br><span class=\"line\">          targetCapacity &#x3D;</span><br><span class=\"line\">              calculateReservationToPlanRatio(clusterResources,</span><br><span class=\"line\">                  planResources,</span><br><span class=\"line\">                  capToAssign);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">          LOG.debug(</span><br><span class=\"line\">              &quot;Assigning capacity of &#123;&#125; to queue &#123;&#125; with target capacity &#123;&#125;&quot;,</span><br><span class=\"line\">              capToAssign, currResId, targetCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; set maxCapacity to 100% unless the job requires gang, in which</span><br><span class=\"line\">        &#x2F;&#x2F; case we stick to capacity (as running early&#x2F;before is likely a</span><br><span class=\"line\">        &#x2F;&#x2F; waste of resources)</span><br><span class=\"line\">        &#x2F;&#x2F; 设置最大配额。感觉是无效代码，不明白为什么要设置最大配额 都为1不行吗，为什么按有无并发来设置最大配额  </span><br><span class=\"line\">        float maxCapacity &#x3D; 1.0f;</span><br><span class=\"line\">        if (res.containsGangs()) &#123;</span><br><span class=\"line\">          maxCapacity &#x3D; targetCapacity;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 设置capacity 和 maxCapacity</span><br><span class=\"line\">          setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);</span><br><span class=\"line\">        &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">          LOG.warn(&quot;Exception while trying to size reservation for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">              currResId, planQueueName, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        totalAssignedCapacity +&#x3D; targetCapacity;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第五步: 设置 default queue 的配额(PlanQueue 的剩余配额)</span><br><span class=\"line\">    float defQCap &#x3D; 1.0f - totalAssignedCapacity;</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;PlanFollowerEditPolicyTask: total Plan Capacity: &#123;&#125; &quot;</span><br><span class=\"line\">          + &quot;currReservation: &#123;&#125; default-queue capacity: &#123;&#125;&quot;, planResources,</span><br><span class=\"line\">          numRes, defQCap);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);</span><br><span class=\"line\">    &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">      LOG.warn(</span><br><span class=\"line\">          &quot;Exception while trying to reclaim default queue capacity for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">          planQueueName, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第六步: 清理过期的预订记录(now-endTime&gt;24h)</span><br><span class=\"line\">    &#x2F;&#x2F; garbage collect finished reservations from plan</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      plan.archiveCompletedReservations(now);</span><br><span class=\"line\">    &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">      LOG.error(&quot;Exception in archiving completed reservations: &quot;, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Finished iteration of plan follower edit policy for plan: &quot;</span><br><span class=\"line\">        + planQueueName);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Extension: update plan with app states,</span><br><span class=\"line\">    &#x2F;&#x2F; useful to support smart replanning</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>先看 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)<br>可用于兼容集群/队列资源骤减的情况，策略是删除最晚接受的部分预约。<br>[SimpleCapacityReplanner.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void plan(Plan plan, List&lt;ReservationDefinition&gt; contracts)</span><br><span class=\"line\">      throws PlanningException &#123;</span><br><span class=\"line\">    if (contracts !&#x3D; null) &#123;</span><br><span class=\"line\">      throw new RuntimeException(</span><br><span class=\"line\">          &quot;SimpleCapacityReplanner cannot handle new reservation contracts&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ResourceCalculator resCalc &#x3D; plan.getResourceCalculator();</span><br><span class=\"line\">    Resource totCap &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    long now &#x3D; clock.getTime();</span><br><span class=\"line\">    &#x2F;&#x2F; lengthOfCheckZone&#x3D;1h </span><br><span class=\"line\">    for (long t &#x3D; now; (t &lt; plan.getLastEndTime() &amp;&amp; t &lt; (now + lengthOfCheckZone)); t +&#x3D;</span><br><span class=\"line\">        plan.getStep()) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 使用 Plan 所有预订的总资源量 - Plan 的 capacity 来检查是否超限</span><br><span class=\"line\">      Resource excessCap &#x3D;</span><br><span class=\"line\">          Resources.subtract(plan.getTotalCommittedResources(t), totCap);</span><br><span class=\"line\">      if (Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 按照 ReservationAllocation.acceptedAt 做比较，最新接受的在前</span><br><span class=\"line\">        Set&lt;ReservationAllocation&gt; curReservations &#x3D;</span><br><span class=\"line\">            new TreeSet&lt;ReservationAllocation&gt;(plan.getReservationsAtTime(t));</span><br><span class=\"line\">        for (Iterator&lt;ReservationAllocation&gt; resIter &#x3D;</span><br><span class=\"line\">            curReservations.iterator(); resIter.hasNext()</span><br><span class=\"line\">            &amp;&amp; Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE);) &#123;</span><br><span class=\"line\">          ReservationAllocation reservation &#x3D; resIter.next();</span><br><span class=\"line\">          &#x2F;&#x2F; 使用 deleteReservation 来调用 Plan.removeReservation 来删除预订。</span><br><span class=\"line\">          &#x2F;&#x2F; removeReservation 的源码分析 见&quot;step 3&quot;</span><br><span class=\"line\">          plan.deleteReservation(reservation.getReservationId());</span><br><span class=\"line\">          excessCap &#x3D;</span><br><span class=\"line\">              Resources.subtract(excessCap, reservation.getResourcesAtTime(t));</span><br><span class=\"line\">          LOG.info(&quot;Removing reservation &quot; + reservation.getReservationId()</span><br><span class=\"line\">              + &quot; to repair physical-resource constraints in the plan: &quot;</span><br><span class=\"line\">              + plan.getQueueName());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>第二步略，第三步: 清除过期预约。<br>[AbstractSchedulerPlanFollower.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void cleanupExpiredQueues(String planQueueName,</span><br><span class=\"line\">      boolean shouldMove, Set&lt;String&gt; toRemove, String defReservationQueue) &#123;</span><br><span class=\"line\">    for (String expiredReservationId : toRemove) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        String expiredReservation &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">            expiredReservationId);</span><br><span class=\"line\">         &#x2F;&#x2F; 将过期预约对应的 ReservationQueue 的 capacity 和 maxCapacity 设置为 0 </span><br><span class=\"line\">        setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);</span><br><span class=\"line\">        &#x2F;&#x2F; 默认配置为 true。将其中的 APP 移动到同级的 default 队列中(xxx-default)。</span><br><span class=\"line\">        if (shouldMove) &#123;</span><br><span class=\"line\">          moveAppsInQueueSync(expiredReservation, defReservationQueue);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (scheduler.getAppsInQueue(expiredReservation).size() &gt; 0) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 移走所有已有 APP 的情况,还有新提交的。暂时保留</span><br><span class=\"line\">          scheduler.killAllAppsInQueue(expiredReservation);</span><br><span class=\"line\">          LOG.info(&quot;Killing applications in queue: &#123;&#125;&quot;, expiredReservation);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 删除队列  </span><br><span class=\"line\">          scheduler.removeQueue(expiredReservation);</span><br><span class=\"line\">          LOG.info(&quot;Queue: &quot; + expiredReservation + &quot; removed&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">        LOG.warn(&quot;Exception while trying to expire reservation: &#123;&#125;&quot;,</span><br><span class=\"line\">            expiredReservationId, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第四步 先调用 sortByDelta 方法使用 ReservationAllocationComparator 对所有正在生效的预订对应的 ReservationQueue 做排序，然后对所有预订通过 setQueueEntitlement 设置或修正 capacity 和 maxCapacity。<br>第五步 通过 setQueueEntitlement 设置 default queue 的 capacity 和 maxCapacity，占有所有剩余资源<br>[ReservationAllocationComparator.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    @Override</span><br><span class=\"line\">    public int compare(ReservationAllocation lhs, ReservationAllocation rhs) &#123;</span><br><span class=\"line\">      Resource lhsRes &#x3D; getUnallocatedReservedResources(lhs);</span><br><span class=\"line\">      Resource rhsRes &#x3D; getUnallocatedReservedResources(rhs);</span><br><span class=\"line\">      &#x2F;&#x2F; 两个 Resource 先比内存再比虚拟核，小的在前。</span><br><span class=\"line\">      return lhsRes.compareTo(rhsRes);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private Resource getUnallocatedReservedResources(</span><br><span class=\"line\">        ReservationAllocation reservation) &#123;</span><br><span class=\"line\">      Resource resResource;</span><br><span class=\"line\">      &#x2F;&#x2F; 查询常规调度器，获取现有 capacity</span><br><span class=\"line\">      Resource reservationResource &#x3D; planFollower</span><br><span class=\"line\">          .getReservationQueueResourceIfExists</span><br><span class=\"line\">              (plan, reservation.getReservationId());</span><br><span class=\"line\">      if (reservationResource !&#x3D; null) &#123;</span><br><span class=\"line\">        resResource &#x3D;</span><br><span class=\"line\">            Resources.subtract(</span><br><span class=\"line\">            &#x2F;&#x2F; 当前时刻需要的 capacity。可能为负</span><br><span class=\"line\">                reservation.getResourcesAtTime(now),</span><br><span class=\"line\">                reservationResource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        resResource &#x3D; reservation.getResourcesAtTime(now);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return resResource;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[AbstractSchedulerPlanFollower.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void setQueueEntitlement(String planQueueName, String currResId,</span><br><span class=\"line\">      float targetCapacity,</span><br><span class=\"line\">      float maxCapacity) throws YarnException &#123;</span><br><span class=\"line\">    String reservationQueueName &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">        currResId);</span><br><span class=\"line\">    scheduler.setEntitlement(reservationQueueName, new QueueEntitlement(</span><br><span class=\"line\">        targetCapacity, maxCapacity));</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void setEntitlement(String inQueue,</span><br><span class=\"line\">      QueueEntitlement entitlement) throws SchedulerDynamicEditException,</span><br><span class=\"line\">      YarnException &#123;</span><br><span class=\"line\">    LeafQueue queue &#x3D; getAndCheckLeafQueue(inQueue);</span><br><span class=\"line\">    ParentQueue parent &#x3D; (ParentQueue) queue.getParent();</span><br><span class=\"line\">    if (!(queue instanceof ReservationQueue)) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(&quot;Entitlement can not be&quot;</span><br><span class=\"line\">          + &quot; modified dynamically since queue &quot; + inQueue</span><br><span class=\"line\">          + &quot; is not a ReservationQueue&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (!(parent instanceof PlanQueue)) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(&quot;The parent of ReservationQueue &quot;</span><br><span class=\"line\">          + inQueue + &quot; must be an PlanQueue&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ReservationQueue newQueue &#x3D; (ReservationQueue) queue;</span><br><span class=\"line\">    float sumChilds &#x3D; ((PlanQueue) parent).sumOfChildCapacities();</span><br><span class=\"line\">    &#x2F;&#x2F; 计算修改后, 所有同级队列的容量之和</span><br><span class=\"line\">    float newChildCap &#x3D; sumChilds - queue.getCapacity() + entitlement.getCapacity();</span><br><span class=\"line\"></span><br><span class=\"line\">    if (newChildCap &gt;&#x3D; 0 &amp;&amp; newChildCap &lt; 1.0f + CSQueueUtils.EPSILON) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 设置前后的 capacity 和 maxCapacity 都一样则 不设置</span><br><span class=\"line\">      if (Math.abs(entitlement.getCapacity() - queue.getCapacity()) &#x3D;&#x3D; 0</span><br><span class=\"line\">          &amp;&amp; Math.abs(entitlement.getMaxCapacity() - queue.getMaximumCapacity()) &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">        return;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;设置 ReservationQueue 的 capacity 和 maxCapacity </span><br><span class=\"line\">      newQueue.setEntitlement(entitlement);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(</span><br><span class=\"line\">          &quot;Sum of child queues would exceed 100% for PlanQueue: &quot;</span><br><span class=\"line\">              + parent.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Set entitlement for ReservationQueue &quot; + inQueue + &quot;  to &quot;</span><br><span class=\"line\">        + queue.getCapacity() + &quot; request was (&quot; + entitlement.getCapacity() + &quot;)&quot;);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[ReservationQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void setEntitlement(QueueEntitlement entitlement)</span><br><span class=\"line\">      throws SchedulerDynamicEditException &#123;</span><br><span class=\"line\">    float capacity &#x3D; entitlement.getCapacity();</span><br><span class=\"line\">    if (capacity &lt; 0 || capacity &gt; 1.0f) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(</span><br><span class=\"line\">          &quot;Capacity demand is not in the [0,1] range: &quot; + capacity);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setCapacity(capacity);</span><br><span class=\"line\">    setAbsoluteCapacity(getParent().getAbsoluteCapacity() * getCapacity());</span><br><span class=\"line\">    setMaxCapacity(entitlement.getMaxCapacity());</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;successfully changed to &quot; + capacity + &quot; for queue &quot;</span><br><span class=\"line\">          + this.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>第六步: 清理过期的预订记录<br>[InMemoryPlan.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void archiveCompletedReservations(long tick) &#123;</span><br><span class=\"line\">    LOG.debug(&quot;Running archival at time: &#123;&#125;&quot;, tick);</span><br><span class=\"line\">    List&lt;InMemoryReservationAllocation&gt; expiredReservations &#x3D;</span><br><span class=\"line\">        new ArrayList&lt;InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">    readLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 删除过期的保留记录  </span><br><span class=\"line\">      long archivalTime &#x3D; tick - policy.getValidWindow();</span><br><span class=\"line\">      ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">          new ReservationInterval(archivalTime, archivalTime);</span><br><span class=\"line\">      SortedMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt; reservations &#x3D;</span><br><span class=\"line\">          currentReservations.headMap(searchInterval, true);</span><br><span class=\"line\">      if (!reservations.isEmpty()) &#123;</span><br><span class=\"line\">        for (Set&lt;InMemoryReservationAllocation&gt; reservationEntries : reservations</span><br><span class=\"line\">            .values()) &#123;</span><br><span class=\"line\">          for (InMemoryReservationAllocation reservation : reservationEntries) &#123;</span><br><span class=\"line\">            &#x2F;&#x2F; 结束时间早于当前时间24h </span><br><span class=\"line\">            if (reservation.getEndTime() &lt;&#x3D; archivalTime) &#123;</span><br><span class=\"line\">              expiredReservations.add(reservation);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      readLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (expiredReservations.isEmpty()) &#123;</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      for (InMemoryReservationAllocation expiredReservation : expiredReservations) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; Plan.removeReservation 源码分析见&quot;step 3&quot;,主要是内存数据结构的维护</span><br><span class=\"line\">        removeReservation(expiredReservation);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>　　　　</p>\n<h3 id=\"step-6\"><a href=\"#step-6\" class=\"headerlink\" title=\"step 6\"></a>step 6</h3><p>用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue)   </p>\n<p>参见 ApplicationSubmissionContext 的数据结构  </p>\n<h3 id=\"step-7\"><a href=\"#step-7\" class=\"headerlink\" title=\"step 7\"></a>step 7</h3><p>常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">    switch(event.getType()) &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    case APP_ADDED:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      AppAddedSchedulerEvent appAddedEvent &#x3D; (AppAddedSchedulerEvent) event;</span><br><span class=\"line\">      &#x2F;&#x2F; 解析 APP 提交的队列</span><br><span class=\"line\">      String queueName &#x3D;</span><br><span class=\"line\">          resolveReservationQueueName(appAddedEvent.getQueue(),</span><br><span class=\"line\">              appAddedEvent.getApplicationId(),</span><br><span class=\"line\">              appAddedEvent.getReservationID());</span><br><span class=\"line\">      if (queueName !&#x3D; null) &#123;</span><br><span class=\"line\">        if (!appAddedEvent.getIsAppRecovering()) &#123;</span><br><span class=\"line\">          addApplication(appAddedEvent.getApplicationId(), queueName,</span><br><span class=\"line\">              appAddedEvent.getUser());</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,</span><br><span class=\"line\">              appAddedEvent.getUser());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private synchronized String resolveReservationQueueName(String queueName,</span><br><span class=\"line\">      ApplicationId applicationId, ReservationId reservationID) &#123;</span><br><span class=\"line\">    CSQueue queue &#x3D; getQueue(queueName);</span><br><span class=\"line\">    if ((queue &#x3D;&#x3D; null) || !(queue instanceof PlanQueue)) &#123;</span><br><span class=\"line\">      return queueName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (reservationID !&#x3D; null) &#123;</span><br><span class=\"line\">      String resQName &#x3D; reservationID.toString();</span><br><span class=\"line\">      &#x2F;&#x2F; 根据 reservationId 获取对应的叶子队列</span><br><span class=\"line\">      queue &#x3D; getQueue(resQName);</span><br><span class=\"line\">      if (queue &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        String message &#x3D;</span><br><span class=\"line\">            &quot;Application &quot;</span><br><span class=\"line\">                + applicationId</span><br><span class=\"line\">                + &quot; submitted to a reservation which is not yet currently active: &quot;</span><br><span class=\"line\">                + resQName;</span><br><span class=\"line\">        this.rmContext.getDispatcher().getEventHandler()</span><br><span class=\"line\">            .handle(new RMAppEvent(applicationId,</span><br><span class=\"line\">                RMAppEventType.APP_REJECTED, message));</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (!queue.getParent().getQueueName().equals(queueName)) &#123;</span><br><span class=\"line\">        String message &#x3D;</span><br><span class=\"line\">            &quot;Application: &quot; + applicationId + &quot; submitted to a reservation &quot;</span><br><span class=\"line\">                + resQName + &quot; which does not belong to the specified queue: &quot;</span><br><span class=\"line\">                + queueName;</span><br><span class=\"line\">        this.rmContext.getDispatcher().getEventHandler()</span><br><span class=\"line\">            .handle(new RMAppEvent(applicationId,</span><br><span class=\"line\">                RMAppEventType.APP_REJECTED, message));</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">     &#x2F;&#x2F; 使用 ReservationId 对应的 ReservationQueue 来运行任务, 调度容器....</span><br><span class=\"line\">      queueName &#x3D; resQName;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 如果指定了 PlanQueue 但是没指定 reservationId, 会提交到 PlanQueue 的 default 队列执行</span><br><span class=\"line\">      queueName &#x3D; queueName + ReservationConstants.DEFAULT_QUEUE_SUFFIX;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return queueName;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"step-8\"><a href=\"#step-8\" class=\"headerlink\" title=\"step 8\"></a>step 8</h3><p>预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用<br>参考”step 5”中 AbstractSchedulerPlanFollower#synchronizePlan 方法中对 plan.getReplanner().plan(plan, null) 的调用和cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue)的逻辑  </p>\n<p>官网解释可能不实，原文如下:<br>The system includes mechanisms to adapt to drop in cluster capacity. This consists in replanning by “moving” the reservation if possible, or rejecting the smallest amount of previously accepted reservation (to ensure that other reservation will receive their full amount).</p>\n<p>但在 hadoop-2.7.3 和 hadoop-3.2.0的 trunk 分支上 AbstractSchedulerPlanFollower/Planner(只有 SimpleReplanner 实现类)逻辑没有改动，且无相应逻辑  </p>\n<h2 id=\"QA\"><a href=\"#QA\" class=\"headerlink\" title=\"QA\"></a>QA</h2><blockquote>\n</blockquote>\n<ol>\n<li>Q: reservable queue 和 leaf queue 区别<br> A: 其一，只有 leaf queue 才能被设置为 reservable queue，通过对 leaf queue 设置 yarn.scheduler.capacity.&lt;queue-path&gt;.reservable=true。其二，在资源管理的层级体系中 reservable queue 对应的数据结构是 PlanQueue extend ParentQueue。即实际上，reservable queue 是在 leaf queue上做了配置的 ParentQueue。  </li>\n<li>Q: PlanQueue 和 ReservationQueue 区别<br> A: PlanQueue 对应一个 reservable queue，ReservationQueue 对应一个 ReservationId，PlanQueue 下面可以创建任意个 ReservationQueue; PlanQueue 是 ParentQueue，ReservationQueue 是 LeafQueue。      </li>\n<li>Q: 在名称为 X 的 reservable queue 上预订了资源，reservationId=reservationId_001,提交APP 时 Queue 名字填什么？<br> A: Queue 名字填 X,且需要在 ApplicationSubmissionContext 中设置 reservationId。常规调度器会根据 reservationId 解析出对应的 ReservationQueue(名称与 ReservationId 相同)。  </li>\n<li>Q: 在1:00-2:00 预约了&lt;100G,10core&gt;的资源 30 分钟，但是在 1:00前或者 2:00后提交任务 会怎样?<br>A:  常规调度器会拒绝任务(APP_REJECT)。因为找不到 reservationId 对应的 ReservationQueue,早于1:00时队列还没创建,晚于2:00时队列被删除。最好在1:00提交，可能会有等待时间(RS 调度时从deadline 向 arrival 尝试分配)       </li>\n<li>Q: 任务运行在哪里?<br>A: 任务运行在与 reservationId 同名的 ReservationQueue 中,如果预约时间过去但是 APP 还没运行完成则默认移动 APP 到与 ReservationQueue 同级的 default 队列中  </li>\n<li>Q: 能不能把 reservable queue 当做 leaf queue 用? 每个任务都不指定 ReservationId。<br> A: 可以。提交到 reservable queue 但是不指定 ReservationId 的任务都会被移动到 reservable queue 下的 default 队列运行。</li>\n<li>Q: 提交到 reservable queue 中的 app 有的指定了 ReservationId,有的没指定。运行时有什么影响?<br> A: PlanQueue 优先为有 ReservationId 队列分配资源,其次再为 default 队列分配资源，即只能使用那些被预订后剩余的资源，在资源紧张时 default 队列无资源或被其他任意 ReservationQueue 抢占。(这是由 ReservationSystem 提供预订的逻辑决定的。1.不预约的资源不在ReservationSystem 管理下,请求预约时 ReservationSystem 会认为自身管理 PlanQueue 所有资源都能预约出去  2.为所有预约分配完资源之后剩下的才归属 default 队列所有)  </li>\n<li>Q: Spark On Yarn 如何使用 ReservationSystem?<br> A: 分两个阶段。第一,使用 Client-RM 协议提交 ReservationSubmissionRequest,预订资源并保存回执的 ReservationId。第二，在 Spark On Yarn 程序中指定 ReservationId 字段，但是此阶段 Spark On Yarn 提交程序尚不完善需要修改源代码。目前 Spark On Yarn 程序使用 createApplicationSubmissionContext 方法来封装 ApplicationSubmissionContext 时, 没有针对 ReservationId 的逻辑。如果有需求的话，可以通过 SparkConf 来传递 ReservationId 字符串,在createApplicationSubmissionContext做解析设置 。    <ol start=\"9\">\n<li>Q: ReservationSystem 的适用场景<br>A: 1.资源紧张时需要保证重要生产任务运行,可以使用预订资源的方式 。ReservationSystem 在 PlanQueue 下创建 ReservationQueue，将预订需要的绝对值资源量转化 ReservationQueue 在当前时刻的capacity 和 maxCapacity，将任务在此队列中调度，本质上还是 LeafQueue。绝对资源量转化成百分比资源量擦掉了容器数量和大小，不修改开源抢占调度逻辑 不密集抢占抢占的情况下，不能解决大容器调度的问题。  </li>\n</ol>\n</li>\n</ol>\n<h2 id=\"配置预订系统\"><a href=\"#配置预订系统\" class=\"headerlink\" title=\"配置预订系统\"></a>配置预订系统</h2><p>目前可以在 yarn-site.xml 中配置开启ReservationSystem,可以在CapacityScheduler 及 FairScheduler中增加对预订的支持，方式是在capacity-scheduler.xml 或者 fair-scheduler.xml中的任何 LeafQueue的”reservable”属性标记为 true，然后该队列的配额就可以用于预订。即使没有预订资源，应用程序仍然可以被提交到该队列上，它们将以”best-effort”的模式运行在 预订资源中运行的作业 剩下的容量中。  </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html</a><br><a href=\"https://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us\" target=\"_blank\" rel=\"noopener\">https://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us</a><br><a href=\"http://gitbook.net/java/util/java_util_treemap.html\" target=\"_blank\" rel=\"noopener\">http://gitbook.net/java/util/java_util_treemap.html</a>  </p>\n","site":{"data":{}},"excerpt":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>ReservationSystem 是 YARN ResourceManager 的组件。YARN 的 ReservationSystem 为用户提供了提前保留资源的能力,以确保重要的生产作业可预测地运行。ReservationSystem 执行仔细的准入控制，并保证绝对资源量(而不是群集大小的百分比)。保留具有组和并发的语义，并且可以有随时间变化的资源需求。  </p>","more":"<h2 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h2><p><img src=\"/img/pictures/reservation/yarn_reservation_system.png\" alt=\"\">  </p>\n<p>参考上图，预订资源的工作流程如下：  </p>\n<ul>\n<li>step 0: 用户提交预订创建请求，并收到包含 ReservationId 的响应。  </li>\n<li>step 1: 用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。这描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求。这可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败</li>\n<li>step 2: ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。  </li>\n<li>step 3: SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  </li>\n<li>step 4: 成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据    </li>\n<li>step 5: PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序 </li>\n<li>step 6: 用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue) </li>\n<li>step 7: 常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   </li>\n<li>step 8: 预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用以重建计划  </li>\n</ul>\n<p>注: step 8和官网解释不同。官网解释可能不实,参见源码解析-“step 8”</p>\n<h2 id=\"源码解析\"><a href=\"#源码解析\" class=\"headerlink\" title=\"源码解析\"></a>源码解析</h2><h3 id=\"涉及的类\"><a href=\"#涉及的类\" class=\"headerlink\" title=\"涉及的类\"></a>涉及的类</h3><ul>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem<br>该类继承了AbstractService,实现了ReservationSystem 并封装了Capacity/Fair 调度器下预订系统的核心实现<br>主要功能是:作为服务启动时加载配置文件中配置的 reservable LeafQueue 转换为 Plan;管理 PlanFollower,确保 Plan与常规调度器的同步  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">setRMContext</td>\n<td align=\"center\">保存 rmContext 指针</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">reinitialize</td>\n<td align=\"center\">重新初始化 ReservationSystem</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getPlan</td>\n<td align=\"center\">获取已被加载的 Plan</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getAllPlans</td>\n<td align=\"center\">获取已被加载的所有 Plan</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">synchronizePlan</td>\n<td align=\"center\">使用 PlanFollower 同步 Plan 与常规调度器</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getPlanFollowerTimeStep</td>\n<td align=\"center\">PlanFollower 步长</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getNewReservationId</td>\n<td align=\"center\">获取一个全局唯一的reservationId</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getQueueForReservation</td>\n<td align=\"center\">获取reservationId关联的队列</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">setQueueForReservation</td>\n<td align=\"center\">为reservationId关联队列</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower<br>该类实现了 PlanFollower与 Runnable 接口<br>主要功能是:周期性同步常规调度器与 Plan。通过将计划中的每个预订的当前资源映射到常规调度器(e.g. 队列的调整能力,设置池权重,调整应用优先级)，来影响调度器的资源分配 进而达到 保证作业与Plan 中预订一致的方式来使用资源。一个关键概念是将预订的绝对值式资源转换为队列的优先级和容量。PlanFollower 也会向 Plan 来同步集群总资源的变化使其作出相应的调整。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"left\">通过 SystemClock,ResourceSchduler 和Plans 来初始化PlanFollower</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">synchronizePlan</td>\n<td align=\"left\">同步指定 Plan和常规调度器</td>\n<td align=\"left\">周期性调用;时间紧迫时同步阻塞调用</td>\n</tr>\n<tr>\n<td align=\"left\">setPlans</td>\n<td align=\"left\">重置 PlanFollower 同步的 Plan 集合</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan<br>该接口实现了 Plan 接口,Plan 接口继承了PlanContext, PlanView, PlanEdit。实现只有 InMemoryPlan<br>Plan 代表着预订系统的核心数据结构,维护着集群资源的工作安排(分配或收回)计划。用户将 ReservationRequest 提交给 RM 之后,RM 委托给 ReservationAgent，ReservationAgent 通过PlanView 接口咨询该 Plan 是否能满足 RDL 时间且资源约束。如果可以分配,则通过 PlanEdit 接口将其存储在该 Plan 中。之后便向用户返回 ReservationId 票据,用户可通过该票据在预订的时间范围使用预订的资源。<br>PlanFollower将会周期性的从 Plan 中读取最新工作安排计划(队列瞬时容量),并同步给常规调度器,进而影响正在运行作业占用的资源。<br>接口中有三类方法:  PlanContext 负责配置信息;PlanView 负责对Plan 状态的只读访问;PlanEdit 负责对 Plan 状态写入访问。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">getStep</td>\n<td align=\"center\">获取 Plan 的时间步长</td>\n<td align=\"left\">PlanContext,同 PlanFollower 的时间步长</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationAgent</td>\n<td align=\"center\">获取 Plan关联的 ReservationAgent</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getReplanner</td>\n<td align=\"center\">使用 Planner 对象来应对Plan 的资源意外减少</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getSharingPolicy</td>\n<td align=\"center\">SharingPolicy 控制多用户共享计划资源</td>\n<td align=\"left\">PlanContext</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationById</td>\n<td align=\"center\">通过 ReservationId 获取 Reservation 的详细信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getReservationsAtTime</td>\n<td align=\"center\">获取指定时间点所有活跃的 Reservation 的详细信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getAllReservations</td>\n<td align=\"center\">获取 Plan 中所有预订信息</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getTotalCommittedResources</td>\n<td align=\"center\">获取指定时间点所有预定的总资源量</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getConsumptionForUser</td>\n<td align=\"center\">获取指定时间点指定用户预定的总资源量</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getEarliestStartTime</td>\n<td align=\"center\">获取计划中最早的预订开始时间</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">getLastEndTime</td>\n<td align=\"center\">获取计划中最晚的预订结束时间</td>\n<td align=\"left\">PlanView</td>\n</tr>\n<tr>\n<td align=\"left\">addReservation</td>\n<td align=\"center\">增加一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">updateReservation</td>\n<td align=\"center\">更新一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">deleteReservation</td>\n<td align=\"center\">删除一个预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n<tr>\n<td align=\"left\">archiveCompletedReservations</td>\n<td align=\"center\">清除所有的过期预订</td>\n<td align=\"left\">PlanEdit</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<p>　　仅列举出核心方法,实际不限于此  </p>\n<ul>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.GreedyReservationAgent<br>实现了ReservationAgent接口。 \n一个简单的贪婪放置策略来满足用户预订的代理。具体方式是:按照 ReservationRequests中的各个 ReservationRequest作为单独的阶段,从deadline 开始向后移动至 arrival 来安排预订请求。该代理不考虑本地性,仅仅考虑容器粒度的验证(e.g. 不能超过最大容器大小)  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">createReservation</td>\n<td align=\"center\">使用此代理尝试创建一个预订</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">updateReservation</td>\n<td align=\"center\">使用此代理更新一个已有预订</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">deleteReservation</td>\n<td align=\"center\">使用此代理删除一个已有预订</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy<br>主要功能是校验 Plan 能否接受用户预订请求 。实现类 CapacityOverTimePolicy使用容量的按时间拓展概念：策略会保证该用户的当前预订申请和已生效的预订申请资源不超过瞬时资源限制(e.g. reservable queue capacity * 1),且在 24h 时间窗口内不能超过平均资源限制(e.g. reservable queue capacity * 0.5)。从某种意义来说,预订可以使用 reservable queue 的大部分容量 但只要保证快速归还以保证平均资源限制，这可以防止资源滥用且增加了灵活性。通过配置瞬时资源限制和平均资源限制以及时间窗口,策略可以使 reservable queue 达到即时执行(max=100%,avg=100%)和完全灵活(max=?,avg=?,保留给其他用户或系统)的效果。   </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"center\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"center\">初始化 Policy</td>\n<td align=\"left\">策略必要的配置读取</td>\n</tr>\n<tr>\n<td align=\"left\">validate</td>\n<td align=\"center\">校验 Plan 能否接受用户预订请求</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">getValidWindow</td>\n<td align=\"center\">预订资源的过期时间</td>\n<td align=\"left\">窗口为[-24h,当前)。预订的 deadline 在此之前的记录将被删除</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.Planner<br>实现类只有SimpleCapacityReplanner,且功能远不及 Planner 的设计(增删改 Plan)，只有删除预订资源的功能：从当前时刻开始直到 min{最后一个预约endtime，一个小时之后}，如果所有用户预订资源总量超过 reservable queue capacity，则删除接受时间较晚的一批预订。 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">init</td>\n<td align=\"left\">初始化 Planner</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">plan</td>\n<td align=\"left\">更新现有的 Plan,或增删改已有预订或增加一个新的预订</td>\n<td align=\"left\">SCR 只支持删除晚的预订以保证预订资源总量不超过队列最低配额</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation<br>预订系统的核心数据结构，是一种运行长度编码(Run Length Encoded)的稀疏数据结构(TreeMap)，可随着时间的推移维护资源分配。维护者各事件点(预订分配 starttime，endtime)时(Plan/当前预订)全局应有的资源总量。  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">getEarliestStartTime</td>\n<td align=\"left\">最早的资源分配的时间戳</td>\n<td align=\"left\">treemap  firstKey</td>\n</tr>\n<tr>\n<td align=\"left\">getLatestEndTime</td>\n<td align=\"left\">最晚的资源分配的时间戳</td>\n<td align=\"left\">treemap lastKey</td>\n</tr>\n<tr>\n<td align=\"left\">getCapacityAtTime</td>\n<td align=\"left\">该时间点的 Plan 已预订资源量</td>\n<td align=\"left\">treemap floorKey</td>\n</tr>\n<tr>\n<td align=\"left\">addInterval</td>\n<td align=\"left\">在treemap 中新增一个时间范围的预约资源记录</td>\n<td align=\"left\">维护开始点和结束点的资源量</td>\n</tr>\n<tr>\n<td align=\"left\">removeInterval</td>\n<td align=\"left\">在 treemap 中删除一个时间范围的预约资源记录</td>\n<td align=\"left\">维护开始点和结束点的资源量</td>\n</tr>\n<tr>\n<td align=\"left\">addCompositeInterval</td>\n<td align=\"left\">在 treemap 中新增一个时间范围的多个预订资源记录</td>\n<td align=\"left\">never used</td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<blockquote>\n</blockquote>\n<p> TreeMap 的floorEntry(targetKey) 和 lowerEntry(targetKey)  区别:<br> floorEntry 返回 key 小于等于 targetKey 的键值对,无则 null<br> lowerEntry 返回 key 严格小于 targetKey 的键值对,无则 null  </p>\n<ul>\n<li>org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation\n实现自 ResourceAllocation。预订分配的结果，内存数据结构，包含预订的整体开始结束时间，ReservationSystem 校验(GA 校验资源/SharingPolicy 校验用户违规)通过 ReservationSubmissionRequest 的时间，&lt;时间段，资源量&gt;的分配细节和RLESparseResourceAllocation    </li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">方法</th>\n<th align=\"left\">功能</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">compareTo</td>\n<td align=\"left\">按 acceptTime比较,晚的在前</td>\n<td align=\"left\">比较器, Plan 队列资源骤减时,删除部分晚的的 ResourceAllocation</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest<br>客户端封装ReservationSubmissionRequest 请求，通过 client-RM 接口提交给 RM，数据结构如下<br>　　queueName:String    //支持预订资源的队列<br>　　rd:ReservationDefinition //预订定义<br>　　　　arrival:long  //预订开始时间的 最早时间<br>　　　　deadline:long   //预订结束时间的 最晚时间<br>　　　　name:String  //名称<br>　　　　reservationRequests:ReservationRequests  //预订请求    　\n　　　　　　reservationResources:List&lt;ReservationRequest&gt;<br>　　　　　　　　capability:Capacity  //每个预订请求的资源量<br>　　　　　　　　numContainers:int  //预订的 container 数量<br>　　　　　　　　concurrency:int //并发度。numContainers 可以分批分配<br>　　　　　　　　duration:long  //使用时间<br>　　　　type:ReservationRequestInterpreter  //多个预订请求之间的依赖关系  </p>\n</li>\n<li><p>org.apache.hadoop.yarn.api.records.ReservationRequestInterpreter<br>枚举类型，用以表示多个预订请求之间的依赖关系(或称: 组关系)      </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">类型</th>\n<th align=\"left\">含义</th>\n<th align=\"left\">场景</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">R_ANY</td>\n<td align=\"left\">仅满足 RDL 中一个预订请求即可</td>\n<td align=\"left\">有多种等效的方式满足要求。e.g. 1个&lt;4G,2core&gt;或2个&lt;2G,1core&gt;。Agent 会决定使用最合适的ReservationRequest</td>\n</tr>\n<tr>\n<td align=\"left\">R_ALL</td>\n<td align=\"left\">需要满足 RDL中所有预订请求,请求的分配没有限制时间先后</td>\n<td align=\"left\">事务</td>\n</tr>\n<tr>\n<td align=\"left\">R_ORDER</td>\n<td align=\"left\">需要满足 RDL 中所有预订请求,且有严格的时间限制。k 位置的分配时间段必须在 k+1位置分配时间段之前(无交集)，且 k 位置分配的结束时间和 k+1位置分配的开始时间可以有任意长的时间间隔</td>\n<td align=\"left\">具有固定依赖的阶段性工作流。e.g. 第一个作业需要 1个&lt;4G,2core&gt; 5min，其输出作为第二个作业的输入，第二个作业需要 2 个&lt;2G,1core&gt; 10min，则两个作业预订的分配时间段必然不能重叠</td>\n</tr>\n<tr>\n<td align=\"left\">R_ORDER_NO_GAP</td>\n<td align=\"left\">R_ORDER 的严格版本，要求 k 位置分配的结束时间和 k+1 位置分配的开始时间相同、不能有任何间隙,即 “zero-size gap”,”no_gap”</td>\n<td align=\"left\">1.当前一个作业输出规模比较大时,避免保留太长时间 2.实时性要求很高的场景,作业间时间差则增大了工作流累计延时</td>\n</tr>\n</tbody></table>\n</li>\n<li><p>org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionResponse<br>客户端提交 ReservationSubmissionRequest 请求后，若RM 校验通过可以分配预订，则返回 reservationId给客户端。数据结构如下<br>　　reservationId:ReservationId<br>　　　　clusterTimestamp:long<br>　　　　id:long   </p>\n</li>\n</ul>\n<h3 id=\"step-0\"><a href=\"#step-0\" class=\"headerlink\" title=\"step 0\"></a>step 0</h3><p>用户提交预订创建请求，并收到包含 ReservationId 的响应  </p>\n<p>[ApplicationClientProtocol.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public ReservationSubmissionResponse submitReservation(</span><br><span class=\"line\">      ReservationSubmissionRequest request) throws YarnException, IOException</span><br></pre></td></tr></table></figure>\n<h3 id=\"step-1\"><a href=\"#step-1\" class=\"headerlink\" title=\"step 1\"></a>step 1</h3><p>用户提交由 RDL(Reservation Definition Language)和 ReservationId 组成的 ReservationRequest。描述了用户对资源(e.g. numContainer)和时间(e.g. duration)的需求,可以通过常规的Client-to-RM协议(ApplicationClientProtocol)以编程方式完成，也可以通过RM的REST API来完成。如果提交的预订具有相同的ReservationId 并且RDL相同，则请求将成功但不会创建新的预订。如果RDL不同，则保留将被拒绝且请求失败<br>以 TestCase 中的代码片段为例  </p>\n<p>[TestClientRMService.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Test</span><br><span class=\"line\">public void testReservationAPIs() &#123;</span><br><span class=\"line\">\t....</span><br><span class=\"line\">    &#x2F;&#x2F;创建一个预订请求</span><br><span class=\"line\">    Clock clock &#x3D; new UTCClock();</span><br><span class=\"line\">    &#x2F;&#x2F;预订最早开始时间</span><br><span class=\"line\">    long arrival &#x3D; clock.getTime();\t   </span><br><span class=\"line\">    &#x2F;&#x2F;使用时长 </span><br><span class=\"line\">    long duration &#x3D; 60000;    </span><br><span class=\"line\">    &#x2F;&#x2F;预订的最晚结束时间</span><br><span class=\"line\">    long deadline &#x3D; (long) (arrival + 1.05 * duration);</span><br><span class=\"line\">    &#x2F;&#x2F;RDL: Rervation Define Language </span><br><span class=\"line\">    &#x2F;&#x2F;请求 4 个 &lt;1G,1core&gt; container,使用时长是 6000ms,使用开始的最早时间是 arrival,使用结束的最晚内时间是 deadline。</span><br><span class=\"line\">    ReservationSubmissionRequest sRequest &#x3D;</span><br><span class=\"line\">        createSimpleReservationRequest(4, arrival, deadline, duration);</span><br><span class=\"line\">    ReservationSubmissionResponse sResponse &#x3D; null;</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      sResponse &#x3D; clientService.submitReservation(sRequest);</span><br><span class=\"line\">    &#125; catch (Exception e) &#123;</span><br><span class=\"line\">      Assert.fail(e.getMessage());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">private ReservationSubmissionRequest createSimpleReservationRequest(</span><br><span class=\"line\">      int numContainers, long arrival, long deadline, long duration) &#123;</span><br><span class=\"line\">    ReservationRequest r &#x3D;</span><br><span class=\"line\">        ReservationRequest.newInstance(Resource.newInstance(1024, 1),</span><br><span class=\"line\">            numContainers, 1, duration);</span><br><span class=\"line\">            &#x2F;&#x2F;R_ALL 满足所有请求</span><br><span class=\"line\">    ReservationRequests reqs &#x3D;</span><br><span class=\"line\">        ReservationRequests.newInstance(Collections.singletonList(r),</span><br><span class=\"line\">            ReservationRequestInterpreter.R_ALL);</span><br><span class=\"line\">    ReservationDefinition rDef &#x3D;</span><br><span class=\"line\">        ReservationDefinition.newInstance(arrival, deadline, reqs,</span><br><span class=\"line\">            &quot;testClientRMService#reservation&quot;);</span><br><span class=\"line\">    ReservationSubmissionRequest request &#x3D;</span><br><span class=\"line\">        ReservationSubmissionRequest.newInstance(rDef,</span><br><span class=\"line\">            ReservationSystemTestUtil.reservationQ);</span><br><span class=\"line\">    return request;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"step-2\"><a href=\"#step-2\" class=\"headerlink\" title=\"step 2\"></a>step 2</h3><p>ReservationSystem 委托 ReservationAgent(图中的GREE) 在计划(Plan)中为 ReservationRequest 找到一个合理的时间分配，计划(Plan)是一个跟踪当前所有已接受的预订请求以及系统中可用资源的内存数据结构。   </p>\n<p>[ClientRMService.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public ReservationSubmissionResponse submitReservation(</span><br><span class=\"line\">      ReservationSubmissionRequest request) throws YarnException, IOException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 检查 ReservactionSystem 是否启用</span><br><span class=\"line\">    checkReservationSytem(AuditConstants.SUBMIT_RESERVATION_REQUEST);</span><br><span class=\"line\">    ReservationSubmissionResponse response &#x3D;</span><br><span class=\"line\">        recordFactory.newRecordInstance(ReservationSubmissionResponse.class);</span><br><span class=\"line\">    &#x2F;&#x2F; 通过 AtmicLong 创建全局唯一的ReservationId</span><br><span class=\"line\">    ReservationId reservationId &#x3D; reservationSystem.getNewReservationId();</span><br><span class=\"line\">    &#x2F;&#x2F; 第一步:  校验</span><br><span class=\"line\">    &#x2F;&#x2F; 1.预订请求指定了 queue; 2.指定的 queue 是 reservable queue,即属于 ReservationSystem 管理  </span><br><span class=\"line\">    &#x2F;&#x2F; 2.Reservation Define Language 校验</span><br><span class=\"line\">    &#x2F;&#x2F;\t2a. 空值检验  2b. deadline 不能早于当前时间 2c. ReservationRequests 空值及空集判断  </span><br><span class=\"line\">    &#x2F;&#x2F;\t2d. 分配所有请求的最短时间(R_ANY,R_ALL:取最大;R_ORDER..取和)不能超过 deadline-arrival </span><br><span class=\"line\">    &#x2F;&#x2F;\t2e. 分配所有请求的最大资源量(concurrency * request capacity)不能超过 reservable queue capacity</span><br><span class=\"line\">    Plan plan &#x3D;</span><br><span class=\"line\">        rValidator.validateReservationSubmissionRequest(reservationSystem,</span><br><span class=\"line\">            request, reservationId);</span><br><span class=\"line\">    &#x2F;&#x2F; 校验 ACL\t</span><br><span class=\"line\">    String queueName &#x3D; request.getQueue();</span><br><span class=\"line\">    String user &#x3D;</span><br><span class=\"line\">        checkReservationACLs(queueName,</span><br><span class=\"line\">            AuditConstants.SUBMIT_RESERVATION_REQUEST);</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 第二步:  使用 ReservationAgent 来尝试放置预订请求</span><br><span class=\"line\">      boolean result &#x3D;</span><br><span class=\"line\">          plan.getReservationAgent().createReservation(reservationId, user,</span><br><span class=\"line\">              plan, request.getReservationDefinition());</span><br><span class=\"line\">      if (result) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 同步 reservationSystem &lt;reservationId,queueName&gt;关系</span><br><span class=\"line\">        reservationSystem.setQueueForReservation(reservationId, queueName);</span><br><span class=\"line\">        &#x2F;&#x2F; create the reservation synchronously if required</span><br><span class=\"line\">        &#x2F;&#x2F; 第三步:  如下,若预留的 arrival 时间早于当前(错过) PlanFollower 一个步长,则同步创建</span><br><span class=\"line\">        refreshScheduler(queueName, request.getReservationDefinition(),</span><br><span class=\"line\">            reservationId.toString());</span><br><span class=\"line\">  \t&#x2F;&#x2F; response 中返回放置预订请求的 reservationId</span><br><span class=\"line\">        response.setReservationId(reservationId);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">     ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    return response;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">   private void refreshScheduler(String planName,</span><br><span class=\"line\">      ReservationDefinition contract, String reservationId) &#123;</span><br><span class=\"line\">    if ((contract.getArrival() - clock.getTime()) &lt; reservationSystem</span><br><span class=\"line\">        .getPlanFollowerTimeStep()) &#123;</span><br><span class=\"line\">      ....</span><br><span class=\"line\">      &#x2F;&#x2F;后续再说源码</span><br><span class=\"line\">      reservationSystem.synchronizePlan(planName);</span><br><span class=\"line\">      ....</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>在第二步中,RMClientService 委托 Plan(reservable queue 的映射) 绑定的 ReservationAgent 来决定是否放置该预订请求<br>ReservationAgent(RA)将预订请求中的每个 ResourceRequest(RR)作为一个单独的 stage 尝试放置,并在放置时考虑 ReservationRequestInterpreter(RRI)组关系的处理。从最后一个 stage 开始向前依次放置，使用 Plan 已有的预订计划累计的数据和当次预订请求的累计中间数据判断能否放置当前 RR来影响当前 RR放置时间策略并存储最终放置策略结果。   </p>\n<p>[GreedyReservationAgent.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean computeAllocation(ReservationId reservationId, String user,</span><br><span class=\"line\">      Plan plan, ReservationDefinition contract,</span><br><span class=\"line\">      ReservationAllocation oldReservation) throws PlanningException,</span><br><span class=\"line\">      ContractValidationException &#123;</span><br><span class=\"line\">    LOG.info(&quot;placing the following ReservationRequest: &quot; + contract);</span><br><span class=\"line\">    &#x2F;&#x2F; reservable queue capacity </span><br><span class=\"line\">    Resource totalCapacity &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    </span><br><span class=\"line\">   &#x2F;&#x2F; 译文:考虑在此增加逻辑来调整&quot;ResourceDefinition&quot; 来解决系统的缺陷(e.g. 大型容器的调度延迟) </span><br><span class=\"line\">   &#x2F;&#x2F;  TODO: 想不出会在这个位置使用什么样的策略来解决大容器调度延迟问题? 逻辑能转移到常规调度吗</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 使用保守策略规整缩短 [arrival,deadline]区间</span><br><span class=\"line\">    long earliestStart &#x3D; contract.getArrival();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    if (earliestStart % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      earliestStart &#x3D; earliestStart + (step - (earliestStart % step));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    long deadline &#x3D;</span><br><span class=\"line\">        contract.getDeadline() - contract.getDeadline() % plan.getStep();</span><br><span class=\"line\">        </span><br><span class=\"line\">    &#x2F;&#x2F; 将每个 ReservationRequest(RR) 作为一个单独的 stage,设置一些阶段间的临时变量</span><br><span class=\"line\">    long curDeadline &#x3D; deadline;</span><br><span class=\"line\">    long oldDeadline &#x3D; -1;</span><br><span class=\"line\">    &#x2F;&#x2F; 当次预订请求，全部 RR 放置时间策略结果的(中间)数据结构 </span><br><span class=\"line\">    Map&lt;ReservationInterval, ReservationRequest&gt; allocations &#x3D;</span><br><span class=\"line\">        new HashMap&lt;ReservationInterval, ReservationRequest&gt;();</span><br><span class=\"line\">     &#x2F;&#x2F; 当次预订请求，全部 RR 放置造成&lt;时间-资源&gt;变动结果的(中间)数据结构  </span><br><span class=\"line\">    RLESparseResourceAllocation tempAssigned &#x3D;</span><br><span class=\"line\">        new RLESparseResourceAllocation(plan.getResourceCalculator(),</span><br><span class=\"line\">            plan.getMinimumAllocation());</span><br><span class=\"line\"></span><br><span class=\"line\">    List&lt;ReservationRequest&gt; stages &#x3D; contract.getReservationRequests()</span><br><span class=\"line\">        .getReservationResources();</span><br><span class=\"line\">    ReservationRequestInterpreter type &#x3D; contract.getReservationRequests()</span><br><span class=\"line\">        .getInterpreter();</span><br><span class=\"line\">    &#x2F;&#x2F; 使用迭代器 从最后一个元素之后的 null 向前迭代</span><br><span class=\"line\">    for (ListIterator&lt;ReservationRequest&gt; li &#x3D; </span><br><span class=\"line\">        stages.listIterator(stages.size()); li.hasPrevious();) &#123;</span><br><span class=\"line\">      ReservationRequest currentReservationStage &#x3D; li.previous();</span><br><span class=\"line\">       &#x2F;&#x2F;第一步:  校验 RR</span><br><span class=\"line\">      &#x2F;&#x2F; 1.concurrency &gt; 0; 2.numContainer&gt;0; 3. numContainer 必须是 concurrency 的倍数; 4.单容器大小不得超过maximumAllocation</span><br><span class=\"line\">      validateInput(plan, currentReservationStage, totalCapacity);</span><br><span class=\"line\">      &#x2F;&#x2F;第二步:  尝试分配单个 RR</span><br><span class=\"line\">      Map&lt;ReservationInterval, ReservationRequest&gt; curAlloc &#x3D;</span><br><span class=\"line\">          placeSingleStage(plan, tempAssigned, currentReservationStage,</span><br><span class=\"line\">              earliestStart, curDeadline, oldReservation, totalCapacity);</span><br><span class=\"line\"></span><br><span class=\"line\">      if (curAlloc &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 组关系是 R_ALL,R_ORDER,R_NO_GAP 时，一个 RR 分配失败 导致整个事务失败</span><br><span class=\"line\">        &#x2F;&#x2F; 组关系是 R_ANY,一个 RR 分配失败可以尝试其他 RR </span><br><span class=\"line\">        if (type !&#x3D; ReservationRequestInterpreter.R_ANY) &#123;</span><br><span class=\"line\">          throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">              + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          continue;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">       &#x2F;&#x2F; 可以放置当前 RR(stage)</span><br><span class=\"line\">       </span><br><span class=\"line\">        allocations.putAll(curAlloc);</span><br><span class=\"line\">       &#x2F;&#x2F; 组关系是 R_ANY 的话,可以终止后续 stage 的尝试了</span><br><span class=\"line\">        if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ANY) &#123;</span><br><span class=\"line\">          break;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; 如果组关系是 R_ODER,R_ODER_NO_GAP，则通过设置 curDeadline 来确保 RR 之间的先后关系 </span><br><span class=\"line\">        &#x2F;&#x2F; 将下一个 RR 的 deadline 设置为当前 RR 分配中最早的开始时间</span><br><span class=\"line\">        if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER</span><br><span class=\"line\">            || type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER_NO_GAP) &#123;</span><br><span class=\"line\">          curDeadline &#x3D; findEarliestTime(curAlloc.keySet());</span><br><span class=\"line\"></span><br><span class=\"line\">          &#x2F;&#x2F; 对于 R_ORDER_NO_GAP,确认当前分配和前一个(后向)分配之间没有空隙</span><br><span class=\"line\">          &#x2F;&#x2F; 空隙并非之前理解的完全相等,[t0,t1) 与 [t1,t2)之间的关系,而是不超过 Plan 的时间步长即可 </span><br><span class=\"line\">          if (type &#x3D;&#x3D; ReservationRequestInterpreter.R_ORDER_NO_GAP</span><br><span class=\"line\">              &amp;&amp; oldDeadline &gt; 0) &#123;</span><br><span class=\"line\">            if (oldDeadline - findLatestTime(curAlloc.keySet()) &gt; plan</span><br><span class=\"line\">                .getStep()) &#123;</span><br><span class=\"line\">              throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">                  + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          &#x2F;&#x2F; keep the variable oldDeadline pointing to the last deadline we</span><br><span class=\"line\">          &#x2F;&#x2F; found</span><br><span class=\"line\">          oldDeadline &#x3D; curDeadline;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 没有为预订请求分配到&lt;时间,资源&gt;,通过 throws exception 的方式来快速失败</span><br><span class=\"line\">    if (allocations.isEmpty()) &#123;</span><br><span class=\"line\">      throw new PlanningException(&quot;The GreedyAgent&quot;</span><br><span class=\"line\">          + &quot; couldn&#39;t find a valid allocation for your request&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#x2F;&#x2F; 第三步:  创建预订</span><br><span class=\"line\">  &#x2F;&#x2F; 在[arrival,earliestTime)之间加入了 &quot;零填充&quot;,以表明其是从arrival time 开始的预订</span><br><span class=\"line\">  &#x2F;&#x2F; TODO:在开头和末尾添加&quot;零填充&quot;的意义是什么?没有的话 会有什么问题  </span><br><span class=\"line\">    ReservationRequest ZERO_RES &#x3D;</span><br><span class=\"line\">        ReservationRequest.newInstance(Resource.newInstance(0, 0), 0);</span><br><span class=\"line\">    long firstStartTime &#x3D; findEarliestTime(allocations.keySet());</span><br><span class=\"line\">    if (firstStartTime &gt; earliestStart) &#123;</span><br><span class=\"line\">      allocations.put(new ReservationInterval(earliestStart,</span><br><span class=\"line\">          firstStartTime), ZERO_RES);</span><br><span class=\"line\">      firstStartTime &#x3D; earliestStart;</span><br><span class=\"line\">      &#x2F;&#x2F; 译: 考虑在[lastEndTime,deadline)添加&quot;零填充&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 译:这有待验证,其他代理可能也在放置(同步问题);有代理不知道的 SharingPolicy</span><br><span class=\"line\">    ReservationAllocation capReservation &#x3D;</span><br><span class=\"line\">        new InMemoryReservationAllocation(reservationId, contract, user,</span><br><span class=\"line\">            plan.getQueueName(), firstStartTime,</span><br><span class=\"line\">            findLatestTime(allocations.keySet()), allocations,</span><br><span class=\"line\">            plan.getResourceCalculator(), plan.getMinimumAllocation());</span><br><span class=\"line\">    &#x2F;&#x2F; 第四步: 新增或更新已有预订分配 。在 step 3 中解释</span><br><span class=\"line\">    if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">      return plan.updateReservation(capReservation);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      return plan.addReservation(capReservation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>再看上述 第二步:  尝试分配单个 RR<br>这是整个流程中最重要的部分: 决定了用户的每个 RR 能不能放置,放置在哪个时间段,放置几个 container<br>核心思想是: 从 deadline(动态 deadline)开始向后尝试，每次尝试都遍历 duration 时间，判断 duration 时间每个时间步长是否超过了 capacity(即判断 指定大小的资源能不能占用当前时间段，故称”放置”)。然后向后移动一个时间步长，直至剩余时间不足以满足 duration 或者放置了 RR 中所有 container 结束       </p>\n<p>[GreedyReservationAgent.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private Map&lt;ReservationInterval, ReservationRequest&gt; placeSingleStage(</span><br><span class=\"line\">      Plan plan, RLESparseResourceAllocation tempAssigned,</span><br><span class=\"line\">      ReservationRequest rr, long earliestStart, long curDeadline,</span><br><span class=\"line\">      ReservationAllocation oldResAllocation, final Resource totalCapacity) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    Map&lt;ReservationInterval, ReservationRequest&gt; allocationRequests &#x3D;</span><br><span class=\"line\">        new HashMap&lt;ReservationInterval, ReservationRequest&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    Resource gang &#x3D; Resources.multiply(rr.getCapability(), rr.getConcurrency());</span><br><span class=\"line\">    long dur &#x3D; rr.getDuration();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    if (dur % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      dur +&#x3D; (step - (dur % step));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 将一个并发作为一个原子性的组分配 </span><br><span class=\"line\">    &#x2F;&#x2F; 每次分配资源量: gang&#x3D;capacity * concurrency </span><br><span class=\"line\">    &#x2F;&#x2F;     共计分配次数: gangsToPlace&#x3D; numContainers&#x2F;concurrency  </span><br><span class=\"line\">    int gangsToPlace &#x3D; rr.getNumContainers() &#x2F; rr.getConcurrency();</span><br><span class=\"line\">    int maxGang &#x3D; 0;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; loop trying to place until we are done, or we are considering</span><br><span class=\"line\">    &#x2F;&#x2F; an invalid range of times</span><br><span class=\"line\">    &#x2F;&#x2F; 循环尝试分配 直至所有的 gang 原子组都分配完或者剩余时间不足以容纳一个 duration</span><br><span class=\"line\">    while (gangsToPlace &gt; 0 &amp;&amp; curDeadline - dur &gt;&#x3D; earliestStart) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; as we run along we remember how many gangs we can fit, and what</span><br><span class=\"line\">      &#x2F;&#x2F; was the most constraining moment in time (we will restart just</span><br><span class=\"line\">      &#x2F;&#x2F; after that to place the next batch)</span><br><span class=\"line\">      maxGang &#x3D; gangsToPlace;</span><br><span class=\"line\">      long minPoint &#x3D; curDeadline;</span><br><span class=\"line\">      int curMaxGang &#x3D; maxGang;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; 尝试在[curDeadline-duration,curDeadline) 长为 duration 的区间内放置未知个 Resource&#x3D;gang 的原子组</span><br><span class=\"line\">      for (long t &#x3D; curDeadline - plan.getStep(); t &gt;&#x3D; curDeadline - dur</span><br><span class=\"line\">          &amp;&amp; maxGang &gt; 0; t &#x3D; t - plan.getStep()) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; 如果之前的预订存在的话,则删除此预订之前的分配(e.g. 更新预订时)</span><br><span class=\"line\">        Resource oldResCap &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        if (oldResAllocation !&#x3D; null) &#123;</span><br><span class=\"line\">          oldResCap &#x3D; oldResAllocation.getResourcesAtTime(t);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#x2F;&#x2F; 计算当前时间点的净可用资源</span><br><span class=\"line\">        Resource netAvailableRes &#x3D; Resources.clone(totalCapacity);</span><br><span class=\"line\">        Resources.addTo(netAvailableRes, oldResCap);</span><br><span class=\"line\">        &#x2F;&#x2F; Plan(reservable queue), 最靠近 t 时刻的累计容量</span><br><span class=\"line\">        Resources.subtractFrom(netAvailableRes,</span><br><span class=\"line\">            plan.getTotalCommittedResources(t)); </span><br><span class=\"line\">        &#x2F;&#x2F;当前预订, 最靠近 t 时刻的累计容量</span><br><span class=\"line\">        Resources.subtractFrom(netAvailableRes,</span><br><span class=\"line\">            tempAssigned.getCapacityAtTime(t));</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#x2F;&#x2F;计算当前时刻能满足的最大数量的 gang</span><br><span class=\"line\">        curMaxGang &#x3D;</span><br><span class=\"line\">            (int) Math.floor(Resources.divide(plan.getResourceCalculator(),</span><br><span class=\"line\">                totalCapacity, netAvailableRes, gang));</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; min&#123;需求的 gang 数量,剩余资源能支撑的 gang 数量&#125;</span><br><span class=\"line\">        curMaxGang &#x3D; Math.min(gangsToPlace, curMaxGang);</span><br><span class=\"line\">\t&#x2F;&#x2F; 记住当前尝试的 duration 内,最小数量的 gang 点(即 队列+reservation 占用资源量最多的点)，作为下次尝试的右边界</span><br><span class=\"line\">        if (curMaxGang &lt;&#x3D; maxGang) &#123;</span><br><span class=\"line\">          maxGang &#x3D; curMaxGang;</span><br><span class=\"line\">          &#x2F;&#x2F;资源最紧俏的时间点。当资源充裕时,放置尽可能多的 gang,也会成为当前最紧俏的时间点，作为下次放置的右边界    </span><br><span class=\"line\">          minPoint &#x3D; t;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; if we were able to place any gang, record this, and decrement</span><br><span class=\"line\">      &#x2F;&#x2F; gangsToPlace maxGang&#x3D;0:没资源了 时间:范围不够了</span><br><span class=\"line\">      &#x2F;&#x2F; 退出上一个循环过程有两种情况: </span><br><span class=\"line\">      &#x2F;&#x2F; 1.  maxGang&#x3D;0,这个 duration 内有一个时间点 净剩余资源不足以放下一个 gang,那么整个 duration 都不能用了</span><br><span class=\"line\">      &#x2F;&#x2F; 2. t&lt;&#x3D;curDeadline - dur 整个 duration 已经遍历完了。此时 maxGang &gt; 0</span><br><span class=\"line\">      &#x2F;&#x2F; 以上退出都有可能,所以需要中间变量 maxGang来做判断。</span><br><span class=\"line\">      if (maxGang &gt; 0) &#123;</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; 能在[curDeadline-duration,curDeadline) 内放置下 maxGang 个gang，即 concurrency * maxGang 个 container</span><br><span class=\"line\">        gangsToPlace -&#x3D; maxGang;</span><br><span class=\"line\">        ReservationInterval reservationInt &#x3D;</span><br><span class=\"line\">            new ReservationInterval(curDeadline - dur, curDeadline);</span><br><span class=\"line\">        ReservationRequest reservationRes &#x3D;</span><br><span class=\"line\">            ReservationRequest.newInstance(rr.getCapability(),</span><br><span class=\"line\">                rr.getConcurrency() * maxGang, rr.getConcurrency(),</span><br><span class=\"line\">                rr.getDuration());</span><br><span class=\"line\">        &#x2F;&#x2F; 记住已占用的空间。后续贴出源码</span><br><span class=\"line\">        &#x2F;&#x2F; reservable queue 对应的 Plan 暂时是只读的，除非能将整个 ReservationRequests 放入到计划中。</span><br><span class=\"line\">        tempAssigned.addInterval(reservationInt, reservationRes);</span><br><span class=\"line\">        allocationRequests.put(reservationInt, reservationRes);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 设置下次放置 gang 的右边界。右边界向右无法再安置一个 gang(如上逻辑所述),尝试在右边界向左放置新的 gang</span><br><span class=\"line\">      curDeadline &#x3D; minPoint;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    if (gangsToPlace &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 放置了所有的 gang </span><br><span class=\"line\">      return allocationRequests;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 不能为当前 ReservationRequest(RR) 在时间&#x2F;容量&#x2F;并发 限制上放置所有的 gang。</span><br><span class=\"line\">      &#x2F;&#x2F; 此次 RR放置失败,如果不是 R_ANY 的话，可以终止整个 预订请求了。</span><br><span class=\"line\">      for (Map.Entry&lt;ReservationInterval, ReservationRequest&gt; tempAllocation :</span><br><span class=\"line\">        allocationRequests.entrySet()) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 清理之前的中间数据。后续贴出源码</span><br><span class=\"line\">        tempAssigned.removeInterval(tempAllocation.getKey(),</span><br><span class=\"line\">            tempAllocation.getValue());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>Plan 中使用 RLESparseResourceAllocation(以 TreeMap 为核心的稀疏数据结构)保存了当前已生效所有Reservation的累积&lt;时间-资源&gt;状态变化图，通过汇总所有 Reservation 的 RR 中每个分配的时间段(startTime,endTime)和资源量 最终得到了 Plan 随时间变化的已分配资源量。对每个 ReservationSubmissionRequest 请求来说 ，在尝试分配所有 RR 的时候，也使用RLESparseResourceAllocation 来保存临时分配好 RR 累积&lt;时间-资源&gt;状态变化图。  </p>\n<p>[RLESparseResourceAllocation.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private TreeMap&lt;Long, Resource&gt; cumulativeCapacity &#x3D; new TreeMap&lt;Long, Resource&gt;();</span><br><span class=\"line\">&#x2F;&#x2F;向 RLESparseResourcAllocation 中增加一个 &lt;时间范围-容量&gt; 的预订</span><br><span class=\"line\">public boolean addInterval(ReservationInterval reservationInterval,</span><br><span class=\"line\">      ReservationRequest capacity) &#123;</span><br><span class=\"line\">    Resource totCap &#x3D;</span><br><span class=\"line\">        Resources.multiply(capacity.getCapability(),</span><br><span class=\"line\">            (float) capacity.getNumContainers());</span><br><span class=\"line\">    if (totCap.equals(ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      long startKey &#x3D; reservationInterval.getStartTime();</span><br><span class=\"line\">      long endKey &#x3D; reservationInterval.getEndTime();</span><br><span class=\"line\">      &#x2F;&#x2F;截取 (?,endKey)之间 一段 TreeMap</span><br><span class=\"line\">      NavigableMap&lt;Long, Resource&gt; ticks &#x3D;</span><br><span class=\"line\">          cumulativeCapacity.headMap(endKey, false);</span><br><span class=\"line\">      if (ticks !&#x3D; null &amp;&amp; !ticks.isEmpty()) &#123;</span><br><span class=\"line\">        Resource updatedCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        &#x2F;&#x2F; 找到时间上小于等于 startKey 的最大 key 的键值对</span><br><span class=\"line\">        Entry&lt;Long, Resource&gt; lowEntry &#x3D; ticks.floorEntry(startKey);</span><br><span class=\"line\">        if (lowEntry &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">\t   &#x2F;&#x2F; 表明 startKey 即为 RLE 全局最小键(最早)</span><br><span class=\"line\">          cumulativeCapacity.put(startKey, totCap);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.add(lowEntry.getValue(), totCap);</span><br><span class=\"line\">          if ((startKey &#x3D;&#x3D; lowEntry.getKey())</span><br><span class=\"line\">              &amp;&amp; (isSameAsPrevious(lowEntry.getKey(), updatedCapacity))) &#123;</span><br><span class=\"line\">             &#x2F;&#x2F; 通过 remove 达到合并区间的。</span><br><span class=\"line\">             &#x2F;&#x2F; e.g. [t0,t1)为 2G, [t1,t2)为 1G;现在在 [t1,t3) 上分配了 1G,那么可以删除 t1 这个点 </span><br><span class=\"line\">            cumulativeCapacity.remove(lowEntry.getKey());</span><br><span class=\"line\">          &#125; else &#123;</span><br><span class=\"line\">             &#x2F;&#x2F;新增 一个时间点的容量 记录</span><br><span class=\"line\">            cumulativeCapacity.put(startKey, updatedCapacity);</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; [startKey,endKey)时间段内的所有已有记录都要增加 totCap 作为最终状态 </span><br><span class=\"line\">        &#x2F;&#x2F; cumulativeCapacity 维护的是全局状态量,而不是增量或者单个 reservation 的数据</span><br><span class=\"line\">        Set&lt;Entry&lt;Long, Resource&gt;&gt; overlapSet &#x3D;</span><br><span class=\"line\">            ticks.tailMap(startKey, false).entrySet(); &#x2F;&#x2F;startKey 已经增加过了 </span><br><span class=\"line\">        for (Entry&lt;Long, Resource&gt; entry : overlapSet) &#123;</span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.add(entry.getValue(), totCap);</span><br><span class=\"line\">          entry.setValue(updatedCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        cumulativeCapacity.put(startKey, totCap);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      Resource nextTick &#x3D; cumulativeCapacity.get(endKey);</span><br><span class=\"line\">      if (nextTick !&#x3D; null) &#123;</span><br><span class=\"line\">        if (isSameAsPrevious(endKey, nextTick)) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 合并 endKey</span><br><span class=\"line\">          cumulativeCapacity.remove(endKey);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; endKey 时还回资源,所以在 endKey 前面记录都加了 totCap,此处会减去  </span><br><span class=\"line\">        cumulativeCapacity.put(endKey, Resources.subtract(cumulativeCapacity</span><br><span class=\"line\">            .floorEntry(endKey).getValue(), totCap));</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  &#x2F;&#x2F; 在 RLESparseResourcAllocation 中删除一个 &lt;时间范围-容量&gt; 的预订</span><br><span class=\"line\">  public boolean removeInterval(ReservationInterval reservationInterval,</span><br><span class=\"line\">      ReservationRequest capacity) &#123;</span><br><span class=\"line\">    Resource totCap &#x3D;</span><br><span class=\"line\">        Resources.multiply(capacity.getCapability(),</span><br><span class=\"line\">            (float) capacity.getNumContainers());</span><br><span class=\"line\">    if (totCap.equals(ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      long startKey &#x3D; reservationInterval.getStartTime();</span><br><span class=\"line\">      long endKey &#x3D; reservationInterval.getEndTime();</span><br><span class=\"line\">      NavigableMap&lt;Long, Resource&gt; ticks &#x3D;</span><br><span class=\"line\">          cumulativeCapacity.headMap(endKey, false);</span><br><span class=\"line\">      &#x2F;&#x2F;取出[startKey,endKey)区间内的所有记录</span><br><span class=\"line\">      SortedMap&lt;Long, Resource&gt; overlapSet &#x3D; ticks.tailMap(startKey);</span><br><span class=\"line\">      if (overlapSet !&#x3D; null &amp;&amp; !overlapSet.isEmpty()) &#123;</span><br><span class=\"line\">        Resource updatedCapacity &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">        long currentKey &#x3D; -1;</span><br><span class=\"line\">        for (Iterator&lt;Entry&lt;Long, Resource&gt;&gt; overlapEntries &#x3D;</span><br><span class=\"line\">            overlapSet.entrySet().iterator(); overlapEntries.hasNext();) &#123;</span><br><span class=\"line\">          Entry&lt;Long, Resource&gt; entry &#x3D; overlapEntries.next();</span><br><span class=\"line\">          currentKey &#x3D; entry.getKey();</span><br><span class=\"line\">          &#x2F;&#x2F; 在每个时间点的减去该预订已分配的资源量  </span><br><span class=\"line\">          updatedCapacity &#x3D; Resources.subtract(entry.getValue(), totCap);</span><br><span class=\"line\">          cumulativeCapacity.put(currentKey, updatedCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; 左右边界点存在性判断</span><br><span class=\"line\">        Long firstKey &#x3D; overlapSet.firstKey();</span><br><span class=\"line\">        if (isSameAsPrevious(firstKey, overlapSet.get(firstKey))) &#123;</span><br><span class=\"line\">          cumulativeCapacity.remove(firstKey);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if ((currentKey !&#x3D; -1) &amp;&amp; (isSameAsNext(currentKey, updatedCapacity))) &#123;</span><br><span class=\"line\">          cumulativeCapacity.remove(cumulativeCapacity.higherKey(currentKey));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>举个简单例子，为便于讨论:<br>1.容量方面:  reservable queue 有 &lt;2G,2core&gt;的 capacity，所有预订请求中 container 容量都是&lt;1G,1core&gt;，即 queue 最多有 2 个container<br>2.时间方面: 从 t0至 t5，时间单位是 PlanFollower 的时间步长 step(默认是 1s)。t0 为当前时刻<br>3.请求方面: 已有的 reservation 不讨论 arrival,deadline，只给出 ReservationSystem 给出的分配&lt;时间范围-容量&gt;<br>4.ReservationSystem 已有的预订如下  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">预订 Id</th>\n<th align=\"left\">开始时间</th>\n<th align=\"left\">结束时间</th>\n<th align=\"left\">占用时间</th>\n<th align=\"left\">总资源量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">r0</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">1 个单位</td>\n<td align=\"left\">1个 container</td>\n</tr>\n<tr>\n<td align=\"left\">r1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">1个单位</td>\n<td align=\"left\">2个 container</td>\n</tr>\n<tr>\n<td align=\"left\">r2</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">1个单位</td>\n<td align=\"left\">1个 container</td>\n</tr>\n</tbody></table>\n<p>由此可以绘出 Plan 对应的 RLESparseResourceAllocation 中维护的累计&lt;时间-资源&gt;状态变化图  </p>\n<p><img src=\"/img/pictures/reservation/reservationSystem.png\" alt=\"\">  </p>\n<p>即: r2 在 t1 时刻拿到1 个 container,因此图中 t1 时刻已分配资源状态是 1 个 container。<br>      r2 在 t2 时刻释放 1 个 container,r1 拿到两个 2 个 container,因此 t2 时刻已分配资源状态是 2 个 container </p>\n<p> 假设 ReservationSystem接收到一个请求，ReservationDefinition定义的预订请求内容为：\narrival=t0,deadline=t5<br>ReservationRequests(RRS) 中有一个 ReservationRequest(RR)<br>　　capacity&lt;1G,1core&gt;,numContainer=2,concurrency=1,duration=2<br>ReservationRequestInterpreter=R_ALL</p>\n<p>来模拟一下GreedyRerservationAgent#placeSingleStage是如何尝试放置这一个 RR 的<br>初始值:gang=concurrency * capacity=&lt;1G,1core&gt;  gangsToPlace=numContainers/concurrency=2  maxGang=2  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t5</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">2-0-0=2 contianer</td>\n<td align=\"left\">2</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">是</td>\n</tr>\n<tr>\n<td align=\"left\">第二次次循环</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t4</td>\n<td align=\"left\">2</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">2-1-0=1 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">否,duration 超限退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束时，maxGang=1，即可以在[t3,t5)这个时间段放下 1 个 gang，在此为 1 个 container，外层循环将此分配封装成&lt;ReservationInterval,ReservationRequest&gt;加入到 当前预订累计分配的中间数据结构中(tmpAssigned:RLESparseResourceAllocation)。  </p>\n<p>此时: gangsToPlace=1,curDeadline=t3,进行下一次外层循环  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t3</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">2-2-0=0 contianer</td>\n<td align=\"left\">0</td>\n<td align=\"left\">0</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">否,maxGang=0退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束后，maxGang=0，不能在[t1,t3)时间段放置任何一个 gang。  </p>\n<p>此时: gangsToPlace=1,curDeadline=t2，进行下一次外层循环  </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">状态</th>\n<th align=\"left\">循环开始前 maxGang</th>\n<th align=\"left\">开始前 minPoint</th>\n<th align=\"left\">开始前 curMaxGang</th>\n<th align=\"left\">循环变量 t</th>\n<th align=\"left\">净可用资源(队列容量-Plan 累计-当前预订累计)</th>\n<th align=\"left\">最终curMaxGang</th>\n<th align=\"left\">最终 maxGang</th>\n<th align=\"left\">最终 minPoint</th>\n<th align=\"left\">是否进行下次循环</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">第一次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t2</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">2-1-0=1 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">是</td>\n</tr>\n<tr>\n<td align=\"left\">第二次循环</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t0</td>\n<td align=\"left\">2-0-0=2 contianer</td>\n<td align=\"left\">1</td>\n<td align=\"left\">1</td>\n<td align=\"left\">t0</td>\n<td align=\"left\">否,duration 超限退出</td>\n</tr>\n</tbody></table>\n<p>内层循环结束后，maxGang=1，即可以在[t0,t2)时间段放下一个 gang，即 1 个 container，外层循环将此分配封装好之后再次加入到 tmpAssigned 中。   </p>\n<p>此时: gangsToPlace=0终止了外层循环。\n得到的最终结论是: 在[t3,t5)时间段放下一个 container&lt;1G,1core&gt;,在[t0,t2)时间段放下一个 container&lt;1G,1core&gt;。该 RR 整体可以分配，满足组关系 R_ALL，即不考虑用户的情况可以分配该 RR。</p>\n<h3 id=\"step-3\"><a href=\"#step-3\" class=\"headerlink\" title=\"step 3\"></a>step 3</h3><p>SharingPolicy 提供了一种在预订请求上强制保证统计量的方法，决定接受或者拒绝预订。例如，CapacityOvertimePolicy允许强制保证用户可以在其所有预订中请求的瞬时最大容量，以及一段时期内对资源整体的限制，例如，用户所有的预订最多可以达到瞬时50％集群最大容量，但是在一天内，其平均值不能超过10％。(The SharingPolicy provides a way to enforce invariants on the reservation being accepted, potentially rejecting reservations. For example, the CapacityOvertimePolicy allows enforcement of both instantaneous max-capacity a user can request across all of his/her reservations and a limit on the integral of resources over a period of time, e.g., the user can reserve up to 50% of the cluster capacity instantanesouly, but in any 24h period of time he/she cannot exceed 10% average)  </p>\n<p>在 step 2中，ReservationSystem 尝试对预订请求分配时间和资源。若分配成功，则尝试将其加入到 Plan 中或是更新 Plan。    </p>\n<p>[GreedyReservationAgent.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private boolean computeAllocation(ReservationId reservationId, String user,</span><br><span class=\"line\">      Plan plan, ReservationDefinition contract,</span><br><span class=\"line\">      ReservationAllocation oldReservation) throws PlanningException,</span><br><span class=\"line\">      ContractValidationException &#123;</span><br><span class=\"line\">    LOG.info(&quot;placing the following ReservationRequest: &quot; + contract);</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    ReservationAllocation capReservation &#x3D;</span><br><span class=\"line\">        new InMemoryReservationAllocation(reservationId, contract, user,</span><br><span class=\"line\">            plan.getQueueName(), firstStartTime,</span><br><span class=\"line\">            findLatestTime(allocations.keySet()), allocations,</span><br><span class=\"line\">            plan.getResourceCalculator(), plan.getMinimumAllocation());</span><br><span class=\"line\">    if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">     &#x2F;&#x2F; 更新</span><br><span class=\"line\">      return plan.updateReservation(capReservation);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 新增</span><br><span class=\"line\">      return plan.addReservation(capReservation);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>updateReservation 事务性更新 reservation:  先删除旧的reservation，再新增更新的 reservation(当次ReservationSubmissionRequest)。如果新增失败则再把旧的reservation 回滚。 \n所以两处的逻辑的核心在于 addReservation 和 removeReservation。  </p>\n<p>[InMemoryPlan.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   &#x2F;&#x2F;按时间段索引 ReservationAllocation 信息  </span><br><span class=\"line\">private TreeMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt; currentReservations &#x3D;</span><br><span class=\"line\">     new TreeMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt;();</span><br><span class=\"line\">  &#x2F;&#x2F; Plan 整体的&lt;时间-资源&gt;状态变化图</span><br><span class=\"line\"> private RLESparseResourceAllocation rleSparseVector;</span><br><span class=\"line\"> &#x2F;&#x2F; 细分用户的&lt;时间-资源&gt;状态变化图</span><br><span class=\"line\"> private Map&lt;String, RLESparseResourceAllocation&gt; userResourceAlloc &#x3D;</span><br><span class=\"line\">     new HashMap&lt;String, RLESparseResourceAllocation&gt;();</span><br><span class=\"line\"> &#x2F;&#x2F; 按 reservationId 索引ReservationAllocation 信息</span><br><span class=\"line\"> private Map&lt;ReservationId, InMemoryReservationAllocation&gt; reservationTable &#x3D;</span><br><span class=\"line\">     new HashMap&lt;ReservationId, InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">     </span><br><span class=\"line\">public boolean addReservation(ReservationAllocation reservation)</span><br><span class=\"line\">     throws PlanningException &#123;</span><br><span class=\"line\">   InMemoryReservationAllocation inMemReservation &#x3D;</span><br><span class=\"line\">       (InMemoryReservationAllocation) reservation;</span><br><span class=\"line\">   if (inMemReservation.getUser() &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">     String errMsg &#x3D;</span><br><span class=\"line\">         &quot;The specified Reservation with ID &quot;</span><br><span class=\"line\">             + inMemReservation.getReservationId()</span><br><span class=\"line\">             + &quot; is not mapped to any user&quot;;</span><br><span class=\"line\">     LOG.error(errMsg);</span><br><span class=\"line\">     throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   writeLock.lock();</span><br><span class=\"line\">   try &#123;</span><br><span class=\"line\">     if (reservationTable.containsKey(inMemReservation.getReservationId())) &#123;</span><br><span class=\"line\">       String errMsg &#x3D;</span><br><span class=\"line\">           &quot;The specified Reservation with ID &quot;</span><br><span class=\"line\">               + inMemReservation.getReservationId() + &quot; already exists&quot;;</span><br><span class=\"line\">       LOG.error(errMsg);</span><br><span class=\"line\">       throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     &#x2F;&#x2F; 第一步: 使用 SharingPolicy 校验 Plan 能否接受该 reservation。后续解释</span><br><span class=\"line\">     policy.validate(this, inMemReservation);</span><br><span class=\"line\">     &#x2F;&#x2F; 第二步: 记录接受 reservation 的时间</span><br><span class=\"line\">     reservation.setAcceptanceTimestamp(clock.getTime());</span><br><span class=\"line\">     &#x2F;&#x2F; 第三步: 内存数据结构维护</span><br><span class=\"line\">     ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">         new ReservationInterval(inMemReservation.getStartTime(),</span><br><span class=\"line\">             inMemReservation.getEndTime());</span><br><span class=\"line\">     Set&lt;InMemoryReservationAllocation&gt; reservations &#x3D;</span><br><span class=\"line\">         currentReservations.get(searchInterval);</span><br><span class=\"line\">     if (reservations &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">       reservations &#x3D; new HashSet&lt;InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     if (!reservations.add(inMemReservation)) &#123;</span><br><span class=\"line\">       LOG.error(&quot;Unable to add reservation: &#123;&#125; to plan.&quot;,</span><br><span class=\"line\">           inMemReservation.getReservationId());</span><br><span class=\"line\">       return false;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     currentReservations.put(searchInterval, reservations);</span><br><span class=\"line\">     reservationTable.put(inMemReservation.getReservationId(),</span><br><span class=\"line\">         inMemReservation);</span><br><span class=\"line\">     &#x2F;&#x2F; 第四步: 维护Plan 整体的&lt;时间-资源&gt;状态变化图;维护细分用户&lt;时间-资源&gt;状态变化图  </span><br><span class=\"line\">     incrementAllocation(inMemReservation);</span><br><span class=\"line\">     LOG.info(&quot;Sucessfully added reservation: &#123;&#125; to plan.&quot;,</span><br><span class=\"line\">         inMemReservation.getReservationId());</span><br><span class=\"line\">     return true;</span><br><span class=\"line\">   &#125; finally &#123;</span><br><span class=\"line\">     writeLock.unlock();</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"> private boolean removeReservation(ReservationAllocation reservation) &#123;</span><br><span class=\"line\">   assert (readWriteLock.isWriteLockedByCurrentThread());</span><br><span class=\"line\">   ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">       new ReservationInterval(reservation.getStartTime(),</span><br><span class=\"line\">           reservation.getEndTime());</span><br><span class=\"line\">   Set&lt;InMemoryReservationAllocation&gt; reservations &#x3D;</span><br><span class=\"line\">       currentReservations.get(searchInterval);</span><br><span class=\"line\">    &#x2F;&#x2F; Plan 成员变量维护的信息 维护</span><br><span class=\"line\">   if (reservations !&#x3D; null) &#123;</span><br><span class=\"line\">     if (!reservations.remove(reservation)) &#123;</span><br><span class=\"line\">       LOG.error(&quot;Unable to remove reservation: &#123;&#125; from plan.&quot;,</span><br><span class=\"line\">           reservation.getReservationId());</span><br><span class=\"line\">       return false;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     if (reservations.isEmpty()) &#123;</span><br><span class=\"line\">       currentReservations.remove(searchInterval);</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125; else &#123;</span><br><span class=\"line\">     String errMsg &#x3D;</span><br><span class=\"line\">         &quot;The specified Reservation with ID &quot; + reservation.getReservationId()</span><br><span class=\"line\">             + &quot; does not exist in the plan&quot;;</span><br><span class=\"line\">     LOG.error(errMsg);</span><br><span class=\"line\">     throw new IllegalArgumentException(errMsg);</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   reservationTable.remove(reservation.getReservationId());</span><br><span class=\"line\">   &#x2F;&#x2F; 1.维护细分用户&lt;时间-资源&gt;状态变化图;维护Plan 整体&lt;时间-资源&gt;状态变化图  </span><br><span class=\"line\">   decrementAllocation(reservation);</span><br><span class=\"line\">   LOG.info(&quot;Sucessfully deleted reservation: &#123;&#125; in plan.&quot;,</span><br><span class=\"line\">       reservation.getReservationId());</span><br><span class=\"line\">   return true;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n\n<p>Plan 在 addReservation 和 updateReservation 时都使用了 SharingPolicy 来校验用户资源使用是否违规。CapacityOverTimePolicy 是默认 SharingPolicy，主要校验两个方面资源使用是否违规：瞬时用量不超过 reservable queue capaciy 的 1%(默认);24h(默认)平均用量不超过reservable queue capacity 的 1%(默认)。上述的 1%和 24h 是源码内常量,无法用配置修改      </p>\n<p>[CapacityOverTimePolicy.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void validate(Plan plan, ReservationAllocation reservation)</span><br><span class=\"line\">      throws PlanningException &#123;</span><br><span class=\"line\">    ReservationAllocation oldReservation &#x3D;</span><br><span class=\"line\">        plan.getReservationById(reservation.getReservationId());</span><br><span class=\"line\">    if (oldReservation !&#x3D; null</span><br><span class=\"line\">        &amp;&amp; !oldReservation.getUser().equals(reservation.getUser())) &#123;</span><br><span class=\"line\">      throw new MismatchedUserException(</span><br><span class=\"line\">          &quot;Updating an existing reservation with mismatched user:&quot;</span><br><span class=\"line\">              + oldReservation.getUser() + &quot; !&#x3D; &quot; + reservation.getUser());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    long startTime &#x3D; reservation.getStartTime();</span><br><span class=\"line\">    long endTime &#x3D; reservation.getEndTime();</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    &#x2F;&#x2F; reservable queue capacity </span><br><span class=\"line\">    Resource planTotalCapacity &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    &#x2F;&#x2F; 最大瞬时容量和最大平均容量\t</span><br><span class=\"line\">    Resource maxAvgRes &#x3D; Resources.multiply(planTotalCapacity, maxAvg);</span><br><span class=\"line\">    Resource maxInsRes &#x3D; Resources.multiply(planTotalCapacity, maxInst);</span><br><span class=\"line\">    \t</span><br><span class=\"line\">    IntegralResource runningTot &#x3D; new IntegralResource(0L, 0L);</span><br><span class=\"line\">    &#x2F;&#x2F; 平均(乘法,即积分面积)容量限制</span><br><span class=\"line\">    IntegralResource maxAllowed &#x3D; new IntegralResource(maxAvgRes);</span><br><span class=\"line\">    maxAllowed.multiplyBy(validWindow &#x2F; step);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 检查与该分配有重叠区域且长度为 validWindow(24h)的任何窗口,</span><br><span class=\"line\">    &#x2F;&#x2F;提供给用户的资源是否超过瞬时容量和平均容量限制</span><br><span class=\"line\">    for (long t &#x3D; startTime - validWindow; t &lt; endTime + validWindow; t +&#x3D; step) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">      Resource currExistingAllocTot &#x3D; plan.getTotalCommittedResources(t);</span><br><span class=\"line\">      Resource currExistingAllocForUser &#x3D;</span><br><span class=\"line\">          plan.getConsumptionForUser(reservation.getUser(), t);</span><br><span class=\"line\">      Resource currNewAlloc &#x3D; reservation.getResourcesAtTime(t);</span><br><span class=\"line\">      Resource currOldAlloc &#x3D; Resources.none();</span><br><span class=\"line\">      if (oldReservation !&#x3D; null) &#123;</span><br><span class=\"line\">        currOldAlloc &#x3D; oldReservation.getResourcesAtTime(t);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F; 所有用户累计 reservation 不能超过 reservable queue  capacity</span><br><span class=\"line\">      Resource inst &#x3D;</span><br><span class=\"line\">          Resources.subtract(Resources.add(currExistingAllocTot, currNewAlloc),</span><br><span class=\"line\">              currOldAlloc);</span><br><span class=\"line\">      if (Resources.greaterThan(plan.getResourceCalculator(),</span><br><span class=\"line\">          planTotalCapacity, inst, planTotalCapacity)) &#123;</span><br><span class=\"line\">        throw new ResourceOverCommitException(&quot; Resources at time &quot; + t</span><br><span class=\"line\">            + &quot; would be overcommitted (&quot; + inst + &quot; over &quot;</span><br><span class=\"line\">            + plan.getTotalCapacity() + &quot;) by accepting reservation: &quot;</span><br><span class=\"line\">            + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      &#x2F;&#x2F; 细分用户 容量不能超过 瞬时容量限制</span><br><span class=\"line\">      if (Resources.greaterThan(plan.getResourceCalculator(),</span><br><span class=\"line\">          planTotalCapacity, Resources.subtract(</span><br><span class=\"line\">              Resources.add(currExistingAllocForUser, currNewAlloc),</span><br><span class=\"line\">              currOldAlloc), maxInsRes)) &#123;</span><br><span class=\"line\">        throw new PlanningQuotaException(&quot;Instantaneous quota capacity &quot;</span><br><span class=\"line\">            + maxInst + &quot; would be passed at time &quot; + t</span><br><span class=\"line\">            + &quot; by accepting reservation: &quot; + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      </span><br><span class=\"line\">      &#x2F;&#x2F; 相当于增加了 (用户分配容量[已有的+当前reservation更新后的-当前reservation更新前的]) * (一个时间步长)的面积</span><br><span class=\"line\">      &#x2F;&#x2F; 当前reservation更新前的 实际上在 已有的 之中,所以需要减去</span><br><span class=\"line\">      runningTot.add(currExistingAllocForUser);</span><br><span class=\"line\">      runningTot.add(currNewAlloc);</span><br><span class=\"line\">      runningTot.subtract(currOldAlloc);</span><br><span class=\"line\">\t</span><br><span class=\"line\">      &#x2F;&#x2F; 老化掉一个24h窗口之前的那个时间点的面积</span><br><span class=\"line\">      if (t &gt; startTime) &#123;</span><br><span class=\"line\">        Resource pastOldAlloc &#x3D;</span><br><span class=\"line\">            plan.getConsumptionForUser(reservation.getUser(), t - validWindow);</span><br><span class=\"line\">        Resource pastNewAlloc &#x3D; reservation.getResourcesAtTime(t - validWindow);</span><br><span class=\"line\">        runningTot.subtract(pastOldAlloc);</span><br><span class=\"line\">        runningTot.subtract(pastNewAlloc);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (maxAllowed.compareTo(runningTot) &lt; 0) &#123;</span><br><span class=\"line\">        throw new PlanningQuotaException(</span><br><span class=\"line\">            &quot;Integral (avg over time) quota capacity &quot; + maxAvg</span><br><span class=\"line\">                + &quot; over a window of &quot; + validWindow &#x2F; 1000 + &quot; seconds, &quot;</span><br><span class=\"line\">                + &quot; would be passed at time &quot; + t + &quot;(&quot; + new Date(t)</span><br><span class=\"line\">                + &quot;) by accepting reservation: &quot;</span><br><span class=\"line\">                + reservation.getReservationId());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>上述 CapacityOverTimePolicy 用积分面积的方式判断 是否超过平均容量限制的方式，很容易和 RLESparseResourceAllocation 混淆。  RLESparseResourceAllocation 存储的是&lt;时间点-资源&gt;的状态量,是累积状态量不是增量。而积分面积用的是 RLESparseResourceAllocation 绘成的状态图算的积分面积，每向后移动一个时间步长则增加 (RLE 那个时间点资源量) * (一个时间步长) 的积分面积，并减去(RLE 24h前那个时间点资源量) * (一个时间步长)的积分面积。从而达到随着时间增加，积分面积始终是 RLESparseResourceAllocation 图中 [now-24h,now]这个区间的面积，然后和(平均资源限制容量) * (24h) 来比较。很巧妙，第一次见定积分可以在程序中这么用           </p>\n<h3 id=\"step-4\"><a href=\"#step-4\" class=\"headerlink\" title=\"step 4\"></a>step 4</h3><p>成功验证后，ReservationSystem 会向用户返回一个ReservationId 作为票据\n参考 “step 1” 的接口定义和 “step 2” ReservationAgent 分配成功后的处理  </p>\n<h3 id=\"step-5\"><a href=\"#step-5\" class=\"headerlink\" title=\"step 5\"></a>step 5</h3><p>PlanFollower(线程周期调度)通过动态创建/调整/销毁队列将计划的状态发布到调度程序   </p>\n<p>PlanFollower(Runnable) 随 ResourceManager 初始化时创建的 ReservationSystem(服务) 初始化而初始化 启动而启动，调用栈如下  </p>\n<p>-&gt;ResourceManager$RMActiveServices#serviceInit<br>　　-&gt;ResourceManager#createReservationSystem<br>　　　　-&gt;AbstractReservationSystem#serviceInit<br>　　　　　　-&gt;AbstractReservationSystem#createPlanFollower  </p>\n<p>[AbstractReservationSystem.java]　</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void serviceInit(Configuration conf) throws Exception &#123;</span><br><span class=\"line\">  Configuration configuration &#x3D; new Configuration(conf);</span><br><span class=\"line\">  &#x2F;&#x2F; 第一步: 初始化所有的 reservable queue 。</span><br><span class=\"line\">  &#x2F;&#x2F; 指定 SharingPolicy,Planner,capacity,planstep...</span><br><span class=\"line\">  reinitialize(configuration, rmContext);</span><br><span class=\"line\">  &#x2F;&#x2F; 第二步: 创建 PlanFollower。加载类:CS 调度器对应 CapacitySchedulerPlanFollower...</span><br><span class=\"line\">  planFollower &#x3D; createPlanFollower();</span><br><span class=\"line\">  if (planFollower !&#x3D; null) &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步:初始化 PlanFollower </span><br><span class=\"line\">    planFollower.init(clock, scheduler, plans.values());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  super.serviceInit(conf);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public void serviceStart() throws Exception &#123;</span><br><span class=\"line\">  if (planFollower !&#x3D; null) &#123;</span><br><span class=\"line\">    scheduledExecutorService &#x3D; new ScheduledThreadPoolExecutor(1);</span><br><span class=\"line\">    &#x2F;&#x2F;定时调度,调度周期是 planStepSize(默认1s)</span><br><span class=\"line\">    scheduledExecutorService.scheduleWithFixedDelay(planFollower, 0L,</span><br><span class=\"line\">        planStepSize, TimeUnit.MILLISECONDS);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  super.serviceStart();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>PlanFollower的核心逻辑在 AbstractSchedulerPlanFollower，用于 PlanFollower 与常规调度器同步预订分配的信息，下有 CapacitySchedulerPlanFollower 和 FairCapacityPlanFollower 两个子类。  </p>\n<p>[AbstractSchedulerPlanFollower.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized void run() &#123;</span><br><span class=\"line\">    for (Plan plan : plans) &#123;</span><br><span class=\"line\">      synchronizePlan(plan);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">@Override</span><br><span class=\"line\">  public synchronized void synchronizePlan(Plan plan) &#123;</span><br><span class=\"line\">     String planQueueName &#x3D; plan.getQueueName();</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;Running plan follower edit policy for plan: &quot; + planQueueName);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    long step &#x3D; plan.getStep();</span><br><span class=\"line\">    long now &#x3D; clock.getTime();</span><br><span class=\"line\">    if (now % step !&#x3D; 0) &#123;</span><br><span class=\"line\">      now +&#x3D; step - (now % step);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    Queue planQueue &#x3D; getPlanQueue(planQueueName);</span><br><span class=\"line\">    if (planQueue &#x3D;&#x3D; null) return;</span><br><span class=\"line\">    </span><br><span class=\"line\">    Resource clusterResources &#x3D; scheduler.getClusterResource();</span><br><span class=\"line\">    &#x2F;&#x2F; plan 对应的 reservable queue 的容量</span><br><span class=\"line\">    Resource planResources &#x3D; getPlanResources(plan, planQueue,</span><br><span class=\"line\">        clusterResources);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的预订(当前时间介于 startTime 和 endTime 之间的所有 Reservation) </span><br><span class=\"line\">    Set&lt;ReservationAllocation&gt; currentReservations &#x3D;</span><br><span class=\"line\">        plan.getReservationsAtTime(now);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的 reservationId 集合</span><br><span class=\"line\">    Set&lt;String&gt; curReservationNames &#x3D; new HashSet&lt;String&gt;();</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效预订的 resource</span><br><span class=\"line\">    Resource reservedResources &#x3D; Resource.newInstance(0, 0);</span><br><span class=\"line\">    &#x2F;&#x2F; 当前正在生效的 reservation;</span><br><span class=\"line\">    int numRes &#x3D; getReservedResources(now, currentReservations,</span><br><span class=\"line\">        curReservationNames, reservedResources);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 创建 reservable queue 的默认队列: xxx-default</span><br><span class=\"line\">    String defReservationId &#x3D; getReservationIdFromQueueName(planQueueName) +</span><br><span class=\"line\">        ReservationConstants.DEFAULT_QUEUE_SUFFIX;</span><br><span class=\"line\">    String defReservationQueue &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">        defReservationId);</span><br><span class=\"line\">    createDefaultReservationQueue(planQueueName, planQueue,</span><br><span class=\"line\">        defReservationId);</span><br><span class=\"line\">    curReservationNames.add(defReservationId);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)</span><br><span class=\"line\">    &#x2F;&#x2F; 如果专用于预订资源的队列骤减(或配置或节点宕机),通过调用 Planner 来删除部分已有预约</span><br><span class=\"line\">    if (arePlanResourcesLessThanReservations(clusterResources, planResources,</span><br><span class=\"line\">        reservedResources)) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        plan.getReplanner().plan(plan, null);</span><br><span class=\"line\">      &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">        LOG.warn(&quot;Exception while trying to replan: &#123;&#125;&quot;, planQueueName, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    &#x2F;&#x2F; 第二步: 标记当前预约中，哪些旧的预约需要过期删除，哪些新的预约需要创建队列  </span><br><span class=\"line\">    List&lt;? extends Queue&gt; resQueues &#x3D; getChildReservationQueues(planQueue);</span><br><span class=\"line\">    Set&lt;String&gt; expired &#x3D; new HashSet&lt;String&gt;();</span><br><span class=\"line\">    for (Queue resQueue : resQueues) &#123;</span><br><span class=\"line\">      String resQueueName &#x3D; resQueue.getQueueName();</span><br><span class=\"line\">      String reservationId &#x3D; getReservationIdFromQueueName(resQueueName);</span><br><span class=\"line\">      if (curReservationNames.contains(reservationId)) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 当前活跃的预订包含此此预约，因此不用为它创建对应的队列</span><br><span class=\"line\">        curReservationNames.remove(reservationId);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">\t&#x2F;&#x2F; 预约已经结束，准备清除</span><br><span class=\"line\">        expired.add(reservationId);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第三步: 清除过期预约。 getMoveOnExpiry 默认为 true</span><br><span class=\"line\">    cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,</span><br><span class=\"line\">        defReservationQueue);</span><br><span class=\"line\"></span><br><span class=\"line\">    float totalAssignedCapacity &#x3D; 0f;</span><br><span class=\"line\">    if (currentReservations !&#x3D; null) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 释放默认队列中的所有多余容量  </span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        setQueueEntitlement(planQueueName, defReservationQueue, 0f, 1.0f);</span><br><span class=\"line\">      &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">        LOG.warn(</span><br><span class=\"line\">            &quot;Exception while trying to release default queue capacity for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">            planQueueName, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">       &#x2F;&#x2F; 第四步: 增加新的预约或更新已有的预约 对应的队列配额</span><br><span class=\"line\">       &#x2F;&#x2F; 4.1 首先按照(当前时间待分配的容量 - 已有的容量)从小到大(从负到正)的方式排序一次调整 capacity。</span><br><span class=\"line\">       &#x2F;&#x2F; 这种排序方式用以避免分配过程中瞬时容量超出 100%的 capacity(猜测可能是为了减少干扰并发预订和抢占介入)</span><br><span class=\"line\">      List&lt;ReservationAllocation&gt; sortedAllocations &#x3D;</span><br><span class=\"line\">          sortByDelta(</span><br><span class=\"line\">              new ArrayList&lt;ReservationAllocation&gt;(currentReservations), now,</span><br><span class=\"line\">              plan);</span><br><span class=\"line\">      &#x2F;&#x2F; 4.2 为每个队列设置或更新 capacity</span><br><span class=\"line\">      for (ReservationAllocation res : sortedAllocations) &#123;</span><br><span class=\"line\">        String currResId &#x3D; res.getReservationId().toString();</span><br><span class=\"line\">        if (curReservationNames.contains(currResId)) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; 每个reservationId 对应着常规调度器中 PlanQueue 下的一个 ReservationQueue</span><br><span class=\"line\">          addReservationQueue(planQueueName, planQueue, currResId);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; 当前生效的 ResourceAllocation 需要(或RS已分配给Reservation)的 capacity</span><br><span class=\"line\">        Resource capToAssign &#x3D; res.getResourcesAtTime(now);</span><br><span class=\"line\">        float targetCapacity &#x3D; 0f;</span><br><span class=\"line\">        if (planResources.getMemory() &gt; 0</span><br><span class=\"line\">            &amp;&amp; planResources.getVirtualCores() &gt; 0) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; 绝对值式的容量相对 PlanQueue capacity 计算相对容量，capToAssign&#x2F;planResources</span><br><span class=\"line\">          targetCapacity &#x3D;</span><br><span class=\"line\">              calculateReservationToPlanRatio(clusterResources,</span><br><span class=\"line\">                  planResources,</span><br><span class=\"line\">                  capToAssign);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">          LOG.debug(</span><br><span class=\"line\">              &quot;Assigning capacity of &#123;&#125; to queue &#123;&#125; with target capacity &#123;&#125;&quot;,</span><br><span class=\"line\">              capToAssign, currResId, targetCapacity);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        &#x2F;&#x2F; set maxCapacity to 100% unless the job requires gang, in which</span><br><span class=\"line\">        &#x2F;&#x2F; case we stick to capacity (as running early&#x2F;before is likely a</span><br><span class=\"line\">        &#x2F;&#x2F; waste of resources)</span><br><span class=\"line\">        &#x2F;&#x2F; 设置最大配额。感觉是无效代码，不明白为什么要设置最大配额 都为1不行吗，为什么按有无并发来设置最大配额  </span><br><span class=\"line\">        float maxCapacity &#x3D; 1.0f;</span><br><span class=\"line\">        if (res.containsGangs()) &#123;</span><br><span class=\"line\">          maxCapacity &#x3D; targetCapacity;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 设置capacity 和 maxCapacity</span><br><span class=\"line\">          setQueueEntitlement(planQueueName, currResId, targetCapacity, maxCapacity);</span><br><span class=\"line\">        &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">          LOG.warn(&quot;Exception while trying to size reservation for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">              currResId, planQueueName, e);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        totalAssignedCapacity +&#x3D; targetCapacity;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第五步: 设置 default queue 的配额(PlanQueue 的剩余配额)</span><br><span class=\"line\">    float defQCap &#x3D; 1.0f - totalAssignedCapacity;</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;PlanFollowerEditPolicyTask: total Plan Capacity: &#123;&#125; &quot;</span><br><span class=\"line\">          + &quot;currReservation: &#123;&#125; default-queue capacity: &#123;&#125;&quot;, planResources,</span><br><span class=\"line\">          numRes, defQCap);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      setQueueEntitlement(planQueueName, defReservationQueue, defQCap, 1.0f);</span><br><span class=\"line\">    &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">      LOG.warn(</span><br><span class=\"line\">          &quot;Exception while trying to reclaim default queue capacity for plan: &#123;&#125;&quot;,</span><br><span class=\"line\">          planQueueName, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#x2F;&#x2F; 第六步: 清理过期的预订记录(now-endTime&gt;24h)</span><br><span class=\"line\">    &#x2F;&#x2F; garbage collect finished reservations from plan</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      plan.archiveCompletedReservations(now);</span><br><span class=\"line\">    &#125; catch (PlanningException e) &#123;</span><br><span class=\"line\">      LOG.error(&quot;Exception in archiving completed reservations: &quot;, e);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Finished iteration of plan follower edit policy for plan: &quot;</span><br><span class=\"line\">        + planQueueName);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; Extension: update plan with app states,</span><br><span class=\"line\">    &#x2F;&#x2F; useful to support smart replanning</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>先看 第一步: 若正在生效的预约资源量大于队列容量，则删除晚接受的部分预约(accept_time)<br>可用于兼容集群/队列资源骤减的情况，策略是删除最晚接受的部分预约。<br>[SimpleCapacityReplanner.java] </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void plan(Plan plan, List&lt;ReservationDefinition&gt; contracts)</span><br><span class=\"line\">      throws PlanningException &#123;</span><br><span class=\"line\">    if (contracts !&#x3D; null) &#123;</span><br><span class=\"line\">      throw new RuntimeException(</span><br><span class=\"line\">          &quot;SimpleCapacityReplanner cannot handle new reservation contracts&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ResourceCalculator resCalc &#x3D; plan.getResourceCalculator();</span><br><span class=\"line\">    Resource totCap &#x3D; plan.getTotalCapacity();</span><br><span class=\"line\">    long now &#x3D; clock.getTime();</span><br><span class=\"line\">    &#x2F;&#x2F; lengthOfCheckZone&#x3D;1h </span><br><span class=\"line\">    for (long t &#x3D; now; (t &lt; plan.getLastEndTime() &amp;&amp; t &lt; (now + lengthOfCheckZone)); t +&#x3D;</span><br><span class=\"line\">        plan.getStep()) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 使用 Plan 所有预订的总资源量 - Plan 的 capacity 来检查是否超限</span><br><span class=\"line\">      Resource excessCap &#x3D;</span><br><span class=\"line\">          Resources.subtract(plan.getTotalCommittedResources(t), totCap);</span><br><span class=\"line\">      if (Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE)) &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 按照 ReservationAllocation.acceptedAt 做比较，最新接受的在前</span><br><span class=\"line\">        Set&lt;ReservationAllocation&gt; curReservations &#x3D;</span><br><span class=\"line\">            new TreeSet&lt;ReservationAllocation&gt;(plan.getReservationsAtTime(t));</span><br><span class=\"line\">        for (Iterator&lt;ReservationAllocation&gt; resIter &#x3D;</span><br><span class=\"line\">            curReservations.iterator(); resIter.hasNext()</span><br><span class=\"line\">            &amp;&amp; Resources.greaterThan(resCalc, totCap, excessCap, ZERO_RESOURCE);) &#123;</span><br><span class=\"line\">          ReservationAllocation reservation &#x3D; resIter.next();</span><br><span class=\"line\">          &#x2F;&#x2F; 使用 deleteReservation 来调用 Plan.removeReservation 来删除预订。</span><br><span class=\"line\">          &#x2F;&#x2F; removeReservation 的源码分析 见&quot;step 3&quot;</span><br><span class=\"line\">          plan.deleteReservation(reservation.getReservationId());</span><br><span class=\"line\">          excessCap &#x3D;</span><br><span class=\"line\">              Resources.subtract(excessCap, reservation.getResourcesAtTime(t));</span><br><span class=\"line\">          LOG.info(&quot;Removing reservation &quot; + reservation.getReservationId()</span><br><span class=\"line\">              + &quot; to repair physical-resource constraints in the plan: &quot;</span><br><span class=\"line\">              + plan.getQueueName());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>第二步略，第三步: 清除过期预约。<br>[AbstractSchedulerPlanFollower.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void cleanupExpiredQueues(String planQueueName,</span><br><span class=\"line\">      boolean shouldMove, Set&lt;String&gt; toRemove, String defReservationQueue) &#123;</span><br><span class=\"line\">    for (String expiredReservationId : toRemove) &#123;</span><br><span class=\"line\">      try &#123;</span><br><span class=\"line\">        String expiredReservation &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">            expiredReservationId);</span><br><span class=\"line\">         &#x2F;&#x2F; 将过期预约对应的 ReservationQueue 的 capacity 和 maxCapacity 设置为 0 </span><br><span class=\"line\">        setQueueEntitlement(planQueueName, expiredReservation, 0.0f, 0.0f);</span><br><span class=\"line\">        &#x2F;&#x2F; 默认配置为 true。将其中的 APP 移动到同级的 default 队列中(xxx-default)。</span><br><span class=\"line\">        if (shouldMove) &#123;</span><br><span class=\"line\">          moveAppsInQueueSync(expiredReservation, defReservationQueue);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (scheduler.getAppsInQueue(expiredReservation).size() &gt; 0) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 移走所有已有 APP 的情况,还有新提交的。暂时保留</span><br><span class=\"line\">          scheduler.killAllAppsInQueue(expiredReservation);</span><br><span class=\"line\">          LOG.info(&quot;Killing applications in queue: &#123;&#125;&quot;, expiredReservation);</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; 删除队列  </span><br><span class=\"line\">          scheduler.removeQueue(expiredReservation);</span><br><span class=\"line\">          LOG.info(&quot;Queue: &quot; + expiredReservation + &quot; removed&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; catch (YarnException e) &#123;</span><br><span class=\"line\">        LOG.warn(&quot;Exception while trying to expire reservation: &#123;&#125;&quot;,</span><br><span class=\"line\">            expiredReservationId, e);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>第四步 先调用 sortByDelta 方法使用 ReservationAllocationComparator 对所有正在生效的预订对应的 ReservationQueue 做排序，然后对所有预订通过 setQueueEntitlement 设置或修正 capacity 和 maxCapacity。<br>第五步 通过 setQueueEntitlement 设置 default queue 的 capacity 和 maxCapacity，占有所有剩余资源<br>[ReservationAllocationComparator.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    @Override</span><br><span class=\"line\">    public int compare(ReservationAllocation lhs, ReservationAllocation rhs) &#123;</span><br><span class=\"line\">      Resource lhsRes &#x3D; getUnallocatedReservedResources(lhs);</span><br><span class=\"line\">      Resource rhsRes &#x3D; getUnallocatedReservedResources(rhs);</span><br><span class=\"line\">      &#x2F;&#x2F; 两个 Resource 先比内存再比虚拟核，小的在前。</span><br><span class=\"line\">      return lhsRes.compareTo(rhsRes);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private Resource getUnallocatedReservedResources(</span><br><span class=\"line\">        ReservationAllocation reservation) &#123;</span><br><span class=\"line\">      Resource resResource;</span><br><span class=\"line\">      &#x2F;&#x2F; 查询常规调度器，获取现有 capacity</span><br><span class=\"line\">      Resource reservationResource &#x3D; planFollower</span><br><span class=\"line\">          .getReservationQueueResourceIfExists</span><br><span class=\"line\">              (plan, reservation.getReservationId());</span><br><span class=\"line\">      if (reservationResource !&#x3D; null) &#123;</span><br><span class=\"line\">        resResource &#x3D;</span><br><span class=\"line\">            Resources.subtract(</span><br><span class=\"line\">            &#x2F;&#x2F; 当前时刻需要的 capacity。可能为负</span><br><span class=\"line\">                reservation.getResourcesAtTime(now),</span><br><span class=\"line\">                reservationResource);</span><br><span class=\"line\">      &#125; else &#123;</span><br><span class=\"line\">        resResource &#x3D; reservation.getResourcesAtTime(now);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return resResource;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<p>[AbstractSchedulerPlanFollower.java]  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">protected void setQueueEntitlement(String planQueueName, String currResId,</span><br><span class=\"line\">      float targetCapacity,</span><br><span class=\"line\">      float maxCapacity) throws YarnException &#123;</span><br><span class=\"line\">    String reservationQueueName &#x3D; getReservationQueueName(planQueueName,</span><br><span class=\"line\">        currResId);</span><br><span class=\"line\">    scheduler.setEntitlement(reservationQueueName, new QueueEntitlement(</span><br><span class=\"line\">        targetCapacity, maxCapacity));</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[CapacityScheduler.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void setEntitlement(String inQueue,</span><br><span class=\"line\">      QueueEntitlement entitlement) throws SchedulerDynamicEditException,</span><br><span class=\"line\">      YarnException &#123;</span><br><span class=\"line\">    LeafQueue queue &#x3D; getAndCheckLeafQueue(inQueue);</span><br><span class=\"line\">    ParentQueue parent &#x3D; (ParentQueue) queue.getParent();</span><br><span class=\"line\">    if (!(queue instanceof ReservationQueue)) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(&quot;Entitlement can not be&quot;</span><br><span class=\"line\">          + &quot; modified dynamically since queue &quot; + inQueue</span><br><span class=\"line\">          + &quot; is not a ReservationQueue&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (!(parent instanceof PlanQueue)) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(&quot;The parent of ReservationQueue &quot;</span><br><span class=\"line\">          + inQueue + &quot; must be an PlanQueue&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ReservationQueue newQueue &#x3D; (ReservationQueue) queue;</span><br><span class=\"line\">    float sumChilds &#x3D; ((PlanQueue) parent).sumOfChildCapacities();</span><br><span class=\"line\">    &#x2F;&#x2F; 计算修改后, 所有同级队列的容量之和</span><br><span class=\"line\">    float newChildCap &#x3D; sumChilds - queue.getCapacity() + entitlement.getCapacity();</span><br><span class=\"line\"></span><br><span class=\"line\">    if (newChildCap &gt;&#x3D; 0 &amp;&amp; newChildCap &lt; 1.0f + CSQueueUtils.EPSILON) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 设置前后的 capacity 和 maxCapacity 都一样则 不设置</span><br><span class=\"line\">      if (Math.abs(entitlement.getCapacity() - queue.getCapacity()) &#x3D;&#x3D; 0</span><br><span class=\"line\">          &amp;&amp; Math.abs(entitlement.getMaxCapacity() - queue.getMaximumCapacity()) &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">        return;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      &#x2F;&#x2F;设置 ReservationQueue 的 capacity 和 maxCapacity </span><br><span class=\"line\">      newQueue.setEntitlement(entitlement);</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(</span><br><span class=\"line\">          &quot;Sum of child queues would exceed 100% for PlanQueue: &quot;</span><br><span class=\"line\">              + parent.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    LOG.info(&quot;Set entitlement for ReservationQueue &quot; + inQueue + &quot;  to &quot;</span><br><span class=\"line\">        + queue.getCapacity() + &quot; request was (&quot; + entitlement.getCapacity() + &quot;)&quot;);</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>[ReservationQueue.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public synchronized void setEntitlement(QueueEntitlement entitlement)</span><br><span class=\"line\">      throws SchedulerDynamicEditException &#123;</span><br><span class=\"line\">    float capacity &#x3D; entitlement.getCapacity();</span><br><span class=\"line\">    if (capacity &lt; 0 || capacity &gt; 1.0f) &#123;</span><br><span class=\"line\">      throw new SchedulerDynamicEditException(</span><br><span class=\"line\">          &quot;Capacity demand is not in the [0,1] range: &quot; + capacity);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setCapacity(capacity);</span><br><span class=\"line\">    setAbsoluteCapacity(getParent().getAbsoluteCapacity() * getCapacity());</span><br><span class=\"line\">    setMaxCapacity(entitlement.getMaxCapacity());</span><br><span class=\"line\">    if (LOG.isDebugEnabled()) &#123;</span><br><span class=\"line\">      LOG.debug(&quot;successfully changed to &quot; + capacity + &quot; for queue &quot;</span><br><span class=\"line\">          + this.getQueueName());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>第六步: 清理过期的预订记录<br>[InMemoryPlan.java]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void archiveCompletedReservations(long tick) &#123;</span><br><span class=\"line\">    LOG.debug(&quot;Running archival at time: &#123;&#125;&quot;, tick);</span><br><span class=\"line\">    List&lt;InMemoryReservationAllocation&gt; expiredReservations &#x3D;</span><br><span class=\"line\">        new ArrayList&lt;InMemoryReservationAllocation&gt;();</span><br><span class=\"line\">    readLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 删除过期的保留记录  </span><br><span class=\"line\">      long archivalTime &#x3D; tick - policy.getValidWindow();</span><br><span class=\"line\">      ReservationInterval searchInterval &#x3D;</span><br><span class=\"line\">          new ReservationInterval(archivalTime, archivalTime);</span><br><span class=\"line\">      SortedMap&lt;ReservationInterval, Set&lt;InMemoryReservationAllocation&gt;&gt; reservations &#x3D;</span><br><span class=\"line\">          currentReservations.headMap(searchInterval, true);</span><br><span class=\"line\">      if (!reservations.isEmpty()) &#123;</span><br><span class=\"line\">        for (Set&lt;InMemoryReservationAllocation&gt; reservationEntries : reservations</span><br><span class=\"line\">            .values()) &#123;</span><br><span class=\"line\">          for (InMemoryReservationAllocation reservation : reservationEntries) &#123;</span><br><span class=\"line\">            &#x2F;&#x2F; 结束时间早于当前时间24h </span><br><span class=\"line\">            if (reservation.getEndTime() &lt;&#x3D; archivalTime) &#123;</span><br><span class=\"line\">              expiredReservations.add(reservation);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      readLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (expiredReservations.isEmpty()) &#123;</span><br><span class=\"line\">      return;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    writeLock.lock();</span><br><span class=\"line\">    try &#123;</span><br><span class=\"line\">      for (InMemoryReservationAllocation expiredReservation : expiredReservations) &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; Plan.removeReservation 源码分析见&quot;step 3&quot;,主要是内存数据结构的维护</span><br><span class=\"line\">        removeReservation(expiredReservation);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; finally &#123;</span><br><span class=\"line\">      writeLock.unlock();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>　　　　</p>\n<h3 id=\"step-6\"><a href=\"#step-6\" class=\"headerlink\" title=\"step 6\"></a>step 6</h3><p>用户可以在(多个)应用程序的 ApplicationSubmissionContext 中指定 ReservationId 提交到可预订的队列(PlanQueue,具有 reservable 属性的 LeafQueue)   </p>\n<p>参见 ApplicationSubmissionContext 的数据结构  </p>\n<h3 id=\"step-7\"><a href=\"#step-7\" class=\"headerlink\" title=\"step 7\"></a>step 7</h3><p>常规调度器将从创建的特殊队列中提供容器,以确保遵守资源预定。在预订的时间和资源限制下，用户的(多个)应用程序可以以容量/公平的方式共享资源   </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public void handle(SchedulerEvent event) &#123;</span><br><span class=\"line\">    switch(event.getType()) &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    case APP_ADDED:</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      AppAddedSchedulerEvent appAddedEvent &#x3D; (AppAddedSchedulerEvent) event;</span><br><span class=\"line\">      &#x2F;&#x2F; 解析 APP 提交的队列</span><br><span class=\"line\">      String queueName &#x3D;</span><br><span class=\"line\">          resolveReservationQueueName(appAddedEvent.getQueue(),</span><br><span class=\"line\">              appAddedEvent.getApplicationId(),</span><br><span class=\"line\">              appAddedEvent.getReservationID());</span><br><span class=\"line\">      if (queueName !&#x3D; null) &#123;</span><br><span class=\"line\">        if (!appAddedEvent.getIsAppRecovering()) &#123;</span><br><span class=\"line\">          addApplication(appAddedEvent.getApplicationId(), queueName,</span><br><span class=\"line\">              appAddedEvent.getUser());</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,</span><br><span class=\"line\">              appAddedEvent.getUser());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    break;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">private synchronized String resolveReservationQueueName(String queueName,</span><br><span class=\"line\">      ApplicationId applicationId, ReservationId reservationID) &#123;</span><br><span class=\"line\">    CSQueue queue &#x3D; getQueue(queueName);</span><br><span class=\"line\">    if ((queue &#x3D;&#x3D; null) || !(queue instanceof PlanQueue)) &#123;</span><br><span class=\"line\">      return queueName;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if (reservationID !&#x3D; null) &#123;</span><br><span class=\"line\">      String resQName &#x3D; reservationID.toString();</span><br><span class=\"line\">      &#x2F;&#x2F; 根据 reservationId 获取对应的叶子队列</span><br><span class=\"line\">      queue &#x3D; getQueue(resQName);</span><br><span class=\"line\">      if (queue &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        String message &#x3D;</span><br><span class=\"line\">            &quot;Application &quot;</span><br><span class=\"line\">                + applicationId</span><br><span class=\"line\">                + &quot; submitted to a reservation which is not yet currently active: &quot;</span><br><span class=\"line\">                + resQName;</span><br><span class=\"line\">        this.rmContext.getDispatcher().getEventHandler()</span><br><span class=\"line\">            .handle(new RMAppEvent(applicationId,</span><br><span class=\"line\">                RMAppEventType.APP_REJECTED, message));</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      if (!queue.getParent().getQueueName().equals(queueName)) &#123;</span><br><span class=\"line\">        String message &#x3D;</span><br><span class=\"line\">            &quot;Application: &quot; + applicationId + &quot; submitted to a reservation &quot;</span><br><span class=\"line\">                + resQName + &quot; which does not belong to the specified queue: &quot;</span><br><span class=\"line\">                + queueName;</span><br><span class=\"line\">        this.rmContext.getDispatcher().getEventHandler()</span><br><span class=\"line\">            .handle(new RMAppEvent(applicationId,</span><br><span class=\"line\">                RMAppEventType.APP_REJECTED, message));</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">     &#x2F;&#x2F; 使用 ReservationId 对应的 ReservationQueue 来运行任务, 调度容器....</span><br><span class=\"line\">      queueName &#x3D; resQName;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      &#x2F;&#x2F; 如果指定了 PlanQueue 但是没指定 reservationId, 会提交到 PlanQueue 的 default 队列执行</span><br><span class=\"line\">      queueName &#x3D; queueName + ReservationConstants.DEFAULT_QUEUE_SUFFIX;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return queueName;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"step-8\"><a href=\"#step-8\" class=\"headerlink\" title=\"step 8\"></a>step 8</h3><p>预订系统可以兼容容量下降的情况。包括拒绝之前接受最晚的预订兼容 reservable queue 的容量骤减，移动预订到 reservable queue 下的 default队列来兼容超时(预订到期但app 没结束)应用<br>参考”step 5”中 AbstractSchedulerPlanFollower#synchronizePlan 方法中对 plan.getReplanner().plan(plan, null) 的调用和cleanupExpiredQueues(planQueueName, plan.getMoveOnExpiry(), expired,\n        defReservationQueue)的逻辑  </p>\n<p>官网解释可能不实，原文如下:<br>The system includes mechanisms to adapt to drop in cluster capacity. This consists in replanning by “moving” the reservation if possible, or rejecting the smallest amount of previously accepted reservation (to ensure that other reservation will receive their full amount).</p>\n<p>但在 hadoop-2.7.3 和 hadoop-3.2.0的 trunk 分支上 AbstractSchedulerPlanFollower/Planner(只有 SimpleReplanner 实现类)逻辑没有改动，且无相应逻辑  </p>\n<h2 id=\"QA\"><a href=\"#QA\" class=\"headerlink\" title=\"QA\"></a>QA</h2><blockquote>\n</blockquote>\n<ol>\n<li>Q: reservable queue 和 leaf queue 区别<br> A: 其一，只有 leaf queue 才能被设置为 reservable queue，通过对 leaf queue 设置 yarn.scheduler.capacity.&lt;queue-path&gt;.reservable=true。其二，在资源管理的层级体系中 reservable queue 对应的数据结构是 PlanQueue extend ParentQueue。即实际上，reservable queue 是在 leaf queue上做了配置的 ParentQueue。  </li>\n<li>Q: PlanQueue 和 ReservationQueue 区别<br> A: PlanQueue 对应一个 reservable queue，ReservationQueue 对应一个 ReservationId，PlanQueue 下面可以创建任意个 ReservationQueue; PlanQueue 是 ParentQueue，ReservationQueue 是 LeafQueue。      </li>\n<li>Q: 在名称为 X 的 reservable queue 上预订了资源，reservationId=reservationId_001,提交APP 时 Queue 名字填什么？<br> A: Queue 名字填 X,且需要在 ApplicationSubmissionContext 中设置 reservationId。常规调度器会根据 reservationId 解析出对应的 ReservationQueue(名称与 ReservationId 相同)。  </li>\n<li>Q: 在1:00-2:00 预约了&lt;100G,10core&gt;的资源 30 分钟，但是在 1:00前或者 2:00后提交任务 会怎样?<br>A:  常规调度器会拒绝任务(APP_REJECT)。因为找不到 reservationId 对应的 ReservationQueue,早于1:00时队列还没创建,晚于2:00时队列被删除。最好在1:00提交，可能会有等待时间(RS 调度时从deadline 向 arrival 尝试分配)       </li>\n<li>Q: 任务运行在哪里?<br>A: 任务运行在与 reservationId 同名的 ReservationQueue 中,如果预约时间过去但是 APP 还没运行完成则默认移动 APP 到与 ReservationQueue 同级的 default 队列中  </li>\n<li>Q: 能不能把 reservable queue 当做 leaf queue 用? 每个任务都不指定 ReservationId。<br> A: 可以。提交到 reservable queue 但是不指定 ReservationId 的任务都会被移动到 reservable queue 下的 default 队列运行。</li>\n<li>Q: 提交到 reservable queue 中的 app 有的指定了 ReservationId,有的没指定。运行时有什么影响?<br> A: PlanQueue 优先为有 ReservationId 队列分配资源,其次再为 default 队列分配资源，即只能使用那些被预订后剩余的资源，在资源紧张时 default 队列无资源或被其他任意 ReservationQueue 抢占。(这是由 ReservationSystem 提供预订的逻辑决定的。1.不预约的资源不在ReservationSystem 管理下,请求预约时 ReservationSystem 会认为自身管理 PlanQueue 所有资源都能预约出去  2.为所有预约分配完资源之后剩下的才归属 default 队列所有)  </li>\n<li>Q: Spark On Yarn 如何使用 ReservationSystem?<br> A: 分两个阶段。第一,使用 Client-RM 协议提交 ReservationSubmissionRequest,预订资源并保存回执的 ReservationId。第二，在 Spark On Yarn 程序中指定 ReservationId 字段，但是此阶段 Spark On Yarn 提交程序尚不完善需要修改源代码。目前 Spark On Yarn 程序使用 createApplicationSubmissionContext 方法来封装 ApplicationSubmissionContext 时, 没有针对 ReservationId 的逻辑。如果有需求的话，可以通过 SparkConf 来传递 ReservationId 字符串,在createApplicationSubmissionContext做解析设置 。    <ol start=\"9\">\n<li>Q: ReservationSystem 的适用场景<br>A: 1.资源紧张时需要保证重要生产任务运行,可以使用预订资源的方式 。ReservationSystem 在 PlanQueue 下创建 ReservationQueue，将预订需要的绝对值资源量转化 ReservationQueue 在当前时刻的capacity 和 maxCapacity，将任务在此队列中调度，本质上还是 LeafQueue。绝对资源量转化成百分比资源量擦掉了容器数量和大小，不修改开源抢占调度逻辑 不密集抢占抢占的情况下，不能解决大容器调度的问题。  </li>\n</ol>\n</li>\n</ol>\n<h2 id=\"配置预订系统\"><a href=\"#配置预订系统\" class=\"headerlink\" title=\"配置预订系统\"></a>配置预订系统</h2><p>目前可以在 yarn-site.xml 中配置开启ReservationSystem,可以在CapacityScheduler 及 FairScheduler中增加对预订的支持，方式是在capacity-scheduler.xml 或者 fair-scheduler.xml中的任何 LeafQueue的”reservable”属性标记为 true，然后该队列的配额就可以用于预订。即使没有预订资源，应用程序仍然可以被提交到该队列上，它们将以”best-effort”的模式运行在 预订资源中运行的作业 剩下的容量中。  </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html\" target=\"_blank\" rel=\"noopener\">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ReservationSystem.html</a><br><a href=\"https://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us\" target=\"_blank\" rel=\"noopener\">https://www.slideshare.net/Hadoop_Summit/reservations-based-scheduling-if-youre-late-dont-blame-us</a><br><a href=\"http://gitbook.net/java/util/java_util_treemap.html\" target=\"_blank\" rel=\"noopener\">http://gitbook.net/java/util/java_util_treemap.html</a>  </p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cke2rms4v0007y7c7c711eai2","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms51000ay7c785g62zya"},{"post_id":"cke2rms470000y7c73cf7dhjf","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms5d000ey7c78r145vsn"},{"post_id":"cke2rms470000y7c73cf7dhjf","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms5f000gy7c797do92br"},{"post_id":"cke2rms4g0002y7c7gjeh8i34","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms5o000my7c73a3a91q6"},{"post_id":"cke2rms4g0002y7c7gjeh8i34","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms5q000oy7c7483s3gk6"},{"post_id":"cke2rms4p0005y7c7a9cr1r05","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms63000xy7c76jaza9tt"},{"post_id":"cke2rms4p0005y7c7a9cr1r05","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms68000zy7c762ncdmee"},{"post_id":"cke2rms4p0005y7c7a9cr1r05","tag_id":"cke2rms5u000sy7c7cvrlagrv","_id":"cke2rms6a0012y7c7b5ua5p03"},{"post_id":"cke2rms4t0006y7c7hita713l","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms6g0015y7c76y4b06u1"},{"post_id":"cke2rms4t0006y7c7hita713l","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms6g0016y7c7cstcgki7"},{"post_id":"cke2rms4y0009y7c74qy2emgl","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms6h0019y7c7ekk4gecj"},{"post_id":"cke2rms4y0009y7c74qy2emgl","tag_id":"cke2rms6g0017y7c7ak67bo7t","_id":"cke2rms6h001ay7c7016z6h1r"},{"post_id":"cke2rms52000by7c7d3fq86hg","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6j001cy7c7dolf48u1"},{"post_id":"cke2rms52000by7c7d3fq86hg","tag_id":"cke2rms6h0018y7c73wsn41td","_id":"cke2rms6k001dy7c742uw0pjb"},{"post_id":"cke2rms52000by7c7d3fq86hg","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms6l001fy7c73wib2tsi"},{"post_id":"cke2rms59000dy7c703as3cw0","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6l001gy7c79bhj8jbi"},{"post_id":"cke2rms59000dy7c703as3cw0","tag_id":"cke2rms6h0018y7c73wsn41td","_id":"cke2rms6m001iy7c7ermsd9fr"},{"post_id":"cke2rms59000dy7c703as3cw0","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms6m001jy7c7d6e9591t"},{"post_id":"cke2rms5e000fy7c7cijbczul","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6p001ly7c7f4bkeq6p"},{"post_id":"cke2rms5e000fy7c7cijbczul","tag_id":"cke2rms6h0018y7c73wsn41td","_id":"cke2rms6p001my7c77zo57x1o"},{"post_id":"cke2rms5e000fy7c7cijbczul","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms6q001oy7c74qwa1hnz"},{"post_id":"cke2rms5i000iy7c79k542l2z","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6q001py7c7g4v9h7mj"},{"post_id":"cke2rms5i000iy7c79k542l2z","tag_id":"cke2rms6h0018y7c73wsn41td","_id":"cke2rms6r001ry7c7h0bj6dbi"},{"post_id":"cke2rms5i000iy7c79k542l2z","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms6r001sy7c7hm137qgm"},{"post_id":"cke2rms5k000jy7c7gba1ac0u","tag_id":"cke2rms6m001ky7c736vtb5qm","_id":"cke2rms6r001uy7c7a3jze4ov"},{"post_id":"cke2rms5m000ly7c737aiargo","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6r001vy7c7cq8g5e16"},{"post_id":"cke2rms5m000ly7c737aiargo","tag_id":"cke2rms6p001ny7c7aax34ik6","_id":"cke2rms6s001xy7c7czivfnkw"},{"post_id":"cke2rms5o000ny7c77rdx2p6k","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6s001yy7c735dnhgbp"},{"post_id":"cke2rms5o000ny7c77rdx2p6k","tag_id":"cke2rms6g0017y7c7ak67bo7t","_id":"cke2rms6t0020y7c763zu296s"},{"post_id":"cke2rms5r000qy7c76k2t7rh8","tag_id":"cke2rms6r001ty7c7582f3kps","_id":"cke2rms6u0021y7c7ez9375c9"},{"post_id":"cke2rms5t000ry7c7gio5acn0","tag_id":"cke2rms6r001ty7c7582f3kps","_id":"cke2rms6u0023y7c7gr6wbe2d"},{"post_id":"cke2rms5v000ty7c77q6ggt99","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6v0024y7c78csg177u"},{"post_id":"cke2rms5v000ty7c77q6ggt99","tag_id":"cke2rms6p001ny7c7aax34ik6","_id":"cke2rms6v0026y7c7efzs90ft"},{"post_id":"cke2rms5w000uy7c78u3e8kek","tag_id":"cke2rms6r001ty7c7582f3kps","_id":"cke2rms6w0027y7c75q8babd7"},{"post_id":"cke2rms61000wy7c7fgg246ju","tag_id":"cke2rms4l0004y7c742v84klq","_id":"cke2rms6x0029y7c78bwcesma"},{"post_id":"cke2rms61000wy7c7fgg246ju","tag_id":"cke2rms6g0017y7c7ak67bo7t","_id":"cke2rms6x002ay7c7595kda3q"},{"post_id":"cke2rms64000yy7c74xqm08ga","tag_id":"cke2rms6w0028y7c741c0b0t9","_id":"cke2rms6x002cy7c781nc4qjw"},{"post_id":"cke2rms690011y7c7exja3fjr","tag_id":"cke2rms6w0028y7c741c0b0t9","_id":"cke2rms6y002dy7c7hfz90vho"},{"post_id":"cke2rms8o002fy7c7eloc4gjr","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms8u002hy7c7b2hv3gda"},{"post_id":"cke2rms8o002fy7c7eloc4gjr","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms8x002iy7c72igzbfh7"},{"post_id":"cke2rms8s002gy7c7ayro5wyl","tag_id":"cke2rms53000cy7c70nqd5b8h","_id":"cke2rms8z002jy7c7ack30eom"},{"post_id":"cke2rms8s002gy7c7ayro5wyl","tag_id":"cke2rms4x0008y7c7ggxo2z3m","_id":"cke2rms8z002ky7c7agzd099h"}],"Tag":[{"name":"Spark 2.3.0","_id":"cke2rms4l0004y7c742v84klq"},{"name":"源码解析","_id":"cke2rms4x0008y7c7ggxo2z3m"},{"name":"Yarn 2.7.3","_id":"cke2rms53000cy7c70nqd5b8h"},{"name":"Spark On Yarn","_id":"cke2rms5u000sy7c7cvrlagrv"},{"name":"方案","_id":"cke2rms6g0017y7c7ak67bo7t"},{"name":"Spark SQL","_id":"cke2rms6h0018y7c73wsn41td"},{"name":"Spark 3.0","_id":"cke2rms6m001ky7c736vtb5qm"},{"name":"Adaptive Execution","_id":"cke2rms6p001ny7c7aax34ik6"},{"name":"tools","_id":"cke2rms6r001ty7c7582f3kps"},{"name":"Flink 1.12","_id":"cke2rms6w0028y7c741c0b0t9"}]}}